/* LOGS:
Downloading image for github-jina-ai-serve from https://github.com/jina-ai.png
Image converted to WebP: data/images/github-jina-ai-serve.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-amusi-CVPR2025-Papers-with-Code', 'github--amusi--cvpr2025-papers-with-code', 'CVPR2025-Papers-with-Code', 'amusi', 'CVPR 2025 decisions are now available on OpenReview！22.1% = 2878 / 13008 > 注1：欢迎各位大佬提交issue，分享CVPR 2025论文和开源项目！ > > 注2：关于往年CV顶会论文以及其他优质CV论文和大盘点，详见： https://github.com/amusi/daily-paper-computer-vision > > - ICCV 2025 > - ECCV 2024 > - CVPR 2024 欢迎扫码加入【CVer学术交流群】，可以获取CVPR 2025等最前沿工作！这是最大的计算机视觉AI知识星球！每日更新，第一时间分享最新最前沿的计算机视觉、AIGC、扩散模型、多模态、深度学习、自动驾驶、医疗影像和遥感等方向的学习资料，快加入学起来！ - 3DGS(Gaussian Splatting) - Agent) - Avatars - Backbone - CLIPEVOS - Mamba - Embodied AI - GAN - GNN - 多模态大语言模型(MLLM) - 大语言模型...', '["computer-vision","cvpr","cvpr2020","cvpr2021","cvpr2022","cvpr2023","cvpr2024","cvpr2025","deep-learning","image-processing","image-segmentation","machine-learning","object-detection","paper","python","semantic-segmentation","transformer","transformers","visual-tracking"]', 'other', 21599, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/amusi/CVPR2025-Papers-with-Code","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# CVPR 2025 论文和开源项目合集(Papers with Code)\n\nCVPR 2025 decisions are now available on OpenReview！22.1% = 2878 / 13008\n\n\n> 注1：欢迎各位大佬提交issue，分享CVPR 2025论文和开源项目！\n>\n> 注2：关于往年CV顶会论文以及其他优质CV论文和大盘点，详见： https://github.com/amusi/daily-paper-computer-vision\n>\n> - [ICCV 2025](https://github.com/amusi/ICCV2025-Papers-with-Code)\n> - [ECCV 2024](https://github.com/amusi/ECCV2024-Papers-with-Code)\n> - [CVPR 2024](CVPR2024-Papers-with-Code.md)\n\n欢迎扫码加入【CVer学术交流群】，可以获取CVPR 2025等最前沿工作！这是最大的计算机视觉AI知识星球！每日更新，第一时间分享最新最前沿的计算机视觉、AIGC、扩散模型、多模态、深度学习、自动驾驶、医疗影像和遥感等方向的学习资料，快加入学起来！\n\n![](CVer学术交流群.png)\n\n# 【CVPR 2025 论文开源目录】\n\n- [3DGS(Gaussian Splatting)](#3DGS)\n- [Agent)](#Agent)\n- [Avatars](#Avatars)\n- [Backbone](#Backbone)\n- [CLIP](#CLIP)EVOS\n- [Mamba](#Mamba)\n- [Embodied AI](#Embodied-AI)\n- [GAN](#GAN)\n- [GNN](#GNN)\n- [多模态大语言模型(MLLM)](#MLLM)\n- [大语言模型(LLM)](#LLM)\n- [NAS](#NAS)\n- [OCR](#OCR)\n- [NeRF](#NeRF)\n- [DETR](#DETR)\n- [扩散模型(Diffusion Models)](#Diffusion)\n- [ReID(重识别)](#ReID)\n- [长尾分布(Long-Tail)](#Long-Tail)\n- [Vision Transformer](#Vision-Transformer)\n- [视觉和语言(Vision-Language)](#VL)\n- [自监督学习(Self-supervised Learning)](#SSL)\n- [数据增强(Data Augmentation)](#DA)\n- [目标检测(Object Detection)](#Object-Detection)\n- [异常检测(Anomaly Detection)](#Anomaly-Detection)\n- [目标跟踪(Visual Tracking)](#VT)\n- [语义分割(Semantic Segmentation)](#Semantic-Segmentation)\n- [实例分割(Instance Segmentation)](#Instance-Segmentation)\n- [全景分割(Panoptic Segmentation)](#Panoptic-Segmentation)\n- [医学图像(Medical Image)](#MI)\n- [医学图像分割(Medical Image Segmentation)](#MIS)\n- [视频目标分割(Video Object Segmentation)](#VOS)\n- [视频实例分割(Video Instance Segmentation)](#VIS)\n- [参考图像分割(Referring Image Segmentation)](#RIS)\n- [图像抠图(Image Matting)](#Matting)\n- [图像编辑(Image Editing)](#Image-Editing)\n- [Low-level Vision](#LLV)\n- [超分辨率(Super-Resolution)](#SR)\n- [去噪(Denoising)](#Denoising)\n- [去模糊(Deblur)](#Deblur)\n- [自动驾驶(Autonomous Driving)](#Autonomous-Driving)\n- [3D点云(3D Point Cloud)](#3D-Point-Cloud)\n- [3D目标检测(3D Object Detection)](#3DOD)\n- [3D语义分割(3D Semantic Segmentation)](#3DSS)\n- [3D目标跟踪(3D Object Tracking)](#3D-Object-Tracking)\n- [3D语义场景补全(3D Semantic Scene Completion)](#3DSSC)\n- [3D配准(3D Registration)](#3D-Registration)\n- [3D人体姿态估计(3D Human Pose Estimation)](#3D-Human-Pose-Estimation)\n- [3D人体Mesh估计(3D Human Mesh Estimation)](#3D-Human-Pose-Estimation)\n- [3D Visual Grounding(3D视觉定位)](#3DVG)\n- [医学图像(Medical Image)](#Medical-Image)\n- [图像生成(Image Generation)](#Image-Generation)\n- [视频生成(Video Generation)](#Video-Generation)\n- [3D生成(3D Generation)](#3D-Generation)\n- [视频理解(Video Understanding)](#Video-Understanding)\n- [行为检测(Action Detection)](#Action-Detection)\n- [具身智能(Embodied AI)](#Embodied)\n- [文本检测(Text Detection)](#Text-Detection)\n- [知识蒸馏(Knowledge Distillation)](#KD)\n- [模型剪枝(Model Pruning)](#Pruning)\n- [图像压缩(Image Compression)](#IC)\n- [三维重建(3D Reconstruction)](#3D-Reconstruction)\n- [深度估计(Depth Estimation)](#Depth-Estimation)\n- [轨迹预测(Trajectory Prediction)](#TP)\n- [车道线检测(Lane Detection)](#Lane-Detection)\n- [图像描述(Image Captioning)](#Image-Captioning)\n- [视觉问答(Visual Question Answering)](#VQA)\n- [手语识别(Sign Language Recognition)](#SLR)\n- [视频预测(Video Prediction)](#Video-Prediction)\n- [新视点合成(Novel View Synthesis)](#NVS)\n- [Zero-Shot Learning(零样本学习)](#ZSL)\n- [立体匹配(Stereo Matching)](#Stereo-Matching)\n- [特征匹配(Feature Matching)](#Feature-Matching)\n- [暗光图像增强(Low-light Image Enhancement)](#Low-light)\n- [场景图生成(Scene Graph Generation)](#SGG)\n- [风格迁移(Style Transfer)](#ST)\n- [隐式神经表示(Implicit Neural Representations)](#INR)\n- [图像质量评价(Image Quality Assessment)](#IQA)\n- [视频质量评价(Video Quality Assessment)](#Video-Quality-Assessment)\n- [压缩感知(Compressive Sensing)](#CS)\n- [数据集(Datasets)](#Datasets)\n- [新任务(New Tasks)](#New-Tasks)\n- [其他(Others)](#Others)\n\n<a name="3DGS"></a>\n\n# 3DGS(Gaussian Splatting)\n\n\n<a name="Agent"></a>\n\n# Agent\n\n**SpiritSight Agent: Advanced GUI Agent with One Look**\n\n- Paper: https://arxiv.org/abs/2503.03196\n- Code: https://hzhiyuan.github.io/SpiritSight-Agent\n\n\n<a name="Avatars"></a>\n\n# Avatars\n\n\n# Backbone\n\n**Building Vision Models upon Heat Conduction**\n\n- Paper: https://arxiv.org/abs/2405.16555\n- Code: https://github.com/MzeroMiko/vHeat\n\n**LSNet: See Large, Focus Small**\n\n- Paper: https://arxiv.org/abs/2503.23135\n- Code: https://github.com/jameslahm/lsnet\n\n\n<a name="CLIP"></a>\n\n# CLIP\n\n\n\n<a name="Mamba"></a>\n\n# Mamba\n\n\n**MambaVision: A Hybrid Mamba-Transformer Vision Backbone**\n\n- Paper: https://arxiv.org/abs/2407.08083\n- Code: https://github.com/NVlabs/MambaVision\n\n**MobileMamba: Lightweight Multi-Receptive Visual Mamba Network**\n\n- Paper: https://arxiv.org/abs/2411.15941\n- Code: https://github.com/lewandofskee/MobileMamba\n\n**MambaIC: State Space Models for High-Performance Learned Image Compression**\n\n- Paper: https://arxiv.org/abs/2503.12461\n- Code: https://arxiv.org/abs/2503.12461\n\n<a name="Embodied-AI"></a>\n\n# Embodied AI\n\n**CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos**\n\n- Project: https://ai4ce.github.io/CityWalker/\n- Paper: https://arxiv.org/abs/2411.17820\n- Code: https://github.com/ai4ce/CityWalker\n\n\n<a name="GAN"></a>\n\n# GAN\n\n<a name="OCR"></a>\n\n# OCR\n\n\n<a name="NeRF"></a>\n\n# NeRF\n\n\n\n<a name="DETR"></a>\n\n# DETR\n\n**Mr. DETR: Instructive Multi-Route Training for Detection Transformers**\n\n- Paper: https://arxiv.org/abs/2412.10028\n- Code: https://github.com/Visual-AI/Mr.DETR\n\n\n<a name="Prompt"></a>\n\n# Prompt\n\n<a name="MLLM"></a>\n\n# 多模态大语言模型(MLLM)\n\n**LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences**\n\n- Paper： https://arxiv.org/abs/2412.01292\n- Code: https://github.com/Hoyyyaard/LSceneLLM\n\n\n**DynRefer: Delving into Region-level Multimodal Tasks via Dynamic Resolution**\n\n- Paper: https://arxiv.org/abs/2405.16071\n- Code: https://github.com/callsys/DynRefer\n\n\n**Retrieval-Augmented Personalization for Multimodal Large Language Models**\n\n- Project Page: https://hoar012.github.io/RAP-Project/\n- Paper: https://arxiv.org/abs/2410.13360\n- Code: https://github.com/Hoar012/RAP-MLLM\n\n**BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models**\n\n- Paper: https://arxiv.org/abs/2411.15232\n- Code: https://github.com/HealthX-Lab/BiomedCoOp\n\n**FlashSloth: Lightning Multimodal Large Language Models via Embedded Visual Compression**\n\n- Paper: https://arxiv.org/abs/2412.04317\n- Code: https://github.com/codefanw/FlashSloth\n\n**MMRL: Multi-Modal Representation Learning for Vision-Language Models**\n\n- Paper: https://arxiv.org/abs/2503.08497\n- Code: https://github.com/yunncheng/MMRL\n\n**PAVE: Patching and Adapting Video Large Language Models**\n\n- Paper: https://arxiv.org/abs/2503.19794\n- Code: https://github.com/dragonlzm/PAVE\n\n**AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization**\n\n- Paper: https://arxiv.org/abs/2503.23733\n- Code: https://github.com/THUNLP-MT/AdaMMS\n\n\n<a name="LLM"></a>\n\n# 大语言模型(LLM)\n\n\n\n\n<a name="NAS"></a>\n\n# NAS\n\n<a name="ReID"></a>\n\n# ReID(重识别)\n\n**From Poses to Identity: Training-Free Person Re-Identification via Feature Centralization**\n\n- Paper: https://arxiv.org/abs/2503.00938\n- Code: https://github.com/yuanc3/Pose2ID\n\n\n**AirRoom: Objects Matter in Room Reidentification**\n\n- Project: https://sairlab.org/airroom/\n- Paper: https://arxiv.org/abs/2503.01130\n\n\n**IDEA: Inverted Text with Cooperative Deformable Aggregation for Multi-modal Object Re-Identification**\n\n- Paper: https://arxiv.org/abs/2503.10324\n- Code: https://github.com/924973292/IDEA\n\n\n\n<a name="Diffusion"></a>\n\n# 扩散模型(Diffusion Models)\n\n**TinyFusion: Diffusion Transformers Learned Shallow**\n\n- Paper: https://arxiv.org/abs/2412.01199\n- Code: https://github.com/VainF/TinyFusion\n\n**DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any Architecture**\n\n- Paper: https://arxiv.org/abs/2409.03550\n- Code: https://github.com/qianlong0502/DKDM\n\n**Tiled Diffusion**\n\n- Homepage: https://madaror.github.io/tiled-diffusion.github.io/\n- Paper: https://arxiv.org/abs/2412.15185\n- Code: https://github.com/madaror/tiled-diffusion\n\n\n<a name="Vision-Transformer"></a>\n\n# Vision Transformer\n\n\n\n<a name="VL"></a>\n\n# 视觉和语言(Vision-Language)\n\n**NLPrompt: Noise-Label Prompt Learning for Vision-Language Models**\n\n- Paper: https://arxiv.org/abs/2412.01256\n- Code: https://github.com/qunovo/NLPrompt\n\n**PhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability**\n\n- Paper: https://arxiv.org/abs/2503.08481\n- Code: https://github.com/unira-zwj/PhysVLM\n\n**MMRL: Multi-Modal Representation Learning for Vision-Language Models**\n\n- Paper: https://arxiv.org/abs/2503.08497\n- Code: https://github.com/yunncheng/MMRL\n\n\n<a name="Object-Detection"></a>\n\n# 目标检测(Object Detection)\n\n\n**LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models**\n\n- Paper: https://arxiv.org/abs/2501.18954\n- Code：https://github.com/iSEE-Laboratory/LLMDet\n\n**Mr. DETR: Instructive Multi-Route Training for Detection Transformers**\n\n- Paper: https://arxiv.org/abs/2412.10028\n- Code: https://github.com/Visual-AI/Mr.DETR\n\n\n<a name="Anomaly-Detection"></a>\n\n# 异常检测(Anomaly Detection)\n\n\n\n<a name="VT"></a>\n\n# 目标跟踪(Object Tracking)\n\n**Multiple Object Tracking as ID Prediction**\n\n- Paper：https://arxiv.org/abs/2403.16848\n- Code: https://github.com/MCG-NJU/MOTIP\n\n**Omnidirectional Multi-Object Tracking**\n\n- Paper:https://arxiv.org/abs/2503.04565\n- Code:https://github.com/xifen523/OmniTrack\n\n\n<a name="MI"></a>\n\n# 医学图像(Medical Image)\n\n\n**BrainMVP: Multi-modal Vision Pre-training for Medical Image Analysis**\n\n- Paper: https://arxiv.org/abs/2410.10604\n- Code: https://github.com/shaohao011/BrainMVP\n\n\n# 医学图像分割(Medical Image Segmentation)\n\n**Test-Time Domain Generalization via Universe Learning: A Multi-Graph Matching Approach for Medical Image Segmentation**\n\n- Paper: https://arxiv.org/abs/2503.13012\n- Code: https://github.com/Yore0/TTDG-MGM\n\n\n<a name="Autonomous-Driving"></a>\n\n# 自动驾驶(Autonomous Driving)\n\n**LiMoE: Mixture of LiDAR Representation Learners from Automotive Scenes**\n\n- Project: https://ldkong.com/LiMoE\n- Paper: https://arxiv.org/abs/2501.04004\n- Code: https://github.com/Xiangxu-0103/LiMoE\n\n\n\n# 3D点云(3D-Point-Cloud)\n\n**Unlocking Generalization Power in LiDAR Point Cloud Registration**\n\n- Paper: https://arxiv.org/abs/2503.10149\n- Code: https://github.com/peakpang/UGP\n\n\n<a name="3DOD"></a>\n\n# 3D目标检测(3D Object Detection)\n\n\n\n<a name="3DOD"></a>\n\n# 3D语义分割(3D Semantic Segmentation)\n\n\n\n\n\n<a name="LLV"></a>\n\n# Low-level Vision\n\n\n\n<a name="SR"></a>\n\n# 超分辨率(Super-Resolution)\n\n**AESOP: Auto-Encoded Supervision for Perceptual Image Super-Resolution**\n\n- Paper: https://arxiv.org/abs/2412.00124\n- Code: https://github.com/2minkyulee/AESOP-Auto-Encoded-Supervision-for-Perceptual-Image-Super-Resolution\n\n\n<a name="Denoising"></a>\n\n# 去噪(Denoising)\n\n## 图像去噪(Image Denoising)\n\n<a name="3D-Human-Pose-Estimation"></a>\n\n# 3D人体姿态估计(3D Human Pose Estimation)\n\n**Reconstructing Humans with a Biomechanically Accurate Skeleton**\n\n- Homepage: https://isshikihugh.github.io/HSMR/\n- Code: https://github.com/IsshikiHugh/HSMR\n\n<a name="3DVG"></a>\n\n#3D Visual Grounding(3D视觉定位)\n\n**ProxyTransformation: Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding**\n\n- Homepage: https://pqh22.github.io/projects/ProxyTransformation/index.html\n\n- Code: https://github.com/pqh22/ProxyTransformation\n\n- Paper: https://arxiv.org/abs/2502.19247\n\n\n<a name="Image-Generation"></a>\n\n# 图像生成(Image Generation)\n\n**Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models**\n\n- Paper: https://arxiv.org/abs/2501.01423\n- Code: https://github.com/hustvl/LightningDiT\n\n**SleeperMark: Towards Robust Watermark against Fine-Tuning Text-to-image Diffusion Models**\n\n- Paper: https://arxiv.org/abs/2412.04852\n- Code: https://github.com/taco-group/SleeperMark\n\n\n**TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation**\n\n- Homepage: https://byteflow-ai.github.io/TokenFlow/\n- Code: https://github.com/ByteFlow-AI/TokenFlow\n- Paper:https://arxiv.org/abs/2412.03069\n\n**PAR: Parallelized Autoregressive Visual Generation**\n\n- Project: https://epiphqny.github.io/PAR-project/\n- Paper: https://arxiv.org/abs/2412.15119\n- Code: https://github.com/Epiphqny/PAR\n\n\n**Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis**\n\n- Project: https://generative-photography.github.io/project/\n- Paper: https://arxiv.org/abs/2412.02168\n- Code: https://github.com/pandayuanyu/generative-photography\n\n\n**OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation**\n\n- Project Page: https://opening-benchmark.github.io/\n- Paper: https://arxiv.org/abs/2411.18499).\n- Code: https://github.com/LanceZPF/OpenING\n\n\n\n\n<a name="Video-Generation"></a>\n\n# 视频生成(Video Generation)\n\n**Identity-Preserving Text-to-Video Generation by Frequency Decomposition**\n\n- Paper: https://arxiv.org/abs/2411.17440\n- Code: https://github.com/PKU-YuanGroup/ConsisID\n\n\n**Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models**\n\n- Paper: https://arxiv.org/abs/2407.15642\n- Code: https://github.com/maxin-cn/Cinemo\n\n**X-Dyna: Expressive Dynamic Human Image Animation**\n\n- Paper: https://arxiv.org/abs/2501.10021\n- Code: https://github.com/bytedance/X-Dyna\n\n**PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation**\n\n- Paper: https://arxiv.org/pdf/2412.00596\n- Code: https://github.com/pittisl/PhyT2V\n\n\n**Timestep Embedding Tells: It''s Time to Cache for Video Diffusion Model**\n\n- Project: https://liewfeng.github.io/TeaCache/\n- Paper: https://arxiv.org/abs/2411.19108\n- Code: https://github.com/ali-vilab/TeaCache\n\n\n**AR-Diffusion: Asynchronous Video Generation with Auto-Regressive Diffusion**\n\n- Project: https://iva-mzsun.github.io/AR-Diffusion\n- Paper: https://arxiv.org/abs/2503.07418\n- Code: https://github.com/iva-mzsun/AR-Diffusion\n\n\n<a name="Image-Editing"></a>\n\n# 图像编辑(Image Editing)\n\n**Edit Away and My Face Will not Stay: Personal Biometric Defense against Malicious Generative Editing**\n\n- Paper: https://arxiv.org/abs/2411.16832\n- Code: https://github.com/taco-group/FaceLock\n\n\n**h-Edit: Effective and Flexible Diffusion-Based Editing via Doob’s h-Transform**\n\n- Paper: https://arxiv.org/abs/2503.02187\n- Code: https://github.com/nktoan/h-edit\n\n\n<a name="Video-Editing"></a>\n\n# 视频编辑(Video Editing)\n\n\n\n<a name="3D-Generation"></a>\n\n# 3D生成(3D Generation)\n\n\n**Generative Gaussian Splatting for Unbounded 3D City Generation**\n\n- Project: https://haozhexie.com/project/gaussian-city\n- Paper: https://arxiv.org/abs/2406.06526\n- Code: https://github.com/hzxie/GaussianCity\n\n**StdGEN: Semantic-Decomposed 3D Character Generation from Single Images**\n\n- Project: https://stdgen.github.io/\n- Paper: https://arxiv.org/abs/2411.05738\n- Code: https://github.com/hyz317/StdGEN\n\n\n<a name="3D-Reconstruction"></a>\n\n# 3D重建(3D Reconstruction)\n\n**Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass**\n\n- Project: https://fast3r-3d.github.io/\n- Paper: https://arxiv.org/abs/2501.13928\n\n\n<a name="HMG"></a>\n\n# 人体运动生成(Human Motion Generation)\n\n**SemGeoMo: Dynamic Contextual Human Motion Generation with Semantic and Geometric Guidance**\n\n- Project: https://4dvlab.github.io/project_page/semgeomo/\n- Paper: https://arxiv.org/abs/2503.01291\n- https://github.com/4DVLab/SemGeoMo\n\n<a name="Video-Understanding"></a>\n\n# 视频理解(Video Understanding)\n\n**Temporal Grounding Videos like Flipping Manga**\n\n- Paper: https://arxiv.org/abs/2411.10332\n- Code: https://github.com/yongliang-wu/NumPro\n\n<a name="Embodied"></a>\n\n# 具身智能(Embodied AI)\n\n**Universal Actions for Enhanced Embodied Foundation Models**\n\n- Project: https://2toinf.github.io/UniAct/\n- Paper: https://arxiv.org/abs/2501.10105\n- Code: https://github.com/2toinf/UniAct\n\n**PhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability**\n\n- Paper: https://arxiv.org/abs/2503.08481\n- Code: https://github.com/unira-zwj/PhysVLM\n\n\n<a name="KD"></a>\n\n# 知识蒸馏(Knowledge Distillation)\n\n<a name="Depth-Estimation"></a>\n\n\n# 深度估计(Depth Estimation)\n\n**DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos**\n\n- Project: https://depthcrafter.github.io\n- Paper: https://arxiv.org/abs/2409.02095\n- Code: https://github.com/Tencent/DepthCrafter\n\n\n**MonSter: Marry Monodepth to Stereo Unleashes Power**\n\n- Paper: https://arxiv.org/abs/2501.08643\n- Code: https://github.com/Junda24/MonSter\n\n**DEFOM-Stereo: Depth Foundation Model Based Stereo Matching**\n\n- Project: https://insta360-research-team.github.io/DEFOM-Stereo/\n- Paper: https://arxiv.org/abs/2501.09466\n- Code: https://github.com/Insta360-Research-Team/DEFOM-Stereo\n\n\n<a name="Stereo-Matching"></a>\n\n# 立体匹配(Stereo Matching)\n\n**MonSter: Marry Monodepth to Stereo Unleashes Power**\n\n- Paper: https://arxiv.org/abs/2501.08643\n- Code: https://github.com/Junda24/MonSter\n\n\n<a name="Low-light"></a>\n\n# 暗光图像增强(Low-light Image Enhancement)\n\n\n**HVI: A New color space for Low-light Image Enhancement**\n\n- Paper: https://arxiv.org/abs/2502.20272\n- Code: https://github.com/Fediory/HVI-CIDNet\n- Demo: https://huggingface.co/spaces/Fediory/HVI-CIDNet_Low-light-Image-Enhancement_\n\n**ReDDiT: Efficient Diffusion as Low Light Enhancer**\n\n- Paper: https://arxiv.org/abs/2410.12346\n- Code: https://github.com/lgz-0713/ReDDiT\n\n\n\n<a name="IC"></a>\n\n# 图像压缩(Image Compression)](#IC)\n\n**MambaIC: State Space Models for High-Performance Learned Image Compression**\n\n- Paper: https://arxiv.org/abs/2503.12461\n- Code: https://arxiv.org/abs/2503.12461\n\n\n<a name="SGG"></a>\n\n# 场景图生成(Scene Graph Generation)\n\n\n\n<a name="ST"></a>\n\n# 风格迁移(Style Transfer)\n\n**StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements**\n\n- Project: https://stylestudio-official.github.io/\n- Paper: https://arxiv.org/abs/2412.08503\n- Code: https://github.com/Westlake-AGI-Lab/StyleStudio\n\n\n<a name="IQA"></a>\n\n# 图像质量评价(Image Quality Assessment)\n\n**Auto Cherry-Picker: Learning from High-quality Generative Data Driven by Language**\n\n- Homepage: https://yichengchen24.github.io/projects/autocherrypicker\n- Paper: https://arxiv.org/pdf/2406.20085\n- Code: https://github.com/yichengchen24/ACP\n\n<a name="Video-Quality-Assessment"></a>\n\n# 视频质量评价(Video Quality Assessment)\n\n<a name="CS"></a>\n\n# 压缩感知(Compressive Sensing)\n\n**Using Powerful Prior Knowledge of Diffusion Model in Deep Unfolding Networks for Image Compressive Sensing**\n\n- Paper: https://arxiv.org/abs/2503.08429\n- Code: https://github.com/FengodChen/DMP-DUN-CVPR2025\n\n\n<a name="Datasets"></a>\n\n# 数据集(Datasets)\n\n\n**Objaverse++: Curated 3D Object Dataset with Quality Annotations**\n\n- Paper: https://arxiv.org/abs/2504.07334\n- Code: https://github.com/TCXX/ObjaversePlusPlus\n\n\n<a name="Others"></a>\n\n# 其他(Others)\n\n\n**DTGBrepGen: A Novel B-rep Generative Model through Decoupling Topology and Geometry**\n\n- Paper: https://arxiv.org/abs/2503.13110\n- Code: https://github.com/jinli99/DTGBrepGen\n\n\n**Analyzing the Synthetic-to-Real Domain Gap in 3D Hand Pose Estimation**\n\n- Paper: https://arxiv.org/abs/2503.19307\n- Code: https://github.com/delaprada/HandSynthesis.git\n\n**EVOS: Efficient Implicit Neural Training via EVOlutionary Selector**\n\n- Homepage: https://weixiang-zhang.github.io/proj-evos/\n- Paper: https://arxiv.org/abs/2412.10153\n- Code: https://github.com/zwx-open/EVOS-INR\n  ', '{"language":null,"stars":21599,"forks":2770,"watchers":21599,"open_issues":22,"topics":["computer-vision","cvpr","cvpr2020","cvpr2021","cvpr2022","cvpr2023","cvpr2024","cvpr2025","deep-learning","image-processing","image-segmentation","machine-learning","object-detection","paper","python","semantic-segmentation","transformer","transformers","visual-tracking"],"default_branch":"main","size_kb":463,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:amusi:daily-paper-computer-vision","source_url":"https://github.com/amusi/daily-paper-computer-vision"},{"type":"has_code","target_id":"github:amusi:ICCV2025-Papers-with-Code","source_url":"https://github.com/amusi/ICCV2025-Papers-with-Code"},{"type":"has_code","target_id":"github:amusi:ECCV2024-Papers-with-Code","source_url":"https://github.com/amusi/ECCV2024-Papers-with-Code"},{"type":"has_code","target_id":"github:MzeroMiko:vHeat","source_url":"https://github.com/MzeroMiko/vHeat"},{"type":"has_code","target_id":"github:jameslahm:lsnet","source_url":"https://github.com/jameslahm/lsnet"},{"type":"has_code","target_id":"github:NVlabs:MambaVision","source_url":"https://github.com/NVlabs/MambaVision"},{"type":"has_code","target_id":"github:lewandofskee:MobileMamba","source_url":"https://github.com/lewandofskee/MobileMamba"},{"type":"has_code","target_id":"github:ai4ce:CityWalker","source_url":"https://github.com/ai4ce/CityWalker"},{"type":"has_code","target_id":"github:Visual-AI:Mr.DETR","source_url":"https://github.com/Visual-AI/Mr.DETR"},{"type":"has_code","target_id":"github:Hoyyyaard:LSceneLLM","source_url":"https://github.com/Hoyyyaard/LSceneLLM"},{"type":"has_code","target_id":"github:callsys:DynRefer","source_url":"https://github.com/callsys/DynRefer"},{"type":"has_code","target_id":"github:Hoar012:RAP-MLLM","source_url":"https://github.com/Hoar012/RAP-MLLM"},{"type":"has_code","target_id":"github:HealthX-Lab:BiomedCoOp","source_url":"https://github.com/HealthX-Lab/BiomedCoOp"},{"type":"has_code","target_id":"github:codefanw:FlashSloth","source_url":"https://github.com/codefanw/FlashSloth"},{"type":"has_code","target_id":"github:yunncheng:MMRL","source_url":"https://github.com/yunncheng/MMRL"},{"type":"has_code","target_id":"github:dragonlzm:PAVE","source_url":"https://github.com/dragonlzm/PAVE"},{"type":"has_code","target_id":"github:THUNLP-MT:AdaMMS","source_url":"https://github.com/THUNLP-MT/AdaMMS"},{"type":"has_code","target_id":"github:yuanc3:Pose2ID","source_url":"https://github.com/yuanc3/Pose2ID"},{"type":"has_code","target_id":"github:924973292:IDEA","source_url":"https://github.com/924973292/IDEA"},{"type":"has_code","target_id":"github:VainF:TinyFusion","source_url":"https://github.com/VainF/TinyFusion"},{"type":"has_code","target_id":"github:qianlong0502:DKDM","source_url":"https://github.com/qianlong0502/DKDM"},{"type":"has_code","target_id":"github:madaror:tiled-diffusion","source_url":"https://github.com/madaror/tiled-diffusion"},{"type":"has_code","target_id":"github:qunovo:NLPrompt","source_url":"https://github.com/qunovo/NLPrompt"},{"type":"has_code","target_id":"github:unira-zwj:PhysVLM","source_url":"https://github.com/unira-zwj/PhysVLM"},{"type":"has_code","target_id":"github:yunncheng:MMRL","source_url":"https://github.com/yunncheng/MMRL"},{"type":"has_code","target_id":"github:iSEE-Laboratory:LLMDet","source_url":"https://github.com/iSEE-Laboratory/LLMDet"},{"type":"has_code","target_id":"github:Visual-AI:Mr.DETR","source_url":"https://github.com/Visual-AI/Mr.DETR"},{"type":"has_code","target_id":"github:MCG-NJU:MOTIP","source_url":"https://github.com/MCG-NJU/MOTIP"},{"type":"has_code","target_id":"github:xifen523:OmniTrack","source_url":"https://github.com/xifen523/OmniTrack"},{"type":"has_code","target_id":"github:shaohao011:BrainMVP","source_url":"https://github.com/shaohao011/BrainMVP"},{"type":"has_code","target_id":"github:Yore0:TTDG-MGM","source_url":"https://github.com/Yore0/TTDG-MGM"},{"type":"has_code","target_id":"github:Xiangxu-0103:LiMoE","source_url":"https://github.com/Xiangxu-0103/LiMoE"},{"type":"has_code","target_id":"github:peakpang:UGP","source_url":"https://github.com/peakpang/UGP"},{"type":"has_code","target_id":"github:2minkyulee:AESOP-Auto-Encoded-Supervision-for-Perceptual-Image-Super-Resolution","source_url":"https://github.com/2minkyulee/AESOP-Auto-Encoded-Supervision-for-Perceptual-Image-Super-Resolution"},{"type":"has_code","target_id":"github:IsshikiHugh:HSMR","source_url":"https://github.com/IsshikiHugh/HSMR"},{"type":"has_code","target_id":"github:pqh22:ProxyTransformation","source_url":"https://github.com/pqh22/ProxyTransformation"},{"type":"has_code","target_id":"github:hustvl:LightningDiT","source_url":"https://github.com/hustvl/LightningDiT"},{"type":"has_code","target_id":"github:taco-group:SleeperMark","source_url":"https://github.com/taco-group/SleeperMark"},{"type":"has_code","target_id":"github:ByteFlow-AI:TokenFlow","source_url":"https://github.com/ByteFlow-AI/TokenFlow"},{"type":"has_code","target_id":"github:Epiphqny:PAR","source_url":"https://github.com/Epiphqny/PAR"},{"type":"has_code","target_id":"github:pandayuanyu:generative-photography","source_url":"https://github.com/pandayuanyu/generative-photography"},{"type":"has_code","target_id":"github:LanceZPF:OpenING","source_url":"https://github.com/LanceZPF/OpenING"},{"type":"has_code","target_id":"github:PKU-YuanGroup:ConsisID","source_url":"https://github.com/PKU-YuanGroup/ConsisID"},{"type":"has_code","target_id":"github:maxin-cn:Cinemo","source_url":"https://github.com/maxin-cn/Cinemo"},{"type":"has_code","target_id":"github:bytedance:X-Dyna","source_url":"https://github.com/bytedance/X-Dyna"},{"type":"has_code","target_id":"github:pittisl:PhyT2V","source_url":"https://github.com/pittisl/PhyT2V"},{"type":"has_code","target_id":"github:ali-vilab:TeaCache","source_url":"https://github.com/ali-vilab/TeaCache"},{"type":"has_code","target_id":"github:iva-mzsun:AR-Diffusion","source_url":"https://github.com/iva-mzsun/AR-Diffusion"},{"type":"has_code","target_id":"github:taco-group:FaceLock","source_url":"https://github.com/taco-group/FaceLock"},{"type":"has_code","target_id":"github:nktoan:h-edit","source_url":"https://github.com/nktoan/h-edit"},{"type":"has_code","target_id":"github:hzxie:GaussianCity","source_url":"https://github.com/hzxie/GaussianCity"},{"type":"has_code","target_id":"github:hyz317:StdGEN","source_url":"https://github.com/hyz317/StdGEN"},{"type":"has_code","target_id":"github:4DVLab:SemGeoMo","source_url":"https://github.com/4DVLab/SemGeoMo"},{"type":"has_code","target_id":"github:yongliang-wu:NumPro","source_url":"https://github.com/yongliang-wu/NumPro"},{"type":"has_code","target_id":"github:2toinf:UniAct","source_url":"https://github.com/2toinf/UniAct"},{"type":"has_code","target_id":"github:unira-zwj:PhysVLM","source_url":"https://github.com/unira-zwj/PhysVLM"},{"type":"has_code","target_id":"github:Tencent:DepthCrafter","source_url":"https://github.com/Tencent/DepthCrafter"},{"type":"has_code","target_id":"github:Junda24:MonSter","source_url":"https://github.com/Junda24/MonSter"},{"type":"has_code","target_id":"github:Insta360-Research-Team:DEFOM-Stereo","source_url":"https://github.com/Insta360-Research-Team/DEFOM-Stereo"},{"type":"has_code","target_id":"github:Junda24:MonSter","source_url":"https://github.com/Junda24/MonSter"},{"type":"has_code","target_id":"github:Fediory:HVI-CIDNet","source_url":"https://github.com/Fediory/HVI-CIDNet"},{"type":"has_code","target_id":"github:lgz-0713:ReDDiT","source_url":"https://github.com/lgz-0713/ReDDiT"},{"type":"has_code","target_id":"github:Westlake-AGI-Lab:StyleStudio","source_url":"https://github.com/Westlake-AGI-Lab/StyleStudio"},{"type":"has_code","target_id":"github:yichengchen24:ACP","source_url":"https://github.com/yichengchen24/ACP"},{"type":"has_code","target_id":"github:FengodChen:DMP-DUN-CVPR2025","source_url":"https://github.com/FengodChen/DMP-DUN-CVPR2025"},{"type":"has_code","target_id":"github:TCXX:ObjaversePlusPlus","source_url":"https://github.com/TCXX/ObjaversePlusPlus"},{"type":"has_code","target_id":"github:jinli99:DTGBrepGen","source_url":"https://github.com/jinli99/DTGBrepGen"},{"type":"has_code","target_id":"github:delaprada:HandSynthesis.git","source_url":"https://github.com/delaprada/HandSynthesis.git"},{"type":"has_code","target_id":"github:zwx-open:EVOS-INR","source_url":"https://github.com/zwx-open/EVOS-INR"}]', NULL, NULL, 'pending', 70, '63b929d42943ef5a29ef590da4e3a1c8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-amusi-CVPR2025-Papers-with-Code from https://github.com/amusi.png
Image converted to WebP: data/images/github-amusi-CVPR2025-Papers-with-Code.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-zergtant-pytorch-handbook', 'github--zergtant--pytorch-handbook', 'pytorch-handbook', 'zergtant', '!pytorch 这是一本开源的书籍，目标是帮助那些希望和使用PyTorch进行深度学习开发和研究的朋友快速入门。 由于本人水平有限，在写此教程的时候参考了一些网上的资料，在这里对他们表示敬意，我会在每个引用中附上原文地址，方便大家参考。 深度学习的技术在飞速的发展，同时PyTorch也在不断更新，且本人会逐步完善相关内容。 由于PyTorch版本更迭，教程的版本会与PyTorch版本，保持一致。 pytorch大版本更新的主要变动总结 当前版本 1.11 国内的镜像，速度很快，不会被墙：https://www.pytorch.wiki/ PDF文件目前还没有找到好的生成方法，有熟悉这方面的朋友可以联系我，感激不尽 群号：760443051 !QR 点击链接加入群聊【PyTorch Handbook 交流6群】：https://jq.qq.com/?_wv=1027&k=X4Ro6uWv 1群(985896536)已满，2群(681980831) 3群(773681699)已满 4群(884017356)已满 5群(894059877)已满 不要再加了 公众账号每日分享干货文章 ...', '["deep-learning","machine-learning","neural-network","pytorch","pytorch-handbook","pytorch-tutorials","jupyter notebook"]', 'other', 21442, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/zergtant/pytorch-handbook","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# PyTorch 中文手册（pytorch handbook）\n![pytorch](pytorch-logo-dark.png)\n\n## 书籍介绍\n这是一本开源的书籍，目标是帮助那些希望和使用PyTorch进行深度学习开发和研究的朋友快速入门。\n\n由于本人水平有限，在写此教程的时候参考了一些网上的资料，在这里对他们表示敬意，我会在每个引用中附上原文地址，方便大家参考。\n\n深度学习的技术在飞速的发展，同时PyTorch也在不断更新，且本人会逐步完善相关内容。\n\n## 版本说明\n由于PyTorch版本更迭，教程的版本会与PyTorch版本，保持一致。\n\n[pytorch大版本更新的主要变动总结](pytorch-changelog.md)  当前版本 1.11\n\n## 在线版本和PDF\n\n国内的镜像，速度很快，不会被墙：https://www.pytorch.wiki/\n\nPDF文件目前还没有找到好的生成方法，有熟悉这方面的朋友可以联系我，感激不尽\n\n## QQ 6群 \n\n群号：760443051\n\n\n![QR](PyTorch-Handbook-6.png) \n\n点击链接加入群聊【PyTorch Handbook 交流6群】：https://jq.qq.com/?_wv=1027&k=X4Ro6uWv\n\n\n1群(985896536)已满，2群(681980831) 3群(773681699)已满  4群(884017356)已满  5群(894059877)已满\n\n不要再加了\n\n## 新福利\n\n公众账号每日分享干货文章\n![weixin QR](deephub.jpg) \n\n\n\n## 说明\n\n- 修改错别字请直接提issue或PR\n\n- PR时请注意版本\n\n- 有问题也请直接提issue\n\n感谢\n\n## 目录\n\n### 第一章：PyTorch 入门\n\n1. [PyTorch 简介](chapter1/1.1-pytorch-introduction.md)\n2. [PyTorch 环境搭建](chapter1/1.2-pytorch-installation.md)\n3. [PyTorch 深度学习：60分钟快速入门（官方）](chapter1/1.3-deep-learning-with-pytorch-60-minute-blitz.md)\n    - [张量](chapter1/1_tensor_tutorial.ipynb)\n    - [Autograd：自动求导](chapter1/2_autograd_tutorial.ipynb) \n    - [神经网络](chapter1/3_neural_networks_tutorial.ipynb)\n    - [训练一个分类器](chapter1/4_cifar10_tutorial.ipynb)\n    - [选读：数据并行处理（多GPU）](chapter1/5_data_parallel_tutorial.ipynb)\n4. [相关资源介绍](chapter1/1.4-pytorch-resource.md)\n\n### 第二章 基础\n#### 第一节 PyTorch 基础\n1. [张量](chapter2/2.1.1.pytorch-basics-tensor.ipynb)\n2. [自动求导](chapter2/2.1.2-pytorch-basics-autograd.ipynb)\n3. [神经网络包nn和优化器optm](chapter2/2.1.3-pytorch-basics-nerual-network.ipynb)\n4. [数据的加载和预处理](chapter2/2.1.4-pytorch-basics-data-loader.ipynb)\n#### 第二节 深度学习基础及数学原理\n\n[深度学习基础及数学原理](chapter2/2.2-deep-learning-basic-mathematics.ipynb)\n\n#### 第三节 神经网络简介\n\n[神经网络简介](chapter2/2.3-deep-learning-neural-network-introduction.ipynb)  注：本章在本地使用微软的Edge打开会崩溃，请使Chrome Firefox打开查看\n\n#### 第四节 卷积神经网络\n\n[卷积神经网络](chapter2/2.4-cnn.ipynb)\n\n#### 第五节 循环神经网络\n\n[循环神经网络](chapter2/2.5-rnn.ipynb)\n\n### 第三章 实践\n#### 第一节 logistic回归二元分类\n\n[logistic回归二元分类](chapter3/3.1-logistic-regression.ipynb)\n\n\n#### 第二节 CNN:MNIST数据集手写数字识别\n\n[CNN:MNIST数据集手写数字识别](chapter3/3.2-mnist.ipynb)\n\n#### 第三节 RNN实例：通过Sin预测Cos\n\n[RNN实例：通过Sin预测Cos](chapter3/3.3-rnn.ipynb)\n\n### 第四章 提高\n#### 第一节 Fine-tuning\n\n[Fine-tuning](chapter4/4.1-fine-tuning.ipynb)\n\n#### 第二节 可视化\n\n[visdom](chapter4/4.2.1-visdom.ipynb)\n\n[tensorboardx](chapter4/4.2.2-tensorboardx.ipynb) \n\n[可视化理解卷积神经网络](chapter4/4.2.3-cnn-visualizing.ipynb)\n\n#### 第三节 Fast.ai\n[Fast.ai](chapter4/4.3-fastai.ipynb)\n#### 第四节 训练的一些技巧\n\n#### 第五节 多GPU并行训练\n[多GPU并行计算](chapter4/4.5-multiply-gpu-parallel-training.ipynb)\n\n#### 补充翻译文章：在PyTorch中使用DistributedDataParallel进行多GPU分布式模型训练\n[在PyTorch中使用DistributedDataParallel进行多GPU分布式模型训练](chapter4/distributeddataparallel)\n\n\n### 第五章 应用\n#### 第一节 Kaggle介绍\n[Kaggle介绍](chapter5/5.1-kaggle.md)\n#### 第二节 结构化数据\n[Pytorch处理结构化数据](chapter5/5.2-Structured-Data.ipynb)\n#### 第三节 计算机视觉\n[Fashion MNIST 图像分类](chapter5/5.3-Fashion-MNIST.ipynb)\n#### 第四节 自然语言处理\n#### 第五节 协同过滤\n\n### 第六章 资源\n\n[torchaudio](torchaudio/intro.ipynb)\n\n\n### 第七章 附录\n\n[树莓派编译安装 pytorch 1.4](pi/)\n\ntransforms的常用操作总结\n\npytorch的损失函数总结\n\npytorch的优化器总结\n\n\n## Script\nscript目录是我写的将ipynb转换成在线的版本和pdf文件的脚本，因为还在测试阶段，所以有什么问题请大家提出\n\n\n## License\n\n![](https://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png)\n\n[本作品采用知识共享署名-非商业性使用-相同方式共享 3.0  中国大陆许可协议进行许可](http://creativecommons.org/licenses/by-nc-sa/3.0/cn)\n', '{"language":"Jupyter Notebook","stars":21442,"forks":5439,"watchers":21442,"open_issues":57,"topics":["deep-learning","machine-learning","neural-network","pytorch","pytorch-handbook","pytorch-tutorials"],"default_branch":"master","size_kb":149507,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[]', NULL, NULL, 'pending', 55, '71528c1aef572d9eca061933af1b46b8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-zergtant-pytorch-handbook from https://github.com/zergtant.png
Image converted to WebP: data/images/github-zergtant-pytorch-handbook.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-TheAlgorithms-C', 'github--thealgorithms--c', 'C', 'TheAlgorithms', '<!-- the suffix in the above line is required for doxygen to consider this as the index page of the generated documentation site --> !GitHub repo size The repository is a collection of open-source implementations of a variety of algorithms implemented in C and licensed under GPLv3 License. The algorithms span a variety of topics from computer science, mathematics and statistics, data science, machine learning, engineering, etc.. The implementations and their associated documentations are mean...', '["algorithm-challenges","algorithm-competitions","algorithms","c","community-driven","computer-science","data-structures","datastructures","education","educational","hacktoberfest","interview","interview-questions","learn-to-code","machine-learning","machine-learning-algorithms","mathematics","search","sort","c"]', 'other', 21427, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/TheAlgorithms/C","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# The Algorithms - C # {#mainpage}\n<!-- the suffix in the above line is required for doxygen to consider this as the index page of the generated documentation site -->\n\n[![Gitpod Ready-to-Code](https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/TheAlgorithms/C)\n[![CodeQL CI](https://github.com/TheAlgorithms/C/actions/workflows/codeql.yml/badge.svg)](https://github.com/TheAlgorithms/C/actions/workflows/codeql_analysis.yml)\n[![Gitter chat](https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&logo=gitter&style=flat-square)](https://gitter.im/TheAlgorithms)\n[![contributions welcome](https://img.shields.io/static/v1.svg?label=Contributions&message=Welcome&color=0059b3&style=flat-square)](https://github.com/TheAlgorithms/C/blob/master/CONTRIBUTING.md)\n![GitHub repo size](https://img.shields.io/github/repo-size/TheAlgorithms/C?color=red&style=flat-square)\n[![Doxygen CI](https://github.com/TheAlgorithms/C/workflows/Doxygen%20CI/badge.svg)](https://TheAlgorithms.github.io/C)\n[![Awesome CI](https://github.com/TheAlgorithms/C/workflows/Awesome%20CI%20Workflow/badge.svg)](https://github.com/TheAlgorithms/C/actions?query=workflow%3A%22Awesome+CI+Workflow%22)\n[![Income](https://img.shields.io/liberapay/receives/TheAlgorithms.svg?logo=liberapay)](https://liberapay.com/TheAlgorithms)\n[![Discord chat](https://img.shields.io/discord/808045925556682782.svg?logo=discord&colorB=5865F2)](https://the-algorithms.com/discord/)\n[![Donate](https://liberapay.com/assets/widgets/donate.svg)](https://liberapay.com/TheAlgorithms/donate)\n\n## Overview\n\nThe repository is a collection of open-source implementations of a variety of algorithms implemented in C and licensed under [GPLv3 License](https://github.com/TheAlgorithms/C/blob/master/LICENSE). The algorithms span a variety of topics from computer science, mathematics and statistics, data science, machine learning, engineering, etc.. The implementations and their associated documentations are meant to provide a learning resource for educators and students. Hence, one may find more than one implementation for the same objective but using different algorithm strategies and optimizations.\n\n## Features\n\n* The repository provides implementations of various algorithms in one of the most fundamental general purpose languages - [C](https://en.wikipedia.org/wiki/C_(programming_language)).\n* Well documented source code with detailed explanations provide a valuable resource for educators and students alike.\n* Each source code is atomic using standard C library [`libc`](https://en.wikipedia.org/wiki/C_standard_library) and _no external libraries_ are required for their compilation and execution. Thus the fundamentals of the algorithms can be studied in much depth.\n* Source codes are [compiled and tested](https://github.com/TheAlgorithms/C/actions?query=workflow%3A%22Awesome+CI+Workflow%22) for every commit on the latest versions of two major operating systems viz., MacOS and Ubuntu (Linux) using AppleClang 14.0.0 and GNU 11.3.0 respectively.\n* Strict adherence to [C11](https://en.wikipedia.org/wiki/C11_(C_standard_revision)) standard ensures portability of code to embedded systems as well like ESP32, ARM Cortex, etc. with little to no changes.\n* Self-checks within programs ensure correct implementations with confidence.\n* Modular implementations and OpenSource licensing enable the functions to be utilized conveniently in other applications.\n\n## Documentation\n\n[Online Documentation](https://TheAlgorithms.github.io/C) is generated from the repository source codes directly. The documentation contains all resources including source code snippets, details on execution of the programs, diagrammatic representation of program flow, and links to external resources where necessary.\nClick on [Files menu](https://TheAlgorithms.github.io/C/files.html) to see the list of all the files documented with the code.\n\n[Documentation of Algorithms in C](https://thealgorithms.github.io/C) by [The Algorithms Contributors](https://github.com/TheAlgorithms/C/graphs/contributors) is licensed under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/?ref=chooser-v1)<br/>\n<a href="https://creativecommons.org/licenses/by-sa/4.0"><img alt="Creative Commons License" style="height:22px!important;margin-left: 3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg" /><img  alt="Credit must be given to the creator" style="height:22px!important;margin-left: 3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg" /><img alt="Adaptations must be shared under the same terms" style="height:22px!important;margin-left: 3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg" /></a>\n\n## Contributions\n\nAs a community developed and maintained repository, we welcome new un-plagiarized quality contributions. Please read our [Contribution Guidelines](https://github.com/TheAlgorithms/C/blob/master/CONTRIBUTING.md).\n', '{"language":"C","stars":21427,"forks":4696,"watchers":21427,"open_issues":36,"topics":["algorithm-challenges","algorithm-competitions","algorithms","c","community-driven","computer-science","data-structures","datastructures","education","educational","hacktoberfest","interview","interview-questions","learn-to-code","machine-learning","machine-learning-algorithms","mathematics","search","sort"],"default_branch":"master","size_kb":20365,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"}]', NULL, 'GPL-3.0', 'approved', 65, '6f2f12db828b51dbd017b318d69de659', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-TheAlgorithms-C from https://github.com/TheAlgorithms.png
Image converted to WebP: data/images/github-TheAlgorithms-C.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-serengil-deepface', 'github--serengil--deepface', 'deepface', 'serengil', '<div align="center"> <div align="center"> <a href="https://trendshift.io/repositories/4227" target="_blank"><img src="https://trendshift.io/api/badge/repositories/4227" alt="serengil%2Fdeepface | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a> <!-- <a href="https://www.producthunt.com/posts/deepface?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-deepface" target="_blank"> <img src="https://api.producthunt.com/widgets/embed-image/v1/featured...', '["age-prediction","arcface","deep-learning","deepface","deepid","emotion-recognition","face-analysis","face-recognition","facenet","facial-expression-recognition","facial-recognition","gender-prediction","machine-learning","openface","python","race-classification","vgg-face","python"]', 'other', 21235, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/serengil/deepface","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# deepface\n\n<div align="center">\n\n[![Downloads](https://static.pepy.tech/personalized-badge/deepface?period=total&units=international_system&left_color=grey&right_color=blue&left_text=downloads)](https://pepy.tech/project/deepface)\n[![Stars](https://img.shields.io/github/stars/serengil/deepface?color=yellow&style=flat&label=%E2%AD%90%20stars)](https://github.com/serengil/deepface/stargazers)\n[![Pulls](https://img.shields.io/docker/pulls/serengil/deepface?logo=docker)](https://hub.docker.com/r/serengil/deepface)\n[![License](http://img.shields.io/:license-MIT-green.svg?style=flat)](https://github.com/serengil/deepface/blob/master/LICENSE)\n[![Tests](https://github.com/serengil/deepface/actions/workflows/tests.yml/badge.svg)](https://github.com/serengil/deepface/actions/workflows/tests.yml)\n[![DOI](http://img.shields.io/:DOI-10.17671/gazibtd.1399077-blue.svg?style=flat)](https://doi.org/10.17671/gazibtd.1399077)\n\n[![Blog](https://img.shields.io/:blog-sefiks.com-blue.svg?style=flat&logo=wordpress)](https://sefiks.com)\n[![YouTube](https://img.shields.io/:youtube-@sefiks-red.svg?style=flat&logo=youtube)](https://www.youtube.com/@sefiks?sub_confirmation=1)\n[![Twitter](https://img.shields.io/:follow-@serengil-blue.svg?style=flat&logo=x)](https://twitter.com/intent/user?screen_name=serengil)\n\n[![Patreon](https://img.shields.io/:become-patron-f96854.svg?style=flat&logo=patreon)](https://www.patreon.com/serengil?repo=deepface)\n[![GitHub Sponsors](https://img.shields.io/github/sponsors/serengil?logo=GitHub&color=lightgray)](https://github.com/sponsors/serengil)\n[![Buy Me a Coffee](https://img.shields.io/badge/-buy_me_a%C2%A0coffee-gray?logo=buy-me-a-coffee)](https://buymeacoffee.com/serengil)\n\n<div align="center">\n  <a href="https://trendshift.io/repositories/4227" target="_blank"><img src="https://trendshift.io/api/badge/repositories/4227" alt="serengil%2Fdeepface | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n  <!--\n  <a href="https://www.producthunt.com/posts/deepface?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-deepface" target="_blank">\n      <img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=753599&theme=light" alt="DeepFace - A Lightweight Deep Face Recognition Library for Python | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" />\n  </a>\n  -->\n</div>\n\n<!--\n[![Hacker News](https://img.shields.io/badge/dynamic/json?color=orange&label=Hacker%20News&query=score&url=https%3A%2F%2Fhacker-news.firebaseio.com%2Fv0%2Fitem%2F42584896.json&logo=y-combinator)](https://news.ycombinator.com/item?id=42584896)\n[![Product Hunt](https://img.shields.io/badge/Product%20Hunt-%E2%96%B2-orange?logo=producthunt)](https://www.producthunt.com/posts/deepface?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-deepface)\n-->\n\n<!-- [![DOI](http://img.shields.io/:DOI-10.1109/ICEET53442.2021.9659697-blue.svg?style=flat)](https://doi.org/10.1109/ICEET53442.2021.9659697) -->\n<!-- [![DOI](http://img.shields.io/:DOI-10.1109/ASYU50717.2020.9259802-blue.svg?style=flat)](https://doi.org/10.1109/ASYU50717.2020.9259802) -->\n\n</div>\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-icon-labeled.png" width="200" height="240"></p>\n\nDeepFace is a lightweight [face recognition](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/) and facial attribute analysis ([age](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/), [gender](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/), [emotion](https://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/) and [race](https://sefiks.com/2019/11/11/race-and-ethnicity-prediction-in-keras/)) framework for python. It is a hybrid face recognition framework wrapping **state-of-the-art** models: [`VGG-Face`](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/), [`FaceNet`](https://sefiks.com/2018/09/03/face-recognition-with-facenet-in-keras/), [`OpenFace`](https://sefiks.com/2019/07/21/face-recognition-with-openface-in-keras/), [`DeepFace`](https://sefiks.com/2020/02/17/face-recognition-with-facebook-deepface-in-keras/), [`DeepID`](https://sefiks.com/2020/06/16/face-recognition-with-deepid-in-keras/), [`ArcFace`](https://sefiks.com/2020/12/14/deep-face-recognition-with-arcface-in-keras-and-python/), [`Dlib`](https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/), `SFace`, `GhostFaceNet`, `Buffalo_L`.\n\n[A modern face recognition pipeline](https://sefiks.com/2020/05/01/a-gentle-introduction-to-face-recognition-in-deep-learning/) consists of 5 common stages: [detect](https://sefiks.com/2020/08/25/deep-face-detection-with-opencv-in-python/), [align](https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/), [normalize](https://sefiks.com/2020/11/20/facial-landmarks-for-face-recognition-with-dlib/), [represent](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/) and [verify](https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/). While DeepFace handles all these common stages in the background, you don’t need to acquire in-depth knowledge about all the processes behind it. You can just call its verification, find or analysis function with a single line of code.\n\n[`Experiments`](https://github.com/serengil/deepface/tree/master/benchmarks) show that **human beings have 97.53% accuracy** on facial recognition tasks whereas those models already reached and passed that accuracy level.\n\n## Installation [![PyPI](https://img.shields.io/pypi/v/deepface.svg)](https://pypi.org/project/deepface/)\n\nThe easiest way to install deepface is to download it from [`PyPI`](https://pypi.org/project/deepface/). It''s going to install the library itself and its prerequisites as well.\n\n```shell\n$ pip install deepface\n```\n\nAlternatively, you can also install deepface from its source code. Source code may have new features not published in pip release yet.\n\n```shell\n$ git clone https://github.com/serengil/deepface.git\n$ cd deepface\n$ pip install -e .\n```\n\nOnce you installed the library, then you will be able to import it and use its functionalities.\n\n```python\nfrom deepface import DeepFace\n```\n\n**Face Verification** - [`Demo`](https://youtu.be/KRCvkNCOphE)\n\nThis function determines whether two facial images belong to the same person or to different individuals. It accepts exact image file paths as input, but also supports NumPy arrays, base64-encoded images, and URLs. The function returns a dictionary, where the key of interest is `verified`: True indicates the images are of the same person, while False means they are of different people. In addition to this binary classification, the function also provides a [`confidence`](https://youtu.be/QQ4vO6UOsFo) score that reflects the likelihood that the two images represent the same person.\n\n```python\nresult = DeepFace.verify(img1_path = "img1.jpg", img2_path = "img2.jpg")\n```\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/verify-credit.jpg" width="99%"></p>\n\n**Face recognition** - [`Demo`](https://youtu.be/Hrjp-EStM_s)\n\n[Face recognition](https://sefiks.com/2020/05/25/large-scale-face-recognition-for-deep-learning/) requires applying face verification many times. Herein, deepface has an out-of-the-box find function to handle this action. It''s going to look for the identity of input image in the database path and it will return list of pandas data frame as output. Meanwhile, facial embeddings of the facial database are stored in a pickle file to be searched faster in next time. Result is going to be the size of faces appearing in the source image. Besides, target images in the database can have many faces as well.\n\n```python\ndfs = DeepFace.find(img_path = "img1.jpg", db_path = "C:/my_db")\n```\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-6-v2.jpg" width="95%"></p>\n\n**Facial Attribute Analysis** - [`Demo`](https://youtu.be/GT2UeN85BdA)\n\nDeepFace also comes with a strong facial attribute analysis module including [`age`](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/), [`gender`](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/), [`facial expression`](https://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/) (including angry, fear, neutral, sad, disgust, happy and surprise) and [`race`](https://sefiks.com/2019/11/11/race-and-ethnicity-prediction-in-keras/) (including asian, white, middle eastern, indian, latino and black) predictions. Result is going to be the size of faces appearing in the source image.\n\n```python\nobjs = DeepFace.analyze(\n  img_path = "img4.jpg", actions = [''age'', ''gender'', ''race'', ''emotion'']\n)\n```\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-2.jpg" width="95%"></p>\n\nAge model got ± 4.65 MAE; gender model got 97.44% accuracy, 96.29% precision and 95.05% recall as mentioned in its [tutorial](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/).\n\n**Real Time Analysis** - [`Demo`](https://youtu.be/-c9sSJcx6wI), [`React Demo part-i`](https://youtu.be/IXoah6rhxac), [`React Demo part-ii`](https://youtu.be/_waBA-cH2D4)\n\nYou can run deepface for real time videos as well. Stream function will access your webcam and apply both face recognition and facial attribute analysis. The function starts to analyze a frame if it can focus a face sequentially 5 frames. Then, it shows results 5 seconds.\n\n```python\nDeepFace.stream(db_path = "C:/database")\n```\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-3.jpg" width="90%"></p>\n\nEven though face recognition is based on one-shot learning, you can use multiple face pictures of a person as well. You should rearrange your directory structure as illustrated below.\n\n```bash\nuser\n├── database\n│   ├── Alice\n│   │   ├── Alice1.jpg\n│   │   ├── Alice2.jpg\n│   ├── Bob\n│   │   ├── Bob.jpg\n```\n\nIf you intend to perform face verification or analysis tasks directly from your browser, [`deepface-react-ui`](https://github.com/serengil/deepface-react-ui) is a separate repository built using ReactJS depending on deepface api.\n\nHere, you can also find some real time demos for various facial recognition models:\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/refs/heads/master/icon/deepface-realtime.jpg" width="90%"></p>\n\n| Task                 | Model    | Demo                                    |\n| ---                  | ---      | ---                                     |\n| Facial Recognition   | DeepFace | [`Video`](https://youtu.be/YjYIMs5ZOfc) |\n| Facial Recognition   | FaceNet  | [`Video`](https://youtu.be/vB1I5vWgTQg) |\n| Facial Recognition   | VGG-Face | [`Video`](https://youtu.be/tSU_lNi0gQQ) |\n| Facial Recognition   | OpenFace | [`Video`](https://youtu.be/-4z2sL6wzP8) |\n| Age & Gender         | Default  | [`Video`](https://youtu.be/tFI7vZn3P7E) |\n| Race & Ethnicity     | Default  | [`Video`](https://youtu.be/-ztiy5eJha8) |\n| Emotion              | Default  | [`Video`](https://youtu.be/Y7DfLvLKScs) |\n| Celebrity Look-Alike | Default  | [`Video`](https://youtu.be/RMgIKU1H8DY) |\n\n**Embeddings** - [`Tutorial`](https://sefiks.com/2025/06/28/what-are-vector-embeddings-and-why-they-matter-in-ai/), [`Demo`](https://youtu.be/OYialFo7Qo4)\n\nFace recognition models basically represent facial images as multi-dimensional vectors. Sometimes, you need those embedding vectors directly. DeepFace comes with a dedicated representation function. Represent function returns a list of embeddings. Result is going to be the size of faces appearing in the image path.\n\n```python\nembedding_objs = DeepFace.represent(img_path = "img.jpg")\n```\n\nEmbeddings can be [plotted](https://sefiks.com/2020/05/01/a-gentle-introduction-to-face-recognition-in-deep-learning/) as below. Each slot is corresponding to a dimension value and dimension value is emphasized with colors. Similar to 2D barcodes, vertical dimension stores no information in the illustration.\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/embedding.jpg" width="95%"></p>\n\nIn summary, the distance between vector embeddings of the same person should be smaller than that between embeddings of different people. When reduced to two-dimensional space, the clusters become clearly distinguishable.\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/refs/heads/master/icon/facenet-pca.png" width="95%"></p>\n\n**Face recognition models** - [`Demo`](https://youtu.be/eKOZawGR3y0)\n\nDeepFace is a **hybrid** face recognition package. It currently wraps many **state-of-the-art** face recognition models: [`VGG-Face`](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/) , [`FaceNet`](https://sefiks.com/2018/09/03/face-recognition-with-facenet-in-keras/), [`OpenFace`](https://sefiks.com/2019/07/21/face-recognition-with-openface-in-keras/), [`DeepFace`](https://sefiks.com/2020/02/17/face-recognition-with-facebook-deepface-in-keras/), [`DeepID`](https://sefiks.com/2020/06/16/face-recognition-with-deepid-in-keras/), [`ArcFace`](https://sefiks.com/2020/12/14/deep-face-recognition-with-arcface-in-keras-and-python/), [`Dlib`](https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/), `SFace`, `GhostFaceNet` and `Buffalo_L`. The default configuration uses VGG-Face model.\n\n```python\nmodels = [\n    "VGG-Face", "Facenet", "Facenet512", "OpenFace", "DeepFace",\n    "DeepID", "ArcFace", "Dlib", "SFace", "GhostFaceNet",\n    "Buffalo_L",\n]\n\nresult = DeepFace.verify(\n  img1_path = "img1.jpg", img2_path = "img2.jpg", model_name = models[0]\n)\n\ndfs = DeepFace.find(\n  img_path = "img1.jpg", db_path = "C:/my_db", model_name = models[1]\n)\n\nembeddings = DeepFace.represent(\n  img_path = "img.jpg", model_name = models[2]\n)\n```\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/model-portfolio-20240316.jpg" width="95%"></p>\n\nFaceNet, VGG-Face, ArcFace and Dlib are overperforming ones based on experiments - see [`BENCHMARKS`](https://github.com/serengil/deepface/tree/master/benchmarks) for more details. You can find the measured scores of various models in DeepFace and the reported scores from their original studies in the following table.\n\n| Model          | Measured Score | Declared Score     |\n| -------------- | -------------- | ------------------ |\n| Facenet512     | 98.4%          | 99.6%              |\n| Human-beings   | 97.5%          | 97.5%              |\n| Facenet        | 97.4%          | 99.2%              |\n| Dlib           | 96.8%          | 99.3 %             |\n| VGG-Face       | 96.7%          | 98.9%              |\n| ArcFace        | 96.7%          | 99.5%              |\n| GhostFaceNet   | 93.3%          | 99.7%              |\n| SFace          | 93.0%          | 99.5%              |\n| OpenFace       | 78.7%          | 92.9%              |\n| DeepFace       | 69.0%          | 97.3%              |\n| DeepID         | 66.5%          | 97.4%              |\n\nConducting experiments with those models within DeepFace may reveal disparities compared to the original studies, owing to the adoption of distinct detection or normalization techniques. Furthermore, some models have been released solely with their backbones, lacking pre-trained weights. Thus, we are utilizing their re-implementations instead of the original pre-trained weights.\n\n**Face Detection and Alignment** - [`Demo`](https://youtu.be/GZ2p2hj2H5k)\n\nFace detection and alignment are important early stages of a modern face recognition pipeline. [Experiments](https://github.com/serengil/deepface/tree/master/benchmarks) show that detection increases the face recognition accuracy up to 42%, while alignment increases it up to 6%. [`OpenCV`](https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/), [`Ssd`](https://sefiks.com/2020/08/25/deep-face-detection-with-opencv-in-python/), [`Dlib`](https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/),  [`MtCnn`](https://sefiks.com/2020/09/09/deep-face-detection-with-mtcnn-in-python/), `Faster MtCnn`, [`RetinaFace`](https://sefiks.com/2021/04/27/deep-face-detection-with-retinaface-in-python/), [`MediaPipe`](https://sefiks.com/2022/01/14/deep-face-detection-with-mediapipe/), `Yolo`, `YuNet` and `CenterFace` detectors are wrapped in deepface.\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/detector-portfolio-v6.jpg" width="95%"></p>\n\nAll deepface functions accept optional detector backend and align input arguments. You can switch among those detectors and alignment modes with these arguments. OpenCV is the default detector and alignment is on by default.\n\n```python\nbackends = [\n    ''opencv'', ''ssd'', ''dlib'', ''mtcnn'', ''fastmtcnn'',\n    ''retinaface'', ''mediapipe'', ''yolov8n'', ''yolov8m'', \n    ''yolov8l'', ''yolov11n'', ''yolov11s'', ''yolov11m'',\n    ''yolov11l'', ''yolov12n'', ''yolov12s'', ''yolov12m'',\n    ''yolov12l'', ''yunet'', ''centerface'',\n]\ndetector = backends[3]\nalign = True\n\nobj = DeepFace.verify(\n  img1_path = "img1.jpg", img2_path = "img2.jpg", detector_backend = detector, align = align\n)\n\ndfs = DeepFace.find(\n  img_path = "img.jpg", db_path = "my_db", detector_backend = detector, align = align\n)\n\nembedding_objs = DeepFace.represent(\n  img_path = "img.jpg", detector_backend = detector, align = align\n)\n\ndemographies = DeepFace.analyze(\n  img_path = "img4.jpg", detector_backend = detector, align = align\n)\n\nface_objs = DeepFace.extract_faces(\n  img_path = "img.jpg", detector_backend = detector, align = align\n)\n```\n\nFace recognition models are actually CNN models and they expect standard sized inputs. So, resizing is required before representation. To avoid deformation, deepface adds black padding pixels according to the target size argument after detection and alignment.\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/detector-outputs-20240414.jpg" width="90%"></p>\n\n[RetinaFace](https://sefiks.com/2021/04/27/deep-face-detection-with-retinaface-in-python/) and [MtCnn](https://sefiks.com/2020/09/09/deep-face-detection-with-mtcnn-in-python/) seem to overperform in detection and alignment stages but they are much slower. If the speed of your pipeline is more important, then you should use opencv or ssd. On the other hand, if you consider the accuracy, then you should use retinaface or mtcnn.\n\nThe performance of RetinaFace is very satisfactory even in the crowd as seen in the following illustration. Besides, it comes with an incredible facial landmark detection performance. Highlighted red points show some facial landmarks such as eyes, nose and mouth. That''s why, alignment score of RetinaFace is high as well.\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/retinaface-results.jpeg" width="90%">\n<br><em>The Yellow Angels - Fenerbahce Women''s Volleyball Team</em>\n</p>\n\nYou can find out more about RetinaFace on this [repo](https://github.com/serengil/retinaface).\n\n**Face Anti Spoofing** - [`Demo`](https://youtu.be/UiK1aIjOBlQ)\n\nDeepFace also includes an anti-spoofing analysis module to understand given image is real or fake. To activate this feature, set the `anti_spoofing` argument to True in any DeepFace tasks.\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/face-anti-spoofing.jpg" width="40%"></p>\n\n```python\n# anti spoofing test in face detection\nface_objs = DeepFace.extract_faces(img_path="dataset/img1.jpg", anti_spoofing = True)\nassert all(face_obj["is_real"] is True for face_obj in face_objs)\n\n# anti spoofing test in real time analysis\nDeepFace.stream(db_path = "C:/database", anti_spoofing = True)\n```\n\n**Similarity** - [`Demo`](https://youtu.be/1EPoS69fHOc)\n\nFace recognition models are regular [convolutional neural networks](https://sefiks.com/2018/03/23/convolutional-autoencoder-clustering-images-with-neural-networks/) and they are responsible to represent faces as vectors. We expect that a face pair of same person should be [more similar](https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/) than a face pair of different persons.\n\nSimilarity could be calculated by different metrics such as [Cosine Similarity](https://sefiks.com/2018/08/13/cosine-similarity-in-machine-learning/), Angular Distance, Euclidean Distance or L2 normalized Euclidean. The default configuration uses cosine similarity. According to [experiments](https://github.com/serengil/deepface/tree/master/benchmarks), no distance metric is overperforming than other.\n\n```python\nmetrics = ["cosine", "euclidean", "euclidean_l2", "angular"]\n\nresult = DeepFace.verify(\n  img1_path = "img1.jpg", img2_path = "img2.jpg", distance_metric = metrics[1]\n)\n\ndfs = DeepFace.find(\n  img_path = "img1.jpg", db_path = "C:/my_db", distance_metric = metrics[2]\n)\n```\n\n**API** - [`Demo`](https://youtu.be/HeKCQ6U9XmI), [`Docker Demo`](https://youtu.be/9Tk9lRQareA)\n\nDeepFace serves an API as well - see [`api folder`](https://github.com/serengil/deepface/tree/master/deepface/api/src) for more details. You can clone deepface source code and run the api with the following command. It will use gunicorn server to get a rest service up. In this way, you can call deepface from an external system such as mobile app or web.\n\n```shell\ncd scripts\n\n# run the service directly\n./service.sh\n\n# run the service via docker\n./dockerize.sh\n```\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-api.jpg" width="90%"></p>\n\nFace recognition, facial attribute analysis and vector representation functions are covered in the API. You are expected to call these functions as http post methods. Default service endpoints will be `http://localhost:5005/verify` for face recognition, `http://localhost:5005/analyze` for facial attribute analysis, and `http://localhost:5005/represent` for vector representation. The API accepts images as file uploads (via form data), or as exact image paths, URLs, or base64-encoded strings (via either JSON or form data), providing versatile options for different client requirements. [Here](https://github.com/serengil/deepface/tree/master/deepface/api/postman), you can find a postman project to find out how these methods should be called.\n\n**Large Scale Facial Recognition** - [`Playlist`](https://www.youtube.com/playlist?list=PLsS_1RYmYQQGSJu_Z3OVhXhGmZ86_zuIm)\n\nIf your task requires facial recognition on large datasets, you should combine DeepFace with a vector index or vector database. This setup will perform [approximate nearest neighbor](https://youtu.be/c10w0Ptn_CU) searches instead of exact ones, allowing you to identify a face in a database containing billions of entries within milliseconds. Common vector index solutions include [Annoy](https://youtu.be/Jpxm914o2xk), [Faiss](https://youtu.be/6AmEvDTKT-k), [Voyager](https://youtu.be/2ZYTV9HlFdU), [NMSLIB](https://youtu.be/EVBhO8rbKbg), [ElasticSearch](https://youtu.be/i4GvuOmzKzo). For vector databases, popular options are [Postgres with its pgvector extension](https://youtu.be/Xfv4hCWvkp0) and [RediSearch](https://youtu.be/yrXlS0d6t4w).\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-big-data.jpg" width="90%"></p>\n\nConversely, if your task involves facial recognition on small to moderate-sized databases, you can adopt use relational databases such as [Postgres](https://youtu.be/f41sLxn1c0k) or [SQLite](https://youtu.be/_1ShBeWToPg), or NoSQL databases like [Mongo](https://youtu.be/dmprgum9Xu8), [Redis](https://youtu.be/X7DSpUMVTsw) or [Cassandra](https://youtu.be/J_yXpc3Y8Ec) to perform exact nearest neighbor search.\n\n**Encrypt Embeddings** - [`Demo with PHE`](https://youtu.be/8VCu39jFZ7k), [`Tutorial for PHE`](https://sefiks.com/2025/03/04/vector-similarity-search-with-partially-homomorphic-encryption-in-python/), [`Demo with FHE`](https://youtu.be/njjw0PEhH00), [`Tutorial for FHE`](https://sefiks.com/2021/12/01/homomorphic-facial-recognition-with-tenseal/)\n\nVector embeddings, though not reversible, carry sensitive information like fingerprints, making their security crucial. Encrypting them prevents adversarial misuse. Traditional encryption (e.g., AES) is secure but unsuitable for cloud-based distance calculations.\n\n[Homomorphic encryption](https://youtu.be/3ejI0zNPMEQ) allows computations on encrypted data without revealing content—ideal for secure cloud processing. For example, the cloud can compute encrypted similarity without knowing the data, while only the key holder can decrypt the result. See the  [`LightPHE`](https://github.com/serengil/LightPHE) library for partially homomorphic encryption.\n\n```python\nfrom lightphe import LightPHE\n\n# build an additively homomorphic cryptosystem (e.g. Paillier) on-prem\ncs = LightPHE(algorithm_name = "Paillier", precision = 19)\n\n# define encrypted and plain vectors\nencrypted_alpha = DeepFace.represent("source.jpg", cryptosystem=cs)[0]["encrypted_embedding"]\nbeta = DeepFace.represent("target.jpg")[0]["embedding"]\n\n# dot product of encrypted & plain embedding in cloud - private key not required\nencrypted_cosine_similarity = encrypted_alpha @ beta\n\n# decrypt similarity on-prem - private key required\ncalculated_similarity = cs.decrypt(encrypted_cosine_similarity)[0]\n\n# verification\nprint("same person" if calculated_similarity >= 1 - threshold else "different persons")\n```\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/refs/heads/master/icon/encrypt-embeddings.jpg" width="60%"></p>\n\nFor stronger privacy, fully homomorphic encryption enables dot product computations between encrypted embeddings, but it''s far more computationally intensive. Explore [`CipherFace`](https://github.com/serengil/cipherface) for FHE-based approaches.\n\n### Extended Applications\n\nDeepFace can also be used for fun and insightful applications such as\n\n**Find Your Celebrity Look-Alike** - [`Demo`](https://youtu.be/jaxkEn-Kieo), [`Real-Time Demo`](https://youtu.be/RMgIKU1H8DY), [`Tutorial`](https://sefiks.com/2019/05/05/celebrity-look-alike-face-recognition-with-deep-learning-in-keras/)\n\nDeepFace can analyze your facial features and match them with celebrities, letting you discover which famous personality you resemble the most.\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/celebrity-look-alike.jpg" width="55%"></p>\n\n**Find Which Parent a Child Look More** - [`Demo`](https://youtu.be/nza4tmi9vhE), [`Tutorial`](https://sefiks.com/2022/12/22/decide-whom-your-child-looks-like-with-facial-recognition-mommy-or-daddy/)\n\nDeepFace can also be used to compare a child''s face to their parents'' or relatives'' faces to determine which one the child resembles more.\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/parental-look-alike-scaled.jpg" width="90%"></p>\n\n## Contribution\n\nPull requests are more than welcome! If you are planning to contribute a large patch, please create an issue first to get any upfront questions or design decisions out of the way first.\n\nBefore creating a PR, you should run the unit tests and linting locally by running `make test && make lint` command. Once a PR sent, GitHub test workflow will be run automatically and unit test and linting jobs will be available in [GitHub actions](https://github.com/serengil/deepface/actions) before approval.\n\n## Support\n\nThere are many ways to support a project - starring⭐️ the GitHub repo is just one 🙏 It really helps the project get discovered by more people.\n\nIf you do like this work, then you can support it financially on [Patreon](https://www.patreon.com/serengil?repo=deepface), [GitHub Sponsors](https://github.com/sponsors/serengil) or [Buy Me a Coffee](https://buymeacoffee.com/serengil). Also, your company''s logo will be shown on README on GitHub if you become a sponsor in gold, silver or bronze tiers.\n\n<a href="https://www.patreon.com/serengil?repo=deepface">\n<img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/patreon.png" width="30%">\n</a>\n\n<!--\n<a href="https://github.com/sponsors/serengil">\n<img src="https://raw.githubusercontent.com/serengil/deepface/refs/heads/master/icon/github_sponsor_button.png" width="37%">\n</a>\n\n<a href="https://buymeacoffee.com/serengil">\n<img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/bmc-button.png" width="25%">\n</a>\n-->\n\n<!--\nAdditionally, you can help us reach a wider audience by upvoting our posts on Hacker News and Product Hunt.\n\n<div style="display: flex; align-items: center; gap: 10px;">\n  <a href="https://news.ycombinator.com/item?id=42584896">\n    <img src="https://hackerbadge.vercel.app/api?id=42584896&type=orange" style="width: 250px; height: 54px;" width="250" alt="Featured on Hacker News">\n  </a>\n  \n  <a href="https://www.producthunt.com/posts/deepface?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-deepface" target="_blank">\n    <img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=753599&theme=light" alt="DeepFace - A Lightweight Deep Face Recognition Library for Python | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" />\n  </a>\n</div>\n-->\n\n## Citation\n\nPlease cite deepface in your publications if it helps your research.\n\n<details open>\n  <summary>S. Serengil and A. Ozpinar, <b>"A Benchmark of Facial Recognition Pipelines and Co-Usability Performances of Modules"</b>, <i>Journal of Information Technologies</i>, vol. 17, no. 2, pp. 95-107, 2024.</summary>\n  \n  ```BibTeX\n  @article{serengil2024lightface,\n    title     = {A Benchmark of Facial Recognition Pipelines and Co-Usability Performances of Modules},\n    author    = {Serengil, Sefik and Ozpinar, Alper},\n    journal   = {Journal of Information Technologies},\n    volume    = {17},\n    number    = {2},\n    pages     = {95-107},\n    year      = {2024},\n    doi       = {10.17671/gazibtd.1399077},\n    url       = {https://dergipark.org.tr/en/pub/gazibtd/issue/84331/1399077},\n    publisher = {Gazi University}\n  }\n  ```\n</details>\n\n<details>\n  <summary>S. I. Serengil and A. Ozpinar, <b>"LightFace: A Hybrid Deep Face Recognition Framework"</b>, <i>2020 Innovations in Intelligent Systems and Applications Conference (ASYU)</i>, 2020, pp. 23-27.</summary>\n  \n  ```BibTeX\n  @inproceedings{serengil2020lightface,\n    title        = {LightFace: A Hybrid Deep Face Recognition Framework},\n    author       = {Serengil, Sefik Ilkin and Ozpinar, Alper},\n    booktitle    = {2020 Innovations in Intelligent Systems and Applications Conference (ASYU)},\n    pages        = {23-27},\n    year         = {2020},\n    doi          = {10.1109/ASYU50717.2020.9259802},\n    url          = {https://ieeexplore.ieee.org/document/9259802},\n    organization = {IEEE}\n  }\n  ```\n</details>\n\n<details>\n  <summary>S. I. Serengil and A. Ozpinar, <b>"HyperExtended LightFace: A Facial Attribute Analysis Framework"</b>, <i>2021 International Conference on Engineering and Emerging Technologies (ICEET)</i>, 2021, pp. 1-4.</summary>\n  \n  ```BibTeX\n  @inproceedings{serengil2021lightface,\n    title        = {HyperExtended LightFace: A Facial Attribute Analysis Framework},\n    author       = {Serengil, Sefik Ilkin and Ozpinar, Alper},\n    booktitle    = {2021 International Conference on Engineering and Emerging Technologies (ICEET)},\n    pages        = {1-4},\n    year         = {2021},\n    doi          = {10.1109/ICEET53442.2021.9659697},\n    url          = {https://ieeexplore.ieee.org/document/9659697},\n    organization = {IEEE}\n  }\n  ```\n</details>\n\nAlso, if you use deepface in your GitHub projects, please add `deepface` in the `requirements.txt`.\n\n## Licence\n\nDeepFace is licensed under the MIT License - see [`LICENSE`](https://github.com/serengil/deepface/blob/master/LICENSE) for more details.\n\nDeepFace wraps some external face recognition models: [VGG-Face](http://www.robots.ox.ac.uk/~vgg/software/vgg_face/), [Facenet](https://github.com/davidsandberg/facenet/blob/master/LICENSE.md) (both 128d and 512d), [OpenFace](https://github.com/iwantooxxoox/Keras-OpenFace/blob/master/LICENSE), [DeepFace](https://github.com/swghosh/DeepFace), [DeepID](https://github.com/Ruoyiran/DeepID/blob/master/LICENSE.md), [ArcFace](https://github.com/leondgarse/Keras_insightface/blob/master/LICENSE), [Dlib](https://github.com/davisking/dlib/blob/master/dlib/LICENSE.txt), [SFace](https://github.com/opencv/opencv_zoo/blob/master/models/face_recognition_sface/LICENSE), [GhostFaceNet](https://github.com/HamadYA/GhostFaceNets/blob/main/LICENSE) and\n[Buffalo_L](https://github.com/deepinsight/insightface/blob/master/README.md). Besides, age, gender and race / ethnicity models were trained on the backbone of VGG-Face with transfer learning. Similarly, DeepFace wraps many face detectors: [OpenCv](https://github.com/opencv/opencv/blob/4.x/LICENSE), [Ssd](https://github.com/opencv/opencv/blob/master/LICENSE), [Dlib](https://github.com/davisking/dlib/blob/master/LICENSE.txt), [MtCnn](https://github.com/ipazc/mtcnn/blob/master/LICENSE), [Fast MtCnn](https://github.com/timesler/facenet-pytorch/blob/master/LICENSE.md), [RetinaFace](https://github.com/serengil/retinaface/blob/master/LICENSE), [MediaPipe](https://github.com/google/mediapipe/blob/master/LICENSE), [YuNet](https://github.com/ShiqiYu/libfacedetection/blob/master/LICENSE), [Yolo](https://github.com/ultralytics/ultralytics/blob/main/LICENSE) and [CenterFace](https://github.com/Star-Clouds/CenterFace/blob/master/LICENSE). Finally, DeepFace is optionally using [face anti spoofing](https://github.com/minivision-ai/Silent-Face-Anti-Spoofing/blob/master/LICENSE) to determine the given images are real or fake. License types will be inherited when you intend to utilize those models. Please check the license types of those models for production purposes.\n\nDeepFace [logo](https://thenounproject.com/term/face-recognition/2965879/) is created by [Adrien Coquet](https://thenounproject.com/coquet_adrien/) and it is licensed under [Creative Commons: By Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/).\n', '{"language":"Python","stars":21235,"forks":2889,"watchers":21235,"open_issues":10,"topics":["age-prediction","arcface","deep-learning","deepface","deepid","emotion-recognition","face-analysis","face-recognition","facenet","facial-expression-recognition","facial-recognition","gender-prediction","machine-learning","openface","python","race-classification","vgg-face"],"default_branch":"master","size_kb":56775,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:sponsors:serengil","source_url":"https://github.com/sponsors/serengil"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:serengil:deepface.git","source_url":"https://github.com/serengil/deepface.git"},{"type":"has_code","target_id":"github:serengil:deepface-react-ui","source_url":"https://github.com/serengil/deepface-react-ui"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:serengil:retinaface","source_url":"https://github.com/serengil/retinaface"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:serengil:LightPHE","source_url":"https://github.com/serengil/LightPHE"},{"type":"has_code","target_id":"github:serengil:cipherface","source_url":"https://github.com/serengil/cipherface"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:sponsors:serengil","source_url":"https://github.com/sponsors/serengil"},{"type":"has_code","target_id":"github:sponsors:serengil\">","source_url":"https://github.com/sponsors/serengil\">"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:davidsandberg:facenet","source_url":"https://github.com/davidsandberg/facenet"},{"type":"has_code","target_id":"github:iwantooxxoox:Keras-OpenFace","source_url":"https://github.com/iwantooxxoox/Keras-OpenFace"},{"type":"has_code","target_id":"github:swghosh:DeepFace","source_url":"https://github.com/swghosh/DeepFace"},{"type":"has_code","target_id":"github:Ruoyiran:DeepID","source_url":"https://github.com/Ruoyiran/DeepID"},{"type":"has_code","target_id":"github:leondgarse:Keras_insightface","source_url":"https://github.com/leondgarse/Keras_insightface"},{"type":"has_code","target_id":"github:davisking:dlib","source_url":"https://github.com/davisking/dlib"},{"type":"has_code","target_id":"github:opencv:opencv_zoo","source_url":"https://github.com/opencv/opencv_zoo"},{"type":"has_code","target_id":"github:HamadYA:GhostFaceNets","source_url":"https://github.com/HamadYA/GhostFaceNets"},{"type":"has_code","target_id":"github:deepinsight:insightface","source_url":"https://github.com/deepinsight/insightface"},{"type":"has_code","target_id":"github:opencv:opencv","source_url":"https://github.com/opencv/opencv"},{"type":"has_code","target_id":"github:opencv:opencv","source_url":"https://github.com/opencv/opencv"},{"type":"has_code","target_id":"github:davisking:dlib","source_url":"https://github.com/davisking/dlib"},{"type":"has_code","target_id":"github:ipazc:mtcnn","source_url":"https://github.com/ipazc/mtcnn"},{"type":"has_code","target_id":"github:timesler:facenet-pytorch","source_url":"https://github.com/timesler/facenet-pytorch"},{"type":"has_code","target_id":"github:serengil:retinaface","source_url":"https://github.com/serengil/retinaface"},{"type":"has_code","target_id":"github:google:mediapipe","source_url":"https://github.com/google/mediapipe"},{"type":"has_code","target_id":"github:ShiqiYu:libfacedetection","source_url":"https://github.com/ShiqiYu/libfacedetection"},{"type":"has_code","target_id":"github:ultralytics:ultralytics","source_url":"https://github.com/ultralytics/ultralytics"},{"type":"has_code","target_id":"github:Star-Clouds:CenterFace","source_url":"https://github.com/Star-Clouds/CenterFace"},{"type":"has_code","target_id":"github:minivision-ai:Silent-Face-Anti-Spoofing","source_url":"https://github.com/minivision-ai/Silent-Face-Anti-Spoofing"}]', NULL, 'MIT', 'approved', 80, '772741bcea4680a613c0526b4aad24d8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-serengil-deepface from https://github.com/serengil.png
Image converted to WebP: data/images/github-serengil-deepface.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-recommenders-team-recommenders', 'github--recommenders-team--recommenders', 'recommenders', 'recommenders-team', '<!-- Copyright (c) Recommenders contributors. Licensed under the MIT License. --> <img src="https://raw.githubusercontent.com/recommenders-team/artwork/main/color/recommenders_color.svg" width="800"> <img align="left" width="300" src="https://raw.githubusercontent.com/recommenders-team/artwork/main/mix/join_recommenders_slack.svg"> <br> We reached 20,000 stars!! We are happy to announce that we have reached 20,000 stars on GitHub! Thank you for your support and contributions to the Recommende...', '["ai","artificial-intelligence","data-science","deep-learning","jupyter-notebook","kubernetes","machine-learning","operationalization","python","ranking","rating","recommendation","recommendation-algorithm","recommendation-engine","recommendation-system","recommender","tutorial","python"]', 'other', 21208, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/recommenders-team/recommenders","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<!--\nCopyright (c) Recommenders contributors.\nLicensed under the MIT License.\n-->\n<img src="https://raw.githubusercontent.com/recommenders-team/artwork/main/color/recommenders_color.svg" width="800">\n\n\n[![Documentation status](https://github.com/recommenders-team/recommenders/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/recommenders-team/recommenders/actions/workflows/pages/pages-build-deployment)\n[![License](https://img.shields.io/github/license/recommenders-team/recommenders.svg)](https://github.com/recommenders-team/recommenders/blob/main/LICENSE)\n[![Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![PyPI Version](https://img.shields.io/pypi/v/recommenders.svg?logo=pypi&logoColor=white)](https://pypi.org/project/recommenders)\n[![Python Versions](https://img.shields.io/pypi/pyversions/recommenders.svg?logo=python&logoColor=white)](https://pypi.org/project/recommenders)\n\n[<img align="left" width="300" src="https://raw.githubusercontent.com/recommenders-team/artwork/main/mix/join_recommenders_slack.svg">](https://join.slack.com/t/lfaifoundation/shared_invite/zt-2iyl7zyya-g5rOO5K518CBoevyi28W6w)\n\n<br>\n\n## What''s New (April, 2025)\n\nWe reached 20,000 stars!!\n\nWe are happy to announce that we have reached 20,000 stars on GitHub! Thank you for your support and contributions to the Recommenders project. We are excited to continue building and improving this project with your help.\n\nCheck out the release [Recommenders 1.2.1](https://github.com/recommenders-team/recommenders/releases/tag/1.2.1)!\n\nWe fixed a lot of bugs due to dependencies, improved security, reviewed the notebooks and the libraries.\n\n## Introduction\n\nRecommenders objective is to assist researchers, developers and enthusiasts in prototyping, experimenting with and bringing to production a range of classic and state-of-the-art recommendation systems.\n\nRecommenders is a project under the [Linux Foundation of AI and Data](https://lfaidata.foundation/projects/). \n\nThis repository contains examples and best practices for building recommendation systems, provided as Jupyter notebooks. The examples detail our learnings on five key tasks:\n\n- [Prepare Data](examples/01_prepare_data): Preparing and loading data for each recommendation algorithm.\n- [Model](examples/00_quick_start): Building models using various classical and deep learning recommendation algorithms such as Alternating Least Squares ([ALS](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/recommendation.html#ALS)) or eXtreme Deep Factorization Machines ([xDeepFM](https://arxiv.org/abs/1803.05170)).\n- [Evaluate](examples/03_evaluate): Evaluating algorithms with offline metrics.\n- [Model Select and Optimize](examples/04_model_select_and_optimize): Tuning and optimizing hyperparameters for recommendation models.\n- [Operationalize](examples/05_operationalize): Operationalizing models in a production environment on Azure.\n\nSeveral utilities are provided in [recommenders](recommenders) to support common tasks such as loading datasets in the format expected by different algorithms, evaluating model outputs, and splitting training/test data. Implementations of several state-of-the-art algorithms are included for self-study and customization in your own applications. See the [Recommenders documentation](https://readthedocs.org/projects/microsoft-recommenders/).\n\nFor a more detailed overview of the repository, please see the documents on the [wiki page](https://github.com/microsoft/recommenders/wiki/Documents-and-Presentations).\n\nFor some of the practical scenarios where recommendation systems have been applied, see [scenarios](scenarios). \n\n## Getting Started\n\nWe recommend [conda](https://docs.conda.io/projects/conda/en/latest/glossary.html?highlight=environment#conda-environment) for environment management, and [VS Code](https://code.visualstudio.com/) for development. To install the recommenders package and run an example notebook on Linux/WSL:\n\n```bash\n# 1. Install gcc if it is not installed already. On Ubuntu, this could done by using the command\n# sudo apt install gcc\n\n# 2. Create and activate a new conda environment\nconda create -n <environment_name> python=3.9\nconda activate <environment_name>\n\n# 3. Install the core recommenders package. It can run all the CPU notebooks.\npip install recommenders\n\n# 4. create a Jupyter kernel\npython -m ipykernel install --user --name <environment_name> --display-name <kernel_name>\n\n# 5. Clone this repo within VSCode or using command line:\ngit clone https://github.com/recommenders-team/recommenders.git\n\n# 6. Within VSCode:\n#   a. Open a notebook, e.g., examples/00_quick_start/sar_movielens.ipynb;  \n#   b. Select Jupyter kernel <kernel_name>;\n#   c. Run the notebook.\n```\n\nFor more information about setup on other platforms (e.g., Windows and macOS) and different configurations (e.g., GPU, Spark and experimental features), see the [Setup Guide](SETUP.md).\n\nIn addition to the core package, several extras are also provided, including:\n+ `[gpu]`: Needed for running GPU models.\n+ `[spark]`: Needed for running Spark models.\n+ `[dev]`: Needed for development for the repo.\n+ `[all]`: `[gpu]`|`[spark]`|`[dev]`\n+ `[experimental]`: Models that are not thoroughly tested and/or may require additional steps in installation.\n\n## Algorithms\n\nThe table below lists the recommendation algorithms currently available in the repository. Notebooks are linked under the Example column as Quick start, showcasing an easy to run example of the algorithm, or as Deep dive, explaining in detail the math and implementation of the algorithm.\n\n| Algorithm | Type | Description | Example |\n|-----------|------|-------------|---------|\n| Alternating Least Squares (ALS) | Collaborative Filtering | Matrix factorization algorithm for explicit or implicit feedback in large datasets, optimized for scalability and distributed computing capability. It works in the PySpark environment. | [Quick start](examples/00_quick_start/als_movielens.ipynb) / [Deep dive](examples/02_model_collaborative_filtering/als_deep_dive.ipynb) |\n| Attentive Asynchronous Singular Value Decomposition (A2SVD)<sup>*</sup> | Collaborative Filtering | Sequential-based algorithm that aims to capture both long and short-term user preferences using attention mechanism. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Cornac/Bayesian Personalized Ranking (BPR) | Collaborative Filtering | Matrix factorization algorithm for predicting item ranking with implicit feedback. It works in the CPU environment. | [Deep dive](examples/02_model_collaborative_filtering/cornac_bpr_deep_dive.ipynb) |\n| Cornac/Bilateral Variational Autoencoder (BiVAE) | Collaborative Filtering | Generative model for dyadic data (e.g., user-item interactions). It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/cornac_bivae_deep_dive.ipynb) |\n| Convolutional Sequence Embedding Recommendation (Caser) | Collaborative Filtering | Algorithm based on convolutions that aim to capture both user’s general preferences and sequential patterns. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Deep Knowledge-Aware Network (DKN)<sup>*</sup> | Content-Based Filtering | Deep learning algorithm incorporating a knowledge graph and article embeddings for providing news or article recommendations. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/dkn_MIND.ipynb) / [Deep dive](examples/02_model_content_based_filtering/dkn_deep_dive.ipynb) |\n| Extreme Deep Factorization Machine (xDeepFM)<sup>*</sup> | Collaborative Filtering | Deep learning based algorithm for implicit and explicit feedback with user/item features. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/xdeepfm_criteo.ipynb) |\n| Embedding Dot Bias | Collaborative Filtering | General purpose algorithm with embeddings and biases for users and items. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/embdotbias_movielens.ipynb) |\n| LightFM/Factorization Machine | Collaborative Filtering | Factorization Machine algorithm for both implicit and explicit feedbacks. It works in the CPU environment. | [Quick start](examples/02_model_collaborative_filtering/lightfm_deep_dive.ipynb) |\n| LightGBM/Gradient Boosting Tree<sup>*</sup> | Content-Based Filtering | Gradient Boosting Tree algorithm for fast training and low memory usage in content-based problems. It works in the CPU/GPU/PySpark environments. | [Quick start in CPU](examples/00_quick_start/lightgbm_tinycriteo.ipynb) / [Deep dive in PySpark](examples/02_model_content_based_filtering/mmlspark_lightgbm_criteo.ipynb) |\n| LightGCN | Collaborative Filtering | Deep learning algorithm which simplifies the design of GCN for predicting implicit feedback. It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/lightgcn_deep_dive.ipynb) |\n| GeoIMC<sup>*</sup> | Collaborative Filtering | Matrix completion algorithm that takes into account user and item features using Riemannian conjugate gradient optimization and follows a geometric approach. It works in the CPU environment. | [Quick start](examples/00_quick_start/geoimc_movielens.ipynb) |\n| GRU | Collaborative Filtering | Sequential-based algorithm that aims to capture both long and short-term user preferences using recurrent neural networks. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Multinomial VAE | Collaborative Filtering | Generative model for predicting user/item interactions. It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/multi_vae_deep_dive.ipynb) |\n| Neural Recommendation with Long- and Short-term User Representations (LSTUR)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with long- and short-term user interest modeling. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/lstur_MIND.ipynb) |\n| Neural Recommendation with Attentive Multi-View Learning (NAML)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with attentive multi-view learning. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/naml_MIND.ipynb) |\n| Neural Collaborative Filtering (NCF) | Collaborative Filtering | Deep learning algorithm with enhanced performance for user/item implicit feedback. It works in the CPU/GPU environment.| [Quick start](examples/00_quick_start/ncf_movielens.ipynb) / [Deep dive](examples/02_model_collaborative_filtering/ncf_deep_dive.ipynb) |\n| Neural Recommendation with Personalized Attention (NPA)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with personalized attention network. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/npa_MIND.ipynb) |\n| Neural Recommendation with Multi-Head Self-Attention (NRMS)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with multi-head self-attention. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/nrms_MIND.ipynb) |\n| Next Item Recommendation (NextItNet) | Collaborative Filtering | Algorithm based on dilated convolutions and residual network that aims to capture sequential patterns. It considers both user/item interactions and features.  It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Restricted Boltzmann Machines (RBM) | Collaborative Filtering | Neural network based algorithm for learning the underlying probability distribution for explicit or implicit user/item feedback. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/rbm_movielens.ipynb) / [Deep dive](examples/02_model_collaborative_filtering/rbm_deep_dive.ipynb) |\n| Riemannian Low-rank Matrix Completion (RLRMC)<sup>*</sup> | Collaborative Filtering | Matrix factorization algorithm using Riemannian conjugate gradients optimization with small memory consumption to predict user/item interactions. It works in the CPU environment. | [Quick start](examples/00_quick_start/rlrmc_movielens.ipynb) |\n| Simple Algorithm for Recommendation (SAR)<sup>*</sup> | Collaborative Filtering | Similarity-based algorithm for implicit user/item feedback.  It works in the CPU environment. | [Quick start](examples/00_quick_start/sar_movielens.ipynb) / [Deep dive](examples/02_model_collaborative_filtering/sar_deep_dive.ipynb) |\n| Self-Attentive Sequential Recommendation (SASRec) | Collaborative Filtering | Transformer based algorithm for sequential recommendation. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sasrec_amazon.ipynb) |\n| Short-term and Long-term Preference Integrated Recommender (SLi-Rec)<sup>*</sup> | Collaborative Filtering | Sequential-based algorithm that aims to capture both long and short-term user preferences using attention mechanism, a time-aware controller and a content-aware controller. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Multi-Interest-Aware Sequential User Modeling (SUM)<sup>*</sup> | Collaborative Filtering | An enhanced memory network-based sequential user model which aims to capture users'' multiple interests. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Sequential Recommendation Via Personalized Transformer (SSEPT) | Collaborative Filtering | Transformer based algorithm for sequential recommendation with User embedding. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sasrec_amazon.ipynb) |\n| Standard VAE | Collaborative Filtering | Generative Model for predicting user/item interactions.  It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/standard_vae_deep_dive.ipynb) |\n| Surprise/Singular Value Decomposition (SVD) | Collaborative Filtering | Matrix factorization algorithm for predicting explicit rating feedback in small datasets. It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/surprise_svd_deep_dive.ipynb) |\n| Term Frequency - Inverse Document Frequency (TF-IDF) | Content-Based Filtering | Simple similarity-based algorithm for content-based recommendations with text datasets. It works in the CPU environment. | [Quick  start](examples/00_quick_start/tfidf_covid.ipynb) |\n| Vowpal Wabbit (VW)<sup>*</sup> | Content-Based Filtering | Fast online learning algorithms, great for scenarios where user features / context are constantly changing. It uses the CPU for online learning. | [Deep dive](examples/02_model_content_based_filtering/vowpal_wabbit_deep_dive.ipynb) |\n| Wide and Deep | Collaborative Filtering | Deep learning algorithm that can memorize feature interactions and generalize user features. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/wide_deep_movielens.ipynb) |\n| xLearn/Factorization Machine (FM) & Field-Aware FM (FFM) | Collaborative Filtering | Quick and memory efficient algorithm to predict labels with user/item features. It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/fm_deep_dive.ipynb) |\n\n**NOTE**: <sup>*</sup> indicates algorithms invented/contributed by Microsoft.\n\nIndependent or incubating algorithms and utilities are candidates for the [contrib](contrib) folder. This will house contributions which may not easily fit into the core repository or need time to refactor or mature the code and add necessary tests.\n\n| Algorithm | Type | Description | Example |\n|-----------|------|-------------|---------|\n| SARplus <sup>*</sup> | Collaborative Filtering | Optimized implementation of SAR for Spark |  [Quick start](contrib/sarplus/README.md) |\n\n### Algorithm Comparison\n\nWe provide a [benchmark notebook](examples/06_benchmarks/movielens.ipynb) to illustrate how different algorithms could be evaluated and compared. In this notebook, the MovieLens dataset is split into training/test sets at a 75/25 ratio using a stratified split. A recommendation model is trained using each of the collaborative filtering algorithms below. We utilize empirical parameter values reported in literature [here](http://mymedialite.net/examples/datasets.html). For ranking metrics we use `k=10` (top 10 recommended items). We run the comparison on a machine with 4 CPUs, 30Gb of RAM, and 1 GPU GeForce GTX 1660 Ti with 6Gb of memory. Spark ALS is run in local standalone mode. In this table we show the results on Movielens 100k, running the algorithms for 15 epochs.\n\n| Algo | MAP | nDCG@k | Precision@k | Recall@k | RMSE | MAE | R<sup>2</sup> | Explained Variance |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| [ALS](examples/00_quick_start/als_movielens.ipynb) | 0.004732 |	0.044239 |	0.048462 |	0.017796 | 0.965038 |	0.753001 |	0.255647 |	0.251648 |\n| [BiVAE](examples/02_model_collaborative_filtering/cornac_bivae_deep_dive.ipynb) | 0.146126	| 0.475077 |	0.411771 |	0.219145 | N/A |	N/A |	N/A |	N/A |\n| [BPR](examples/02_model_collaborative_filtering/cornac_bpr_deep_dive.ipynb) | 0.132478	| 0.441997 |	0.388229 |	0.212522 | N/A |	N/A |	N/A |	N/A |\n| [embdotbias](examples/00_quick_start/embdotbias_movielens.ipynb) | 0.018954 |	0.117810 |	0.104242 |	0.042450 | 0.992760 | 0.776040 | 0.223344 |	0.223393 |\n| [LightGCN](examples/02_model_collaborative_filtering/lightgcn_deep_dive.ipynb) | 0.088526 | 0.419846 | 0.379626 | 0.144336 | N/A | N/A | N/A | N/A |\n| [NCF](examples/02_model_collaborative_filtering/ncf_deep_dive.ipynb) | 0.107720	| 0.396118 |	0.347296 |	0.180775 | N/A | N/A | N/A | N/A |\n| [SAR](examples/00_quick_start/sar_movielens.ipynb) | 0.110591 |	0.382461 | 	0.330753 | 0.176385 | 1.253805 | 1.048484 |	-0.569363 |	0.030474 |\n| [SVD](examples/02_model_collaborative_filtering/surprise_svd_deep_dive.ipynb) | 0.012873	| 0.095930 |	0.091198 |	0.032783 | 0.938681 | 0.742690 | 0.291967 | 0.291971 |\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Before contributing, please see our [contribution guidelines](CONTRIBUTING.md).\n\nThis project adheres to this [Code of Conduct](CODE_OF_CONDUCT.md) in order to foster a welcoming and inspiring community for all.\n\n<!--\nStopped AzureML MLOps. See #2251\n## Build Status\n\nThese tests are the nightly builds, which compute the asynchronous tests. `main` is our principal branch and `staging` is our development branch. We use [pytest](https://docs.pytest.org/) for testing python utilities in [recommenders](recommenders) and the Recommenders [notebook executor](recommenders/utils/notebook_utils.py) for the [notebooks](examples). \n\nFor more information about the testing pipelines, please see the [test documentation](tests/README.md).\n\n### AzureML Nightly Build Status\n\nThe nightly build tests are run daily on AzureML.\n\n| Build Type | Branch | Status |  | Branch | Status |\n| --- | --- | --- | --- | --- | --- |\n| **Linux CPU** | main | [![azureml-cpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml/badge.svg?branch=main)](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml?query=branch%3Amain) | | staging | [![azureml-cpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml/badge.svg?branch=staging)](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml?query=branch%3Astaging) |\n| **Linux GPU** | main | [![azureml-gpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml/badge.svg?branch=main)](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml?query=branch%3Amain) | | staging | [![azureml-gpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml/badge.svg?branch=staging)](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml?query=branch%3Astaging) |\n| **Linux Spark** | main | [![azureml-spark-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml/badge.svg?branch=main)](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml?query=branch%3Amain) | | staging | [![azureml-spark-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml/badge.svg?branch=staging)](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml?query=branch%3Astaging) |\n-->\n\n## References\n\n- **FREE COURSE**: M. González-Fierro, "Recommendation Systems: A Practical Introduction", LinkedIn Learning, 2024. [Available on this link](https://www.linkedin.com/learning/recommendation-systems-a-practical-introduction).\n- D. Li, J. Lian, L. Zhang, K. Ren, D. Lu, T. Wu, X. Xie, "Recommender Systems: Frontiers and Practices", Springer, Beijing, 2024. [Available on this link](https://www.amazon.com/Recommender-Systems-Frontiers-Practices-Dongsheng/dp/9819989639/).\n- A. Argyriou, M. González-Fierro, and L. Zhang, "Microsoft Recommenders: Best Practices for Production-Ready Recommendation Systems", *WWW 2020: International World Wide Web Conference Taipei*, 2020. Available online: https://dl.acm.org/doi/abs/10.1145/3366424.3382692\n- S. Graham,  J.K. Min, T. Wu, "Microsoft recommenders: tools to accelerate developing recommender systems", *RecSys ''19: Proceedings of the 13th ACM Conference on Recommender Systems*, 2019. Available online: https://dl.acm.org/doi/10.1145/3298689.3346967\n- L. Zhang, T. Wu, X. Xie, A. Argyriou, M. González-Fierro and J. Lian, "Building Production-Ready Recommendation System at Scale", *ACM SIGKDD Conference on Knowledge Discovery and Data Mining 2019 (KDD 2019)*, 2019.\n', '{"language":"Python","stars":21208,"forks":3271,"watchers":21208,"open_issues":171,"topics":["ai","artificial-intelligence","data-science","deep-learning","jupyter-notebook","kubernetes","machine-learning","operationalization","python","ranking","rating","recommendation","recommendation-algorithm","recommendation-engine","recommendation-system","recommender","tutorial"],"default_branch":"main","size_kb":230975,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:recommenders-team:recommenders","source_url":"https://github.com/recommenders-team/recommenders"},{"type":"has_code","target_id":"github:recommenders-team:recommenders","source_url":"https://github.com/recommenders-team/recommenders"},{"type":"has_code","target_id":"github:recommenders-team:recommenders","source_url":"https://github.com/recommenders-team/recommenders"},{"type":"has_code","target_id":"github:psf:black","source_url":"https://github.com/psf/black"},{"type":"has_code","target_id":"github:recommenders-team:recommenders","source_url":"https://github.com/recommenders-team/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:recommenders-team:recommenders.git","source_url":"https://github.com/recommenders-team/recommenders.git"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"}]', NULL, 'MIT', 'approved', 80, '0b9886025a6f9299164396ed5bb14f80', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-recommenders-team-recommenders from https://github.com/recommenders-team.png
Image converted to WebP: data/images/github-recommenders-team-recommenders.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-huggingface-datasets', 'github--huggingface--datasets', 'datasets', 'huggingface', '<p align="center"> <picture> <source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-dark.svg"> <source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-light.svg"> <img alt="Hugging Face Datasets Library" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-light.svg" width="352" heigh...', '["ai","artificial-intelligence","computer-vision","dataset-hub","datasets","deep-learning","huggingface","llm","machine-learning","natural-language-processing","nlp","numpy","pandas","pytorch","speech","tensorflow","python"]', 'other', 20959, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/huggingface/datasets","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<p align="center">\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-dark.svg">\n    <source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-light.svg">\n    <img alt="Hugging Face Datasets Library" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-light.svg" width="352" height="59" style="max-width: 100%;">\n  </picture>\n  <br/>\n  <br/>\n</p>\n\n<p align="center">\n    <a href="https://github.com/huggingface/datasets/actions/workflows/ci.yml?query=branch%3Amain"><img alt="Build" src="https://github.com/huggingface/datasets/actions/workflows/ci.yml/badge.svg?branch=main"></a>\n    <a href="https://github.com/huggingface/datasets/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/datasets.svg?color=blue"></a>\n    <a href="https://huggingface.co/docs/datasets/index.html"><img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/datasets/index.html.svg?down_color=red&down_message=offline&up_message=online"></a>\n    <a href="https://github.com/huggingface/datasets/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/datasets.svg"></a>\n    <a href="https://huggingface.co/datasets/"><img alt="Number of datasets" src="https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/datasets&color=brightgreen"></a>\n    <a href="CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg"></a>\n    <a href="https://zenodo.org/badge/latestdoi/250213286"><img src="https://zenodo.org/badge/250213286.svg" alt="DOI"></a>\n</p>\n\n🤗 Datasets is a lightweight library providing **two** main features:\n\n- **one-line dataloaders for many public datasets**: one-liners to download and pre-process any of the ![number of datasets](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/datasets&color=brightgreen) major public datasets (image datasets, audio datasets, text datasets in 467 languages and dialects, etc.) provided on the [HuggingFace Datasets Hub](https://huggingface.co/datasets). With a simple command like `squad_dataset = load_dataset("rajpurkar/squad")`, get any of these datasets ready to use in a dataloader for training/evaluating a ML model (Numpy/Pandas/PyTorch/TensorFlow/JAX),\n- **efficient data pre-processing**: simple, fast and reproducible data pre-processing for the public datasets as well as your own local datasets in CSV, JSON, text, PNG, JPEG, WAV, MP3, Parquet, HDF5, etc. With simple commands like `processed_dataset = dataset.map(process_example)`, efficiently prepare the dataset for inspection and ML model evaluation and training.\n\n[🎓 **Documentation**](https://huggingface.co/docs/datasets/) [🔎 **Find a dataset in the Hub**](https://huggingface.co/datasets) [🌟 **Share a dataset on the Hub**](https://huggingface.co/docs/datasets/share)\n\n<h3 align="center">\n    <a href="https://hf.co/course"><img src="https://raw.githubusercontent.com/huggingface/datasets/main/docs/source/imgs/course_banner.png"></a>\n</h3>\n\n🤗 Datasets is designed to let the community easily add and share new datasets.\n\n🤗 Datasets has many additional interesting features:\n\n- Thrive on large datasets: 🤗 Datasets naturally frees the user from RAM memory limitation, all datasets are memory-mapped using an efficient zero-serialization cost backend (Apache Arrow).\n- Smart caching: never wait for your data to process several times.\n- Lightweight and fast with a transparent and pythonic API (multi-processing/caching/memory-mapping).\n- Built-in interoperability with NumPy, PyTorch, TensorFlow 2, JAX, Pandas, Polars and more.\n- Native support for audio, image and video data.\n- Enable streaming mode to save disk space and start iterating over the dataset immediately.\n\n🤗 Datasets originated from a fork of the awesome [TensorFlow Datasets](https://github.com/tensorflow/datasets) and the HuggingFace team want to deeply thank the TensorFlow Datasets team for building this amazing library.\n\n# Installation\n\n## With pip\n\n🤗 Datasets can be installed from PyPi and has to be installed in a virtual environment (venv or conda for instance)\n\n```bash\npip install datasets\n```\n\n## With conda\n\n🤗 Datasets can be installed using conda as follows:\n\n```bash\nconda install -c huggingface -c conda-forge datasets\n```\n\nFollow the installation pages of TensorFlow and PyTorch to see how to install them with conda.\n\nFor more details on installation, check the installation page in the documentation: https://huggingface.co/docs/datasets/installation\n\n## Installation to use with Machine Learning & Data frameworks frameworks\n\nIf you plan to use 🤗 Datasets with PyTorch (2.0+), TensorFlow (2.6+) or JAX (3.14+) you should also install PyTorch, TensorFlow or JAX.\n🤗 Datasets is also well integrated with data frameworks like PyArrow, Pandas, Polars and Spark, which should be installed separately.\n\nFor more details on using the library with these frameworks, check the quick start page in the documentation: https://huggingface.co/docs/datasets/quickstart\n\n# Usage\n\n🤗 Datasets is made to be very simple to use - the API is centered around a single function, `datasets.load_dataset(dataset_name, **kwargs)`, that instantiates a dataset.\n\nThis library can be used for text/image/audio/etc. datasets. Here is an example to load a text dataset:\n\nHere is a quick example:\n\n```python\nfrom datasets import load_dataset\n\n# Print all the available datasets\nfrom huggingface_hub import list_datasets\nprint([dataset.id for dataset in list_datasets()])\n\n# Load a dataset and print the first example in the training set\nsquad_dataset = load_dataset(''rajpurkar/squad'')\nprint(squad_dataset[''train''][0])\n\n# Process the dataset - add a column with the length of the context texts\ndataset_with_length = squad_dataset.map(lambda x: {"length": len(x["context"])})\n\n# Process the dataset - tokenize the context texts (using a tokenizer from the 🤗 Transformers library)\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(''bert-base-cased'')\n\ntokenized_dataset = squad_dataset.map(lambda x: tokenizer(x[''context'']), batched=True)\n```\n\nIf your dataset is bigger than your disk or if you don''t want to wait to download the data, you can use streaming:\n\n```python\n# If you want to use the dataset immediately and efficiently stream the data as you iterate over the dataset\nimage_dataset = load_dataset(''timm/imagenet-1k-wds'', streaming=True)\nfor example in image_dataset["train"]:\n    break\n```\n\nFor more details on using the library, check the quick start page in the documentation: https://huggingface.co/docs/datasets/quickstart and the specific pages on:\n\n- Loading a dataset: https://huggingface.co/docs/datasets/loading\n- What''s in a Dataset: https://huggingface.co/docs/datasets/access\n- Processing data with 🤗 Datasets: https://huggingface.co/docs/datasets/process\n    - Processing audio data: https://huggingface.co/docs/datasets/audio_process\n    - Processing image data: https://huggingface.co/docs/datasets/image_process\n    - Processing text data: https://huggingface.co/docs/datasets/nlp_process\n- Streaming a dataset: https://huggingface.co/docs/datasets/stream\n- etc.\n\n# Add a new dataset to the Hub\n\nWe have a very detailed step-by-step guide to add a new dataset to the ![number of datasets](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/datasets&color=brightgreen) datasets already provided on the [HuggingFace Datasets Hub](https://huggingface.co/datasets).\n\nYou can find:\n- [how to upload a dataset to the Hub using your web browser or Python](https://huggingface.co/docs/datasets/upload_dataset) and also\n- [how to upload it using Git](https://huggingface.co/docs/datasets/share).\n\n# Disclaimers\n\nYou can use 🤗 Datasets to load datasets based on versioned git repositories maintained by the dataset authors. For reproducibility reasons, we ask users to pin the `revision` of the repositories they use.\n\nIf you''re a dataset owner and wish to update any part of it (description, citation, license, etc.), or do not want your dataset to be included in the Hugging Face Hub, please get in touch by opening a discussion or a pull request in the Community tab of the dataset page. Thanks for your contribution to the ML community!\n\n## BibTeX\n\nIf you want to cite our 🤗 Datasets library, you can use our [paper](https://huggingface.co/papers/2109.02846):\n\n```bibtex\n@inproceedings{lhoest-etal-2021-datasets,\n    title = "Datasets: A Community Library for Natural Language Processing",\n    author = "Lhoest, Quentin  and\n      Villanova del Moral, Albert  and\n      Jernite, Yacine  and\n      Thakur, Abhishek  and\n      von Platen, Patrick  and\n      Patil, Suraj  and\n      Chaumond, Julien  and\n      Drame, Mariama  and\n      Plu, Julien  and\n      Tunstall, Lewis  and\n      Davison, Joe  and\n      {\v{S}}a{\v{s}}ko, Mario  and\n      Chhablani, Gunjan  and\n      Malik, Bhavitvya  and\n      Brandeis, Simon  and\n      Le Scao, Teven  and\n      Sanh, Victor  and\n      Xu, Canwen  and\n      Patry, Nicolas  and\n      McMillan-Major, Angelina  and\n      Schmid, Philipp  and\n      Gugger, Sylvain  and\n      Delangue, Cl{\''e}ment  and\n      Matussi{\`e}re, Th{\''e}o  and\n      Debut, Lysandre  and\n      Bekman, Stas  and\n      Cistac, Pierric  and\n      Goehringer, Thibault  and\n      Mustar, Victor  and\n      Lagunas, Fran{\c{c}}ois  and\n      Rush, Alexander  and\n      Wolf, Thomas",\n    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",\n    month = nov,\n    year = "2021",\n    address = "Online and Punta Cana, Dominican Republic",\n    publisher = "Association for Computational Linguistics",\n    url = "https://aclanthology.org/2021.emnlp-demo.21",\n    pages = "175--184",\n    abstract = "The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.",\n    eprint={2109.02846},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n}\n```\n\nIf you need to cite a specific version of our 🤗 Datasets library for reproducibility, you can use the corresponding version Zenodo DOI from this [list](https://zenodo.org/search?q=conceptrecid:%224817768%22&sort=-version&all_versions=True).\n', '{"language":"Python","stars":20959,"forks":3025,"watchers":20959,"open_issues":1009,"topics":["ai","artificial-intelligence","computer-vision","dataset-hub","datasets","deep-learning","huggingface","llm","machine-learning","natural-language-processing","nlp","numpy","pandas","pytorch","speech","tensorflow"],"default_branch":"main","size_kb":89723,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:huggingface:datasets","source_url":"https://github.com/huggingface/datasets"},{"type":"has_code","target_id":"github:huggingface:datasets","source_url":"https://github.com/huggingface/datasets"},{"type":"has_code","target_id":"github:huggingface:datasets","source_url":"https://github.com/huggingface/datasets"},{"type":"has_code","target_id":"github:huggingface:datasets","source_url":"https://github.com/huggingface/datasets"},{"type":"has_code","target_id":"github:tensorflow:datasets","source_url":"https://github.com/tensorflow/datasets"},{"type":"has_code","target_id":"github:huggingface:datasets.\",","source_url":"https://github.com/huggingface/datasets.\","}]', NULL, 'Apache-2.0', 'approved', 80, 'e2d8b3cf639b7dd93d544dffe356c5f5', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-huggingface-datasets from https://github.com/huggingface.png
Image converted to WebP: data/images/github-huggingface-datasets.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-RasaHQ-rasa', 'github--rasahq--rasa', 'rasa', 'RasaHQ', '<h1 align="center">Rasa Open Source</h1> <div align="center"> !Documentation Build </div> <hr /> 💡 **We''re migrating issues to Jira** 💡 Starting January 2023, issues for Rasa Open Source are located in this Jira board. You can browse issues without being logged in; if you want to create issues, you''ll need to create a Jira account. <hr /> <img align="right" height="255" src="https://www.rasa.com/assets/img/sara/sara-open-source-2.0.png" alt="An image of Sara, the Rasa mascot bird, holding a...', '["bot","bot-framework","botkit","bots","chatbot","chatbots","chatbots-framework","conversation-driven-development","conversational-agents","conversational-ai","conversational-bots","machine-learning","machine-learning-library","mitie","natural-language-processing","nlp","nlu","rasa","spacy","wit","python"]', 'other', 20912, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/RasaHQ/rasa","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<h1 align="center">Rasa Open Source</h1>\n\n<div align="center">\n\n[![Join the chat on Rasa Community Forum](https://img.shields.io/badge/forum-join%20discussions-brightgreen.svg)](https://forum.rasa.com/?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![PyPI version](https://badge.fury.io/py/rasa.svg)](https://badge.fury.io/py/rasa)\n[![Supported Python Versions](https://img.shields.io/pypi/pyversions/rasa.svg)](https://pypi.python.org/pypi/rasa)\n[![Build Status](https://github.com/RasaHQ/rasa/workflows/Continuous%20Integration/badge.svg)](https://github.com/RasaHQ/rasa/actions)\n[![Coverage Status](https://api.codeclimate.com/v1/badges/756dc6fea1d5d3e127f7/test_coverage)](https://codeclimate.com/github/RasaHQ/rasa/)\n[![Documentation Status](https://img.shields.io/badge/docs-stable-brightgreen.svg)](https://rasa.com/docs)\n![Documentation Build](https://img.shields.io/netlify/d2e447e4-5a5e-4dc7-be5d-7c04ae7ff706?label=Documentation%20Build)\n[![FOSSA Status](https://app.fossa.com/api/projects/custom%2B8141%2Fgit%40github.com%3ARasaHQ%2Frasa.git.svg?type=shield)](https://app.fossa.com/projects/custom%2B8141%2Fgit%40github.com%3ARasaHQ%2Frasa.git?ref=badge_shield)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](https://github.com/orgs/RasaHQ/projects/23)\n\n</div>\n\n<hr />\n\n💡 **We''re migrating issues to Jira** 💡\n\nStarting January 2023, issues for Rasa Open Source are located in\n[this Jira board](https://rasa-open-source.atlassian.net/browse/OSS). You can browse issues without being logged in;\nif you want to create issues, you''ll need to create a Jira account.\n\n<hr />\n\n<img align="right" height="255" src="https://www.rasa.com/assets/img/sara/sara-open-source-2.0.png" alt="An image of Sara, the Rasa mascot bird, holding a flag that reads Open Source with one wing, and a wrench in the other" title="Rasa Open Source">\n\nRasa is an open source machine learning framework to automate text and voice-based conversations. With Rasa, you can build contextual assistants on:\n- Facebook Messenger\n- Slack\n- Google Hangouts\n- Webex Teams\n- Microsoft Bot Framework\n- Rocket.Chat\n- Mattermost\n- Telegram\n- Twilio\n- Your own custom conversational channels\n\nor voice assistants as:\n- Alexa Skills\n- Google Home Actions\n\nRasa helps you build contextual assistants capable of having layered conversations with\nlots of back-and-forth. In order for a human to have a meaningful exchange with a contextual\nassistant, the assistant needs to be able to use context to build on things that were previously\ndiscussed – Rasa enables you to build assistants that can do this in a scalable way.\n\nThere''s a lot more background information in this\n[blog post](https://medium.com/rasa-blog/a-new-approach-to-conversational-software-2e64a5d05f2a).\n\n---\n- 🤔 [Learn more about Rasa](https://rasa.community/)\n\n- 🤓 [Read The Docs](https://rasa.com/docs/rasa/)\n\n- 😁 [Install Rasa](https://rasa.com/docs/rasa/installation/environment-set-up)\n\n- 🚀 [Dive deeper in the learning center](https://learning.rasa.com/)\n\n- 🤗 [Contribute](#how-to-contribute)\n\n- ❓ [Get enterprise-grade support](https://rasa.com/support/)\n\n- 🏢 [Explore the features of our commercial platform](https://rasa.com/product/rasa-platform/)\n\n- 📚 [Learn more about research papers that leverage Rasa](https://scholar.google.com/scholar?oi=bibs&hl=en&authuser=1&cites=16243802403383697687,353275993797024115,14567308604105196228,9067977709825839723,9855847065463746011&as_sdt=5)\n\n\n\n---\n## Where to get help\n\nThere is extensive documentation in the [Rasa Docs](https://rasa.com/docs/rasa).\nMake sure to select the correct version so you are looking at\nthe docs for the version you installed.\n\nPlease use [Rasa Community Forum](https://forum.rasa.com) for quick answers to\nquestions.\n\n### README Contents:\n- [How to contribute](#how-to-contribute)\n- [Development Internals](#development-internals)\n- [Releases](#releases)\n- [License](#license)\n\n### How to contribute\nWe are very happy to receive and merge your contributions into this repository!\n\nTo contribute via pull request, follow these steps:\n\n1. Create an issue describing the feature you want to work on (or\n   have a look at the [contributor board](https://github.com/orgs/RasaHQ/projects/23))\n2. Write your code, tests and documentation, and format them with ``black``\n3. Create a pull request describing your changes\n\nFor more detailed instructions on how to contribute code, check out these [code contributor guidelines](CONTRIBUTING.md).\n\nYou can find more information about how to contribute to Rasa (in lots of\ndifferent ways!) [on our website.](http://rasa.community).\n\nYour pull request will be reviewed by a maintainer, who will get\nback to you about any necessary changes or questions. You will\nalso be asked to sign a\n[Contributor License Agreement](https://cla-assistant.io/RasaHQ/rasa).\n\n\n## Development Internals\n\n### Installing Poetry\n\nRasa uses Poetry for packaging and dependency management. If you want to build it from source,\nyou have to install Poetry first. Please follow\n[the official guide](https://python-poetry.org/docs/#installation) to see all possible options.\n\nTo update an existing poetry version to the [version](.github/poetry_version.txt), currently used in rasa, run:\n```shell\n    poetry self update <version>\n```\n\n### Managing environments\n\nThe official [Poetry guide](https://python-poetry.org/docs/managing-environments/) suggests to use\n[pyenv](https://github.com/pyenv/pyenv) or any other similar tool to easily switch between Python versions.\nThis is how it can be done:\n\n```bash\npyenv install 3.10.10\npyenv local 3.10.10  # Activate Python 3.10.10 for the current project\n```\n*Note*: If you have trouble installing a specific version of python on your system\nit might be worth trying other supported versions.\n\nBy default, Poetry will try to use the currently activated Python version to create the virtual environment\nfor the current project automatically. You can also create and activate a virtual environment manually — in this\ncase, Poetry should pick it up and use it to install the dependencies. For example:\n\n```bash\npython -m venv .venv\nsource .venv/bin/activate\n```\n\nYou can make sure that the environment is picked up by executing\n\n```bash\npoetry env info\n```\n\n### Building from source\n\nTo install dependencies and `rasa` itself in editable mode execute\n\n```bash\nmake install\n```\n\n*Note for macOS users*: under macOS Big Sur we''ve seen some compiler issues for\ndependencies. Using `export SYSTEM_VERSION_COMPAT=1` before the installation helped.\n\n\n#### Installing optional dependencies\n\nIn order to install rasa''s optional dependencies, you need to run:\n\n```bash\nmake install-full\n```\n\n*Note for macOS users*: The command `make install-full` could result in a failure while installing `tokenizers`\n(issue described in depth [here](https://github.com/huggingface/tokenizers/issues/1050)).\n\nIn order to resolve it, you must follow these steps to install a Rust compiler:\n```bash\nbrew install rustup\nrustup-init\n```\n\nAfter initialising the Rust compiler, you should restart the console and check its installation:\n```bash\nrustc --version\n```\n\nIn case the PATH variable had not been automatically setup, run:\n```bash\nexport PATH="$HOME/.cargo/bin:$PATH"\n```\n\n\n### Running and changing the documentation\n\nFirst of all, install all the required dependencies:\n\n```bash\nmake install install-docs\n```\n\nAfter the installation has finished, you can run and view the documentation\nlocally using:\n\n```bash\nmake livedocs\n```\n\nIt should open a new tab with the local version of the docs in your browser;\nif not, visit http://localhost:3000 in your browser.\nYou can now change the docs locally and the web page will automatically reload\nand apply your changes.\n\n### Running the Tests\n\nIn order to run the tests, make sure that you have the development requirements installed:\n\n```bash\nmake prepare-tests-ubuntu # Only on Ubuntu and Debian based systems\nmake prepare-tests-macos  # Only on macOS\n```\n\nThen, run the tests:\n\n```bash\nmake test\n```\n\nThey can also be run at multiple jobs to save some time:\n\n```bash\nJOBS=[n] make test\n```\n\nWhere `[n]` is the number of jobs desired. If omitted, `[n]` will be automatically chosen by pytest.\n\n\n### Running the Integration Tests\n\nIn order to run the integration tests, make sure that you have the development requirements installed:\n\n```bash\nmake prepare-tests-ubuntu # Only on Ubuntu and Debian based systems\nmake prepare-tests-macos  # Only on macOS\n```\n\nThen, you''ll need to start services with the following command which uses\n[Docker Compose](https://docs.docker.com/compose/install/):\n\n```bash\nmake run-integration-containers\n```\n\nFinally, you can run the integration tests like this:\n\n```bash\nmake test-integration\n```\n\n\n### Resolving merge conflicts\n\nPoetry doesn''t include any solution that can help to resolve merge conflicts in\nthe lock file `poetry.lock` by default.\nHowever, there is a great tool called [poetry-merge-lock](https://poetry-merge-lock.readthedocs.io/en/latest/).\nHere is how you can install it:\n\n```bash\npip install poetry-merge-lock\n```\n\nJust execute this command to resolve merge conflicts in `poetry.lock` automatically:\n\n```bash\npoetry-merge-lock\n```\n\n### Build a Docker image locally\n\nIn order to build a Docker image on your local machine execute the following command:\n\n```bash\nmake build-docker\n```\n\nThe Docker image is available on your local machine as `rasa:localdev`.\n\n### Code Style\n\nTo ensure a standardized code style we use the formatter [black](https://github.com/ambv/black).\nTo ensure our type annotations are correct we use the type checker [pytype](https://github.com/google/pytype).\nIf your code is not formatted properly or doesn''t type check, GitHub will fail to build.\n\n#### Formatting\n\nIf you want to automatically format your code on every commit, you can use [pre-commit](https://pre-commit.com/).\nJust install it via `pip install pre-commit` and execute `pre-commit install` in the root folder.\nThis will add a hook to the repository, which reformats files on every commit.\n\nIf you want to set it up manually, install black via `poetry install`.\nTo reformat files execute\n```\nmake formatter\n```\n\n#### Type Checking\n\nIf you want to check types on the codebase, install `mypy` using `poetry install`.\nTo check the types execute\n```\nmake types\n```\n\n### Deploying documentation updates\n\nWe use `Docusaurus v2` to build docs for tagged versions and for the `main` branch.\nTo run Docusaurus, install `Node.js 12.x`.\nThe static site that gets built is pushed to the `documentation` branch of this repo.\n\nWe host the site on netlify. On `main` branch builds (see `.github/workflows/documentation.yml`), we push the built docs to\nthe `documentation` branch. Netlify automatically re-deploys the docs pages whenever there is a change to that branch.\n\n## Releases\nRasa has implemented robust policies governing version naming, as well as release pace for major, minor, and patch releases.\n\nThe values for a given version number (MAJOR.MINOR.PATCH) are incremented as follows:\n- MAJOR version for incompatible API changes or other breaking changes.\n- MINOR version for functionality added in a backward compatible manner.\n- PATCH version for backward compatible bug fixes.\n\nThe following table describes the version types and their expected *release cadence*:\n\n| Version Type |                                                                  Description                                                                  |  Target Cadence |\n|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------|-----------------|\n| Major        | For significant changes, or when any backward-incompatible changes are introduced to the API or data model.                                   | Every 1 - 2 yrs |\n| Minor        | For when new backward-compatible functionality is introduced, a minor feature is introduced, or when a set of smaller features is rolled out. | +/- Quarterly   |\n| Patch        | For backward-compatible bug fixes that fix incorrect behavior.                                                                                | As needed       |\n\nWhile this table represents our target release frequency, we reserve the right to modify it based on changing market conditions and technical requirements.\n\n### Maintenance Policy\nOur End of Life policy defines how long a given release is considered supported, as well as how long a release is\nconsidered to be still in active development or maintenance.\n\nThe maintentance duration and end of life for every release are shown on our website as part of the [Product Release and Maintenance Policy](https://rasa.com/rasa-product-release-and-maintenance-policy/).\n\n### Cutting a Major / Minor release\n#### A week before release day\n\n1. **Make sure the [milestone](https://github.com/RasaHQ/rasa/milestones) already exists and is scheduled for the\ncorrect date.**\n2. **Take a look at the issues & PRs that are in the milestone**: does it look about right for the release highlights\nwe are planning to ship? Does it look like anything is missing? Don''t worry about being aware of every PR that should\nbe in, but it''s useful to take a moment to evaluate what''s assigned to the milestone.\n3. **Post a message on the engineering Slack channel**, letting the team know you''ll be the one cutting the upcoming\nrelease, as well as:\n    1. Providing the link to the appropriate milestone\n    2. Reminding everyone to go over their issues and PRs and please assign them to the milestone\n    3. Reminding everyone of the scheduled date for the release\n\n#### A day before release day\n\n1. **Go over the milestone and evaluate the status of any PR merging that''s happening. Follow up with people on their\nbugs and fixes.** If the release introduces new bugs or regressions that can''t be fixed in time, we should discuss on\nSlack about this and take a decision on how to move forward. If the issue is not ready to be merged in time, we remove the issue / PR from the milestone and notify the PR owner and the product manager on Slack about it. The PR / issue owners are responsible for\ncommunicating any issues which might be release relevant. Postponing the release should be considered as an edge case scenario.\n\n#### Release day! 🚀\n\n1. **At the start of the day, post a small message on slack announcing release day!** Communicate you''ll be handling\nthe release, and the time you''re aiming to start releasing (again, no later than 4pm, as issues may arise and\ncause delays). This message should be posted early in the morning and before moving forward with any of the steps of the release,\n   in order to give enough time to people to check their PRs and issues. That way they can plan any remaining work. A template of the slack message can be found [here](https://rasa-hq.slack.com/archives/C36SS4N8M/p1613032208137500?thread_ts=1612876410.068400&cid=C36SS4N8M).\n   The release time should be communicated transparently so that others can plan potentially necessary steps accordingly. If there are bigger changes this should be communicated.\n2. Make sure the milestone is empty (everything has been either merged or moved to the next milestone)\n3. Once everything in the milestone is taken care of, post a small message on Slack communicating you are about to\nstart the release process (in case anything is missing).\n4. **You may now do the release by following the instructions outlined in the\n[Rasa Open Source README](#steps-to-release-a-new-version) !**\n\n#### After a Major release\n\nAfter a Major release has been completed, please follow [these instructions to complete the documentation update](./docs/README.md#manual-steps-after-a-new-version).\n\n### Steps to release a new version\nReleasing a new version is quite simple, as the packages are build and distributed by GitHub Actions.\n\n*Release steps*:\n1. Make sure all dependencies are up to date (**especially Rasa SDK**)\n    - For Rasa SDK, except in the case of a patch release, that means first creating a [new Rasa SDK release](https://github.com/RasaHQ/rasa-sdk#steps-to-release-a-new-version) (make sure the version numbers between the new Rasa and Rasa SDK releases match)\n    - Once the tag with the new Rasa SDK release is pushed and the package appears on [pypi](https://pypi.org/project/rasa-sdk/), the dependency in the rasa repository can be resolved (see below).\n2. If this is a minor / major release: Make sure all fixes from currently supported minor versions have been merged from their respective release branches (e.g. 3.3.x) back into main.\n3. In case of a minor release, create a new branch that corresponds to the new release, e.g.\n   ```bash\n    git checkout -b 1.2.x\n    git push origin 1.2.x\n    ```\n4. Switch to the branch you want to cut the release from (`main` in case of a major, the `<major>.<minor>.x` branch for minors and patches)\n    - Update the `rasa-sdk` entry in `pyproject.toml` with the new release version and run `poetry update`. This creates a new `poetry.lock` file with all dependencies resolved.\n    - Commit the changes with `git commit -am "bump rasa-sdk dependency"` but do not push them. They will be automatically picked up by the following step.\n5. If this is a major release, update the list of actively maintained versions [in the README](#actively-maintained-versions) and in [the docs](./docs/docs/actively-maintained-versions.mdx).\n6. Run `make release`\n7. Create a PR against the release branch (e.g. `1.2.x`)\n8. Once your PR is merged, tag a new release (this SHOULD always happen on the release branch), e.g. using\n    ```bash\n    git checkout 1.2.x\n    git pull origin 1.2.x\n    git tag 1.2.0 -m "next release"\n    git push origin 1.2.0 --tags\n    ```\n    GitHub will build this tag and publish the build artifacts.\n9. After all the steps are completed and if everything goes well then we should see a message automatically posted in the company''s Slack (`product` channel) like this [one](https://rasa-hq.slack.com/archives/C7B08Q5FX/p1614354499046600)\n10. If no message appears in the channel then you can do the following checks:\n    - Check the workflows in [Github Actions](https://github.com/RasaHQ/rasa/actions) and make sure that the merged PR of the current release is completed successfully. To easily find your PR you can use the filters `event: push` and `branch: <version number>` (example on release 2.4 you can see [here](https://github.com/RasaHQ/rasa/actions/runs/643344876))\n    - If the workflow is not completed, then try to re run the workflow in case that solves the problem\n    - If the problem persists, check also the log files and try to find the root cause of the issue\n    - If you still cannot resolve the error, contact the infrastructure team by providing any helpful information from your investigation\n11.  After the message is posted correctly in the `product` channel, check also in the `product-engineering-alerts` channel if there are any alerts related to the Rasa Open Source release like this [one](https://rasa-hq.slack.com/archives/C01585AN2NP/p1615486087001000)\n\n### Cutting a Patch release\n\nPatch releases are simpler to cut, since they are meant to contain only bugfixes.\n\n**The only things you need to do to cut a patch release are:**\n\n1. Notify the engineering team on Slack that you are planning to cut a patch, in case someone has an important fix\nto add.\n2. Make sure the bugfix(es) are in the release branch you will use (p.e if you are cutting a `2.0.4` patch, you will\nneed your fixes to be on the `2.0.x` release branch). All patch releases must come from a `.x` branch!\n3. Once you''re ready to release the Rasa Open Source patch, checkout the branch, run `make release` and follow the\nsteps + get the PR merged.\n4. Once the PR is in, pull the `.x` branch again and push the tag!\n\n### Additional Release Tasks \n**Note: This is only required if the released version is the highest version available.\nFor instance, perform the following steps when version > [version](https://github.com/RasaHQ/rasa/blob/main/rasa/version.py) on main.**\n\nIn order to check compatibility between the new released Rasa version to the latest version of Rasa X/Enterprise, we perform the following steps:\n1. Following a new Rasa release, an automated pull request is created in [Rasa-X-Demo](https://github.com/RasaHQ/rasa-x-demo/pulls). \n2. Once the above PR is merged, follow instructions [here](https://github.com/RasaHQ/rasa-x-demo/blob/master/.github/VERSION_BUMPER_PR_COMMENT.md), to release a version.\n3. Update the new version in the Rasa X/Enterprise [env file](https://github.com/RasaHQ/rasa-x/blob/main/.env).\nThe [Rasa-X-Demo](https://github.com/RasaHQ/rasa-x-demo) project uses the new updated Rasa version to train and test a model which in turn is used by our CI to run tests in the Rasa X/Enterprise repository, \nthus validating compatibility between Rasa and Rasa X/Enterprise.\n\n### Actively maintained versions\n\nPlease refer to the [Rasa Product Release and Maintenance Policy](https://rasa.com/rasa-product-release-and-maintenance-policy/) page.\n\n## License\nLicensed under the Apache License, Version 2.0.\nCopyright 2022 Rasa Technologies GmbH. [Copy of the license](LICENSE.txt).\n\nA list of the Licenses of the dependencies of the project can be found at\nthe bottom of the\n[Libraries Summary](https://libraries.io/github/RasaHQ/rasa).\n', '{"language":"Python","stars":20912,"forks":4905,"watchers":20912,"open_issues":142,"topics":["bot","bot-framework","botkit","bots","chatbot","chatbots","chatbots-framework","conversation-driven-development","conversational-agents","conversational-ai","conversational-bots","machine-learning","machine-learning-library","mitie","natural-language-processing","nlp","nlu","rasa","spacy","wit"],"default_branch":"3.6.x","size_kb":1611206,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:RasaHQ:rasa","source_url":"https://github.com/RasaHQ/rasa"},{"type":"has_code","target_id":"github:RasaHQ:rasa","source_url":"https://github.com/RasaHQ/rasa"},{"type":"has_code","target_id":"github:orgs:RasaHQ","source_url":"https://github.com/orgs/RasaHQ"},{"type":"has_code","target_id":"github:orgs:RasaHQ","source_url":"https://github.com/orgs/RasaHQ"},{"type":"has_code","target_id":"github:pyenv:pyenv","source_url":"https://github.com/pyenv/pyenv"},{"type":"has_code","target_id":"github:huggingface:tokenizers","source_url":"https://github.com/huggingface/tokenizers"},{"type":"has_code","target_id":"github:ambv:black","source_url":"https://github.com/ambv/black"},{"type":"has_code","target_id":"github:google:pytype","source_url":"https://github.com/google/pytype"},{"type":"has_code","target_id":"github:RasaHQ:rasa","source_url":"https://github.com/RasaHQ/rasa"},{"type":"has_code","target_id":"github:RasaHQ:rasa-sdk","source_url":"https://github.com/RasaHQ/rasa-sdk#steps-to-release-a-new-version"},{"type":"has_code","target_id":"github:RasaHQ:rasa","source_url":"https://github.com/RasaHQ/rasa"},{"type":"has_code","target_id":"github:RasaHQ:rasa","source_url":"https://github.com/RasaHQ/rasa"},{"type":"has_code","target_id":"github:RasaHQ:rasa","source_url":"https://github.com/RasaHQ/rasa"},{"type":"has_code","target_id":"github:RasaHQ:rasa-x-demo","source_url":"https://github.com/RasaHQ/rasa-x-demo"},{"type":"has_code","target_id":"github:RasaHQ:rasa-x-demo","source_url":"https://github.com/RasaHQ/rasa-x-demo"},{"type":"has_code","target_id":"github:RasaHQ:rasa-x","source_url":"https://github.com/RasaHQ/rasa-x"},{"type":"has_code","target_id":"github:RasaHQ:rasa-x-demo","source_url":"https://github.com/RasaHQ/rasa-x-demo"}]', NULL, 'Apache-2.0', 'approved', 80, 'c0d9c8935d68694e56650513baa322b0', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-RasaHQ-rasa from https://github.com/RasaHQ.png
Image converted to WebP: data/images/github-RasaHQ-rasa.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-Zeyi-Lin-HivisionIDPhotos', 'github--zeyi-lin--hivisionidphotos', 'HivisionIDPhotos', 'Zeyi-Lin', '<div align="center"> <img alt="hivision_logo" src="assets/hivision_logo.png" width=120 height=120> <h1>HivisionIDPhoto</h1> English / 中文 / 日本語 / 한국어 [![][release-shield]][release-link] [![][dockerhub-shield]][dockerhub-link] [![][github-stars-shield]][github-stars-link] [![][github-issues-shield]][github-issues-link] [![][github-contributors-shield]][github-contributors-link] [![][github-forks-shield]][github-forks-link] [![][license-shield]][license-link] [![][wechat-shield]][wechat-link] [!...', '["cnn","demo","docker","face-recognition","fastapi","gradio","idphoto","machine-learning","matting","mtcnn","tools","unet","python"]', 'other', 20299, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n\n<img alt="hivision_logo" src="assets/hivision_logo.png" width=120 height=120>\n<h1>HivisionIDPhoto</h1>\n\n[English](README_EN.md) / 中文 / [日本語](README_JP.md) / [한국어](README_KO.md)\n\n[![][release-shield]][release-link]\n[![][dockerhub-shield]][dockerhub-link]\n[![][github-stars-shield]][github-stars-link]\n[![][github-issues-shield]][github-issues-link]\n[![][github-contributors-shield]][github-contributors-link]\n[![][github-forks-shield]][github-forks-link]\n[![][license-shield]][license-link]  \n[![][wechat-shield]][wechat-link]\n[![][spaces-shield]][spaces-link]\n[![][swanhub-demo-shield]][swanhub-demo-link]\n[![][modelscope-shield]][modelscope-link]\n[![][modelers-shield]][modelers-link]\n[![][compshare-shield]][compshare-link]\n\n[![][trendshift-shield]][trendshift-link]\n[![][hellogithub-shield]][hellogithub-link]\n\n<img src="assets/demoImage.jpg" width=900>\n\n</div>\n\n> **相关项目**：\n>\n> - [SwanLab](https://github.com/SwanHubX/SwanLab)：一个开源、现代化设计的深度学习训练跟踪与可视化工具，同时支持云端/离线使用，国内好用的Wandb平替；适配30+主流框架（PyTorch、HuggingFace Transformers、LLaMA Factory、Lightning等），欢迎使用！\n\n\n<br>\n\n# 目录\n\n- [最近更新](#-最近更新)\n- [项目简介](#-项目简介)\n- [社区](#-社区)\n- [准备工作](#-准备工作)\n- [Demo启动](#-运行-gradio-demo)\n- [Python推理](#-python-推理)\n- [API服务部署](#️-部署-api-服务)\n- [Docker部署](#-docker-部署)\n- [联系我们](#-联系我们)\n- [FAQ](#faq)\n- [感谢支持](#-感谢支持)\n- [License](#-lincese)\n- [引用](#-引用)\n\n<br>\n\n# 🤩 最近更新\n\n- 在线体验： [![Spaces](https://img.shields.io/badge/🤗-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/TheEeeeLin/HivisionIDPhotos)、[![][modelscope-shield]][modelscope-link]、[![][modelers-shield]][modelers-link]、[![][compshare-shield]][compshare-link]\n\n- 2024.11.20: Gradio Demo增加**打印排版**选项卡，支持六寸、五寸、A4、3R、4R五种排版尺寸\n- 2024.11.16: API接口增加美颜参数\n- 2024.09.25: 增加**五寸相纸**和**JPEG下载**选项｜默认照片下载支持300DPI\n- 2024.09.24: API接口增加base64图像传入选项 | Gradio Demo增加**排版照裁剪线**功能\n- 2024.09.22: Gradio Demo增加**野兽模式**，可设置内存加载策略 | API接口增加**dpi、face_alignment**参数\n- 2024.09.18: Gradio Demo增加**分享模版照**功能、增加**美式证件照**背景选项\n- 2024.09.17: Gradio Demo增加**自定义底色-HEX输入**功能 | **（社区贡献）C++版本** - [HivisionIDPhotos-cpp](https://github.com/zjkhahah/HivisionIDPhotos-cpp) 贡献 by [zjkhahah](https://github.com/zjkhahah)\n- 2024.09.16: Gradio Demo增加**人脸旋转对齐**功能，自定义尺寸输入支持**毫米**单位\n\n<br>\n\n# 项目简介\n\n> 🚀 谢谢你对我们的工作感兴趣。您可能还想查看我们在图像领域的其他成果，欢迎来信:zeyi.lin@swanhub.co.\n\nHivisionIDPhoto 旨在开发一种实用、系统性的证件照智能制作算法。\n\n它利用一套完善的AI模型工作流程，实现对多种用户拍照场景的识别、抠图与证件照生成。\n\n**HivisionIDPhoto 可以做到：**\n\n1. 轻量级抠图（纯离线，仅需 **CPU** 即可快速推理）\n2. 根据不同尺寸规格生成不同的标准证件照、六寸排版照\n3. 支持 纯离线 或 端云 推理\n4. 美颜\n5. 智能换正装（waiting）\n\n<div align="center">\n<img src="assets/demo.png" width=900>\n</div>\n\n---\n\n如果 HivisionIDPhoto 对你有帮助，请 star 这个 repo 或推荐给你的朋友，解决证件照应急制作问题！\n\n<br>\n\n# 🏠 社区\n\n我们分享了一些由社区构建的HivisionIDPhotos的有趣应用和扩展：\n\n| [HivisionIDPhotos-ComfyUI][community-hivision-comfyui] | [HivisionIDPhotos-wechat-weapp][community-hivision-wechat] |\n| :----------------------------------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------------------------: |\n| <a href="https://github.com/AIFSH/HivisionIDPhotos-ComfyUI"> <img src="assets/comfyui.png" width="900" alt="ComfyUI workflow"> </a>  | <a href="https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp"> <img src="assets/community-wechat-miniprogram.png" width="900" alt="ComfyUI workflow"> </a>  |\n|ComfyUI证件照处理工作流 | 证件照微信小程序（JAVA后端+原生前端） |\n\n| [HivisionIDPhotos-Uniapp][community-hivision-uniapp] | [HivisionIDPhotos-web](https://github.com/jkm199/HivisionIDPhotos-web)|\n| :------------------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------------------------: |\n| <a href="https://github.com/soulerror/HivisionIDPhotos-Uniapp"> <img src="assets/community-uniapp-wechat-miniprogram.png" width="900" alt="HivisionIDPhotos-uniapp"> </a>  | <a href="https://github.com/jkm199/HivisionIDPhotos-web"> <img src="assets/community-web.png" width="900" alt="HivisionIDPhotos-uniapp"> </a>  |\n| 证件照微信小程序（uniapp）| 证件照应用网页版 |\n\n\n- [HivisionIDPhotos-cpp](https://github.com/zjkhahah/HivisionIDPhotos-cpp): HivisionIDphotos C++版本，由 [zjkhahah](https://github.com/zjkhahah) 构建\n- [ai-idphoto](https://github.com/wmlcjj/ai-idphoto): [HivisionIDPhotos-wechat-weapp](https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp) 的uniapp多端兼容版，由 [wmlcjj](https://github.com/wmlcjj) 贡献\n- [HivisionIDPhotos-uniapp-WeChat-gpto1](https://github.com/jkm199/HivisionIDPhotos-uniapp-WeChat-gpto1/): 由gpt-o1辅助完成开发的证件照微信小程序，由 [jkm199](https://github.com/jkm199) 贡献\n- [HivisionIDPhotos-windows-GUI](https://github.com/zhaoyun0071/HivisionIDPhotos-windows-GUI)：Windows客户端应用，由 [zhaoyun0071](https://github.com/zhaoyun0071) 构建\n- [HivisionIDPhotos-NAS](https://github.com/ONG-Leo/HivisionIDPhotos-NAS): 群晖NAS部署中文教程，由 [ONG-Leo](https://github.com/ONG-Leo) 贡献\n\n\n<br>\n\n# 🔧 准备工作\n\n环境安装与依赖：\n- Python >= 3.7（项目主要测试在 python 3.10）\n- OS: Linux, Windows, MacOS\n\n## 1. 克隆项目\n\n```bash\ngit clone https://github.com/Zeyi-Lin/HivisionIDPhotos.git\ncd  HivisionIDPhotos\n```\n\n## 2. 安装依赖环境\n\n> 建议 conda 创建一个 python3.10 虚拟环境后，执行以下命令\n\n```bash\npip install -r requirements.txt\npip install -r requirements-app.txt\n```\n\n## 3. 下载人像抠图模型权重文件\n\n**方式一：脚本下载**\n\n```bash\npython scripts/download_model.py --models all\n# 如需指定下载某个模型\n# python scripts/download_model.py --models modnet_photographic_portrait_matting\n```\n\n**方式二：直接下载**\n\n模型均存到项目的`hivision/creator/weights`目录下：\n\n| 人像抠图模型 | 介绍 | 下载 |\n| -- | -- | -- |\n| MODNet | [MODNet](https://github.com/ZHKKKe/MODNet)官方权重 | [下载](https://github.com/Zeyi-Lin/HivisionIDPhotos/releases/download/pretrained-model/modnet_photographic_portrait_matting.onnx)(24.7MB)|\n| hivision_modnet | 对纯色换底适配性更好的抠图模型 | [下载](https://github.com/Zeyi-Lin/HivisionIDPhotos/releases/download/pretrained-model/hivision_modnet.onnx)(24.7MB) |\n| rmbg-1.4 | [BRIA AI](https://huggingface.co/briaai/RMBG-1.4) 开源的抠图模型 | [下载](https://huggingface.co/briaai/RMBG-1.4/resolve/main/onnx/model.onnx?download=true)(176.2MB)后重命名为`rmbg-1.4.onnx` |\n| birefnet-v1-lite | [ZhengPeng7](https://github.com/ZhengPeng7/BiRefNet) 开源的抠图模型，拥有最好的分割精度 | [下载](https://github.com/ZhengPeng7/BiRefNet/releases/download/v1/BiRefNet-general-bb_swin_v1_tiny-epoch_232.onnx)(224MB)后重命名为`birefnet-v1-lite.onnx` |\n\n> 如果下载网速不顺利：前往[SwanHub](https://swanhub.co/ZeYiLin/HivisionIDPhotos_models/tree/main)下载。\n\n\n## 4. 人脸检测模型配置（可选）\n\n| 拓展人脸检测模型 | 介绍 | 使用文档 |\n| -- | -- | -- |\n| MTCNN | **离线**人脸检测模型，高性能CPU推理（毫秒级），为默认模型，检测精度较低 | Clone此项目后直接使用 |\n| RetinaFace | **离线**人脸检测模型，CPU推理速度中等（秒级），精度较高| [下载](https://github.com/Zeyi-Lin/HivisionIDPhotos/releases/download/pretrained-model/retinaface-resnet50.onnx)后放到`hivision/creator/retinaface/weights`目录下 |\n| Face++ | 旷视推出的在线人脸检测API，检测精度较高，[官方文档](https://console.faceplusplus.com.cn/documents/4888373) | [使用文档](docs/face++_CN.md)|\n\n## 5. 性能参考\n\n> 测试环境为Mac M1 Max 64GB，非GPU加速，测试图片分辨率为 512x715(1) 与 764×1146(2)。\n\n| 模型组合 | 内存占用 | 推理时长(1) | 推理时长(2) |\n| -- | -- | -- | -- |\n| MODNet + mtcnn | 410MB | 0.207s | 0.246s |\n| MODNet + retinaface | 405MB | 0.571s | 0.971s |\n| birefnet-v1-lite + retinaface | 6.20GB | 7.063s | 7.128s |\n\n## 6. GPU推理加速（可选）\n\n在当前版本，可被英伟达GPU加速的模型为`birefnet-v1-lite`，并请确保你有16GB左右的显存。\n\n如需使用英伟达GPU加速推理，在确保你已经安装[CUDA](https://developer.nvidia.com/cuda-downloads)与[cuDNN](https://developer.nvidia.com/cudnn)后，根据[onnxruntime-gpu文档](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#cuda-12x)找到对应的`onnxruntime-gpu`版本安装，以及根据[pytorch官网](https://pytorch.org/get-started/locally/)找到对应的`torch`版本安装。\n\n```bash\n# 假如你的电脑安装的是CUDA 12.x, cuDNN 8\n# 安装torch是可选的，如果你始终配置不好cuDNN，那么试试安装torch\npip install onnxruntime-gpu==1.18.0\npip install torch --index-url https://download.pytorch.org/whl/cu121\n```\n\n完成安装后，调用`birefnet-v1-lite`模型即可利用GPU加速推理。\n\n> TIPS: CUDA 支持向下兼容。比如你的 CUDA 版本为 12.6，`torch` 官方目前支持的最高版本为 12.4（<12.6），`torch`仍可以正常使用CUDA。\n\n<br>\n\n# ⚡️ 运行 Gradio Demo\n\n```bash\npython app.py\n```\n\n运行程序将生成一个本地 Web 页面，在页面中可完成证件照的操作与交互。\n\n<img src="assets/harry.png" width=900>\n\n<br>\n\n# 🚀 Python 推理\n\n核心参数：\n\n- `-i`: 输入图像路径\n- `-o`: 保存图像路径\n- `-t`: 推理类型，有idphoto、human_matting、add_background、generate_layout_photos可选\n- `--matting_model`: 人像抠图模型权重选择\n- `--face_detect_model`: 人脸检测模型选择\n\n更多参数可通过`python inference.py --help`查看\n\n## 1. 证件照制作\n\n输入 1 张照片，获得 1 张标准证件照和 1 张高清证件照的 4 通道透明 png\n\n```python\npython inference.py -i demo/images/test0.jpg -o ./idphoto.png --height 413 --width 295\n```\n\n## 2. 人像抠图\n\n输入 1 张照片，获得 1张 4 通道透明 png\n\n```python\npython inference.py -t human_matting -i demo/images/test0.jpg -o ./idphoto_matting.png --matting_model hivision_modnet\n```\n\n## 3. 透明图增加底色\n\n输入 1 张 4 通道透明 png，获得 1 张增加了底色的 3通道图像\n\n```python\npython inference.py -t add_background -i ./idphoto.png -o ./idphoto_ab.jpg  -c 4f83ce -k 30 -r 1\n```\n\n## 4. 得到六寸排版照\n\n输入 1 张 3 通道照片，获得 1 张六寸排版照\n\n```python\npython inference.py -t generate_layout_photos -i ./idphoto_ab.jpg -o ./idphoto_layout.jpg  --height 413 --width 295 -k 200\n```\n\n## 5. 证件照裁剪\n\n输入 1 张 4 通道照片（抠图好的图像），获得 1 张标准证件照和 1 张高清证件照的 4 通道透明 png\n\n```python\npython inference.py -t idphoto_crop -i ./idphoto_matting.png -o ./idphoto_crop.png --height 413 --width 295\n```\n\n\n<br>\n\n# ⚡️ 部署 API 服务\n\n## 启动后端\n\n```\npython deploy_api.py\n```\n\n## 请求 API 服务\n\n详细请求方式请参考 [API 文档](docs/api_CN.md)，包含以下请求示例：\n- [cURL](docs/api_CN.md#curl-请求示例)\n- [Python](docs/api_CN.md#python-请求示例)\n\n<br>\n\n# 🐳 Docker 部署\n\n## 1. 拉取或构建镜像\n\n> 以下方式三选一\n\n**方式一：拉取最新镜像：**\n\n```bash\ndocker pull linzeyi/hivision_idphotos\n```\n\n**方式二：Dockrfile 直接构建镜像：**\n\n在确保将至少一个[抠图模型权重文件](#3-下载权重文件)放到`hivision/creator/weights`下后，在项目根目录执行：\n\n```bash\ndocker build -t linzeyi/hivision_idphotos .\n```\n\n**方式三：Docker compose 构建：**\n\n在确保将至少一个[抠图模型权重文件](#3-下载权重文件)放到`hivision/creator/weights`下后，在项目根目录下执行：\n\n```bash\ndocker compose build\n```\n\n## 2. 运行服务\n\n**启动 Gradio Demo 服务**\n\n运行下面的命令，在你的本地访问 [http://127.0.0.1:7860](http://127.0.0.1:7860/) 即可使用。\n\n```bash\ndocker run -d -p 7860:7860 linzeyi/hivision_idphotos\n```\n\n**启动 API 后端服务**\n\n```bash\ndocker run -d -p 8080:8080 linzeyi/hivision_idphotos python3 deploy_api.py\n```\n\n**两个服务同时启动**\n\n```bash\ndocker compose up -d\n```\n\n## 环境变量\n\n本项目提供了一些额外的配置项，使用环境变量进行设置：\n\n| 环境变量 | 类型	| 描述 | 示例 |\n|--|--|--|--|\n| FACE_PLUS_API_KEY	 | 可选	| 这是你在 Face++ 控制台申请的 API 密钥	 | `7-fZStDJ····` |\n| FACE_PLUS_API_SECRET	 | 可选	| Face++ API密钥对应的Secret | `VTee824E····` |\n| RUN_MODE | 可选 | 运行模式，可选值为`beast`(野兽模式)。野兽模式下人脸检测和抠图模型将不释放内存，从而获得更快的二次推理速度。建议内存16GB以上尝试。 | `beast` |\n| DEFAULT_LANG | 可选 | Gradio Demo启动时的默认语言| `en` |\n\ndocker使用环境变量示例：\n```bash\ndocker run  -d -p 7860:7860 \\n    -e FACE_PLUS_API_KEY=7-fZStDJ···· \\n    -e FACE_PLUS_API_SECRET=VTee824E···· \\n    -e RUN_MODE=beast \\n    -e DEFAULT_LANG=en \\n    linzeyi/hivision_idphotos  \n```\n\n<br>\n\n# FAQ\n\n## 1. 如何修改预设尺寸和颜色？\n\n- 尺寸：修改[size_list_CN.csv](demo/assets/size_list_CN.csv)后再次运行 `app.py` 即可，其中第一列为尺寸名，第二列为高度，第三列为宽度。\n- 颜色：修改[color_list_CN.csv](demo/assets/color_list_CN.csv)后再次运行 `app.py` 即可，其中第一列为颜色名，第二列为Hex值。\n\n## 2. 如何修改水印字体？\n\n1. 将字体文件放到`hivision/plugin/font`文件夹下\n2. 修改`hivision/plugin/watermark.py`的`font_file`参数值为字体文件名\n\n## 3. 如何添加社交媒体模板照？\n\n1. 将模板图片放到`hivision/plugin/template/assets`文件夹下。模板图片是一个4通道的透明png。\n2. 在`hivision/plugin/template/assets/template_config.json`文件中添加最新的模板信息，其中`width`为模板图宽度(px)，`height`为模板图高度(px)，`anchor_points`为模板中透明区域的四个角的坐标(px)；`rotation`为透明区域相对于垂直方向的旋转角度，>0为逆时针，<0为顺时针。\n3. 在`demo/processor.py`的`_generate_image_template`函数中的`TEMPLATE_NAME_LIST`变量添加最新的模板名\n\n<img src="assets/social_template.png" width="500">\n\n## 4. 如何修改Gradio Demo的顶部导航栏？\n\n- 修改`demo/assets/title.md`\n\n## 5. 如何添加/修改「打印排版」中的尺寸？\n\n- 修改`demo/locales.py`中的`print_switch`字典，添加/修改新的尺寸名称和尺寸参数，然后重新运行`python app.py`\n\n<br>\n\n# 📧 联系我们\n\n如果您有任何问题，请发邮件至 zeyi.lin@swanhub.co\n\n<br>\n\n# 🙏 感谢支持\n\n[![Stargazers repo roster for @Zeyi-Lin/HivisionIDPhotos](https://reporoster.com/stars/Zeyi-Lin/HivisionIDPhotos)](https://github.com/Zeyi-Lin/HivisionIDPhotos/stargazers)\n\n[![Forkers repo roster for @Zeyi-Lin/HivisionIDPhotos](https://reporoster.com/forks/Zeyi-Lin/HivisionIDPhotos)](https://github.com/Zeyi-Lin/HivisionIDPhotos/network/members)\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Zeyi-Lin/HivisionIDPhotos&type=Date)](https://star-history.com/#Zeyi-Lin/HivisionIDPhotos&Date)\n\n贡献者们：\n\n<a href="https://github.com/Zeyi-Lin/HivisionIDPhotos/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=Zeyi-Lin/HivisionIDPhotos" />\n</a>\n\n[Zeyi-Lin](https://github.com/Zeyi-Lin)、[SAKURA-CAT](https://github.com/SAKURA-CAT)、[Feudalman](https://github.com/Feudalman)、[swpfY](https://github.com/swpfY)、[Kaikaikaifang](https://github.com/Kaikaikaifang)、[ShaohonChen](https://github.com/ShaohonChen)、[KashiwaByte](https://github.com/KashiwaByte)\n\n<br>\n\n# 📜 Lincese\n\nThis repository is licensed under the [Apache-2.0 License](LICENSE).\n\n<br>\n\n# 📚 引用\n\n如果您在研究或项目中使用了HivisionIDPhotos，请考虑引用我们的工作。您可以使用以下BibTeX条目：\n\n```bibtex\n@misc{hivisionidphotos,\n      title={{HivisionIDPhotos: A Lightweight and Efficient AI ID Photos Tool}},\n      author={Zeyi Lin and SwanLab Team},\n      year={2024},\n      publisher={GitHub},\n      url = {\url{https://github.com/Zeyi-Lin/HivisionIDPhotos}},\n}\n```\n\n\n\n\n[github-stars-shield]: https://img.shields.io/github/stars/zeyi-lin/hivisionidphotos?color=ffcb47&labelColor=black&style=flat-square\n[github-stars-link]: https://github.com/zeyi-lin/hivisionidphotos/stargazers\n\n[swanhub-demo-shield]: https://swanhub.co/git/repo/SwanHub%2FAuto-README/file/preview?ref=main&path=swanhub.svg\n[swanhub-demo-link]: https://swanhub.co/ZeYiLin/HivisionIDPhotos/demo\n\n[spaces-shield]: https://img.shields.io/badge/🤗-Open%20in%20Spaces-blue\n[spaces-link]: https://huggingface.co/spaces/TheEeeeLin/HivisionIDPhotos\n\n<!-- 微信群链接 -->\n[wechat-shield]: https://img.shields.io/badge/WeChat-微信-4cb55e\n[wechat-link]: https://docs.qq.com/doc/DUkpBdk90eWZFS2JW\n\n<!-- Github Release -->\n[release-shield]: https://img.shields.io/github/v/release/zeyi-lin/hivisionidphotos?color=369eff&labelColor=black&logo=github&style=flat-square\n[release-link]: https://github.com/zeyi-lin/hivisionidphotos/releases\n\n[license-shield]: https://img.shields.io/badge/license-apache%202.0-white?labelColor=black&style=flat-square\n[license-link]: https://github.com/Zeyi-Lin/HivisionIDPhotos/blob/master/LICENSE\n\n[github-issues-shield]: https://img.shields.io/github/issues/zeyi-lin/hivisionidphotos?color=ff80eb&labelColor=black&style=flat-square\n[github-issues-link]: https://github.com/zeyi-lin/hivisionidphotos/issues\n\n[dockerhub-shield]: https://img.shields.io/docker/v/linzeyi/hivision_idphotos?color=369eff&label=docker&labelColor=black&logoColor=white&style=flat-square\n[dockerhub-link]: https://hub.docker.com/r/linzeyi/hivision_idphotos/tags\n\n[trendshift-shield]: https://trendshift.io/api/badge/repositories/11622\n[trendshift-link]: https://trendshift.io/repositories/11622\n\n[hellogithub-shield]: https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=8ea1457289fb4062ba661e5299e733d6&claim_uid=Oh5UaGjfrblg0yZ\n[hellogithub-link]: https://hellogithub.com/repository/8ea1457289fb4062ba661e5299e733d6\n\n[github-contributors-shield]: https://img.shields.io/github/contributors/zeyi-lin/hivisionidphotos?color=c4f042&labelColor=black&style=flat-square\n[github-contributors-link]: https://github.com/zeyi-lin/hivisionidphotos/graphs/contributors\n\n[github-forks-shield]: https://img.shields.io/github/forks/zeyi-lin/hivisionidphotos?color=8ae8ff&labelColor=black&style=flat-square\n[github-forks-link]: https://github.com/zeyi-lin/hivisionidphotos/network/members\n\n[modelscope-shield]: https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&labelColor=white\n[modelscope-link]: https://modelscope.cn/studios/SwanLab/HivisionIDPhotos\n\n[modelers-shield]: https://img.shields.io/badge/Demo_on_Modelers-c42a2a?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMjQiIGhlaWdodD0iNjQiIHZpZXdCb3g9IjAgMCAxMjQgNjQiIGZpbGw9Im5vbmUiPgo8cGF0aCBkPSJNNDIuNzc4MyAwSDI2LjU5NzdWMTUuNzc4N0g0Mi43NzgzVjBaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik0xNi41MDg4IDQuMTc5MkgwLjMyODEyNVYxOS45NTc5SDE2LjUwODhWNC4xNzkyWiIgZmlsbD0iIzI0NDk5QyIvPgo8cGF0aCBkPSJNMTIzLjk1MiA0LjE3OTJIMTA3Ljc3MVYxOS45NTc5SDEyMy45NTJWNC4xNzkyWiIgZmlsbD0iIzI0NDk5QyIvPgo8cGF0aCBkPSJNMTYuNTA4OCA0NS40NjE5SDAuMzI4MTI1VjYxLjI0MDZIMTYuNTA4OFY0NS40NjE5WiIgZmlsbD0iIzI0NDk5QyIvPgo8cGF0aCBkPSJNMTIzLjk1MiA0NS40NjE5SDEwNy43NzFWNjEuMjQwNkgxMjMuOTUyVjQ1LjQ2MTlaIiBmaWxsPSIjMjQ0OTlDIi8+CjxwYXRoIGQ9Ik0zMi43MDggMTUuNzc4OEgxNi41MjczVjMxLjU1NzVIMzIuNzA4VjE1Ljc3ODhaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik01Mi44NDg2IDE1Ljc3ODhIMzYuNjY4VjMxLjU1NzVINTIuODQ4NlYxNS43Nzg4WiIgZmlsbD0iI0RFMDQyOSIvPgo8cGF0aCBkPSJNOTcuNzIzNyAwSDgxLjU0M1YxNS43Nzg3SDk3LjcyMzdWMFoiIGZpbGw9IiNERTA0MjkiLz4KPHBhdGggZD0iTTg3LjY1MzQgMTUuNzc4OEg3MS40NzI3VjMxLjU1NzVIODcuNjUzNFYxNS43Nzg4WiIgZmlsbD0iI0RFMDQyOSIvPgo8cGF0aCBkPSJNMTA3Ljc5NCAxNS43Nzg4SDkxLjYxMzNWMzEuNTU3NUgxMDcuNzk0VjE1Ljc3ODhaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik0yNC42NzQ4IDMxLjU1NzZIOC40OTQxNFY0Ny4zMzYzSDI0LjY3NDhWMzEuNTU3NloiIGZpbGw9IiNERTA0MjkiLz4KPHBhdGggZD0iTTYwLjg3OTkgMzEuNTU3Nkg0NC42OTkyVjQ3LjMzNjNINjAuODc5OVYzMS41NTc2WiIgZmlsbD0iI0RFMDQyOSIvPgo8cGF0aCBkPSJNNzkuNjIwMSAzMS41NTc2SDYzLjQzOTVWNDcuMzM2M0g3OS42MjAxVjMxLjU1NzZaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik0xMTUuODI1IDMxLjU1NzZIOTkuNjQ0NVY0Ny4zMzYzSDExNS44MjVWMzEuNTU3NloiIGZpbGw9IiNERTA0MjkiLz4KPHBhdGggZD0iTTcwLjI1NDkgNDcuMzM1OUg1NC4wNzQyVjYzLjExNDdINzAuMjU0OVY0Ny4zMzU5WiIgZmlsbD0iI0RFMDQyOSIvPgo8L3N2Zz4=&labelColor=white\n[modelers-link]: https://modelers.cn/spaces/SwanLab/HivisionIDPhotos\n\n[compshare-shield]: https://www-s.ucloud.cn/2025/02/dbef8b07ea3d316006d9c22765c3cd53_1740104342584.svg\n[compshare-link]: https://www.compshare.cn/images-detail?ImageID=compshareImage-17jacgm4ju16&ytag=HG_GPU_HivisionIDPhotos\n\n<!-- 社区项目链接 -->\n[community-hivision-comfyui]: https://github.com/AIFSH/HivisionIDPhotos-ComfyUI\n[community-hivision-wechat]: https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp\n[community-hivision-uniapp]: https://github.com/soulerror/HivisionIDPhotos-Uniapp\n[community-hivision-cpp]: https://github.com/zjkhahah/HivisionIDPhotos-cpp\n[community-hivision-windows-gui]: https://github.com/zhaoyun0071/HivisionIDPhotos-windows-GUI\n[community-hivision-nas]: https://github.com/ONG-Leo/HivisionIDPhotos-NAS', '{"language":"Python","stars":20299,"forks":2288,"watchers":20299,"open_issues":91,"topics":["cnn","demo","docker","face-recognition","fastapi","gradio","idphoto","machine-learning","matting","mtcnn","tools","unet"],"default_branch":"master","size_kb":48948,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:SwanHubX:SwanLab","source_url":"https://github.com/SwanHubX/SwanLab"},{"type":"has_code","target_id":"github:zjkhahah:HivisionIDPhotos-cpp","source_url":"https://github.com/zjkhahah/HivisionIDPhotos-cpp"},{"type":"has_code","target_id":"github:AIFSH:HivisionIDPhotos-ComfyUI\">","source_url":"https://github.com/AIFSH/HivisionIDPhotos-ComfyUI\">"},{"type":"has_code","target_id":"github:no1xuan:HivisionIDPhotos-wechat-weapp\">","source_url":"https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp\">"},{"type":"has_code","target_id":"github:jkm199:HivisionIDPhotos-web","source_url":"https://github.com/jkm199/HivisionIDPhotos-web"},{"type":"has_code","target_id":"github:soulerror:HivisionIDPhotos-Uniapp\">","source_url":"https://github.com/soulerror/HivisionIDPhotos-Uniapp\">"},{"type":"has_code","target_id":"github:jkm199:HivisionIDPhotos-web\">","source_url":"https://github.com/jkm199/HivisionIDPhotos-web\">"},{"type":"has_code","target_id":"github:zjkhahah:HivisionIDPhotos-cpp","source_url":"https://github.com/zjkhahah/HivisionIDPhotos-cpp"},{"type":"has_code","target_id":"github:wmlcjj:ai-idphoto","source_url":"https://github.com/wmlcjj/ai-idphoto"},{"type":"has_code","target_id":"github:no1xuan:HivisionIDPhotos-wechat-weapp","source_url":"https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp"},{"type":"has_code","target_id":"github:jkm199:HivisionIDPhotos-uniapp-WeChat-gpto1","source_url":"https://github.com/jkm199/HivisionIDPhotos-uniapp-WeChat-gpto1"},{"type":"has_code","target_id":"github:zhaoyun0071:HivisionIDPhotos-windows-GUI","source_url":"https://github.com/zhaoyun0071/HivisionIDPhotos-windows-GUI"},{"type":"has_code","target_id":"github:ONG-Leo:HivisionIDPhotos-NAS","source_url":"https://github.com/ONG-Leo/HivisionIDPhotos-NAS"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos.git","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos.git"},{"type":"has_code","target_id":"github:ZHKKKe:MODNet","source_url":"https://github.com/ZHKKKe/MODNet"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos"},{"type":"has_code","target_id":"github:ZhengPeng7:BiRefNet","source_url":"https://github.com/ZhengPeng7/BiRefNet"},{"type":"has_code","target_id":"github:ZhengPeng7:BiRefNet","source_url":"https://github.com/ZhengPeng7/BiRefNet"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos}},","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos}},"},{"type":"has_code","target_id":"github:zeyi-lin:hivisionidphotos","source_url":"https://github.com/zeyi-lin/hivisionidphotos"},{"type":"has_code","target_id":"github:zeyi-lin:hivisionidphotos","source_url":"https://github.com/zeyi-lin/hivisionidphotos"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos"},{"type":"has_code","target_id":"github:zeyi-lin:hivisionidphotos","source_url":"https://github.com/zeyi-lin/hivisionidphotos"},{"type":"has_code","target_id":"github:zeyi-lin:hivisionidphotos","source_url":"https://github.com/zeyi-lin/hivisionidphotos"},{"type":"has_code","target_id":"github:zeyi-lin:hivisionidphotos","source_url":"https://github.com/zeyi-lin/hivisionidphotos"},{"type":"has_code","target_id":"github:AIFSH:HivisionIDPhotos-ComfyUI","source_url":"https://github.com/AIFSH/HivisionIDPhotos-ComfyUI"},{"type":"has_code","target_id":"github:no1xuan:HivisionIDPhotos-wechat-weapp","source_url":"https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp"},{"type":"has_code","target_id":"github:soulerror:HivisionIDPhotos-Uniapp","source_url":"https://github.com/soulerror/HivisionIDPhotos-Uniapp"},{"type":"has_code","target_id":"github:zjkhahah:HivisionIDPhotos-cpp","source_url":"https://github.com/zjkhahah/HivisionIDPhotos-cpp"},{"type":"has_code","target_id":"github:zhaoyun0071:HivisionIDPhotos-windows-GUI","source_url":"https://github.com/zhaoyun0071/HivisionIDPhotos-windows-GUI"},{"type":"has_code","target_id":"github:ONG-Leo:HivisionIDPhotos-NAS","source_url":"https://github.com/ONG-Leo/HivisionIDPhotos-NAS"}]', NULL, 'Apache-2.0', 'approved', 80, '21a6274ee7cb01e58c4db02a7a7397e7', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-Zeyi-Lin-HivisionIDPhotos from https://github.com/Zeyi-Lin.png
Image converted to WebP: data/images/github-Zeyi-Lin-HivisionIDPhotos.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-onnx-onnx', 'github--onnx--onnx', 'onnx', 'onnx', '<!-- Copyright (c) ONNX Project Contributors SPDX-License-Identifier: Apache-2.0 --> <p align="center"><img width="40%" src="https://github.com/onnx/onnx/raw/main/docs/onnx-horizontal-color.png" /></p> Open Neural Network Exchange (ONNX) is an open ecosystem that empowers AI developers to choose the right tools as their project evolves. ONNX provides an open source format for AI models, both deep learning and traditional ML. It defines an extensible computation graph model, as well as definit...', '["deep-learning","deep-neural-networks","dnn","keras","machine-learning","ml","neural-network","onnx","pytorch","scikit-learn","tensorflow","python"]', 'other', 19983, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/onnx/onnx","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<!--\nCopyright (c) ONNX Project Contributors\n\nSPDX-License-Identifier: Apache-2.0\n-->\n\n<p align="center"><img width="40%" src="https://github.com/onnx/onnx/raw/main/docs/onnx-horizontal-color.png" /></p>\n\n[![PyPI - Version](https://img.shields.io/pypi/v/onnx.svg)](https://pypi.org/project/onnx)\n[![CI](https://github.com/onnx/onnx/actions/workflows/main.yml/badge.svg)](https://github.com/onnx/onnx/actions/workflows/main.yml)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3313/badge)](https://bestpractices.coreinfrastructure.org/projects/3313)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/onnx/onnx/badge)](https://api.securityscorecards.dev/projects/github.com/onnx/onnx)\n[![REUSE compliant](https://api.reuse.software/badge/github.com/onnx/onnx)](https://api.reuse.software/info/github.com/onnx/onnx)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![abi3 compatible](https://img.shields.io/badge/abi3-compatible-brightgreen)](https://docs.python.org/3/c-api/stable.html)\n\n[Open Neural Network Exchange (ONNX)](https://onnx.ai) is an open ecosystem that empowers AI developers\nto choose the right tools as their project evolves. ONNX provides an open source format for AI models, both deep learning and traditional ML. It defines an extensible computation graph model, as well as definitions of built-in operators and standard\ndata types. Currently we focus on the capabilities needed for inferencing (scoring).\n\nONNX is [widely supported](http://onnx.ai/supported-tools) and can be found in many frameworks, tools, and hardware. Enabling interoperability between different frameworks and streamlining the path from research to production helps increase the speed of innovation in the AI community. We invite the community to join us and further evolve ONNX.\n\n\n# Use ONNX\n\n* [Documentation of ONNX Python Package](https://onnx.ai/onnx/)\n* [Tutorials for creating ONNX models](https://github.com/onnx/tutorials)\n* [Pre-trained ONNX models](https://github.com/onnx/models)\n\n# Learn about the ONNX spec\n\n* [Overview](https://github.com/onnx/onnx/blob/main/docs/Overview.md)\n* [ONNX intermediate representation spec](https://github.com/onnx/onnx/blob/main/docs/IR.md)\n* [Versioning principles of the spec](https://github.com/onnx/onnx/blob/main/docs/Versioning.md)\n* [Operators documentation](https://github.com/onnx/onnx/blob/main/docs/Operators.md)\n* [Operators documentation](https://onnx.ai/onnx/operators/index.html) (latest release)\n* [Python API Overview](https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md)\n\n# Programming utilities for working with ONNX Graphs\n\n* [Shape and Type Inference](https://github.com/onnx/onnx/blob/main/docs/ShapeInference.md)\n* [Graph Optimization](https://github.com/onnx/optimizer)\n* [Opset Version Conversion](https://github.com/onnx/onnx/blob/main/docs/docsgen/source/api/version_converter.md)\n\n# Contribute\n\nONNX is a community project and the open governance model is described [here](https://github.com/onnx/onnx/blob/main/community/readme.md). We encourage you to join the effort and contribute feedback, ideas, and code. You can participate in the [Special Interest Groups](https://github.com/onnx/onnx/blob/main/community/sigs.md) and [Working Groups](https://github.com/onnx/onnx/blob/main/community/working-groups.md) to shape the future of ONNX.\n\nCheck out our [contribution guide](https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md) to get started.\n\nIf you think some operator should be added to ONNX specification, please read\n[this document](https://github.com/onnx/onnx/blob/main/docs/AddNewOp.md).\n\n# Community meetings\n\nThe schedules of the regular meetings of the Steering Committee, the working groups and the SIGs can be found [here](https://onnx.ai/calendar)\n\nCommunity Meetups are held at least once a year. Content from previous community meetups are at:\n\n* 2020.04.09 <https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14091402/LF+AI+Day+-ONNX+Community+Virtual+Meetup+-+Silicon+Valley+-+2020+April+9>\n* 2020.10.14 <https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14092138/LF+AI+Day+-+ONNX+Community+Workshop+-+2020+October+14>\n* 2021.03.24 <https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14092424/Instructions+for+Event+Hosts+-+LF+AI+Data+Day+-+ONNX+Virtual+Community+Meetup+-+March+2021>\n* 2021.10.21 <https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14093194/LF+AI+Data+Day+ONNX+Community+Virtual+Meetup+-+October+2021>\n* 2022.06.24 <https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14093969/ONNX+Community+Day+-+2022+June+24>\n* 2023.06.28 <https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14094507/ONNX+Community+Day+2023+-+June+28>\n\n# Discuss\n\nWe encourage you to open [Issues](https://github.com/onnx/onnx/issues), or use [Slack](https://lfaifoundation.slack.com/) (If you have not joined yet, please use this [link](https://join.slack.com/t/lfaifoundation/shared_invite/zt-o65errpw-gMTbwNr7FnNbVXNVFkmyNA) to join the group) for more real-time discussion.\n\n# Follow Us\n\nStay up to date with the latest ONNX news. [[Facebook](https://www.facebook.com/onnxai/)] [[Twitter/X](https://twitter.com/onnxai)]\n\n# Roadmap\n\nA roadmap process takes place every year. More details can be found [here](https://github.com/onnx/steering-committee/tree/main/roadmap)\n\n# Installation\n\nONNX released packages are published in PyPi.\n\n```sh\npip install onnx # or pip install onnx[reference] for optional reference implementation dependencies\n```\n\n[ONNX weekly packages](https://pypi.org/project/onnx-weekly/) are published in PyPI to enable experimentation and early testing.\n\nDetailed install instructions, including Common Build Options and Common Errors can be found [here](https://github.com/onnx/onnx/blob/main/INSTALL.md)\n\n# Python ABI3 Compatibility\n\nThis package provides [abi3](https://docs.python.org/3/c-api/stable.html)-compatible wheels, allowing a single binary wheel to work across multiple Python versions (from 3.12 onwards).\n\n\n# Testing\n\nONNX uses [pytest](https://docs.pytest.org) as test driver. In order to run tests, you will first need to install `pytest`:\n\n```sh\npip install pytest\n```\n\nAfter installing pytest, use the following command to run tests.\n\n```sh\npytest\n```\n\n# Development\n\nCheck out the [contributor guide](https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md) for instructions.\n\n# Reproducible Builds (Linux)\n\nThis project provides reproducible builds for Linux.\n\nA *reproducible build* means that the same source code will always produce identical binary outputs, no matter who builds it or where it is built.\n\nTo achieve this, we use the [`SOURCE_DATE_EPOCH`](https://reproducible-builds.org/docs/source-date-epoch/) standard. This ensures that build timestamps and other time-dependent information are fixed, making the output bit-for-bit identical across different environments.\n\n### Why this matters\n- **Transparency**: Anyone can verify that the distributed binaries were created from the published source code.\n- **Security**: Prevents tampering or hidden changes in the build process.\n- **Trust**: Users can be confident that the binaries they download are exactly what the maintainers intended.\n\nIf you prefer, you can use the prebuilt reproducible binaries instead of building from source yourself.\n\n# License\n\n[Apache License v2.0](LICENSE)\n\n# Trademark\nCheckout [https://trademarks.justia.com](https://trademarks.justia.com/877/25/onnx-87725026.html) for the trademark.\n\n[General rules of the Linux Foundation on Trademark usage](https://www.linuxfoundation.org/legal/trademark-usage)\n\n# Code of Conduct\n\n[ONNX Open Source Code of Conduct](https://onnx.ai/codeofconduct.html)\n', '{"language":"Python","stars":19983,"forks":3837,"watchers":19983,"open_issues":285,"topics":["deep-learning","deep-neural-networks","dnn","keras","machine-learning","ml","neural-network","onnx","pytorch","scikit-learn","tensorflow"],"default_branch":"main","size_kb":44847,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:astral-sh:ruff","source_url":"https://github.com/astral-sh/ruff"},{"type":"has_code","target_id":"github:onnx:tutorials","source_url":"https://github.com/onnx/tutorials"},{"type":"has_code","target_id":"github:onnx:models","source_url":"https://github.com/onnx/models"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:optimizer","source_url":"https://github.com/onnx/optimizer"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:steering-committee","source_url":"https://github.com/onnx/steering-committee"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"}]', NULL, 'Apache-2.0', 'approved', 65, '189b13342e81ca36f0232e7dc4caeb65', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-onnx-onnx from https://github.com/onnx.png
Image converted to WebP: data/images/github-onnx-onnx.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-EthicalML-awesome-production-machine-learning', 'github--ethicalml--awesome-production-machine-learning', 'awesome-production-machine-learning', 'EthicalML', 'This repository contains a curated list of awesome open source libraries that will help you deploy, monitor, version, scale, and secure your production machine learning 🚀 You can keep up to date by watching this github repo to get a summary of the new production ML libraries added every month via releases 🤩 Additionally, we provide a search toolkit that helps you quickly navigate through the toolchain. | | | | |-|-|-| | 🔧 AutoML | 🧮 Computation & Communication Optimisation | 🏷️ Data Anno...', '["awesome","awesome-list","data-mining","deep-learning","explainability","interpretability","large-scale-machine-learning","large-scale-ml","machine-learning","machine-learning-operations","ml-operations","ml-ops","mlops","privacy-preserving","privacy-preserving-machine-learning","privacy-preserving-ml","production-machine-learning","production-ml","responsible-ai"]', 'other', 19709, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/EthicalML/awesome-production-machine-learning","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)\n[![X](https://img.shields.io/badge/X-%23000000?logo=X&logoColor=white)](https://twitter.com/EthicalML)\n\n# Awesome Production Machine Learning\n\nThis repository contains a curated list of awesome open source libraries that will help you deploy, monitor, version, scale, and secure your production machine learning 🚀\n\nYou can keep up to date by watching this github repo to get a summary of the new production ML libraries added every month [via releases](https://github.com/EthicalML/awesome-production-machine-learning/releases) 🤩\n\nAdditionally, we provide a [search toolkit](https://huggingface.co/spaces/zhiminy/Awesome-Production-Machine-Learning-Search) that helps you quickly navigate through the toolchain.\n\n## Quick links to sections on this page\n\n| | | |\n|-|-|-|\n| [🔧 AutoML](#automl) | [🧮 Computation & Communication Optimisation](#computation-and-communication-optimisation) | [🏷️ Data Annotation & Synthesis](#data-annotation-and-synthesis) |\n| [🧵 Data Pipeline](#data-pipeline) | [📓 Data Science Notebook](#data-science-notebook) | [💾 Data Storage Optimisation](#data-storage-optimisation) |\n| [💸 Data Stream Processing](#data-stream-processing) | [💪 Deployment & Serving](#deployment-and-serving) | [📈 Evaluation & Monitoring](#evaluation-and-monitoring) |\n| [🔍 Explainability & Fairness](#explainability-and-fairness) | [🎁 Feature Store](#feature-store) | [🔴 Industry-strength Anomaly Detection](#industry-strength-anomaly-detection) |\n| [👁️ Industry-strength Computer Vision](#industry-strength-computer-vision) | [🔥 Industry-strength Information Retrieval](#industry-strength-information-retrieval) | [🔠 Industry-strength Natural Language Processing](#industry-strength-nlp) |\n| [🙌 Industry-strength Recommender System](#industry-strength-recommender-system) | [🍕 Industry-strength Reinforcement Learning](#industry-strength-reinforcement-learning) | [📊 Industry-strength Visualisation](#industry-strength-visualisation) |\n| [📅 Metadata Management](#metadata-management) | [📜 Model, Data & Experiment Management](#model-data-and-experiment-management) | [🔩 Model Storage Optimisation](#model-storage-optimisation) |\n| [🏁 Model Training & Orchestration](#model-training-and-orchestration) | [🔏 Privacy & Safety](#privacy-and-safety) |\n\n## Contributing to the list\n\nPlease review our [CONTRIBUTING.md](https://github.com/EthicalML/awesome-production-machine-learning/blob/master/CONTRIBUTING.md) requirements when submitting a PR to help us keep the list clean and up-to-date - thank you to the community for supporting its steady growth 🚀\n\n<picture>\n  <source\n    media="(prefers-color-scheme: dark)"\n    srcset="\n      https://api.star-history.com/svg?repos=EthicalML/awesome-production-machine-learning&type=Date&theme=dark\n    "\n  />\n  <source\n    media="(prefers-color-scheme: light)"\n    srcset="\n      https://api.star-history.com/svg?repos=EthicalML/awesome-production-machine-learning&type=Date\n    "\n  />\n  <img\n    alt="Star History Chart"\n    src="https://api.star-history.com/svg?repos=EthicalML/awesome-production-machine-learning&type=Date"\n  />\n</picture>\n\n## 10 Min Video Overview\n\n<table>\n  <tr>\n    <td width="30%">\n        This <a href="https://www.youtube.com/watch?v=Ynb6X0KZKxY">10 minute video</a> provides an overview of the motivations for machine learning operations as well as a high level overview on some of the tools in this repo. This <a href="https://www.youtube.com/watch?v=NycftytgPnk">newer video</a> covers the an updated 2024 version of the state of MLOps.\n    </td>\n    <td width="70%">\n        <a href="https://www.youtube.com/watch?v=Ynb6X0KZKxY"><img src="images/video.png"></a>\n    </td>\n  </tr>\n</table>\n\n## Want to receive recurrent updates on this repo and other advancements?\n\n<table>\n  <tr>\n    <td width="30%">\n         You can join the <a href="https://ethical.institute/mle.html">Machine Learning Engineer</a> newsletter. Join over 70,000 ML professionals and enthusiasts who receive weekly curated articles & tutorials on production Machine Learning.\n    </td>\n    <td width="70%">\n        <a href="https://ethical.institute/mle.html"><img src="images/mleng.png"></a>\n    </td>\n  </tr>\n  <tr>\n    <td width="30%">\n         Also check out the <a href="https://github.com/EthicalML/awesome-production-genai/">Awesome Production GenAI</a> List, where we aim to map a curated list of awesome open source libraries to deploy, monitor, version and scale your generative artificial intelligence applications and systems.\n    </td>\n    <td width="70%">\n        <a href="https://github.com/EthicalML/awesome-production-genai/"><img src="images/list.jpg"></a>\n    </td>\n  </tr>\n</table>\n\n# Main Content\n\n## AutoML\n* [AutoGluon](https://github.com/autogluon/autogluon) ![](https://img.shields.io/github/stars/autogluon/autogluon.svg?cacheSeconds=86400) - Automated feature, model, and hyperparameter selection for tabular, image, and text data on top of popular machine learning libraries (Scikit-Learn, LightGBM, CatBoost, PyTorch, MXNet).\n* [Autokeras](https://github.com/keras-team/autokeras) ![](https://img.shields.io/github/stars/keras-team/autokeras.svg?cacheSeconds=86400) - AutoML library for Keras based on ["Auto-Keras: Efficient Neural Architecture Search with Network Morphism"](https://arxiv.org/abs/1806.10282).\n* [auto-sklearn](https://github.com/automl/auto-sklearn) ![](https://img.shields.io/github/stars/automl/auto-sklearn.svg?cacheSeconds=86400) - Framework to automate algorithm and hyperparameter tuning for sklearn.\n* [Ax](https://github.com/facebook/Ax) ![](https://img.shields.io/github/stars/facebook/Ax.svg?cacheSeconds=86400) - Ax is an accessible, general-purpose platform for understanding, managing, deploying, and automating adaptive experiments.\n* [BoTorch](https://github.com/meta-pytorch/botorch) ![](https://img.shields.io/github/stars/meta-pytorch/botorch.svg?cacheSeconds=86400) - BoTorch is a library for Bayesian Optimization built on PyTorch.\n* [EvalML](https://github.com/alteryx/evalml) ![](https://img.shields.io/github/stars/alteryx/evalml.svg?cacheSeconds=86400) - EvalML is an AutoML library which builds, optimizes, and evaluates machine learning pipelines using domain-specific objective functions.\n* [Feature Engine](https://github.com/feature-engine/feature_engine) ![](https://img.shields.io/github/stars/feature-engine/feature_engine.svg?cacheSeconds=86400) - Feature-engine is a Python library that contains several transformers to engineer features for use in machine learning models.\n* [Featuretools](https://github.com/alteryx/featuretools) ![](https://img.shields.io/github/stars/alteryx/featuretools.svg?cacheSeconds=86400) - An open source framework for automated feature engineering.\n* [FLAML](https://github.com/microsoft/FLAML) ![](https://img.shields.io/github/stars/microsoft/FLAML.svg?cacheSeconds=86400) - FLAML is a fast library for automated machine learning & tuning.\n* [HEBO](https://github.com/huawei-noah/HEBO) ![](https://img.shields.io/github/stars/huawei-noah/HEBO.svg?cacheSeconds=86400) - Set of open-source hyperparameter optimization frameworks, including the winning submission to the [NeurIPS 2020 Black-Box Optimisation Challenge](https://bbochallenge.com/leaderboard) tested on hyperparameter tuning tasks. \n* [Katib](https://github.com/kubeflow/katib) ![](https://img.shields.io/github/stars/kubeflow/katib.svg?cacheSeconds=86400) - A Kubernetes-based system for Hyperparameter Tuning and Neural Architecture Search.\n* [keras-tuner](https://github.com/keras-team/keras-tuner) ![](https://img.shields.io/github/stars/keras-team/keras-tuner.svg?cacheSeconds=86400) - Keras Tuner is an easy-to-use, distributable hyperparameter optimisation framework that solves the pain points of performing a hyperparameter search. Keras Tuner makes it easy to define a search space and leverage included algorithms to find the best hyperparameter values.\n* [Optuna](https://github.com/optuna/optuna) ![](https://img.shields.io/github/stars/optuna/optuna.svg?cacheSeconds=86400) - Optuna is an automatic hyperparameter optimisation software framework, particularly designed for machine learning.\n* [OSS Vizier](https://github.com/google/vizier) ![](https://img.shields.io/github/stars/google/vizier.svg?cacheSeconds=86400) - OSS Vizier is a Python-based service for black-box optimisation and research, one of the first hyperparameter tuning services designed to work at scale.\n* [TPOT](https://github.com/epistasislab/tpot) ![](https://img.shields.io/github/stars/epistasislab/tpot.svg?cacheSeconds=86400) - Automation of sklearn pipeline creation (including feature selection, pre-processor, etc.).\n* [tsfresh](https://github.com/blue-yonder/tsfresh) ![](https://img.shields.io/github/stars/blue-yonder/tsfresh.svg?cacheSeconds=86400) - Automatic extraction of relevant features from time series.\n\n## Computation and Communication Optimisation\n\n* [Accelerate](https://github.com/huggingface/accelerate) ![](https://img.shields.io/github/stars/huggingface/accelerate.svg?cacheSeconds=86400) - Accelerate abstracts exactly and only the boilerplate code related to multi-GPU/TPU/mixed-precision and leaves the rest of your code unchanged.\n* [Adapters](https://github.com/adapter-hub/adapters) ![](https://img.shields.io/github/stars/adapter-hub/adapters.svg?cacheSeconds=86400) - Adapters is a unified library for parameter-efficient and modular transfer learning.\n* [BitBLAS](https://github.com/microsoft/BitBLAS) ![](https://img.shields.io/github/stars/microsoft/BitBLAS.svg?cacheSeconds=86400) - BitBLAS is a library to support mixed-precision BLAS operations on GPUs\n* [Colossal-AI](https://github.com/hpcaitech/ColossalAI) ![](https://img.shields.io/github/stars/hpcaitech/ColossalAI.svg?cacheSeconds=86400) - A unified deep learning system for big model era, which helps users to efficiently and quickly deploy large AI model training and inference.\n* [Composer](https://github.com/mosaicml/composer) ![](https://img.shields.io/github/stars/mosaicml/composer.svg?cacheSeconds=86400) - Composer is a PyTorch library that enables you to train neural networks faster, at lower cost, and to higher accuracy.\n* [CuDF](https://github.com/rapidsai/cudf) ![](https://img.shields.io/github/stars/rapidsai/cudf.svg?cacheSeconds=86400) - Built based on the Apache Arrow columnar memory format, cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data.\n* [CuML](https://github.com/rapidsai/cuml) ![](https://img.shields.io/github/stars/rapidsai/cuml.svg?cacheSeconds=86400) - cuML is a suite of libraries that implement machine learning algorithms and mathematical primitives functions that share compatible APIs with other RAPIDS projects.\n* [CuPy](https://github.com/cupy/cupy) ![](https://img.shields.io/github/stars/cupy/cupy.svg?cacheSeconds=86400) - An implementation of NumPy-compatible multi-dimensional array on CUDA. CuPy consists of the core multi-dimensional array class, cupy.ndarray, and many functions on it.\n* [DEAP](https://github.com/DEAP/deap) ![](https://img.shields.io/github/stars/DEAP/deap.svg?cacheSeconds=86400) - A novel evolutionary computation framework for rapid prototyping and testing of ideas. It seeks to make algorithms explicit and data structures transparent. It works in perfect harmony with parallelisation mechanisms such as multiprocessing and SCOOP.\n* [DeepEP](https://github.com/deepseek-ai/DeepEP) ![](https://img.shields.io/github/stars/deepseek-ai/DeepEP.svg?cacheSeconds=86400) - DeepEP is a communication library tailored for Mixture-of-Experts (MoE) and expert parallelism (EP). It provides high-throughput and low-latency all-to-all GPU kernels, which are also known as MoE dispatch and combine. The library also supports low-precision operations, including FP8.\n* [DGL](https://github.com/dmlc/dgl) ![](https://img.shields.io/github/stars/dmlc/dgl.svg?cacheSeconds=86400) - DGL is an easy-to-use, high performance and scalable Python package for deep learning on graphs.\n* [DLRover](https://github.com/intelligent-machine-learning/dlrover) ![](https://img.shields.io/github/stars/intelligent-machine-learning/dlrover.svg?cacheSeconds=86400) - DLRover makes the distributed training of large AI models easy, stable, fast and green.\n* [Dask](https://github.com/dask/dask) ![](https://img.shields.io/github/stars/dask/dask.svg?cacheSeconds=86400) - Distributed parallel processing framework for Pandas and NumPy computations.\n* [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) ![](https://img.shields.io/github/stars/deepspeedai/DeepSpeed.svg?cacheSeconds=86400) - DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.\n* [FlagGems](https://github.com/FlagOpen/FlagGems) ![](https://img.shields.io/github/stars/FlagOpen/FlagGems.svg?cacheSeconds=86400) - FlagGems is a high-performance general operator library implemented in OpenAI Triton. It builds on a collection of backend neutral kernels that aims to accelerate LLM training and inference across diverse hardware platforms.\n* [Flashlight](https://github.com/flashlight/flashlight) ![](https://img.shields.io/github/stars/flashlight/flashlight.svg?cacheSeconds=86400) - A fast, flexible machine learning library written entirely in C++ from the Facebook AI Research and the creators of Torch, TensorFlow, Eigen and Deep Speech.\n* [Flax](https://github.com/google/flax) ![](https://img.shields.io/github/stars/google/flax.svg?cacheSeconds=86400) - A neural network library and ecosystem for JAX designed for flexibility.\n* [GPUStack](https://github.com/gpustack/gpustack) ![](https://img.shields.io/github/stars/gpustack/gpustack.svg?cacheSeconds=86400) - GPUStack is an open-source GPU cluster manager for running AI models.\n* [Hivemind](https://github.com/learning-at-home/hivemind) ![](https://img.shields.io/github/stars/learning-at-home/hivemind.svg?cacheSeconds=86400) - Decentralized deep learning in PyTorch.\n* [Horovod](https://github.com/horovod/horovod) ![](https://img.shields.io/github/stars/horovod/horovod.svg?cacheSeconds=86400) - Uber''s distributed training framework for TensorFlow, Keras, and PyTorch.\n* [Jax](https://github.com/jax-ml/jax) ![](https://img.shields.io/github/stars/jax-ml/jax.svg?cacheSeconds=86400) - Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more.\n* [Kompute](https://github.com/lava-nc/lava) ![](https://img.shields.io/github/stars/lava-nc/lava.svg?cacheSeconds=86400) - Blazing fast, lightweight and mobile phone-enabled Vulkan compute framework optimized for advanced GPU data processing usecases.\n* [Lava](https://github.com/KomputeProject/kompute) ![](https://img.shields.io/github/stars/KomputeProject/kompute.svg?cacheSeconds=86400) - Lava is an open source framework to develop applications for neuromorphic hardware architectures.\n* [Liger Kernel](https://github.com/linkedin/Liger-Kernel) ![](https://img.shields.io/github/stars/linkedin/Liger-Kernel.svg?cacheSeconds=86400) - Liger Kernel is a collection of Triton kernels designed specifically for LLM training.\n* [LightGBM](https://github.com/microsoft/LightGBM) ![](https://img.shields.io/github/stars/microsoft/LightGBM.svg?cacheSeconds=86400) - LightGBM is a gradient boosting framework that uses tree based learning algorithms.\n* [MLX](https://github.com/ml-explore/mlx) ![](https://img.shields.io/github/stars/ml-explore/mlx.svg?cacheSeconds=86400) - MLX is an array framework for machine learning on Apple silicon.\n* [Modin](https://github.com/modin-project/modin) ![](https://img.shields.io/github/stars/modin-project/modin.svg?cacheSeconds=86400) - Speed up your Pandas workflows by changing a single line of code.\n* [NVIDIA TensorRT](https://github.com/NVIDIA/TensorRT) ![](https://img.shields.io/github/stars/NVIDIA/TensorRT.svg?cacheSeconds=86400) - TensorRT is a C++ library for high-performance inference on NVIDIA GPUs and deep learning accelerators.\n* [Nevergrad](https://github.com/facebookresearch/nevergrad) ![](https://img.shields.io/github/stars/facebookresearch/nevergrad.svg?cacheSeconds=86400) - Nevergrad is a gradient-free optimisation platform.\n* [Norse](https://github.com/norse/norse) ![](https://img.shields.io/github/stars/norse/norse.svg?cacheSeconds=86400) - Norse aims to exploit the advantages of bio-inspired neural components, which are sparse and event-driven - a fundamental difference from artificial neural networks.\n* [Numba](https://github.com/numba/numba) ![](https://img.shields.io/github/stars/numba/numba.svg?cacheSeconds=86400)  - A compiler for Python array and numerical functions.\n* [Optimum](https://github.com/huggingface/optimum) ![](https://img.shields.io/github/stars/huggingface/optimum.svg?cacheSeconds=86400) - Optimum is an extension of Transformers and Diffusers, providing a set of optimization tools enabling maximum efficiency to train and run models on targeted hardware while keeping things easy to use.\n* [PEFT](https://github.com/huggingface/peft) ![](https://img.shields.io/github/stars/huggingface/peft.svg?cacheSeconds=86400) - Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model''s parameters.\n* [PaddlePaddle](https://github.com/PaddlePaddle/Paddle) ![](https://img.shields.io/github/stars/PaddlePaddle/Paddle.svg?cacheSeconds=86400) - PaddlePaddle is a framework to perform large-scale deep network training, using data sources distributed across hundreds of nodes. \n* [PyG](https://github.com/pyg-team/pytorch_geometric) ![](https://img.shields.io/github/stars/pyg-team/pytorch_geometric.svg?cacheSeconds=86400) - PyG (PyTorch Geometric) is a library built upon PyTorch to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data.\n* [PyTorch Lightning](https://github.com/Lightning-AI/pytorch-lightning) ![](https://img.shields.io/github/stars/Lightning-AI/pytorch-lightning.svg?cacheSeconds=86400) - PyTorch Lightning pretrains, finetunes and deploys AI models on multiple GPUs, TPUs with zero code changes.\n* [PyTorch](https://github.com/pytorch/pytorch) ![](https://img.shields.io/github/stars/pytorch/pytorch.svg?cacheSeconds=86400) - PyTorch is a library to develop and train neural network based deep learning models.\n* [Ray](https://github.com/ray-project/ray) ![](https://img.shields.io/github/stars/ray-project/ray.svg?cacheSeconds=86400) - Ray is a flexible, high-performance distributed execution framework for machine learning.\n* [SetFit](https://github.com/huggingface/setfit) ![](https://img.shields.io/github/stars/huggingface/setfit.svg?cacheSeconds=86400) - SetFit is an efficient and prompt-free framework for few-shot fine-tuning of Sentence Transformers.\n* [Sonnet](https://github.com/google-deepmind/sonnet) ![](https://img.shields.io/github/stars/google-deepmind/sonnet.svg?cacheSeconds=86400) - Sonnet is a library built on top of TensorFlow 2 designed to provide simple, composable abstractions for machine learning research.\n* [Streaming](https://github.com/mosaicml/streaming) ![](https://img.shields.io/github/stars/mosaicml/streaming.svg?cacheSeconds=86400) - A Data Streaming Library for Efficient Neural Network Training.\n* [TensorFlow](https://github.com/tensorflow/tensorflow) ![](https://img.shields.io/github/stars/tensorflow/tensorflow.svg?cacheSeconds=86400) - TensorFlow is a leading library designed for developing and deploying state-of-the-art  machine learning applications.\n* [ThunderKittens](https://github.com/HazyResearch/ThunderKittens) ![](https://img.shields.io/github/stars/HazyResearch/ThunderKittens.svg?cacheSeconds=86400) ThunderKittens is a framework to make it easy to write fast deep learning kernels in CUDA.\n* [TorchOpt](https://github.com/metaopt/torchopt) ![](https://img.shields.io/github/stars/metaopt/torchopt.svg?cacheSeconds=86400) - TorchOpt is an efficient library for differentiable optimization built upon PyTorch.\n* [Triton](https://github.com/triton-lang/triton) ![](https://img.shields.io/github/stars/triton-lang/triton.svg?cacheSeconds=86400) - Triton is a language and compiler for writing highly efficient custom Deep-Learning primitives. The aim of Triton is to provide an open-source environment to write fast code at higher productivity than CUDA, but also with higher flexibility than other existing DSLs.\n* [Vaex](https://github.com/vaexio/vaex) ![](https://img.shields.io/github/stars/vaexio/vaex.svg?cacheSeconds=86400) Vaex is a high performance Python library for lazy Out-of-Core DataFrames (similar to Pandas), to visualize and explore big tabular datasets. Vaex uses memory mapping, zero memory copy policy and lazy computations for best performance (no memory wasted).\n* [Vowpal Wabbit](https://github.com/VowpalWabbit/vowpal_wabbit) ![](https://img.shields.io/github/stars/VowpalWabbit/vowpal_wabbit.svg?cacheSeconds=86400) Vowpal Wabbit is a machine learning system which pushes the frontier of machine learning with techniques such as online, hashing, allreduce, reductions, learning2search, active, and interactive learning.\n* [XGBoost](https://github.com/dmlc/xgboost) ![](https://img.shields.io/github/stars/dmlc/xgboost.svg?cacheSeconds=86400) - XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.\n* [YDF](https://github.com/google/yggdrasil-decision-forests) ![](https://img.shields.io/github/stars/google/yggdrasil-decision-forests.svg?cacheSeconds=86400) - YDF (Yggdrasil Decision Forests) is a library to train, evaluate, interpret, and serve Random Forest, Gradient Boosted Decision Trees, CART and Isolation forest models.\n* [bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes) ![](https://img.shields.io/github/stars/bitsandbytes-foundation/bitsandbytes.svg?cacheSeconds=86400) - Bitsandbytes library is a lightweight Python wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and 8 & 4-bit quantization functions.\n* [einops](https://github.com/arogozhnikov/einops) ![](https://img.shields.io/github/stars/arogozhnikov/einops.svg?cacheSeconds=86400) - Flexible and powerful tensor operations for readable and reliable code.\n* [scikit-learn](https://github.com/scikit-learn/scikit-learn) ![](https://img.shields.io/github/stars/scikit-learn/scikit-learn.svg?cacheSeconds=86400) - Scikit-learn is a powerful machine learning library that provides a wide variety of modules for data access, data preparation and statistical model building. \n* [snnTorch](https://github.com/jeshraghian/snntorch) ![](https://img.shields.io/github/stars/jeshraghian/snntorch.svg?cacheSeconds=86400) - snnTorch is a deep and online learning library with spiking neural networks.\n* [torchdistill](https://github.com/yoshitomo-matsubara/torchdistill) ![](https://img.shields.io/github/stars/yoshitomo-matsubara/torchdistill.svg?cacheSeconds=86400) - torchdistill offers various state-of-the-art knowledge distillation methods and enables you to design (new) experiments simply by editing a declarative yaml config file instead of Python code.\n* [torchkeras](https://github.com/lyhue1991/torchkeras?tab=readme-ov-file) ![](https://img.shields.io/github/stars/lyhue1991/torchkeras?tab=readme-ov-file.svg?cacheSeconds=86400) The torchkeras library is a simple tool for training neural network in pytorch jusk in a keras style.\n* [veScale](https://github.com/volcengine/veScale) ![](https://img.shields.io/github/stars/volcengine/veScale.svg?cacheSeconds=86400) - veScale is a PyTorch native LLM training framework.\n* [yellowbrick](https://github.com/DistrictDataLabs/yellowbrick) ![](https://img.shields.io/github/stars/DistrictDataLabs/yellowbrick.svg?cacheSeconds=86400) - yellowbrick is a matplotlib-based model evaluation plots for scikit-learn and other machine learning libraries.\n\n## Data Annotation and Synthesis\n* [Argilla](https://github.com/argilla-io/argilla) ![](https://img.shields.io/github/stars/argilla-io/argilla.svg?cacheSeconds=86400) - Argilla helps domain experts and data teams to build better NLP datasets in less time.\n* [cleanlab](https://github.com/cleanlab/cleanlab) ![](https://img.shields.io/github/stars/cleanlab/cleanlab.svg?cacheSeconds=86400) - Python library for data-centric AI. Can automatically: find mislabeled data, detect outliers, estimate consensus + annotator-quality for multi-annotator datasets, suggest which data is best to (re)label next.\n* [COCO Annotator](https://github.com/jsbroks/coco-annotator) ![](https://img.shields.io/github/stars/jsbroks/coco-annotator.svg?cacheSeconds=86400) - Web-based image segmentation tool for object detection, localization and keypoints\n* [CVAT](https://github.com/cvat-ai/cvat) ![](https://img.shields.io/github/stars/cvat-ai/cvat.svg?cacheSeconds=86400) - CVAT (Computer Vision Annotation Tool) is OpenCV''s web-based annotation tool for both videos and images for computer algorithms.\n* [Doccano](https://github.com/doccano/doccano) ![](https://img.shields.io/github/stars/doccano/doccano.svg?cacheSeconds=86400) - Open source text annotation tools for humans, providing functionality for sentiment analysis, named entity recognition, and machine translation.\n* [Gretel Synthetics](https://github.com/gretelai/gretel-synthetics) ![](https://img.shields.io/github/stars/gretelai/gretel-synthetics.svg?cacheSeconds=86400) - Gretel Synthetics is a synthetic data generators for structured and unstructured text, featuring differentially private learning.\n* [Label Studio](https://github.com/HumanSignal/label-studio) ![](https://img.shields.io/github/stars/HumanSignal/label-studio.svg?cacheSeconds=86400) - Multi-domain data labeling and annotation tool with standardized output format.\n* [NeMo Curator](https://github.com/NVIDIA/NeMo-Curator) ![](https://img.shields.io/github/stars/NVIDIA/NeMo-Curator.svg?cacheSeconds=86400) - NeMo Curator is a GPU-accelerated framework for efficient large language model data curation.\n* [refinery](https://github.com/code-kern-ai/refinery) ![](https://img.shields.io/github/stars/code-kern-ai/refinery.svg?cacheSeconds=86400) - The data scientist''s open-source choice to scale, assess and maintain natural language data.\n* [SDV](https://github.com/sdv-dev/SDV) ![](https://img.shields.io/github/stars/sdv-dev/SDV.svg?cacheSeconds=86400) - Synthetic Data Vault (SDV) is a Synthetic Data Generation ecosystem of libraries that allows users to easily learn single-table, multi-table and timeseries datasets to later on generate new Synthetic Data that has the same format and statistical properties as the original dataset.\n* [Semantic Segmentation Editor](https://github.com/Hitachi-Automotive-And-Industry-Lab/semantic-segmentation-editor) ![](https://img.shields.io/github/stars/Hitachi-Automotive-And-Industry-Lab/semantic-segmentation-editor.svg?cacheSeconds=86400) - Hitachi''s Open source tool for labelling camera and LIDAR data.\n* [synthcity](https://github.com/vanderschaarlab/synthcity) ![](https://img.shields.io/github/stars/vanderschaarlab/synthcity.svg?cacheSeconds=86400) - synthcity is a library for generating and evaluating synthetic tabular data.\n* [ViPE](https://github.com/nv-tlabs/vipe) ![](https://img.shields.io/github/stars/nv-tlabs/vipe.svg?cacheSeconds=86400) - ViPE is a spatial AI tool for annotating camera poses and dense depth maps from raw videos.\n* [YData Synthetic](https://github.com/ydataai/ydata-synthetic) ![](https://img.shields.io/github/stars/ydataai/ydata-synthetic.svg?cacheSeconds=86400) - YData Synthetic is a package to generate synthetic tabular and time-series data leveraging the state of the art generative models.\n\n## Data Pipeline\n* [Apache Airflow](https://github.com/apache/airflow) ![](https://img.shields.io/github/stars/apache/airflow.svg?cacheSeconds=86400) - Data Pipeline framework built in Python, including scheduler, DAG definition and a UI for visualisation.\n* [Apache Nifi](https://github.com/apache/nifi) ![](https://img.shields.io/github/stars/apache/nifi.svg?cacheSeconds=86400) - Apache NiFi was made for dataflow. It supports highly configurable directed graphs of data routing, transformation, and system mediation logic.\n* [Apache Oozie](https://github.com/apache/oozie) ![](https://img.shields.io/github/stars/apache/oozie.svg?cacheSeconds=86400) - Workflow scheduler for Hadoop jobs.\n* [Argo Workflows](https://github.com/argoproj/argo-workflows) ![](https://img.shields.io/github/stars/argoproj/argo-workflows.svg?cacheSeconds=86400) - Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. Argo Workflows is implemented as a Kubernetes CRD (Custom Resource Definition).\n* [Couler](https://github.com/couler-proj/couler) ![](https://img.shields.io/github/stars/couler-proj/couler.svg?cacheSeconds=86400) - Unified interface for constructing and managing machine learning workflows on different workflow engines, such as Argo Workflows, Tekton Pipelines, and Apache Airflow.\n* [DataTrove](https://github.com/huggingface/datatrove) ![](https://img.shields.io/github/stars/huggingface/datatrove.svg?cacheSeconds=86400) - DataTrove is a library to process, filter and deduplicate text data at a very large scale.\n* [Dagster](https://github.com/dagster-io/dagster) ![](https://img.shields.io/github/stars/dagster-io/dagster.svg?cacheSeconds=86400) - A data orchestrator for machine learning, analytics, and ETL.\n* [DBT](https://github.com/dbt-labs/dbt-core) ![](https://img.shields.io/github/stars/dbt-labs/dbt-core.svg?cacheSeconds=86400) - ETL tool for running transformations inside data warehouses.\n* [Flyte](https://github.com/flyteorg/flyte) ![](https://img.shields.io/github/stars/flyteorg/flyte.svg?cacheSeconds=86400) - Lyft’s Cloud Native Machine Learning and Data Processing Platform - [(Demo)](https://youtu.be/KdUJGSP1h9U?t=1451).\n* [Genie](https://github.com/Netflix/genie) ![](https://img.shields.io/github/stars/Netflix/genie.svg?cacheSeconds=86400) - Job orchestration engine to interface and trigger the execution of jobs from Hadoop-based systems.\n* [Hamilton](https://github.com/dagworks-inc/hamilton) ![](https://img.shields.io/github/stars/dagworks-inc/hamilton.svg?cacheSeconds=86400) - Hamilton is a micro-orchestration framework for defining dataflows. Runs anywhere python runs (e.g. jupyter, fastAPI, spark, ray, dask). Brings software engineering best practices without you knowing it. Use it to define feature engineering transforms, end-to-end model pipelines, and LLM workflows. It complements macro-orchestration systems (e.g. kedro, luigi, airflow, dbt, etc.) as it replaces the code within those macro tasks. Comes with a self-hostable UI that captures lineage & provenance, execution telemetry & data summaries, and builds a self-populating catalog; usable in development as well as production.\n* [Instill VDP](https://github.com/instill-ai/instill-core) ![](https://img.shields.io/github/stars/instill-ai/instill-core.svg?cacheSeconds=86400) - Instill VDP (Versatile Data Pipeline) aims to streamline the data processing pipelines from inception to completion.\n* [Instructor](https://github.com/instructor-ai/instructor) ![](https://img.shields.io/github/stars/instructor-ai/instructor.svg?cacheSeconds=86400) - Instructor makes it easy to get structured data like JSON from LLMs like GPT-3.5, GPT-4, GPT-4-Vision, and open-source models.\n* [Kedro](https://github.com/kedro-org/kedro) ![](https://img.shields.io/github/stars/kedro-org/kedro.svg?cacheSeconds=86400) - Kedro is a workflow development tool that helps you build data pipelines that are robust, scalable, deployable, reproducible and versioned.\n* [Luigi](https://github.com/spotify/luigi) ![](https://img.shields.io/github/stars/spotify/luigi.svg?cacheSeconds=86400) - Luigi is a Python module that helps you build complex pipelines of batch jobs, handling dependency resolution, workflow management, visualisation, etc..\n* [Metaflow](https://github.com/Netflix/metaflow) ![](https://img.shields.io/github/stars/Netflix/metaflow.svg?cacheSeconds=86400) - A framework for data scientists to easily build and manage real-life data science projects.\n* [Pachyderm](https://github.com/pachyderm/pachyderm) ![](https://img.shields.io/github/stars/pachyderm/pachyderm.svg?cacheSeconds=86400) - Open source distributed processing framework build on Kubernetes focused mainly on dynamic building of production machine learning pipelines - [(Video)](https://www.youtube.com/watch?v=LamKVhe2RSM).\n* [Ploomber](https://github.com/ploomber/ploomber) ![](https://img.shields.io/github/stars/ploomber/ploomber.svg?cacheSeconds=86400) - The fastest way to build data pipelines. Develop iteratively, deploy anywhere.\n* [Pixeltable](https://github.com/pixeltable/pixeltable) ![](https://img.shields.io/github/stars/pixeltable/pixeltable.svg?cacheSeconds=86400) – Open-source Python library providing declarative, incremental data infrastructure for building and managing multimodal AI workloads.\n* [Prefect Core](https://github.com/PrefectHQ/prefect) ![](https://img.shields.io/github/stars/PrefectHQ/prefect.svg?cacheSeconds=86400) - Workflow management system that makes it easy to take your data pipelines and add semantics like retries, logging, dynamic mapping, caching, failure notifications, and more.\n* [SeqIO](https://github.com/google/seqio) ![](https://img.shields.io/github/stars/google/seqio.svg?cacheSeconds=86400) - SeqIO is a library for processing sequential data to be fed into downstream sequence models.\n* [Snakemake](https://github.com/snakemake/snakemake) ![](https://img.shields.io/github/stars/snakemake/snakemake.svg?cacheSeconds=86400) - Workflow management system for reproducible and scalable data analyses.\n* [Towhee](https://github.com/towhee-io/towhee) ![](https://img.shields.io/github/stars/towhee-io/towhee.svg?cacheSeconds=86400) - General-purpose machine learning pipeline for generating embedding vectors using one or many ML models.\n* [unstructured](https://github.com/Unstructured-IO/unstructured) ![](https://img.shields.io/github/stars/Unstructured-IO/unstructured.svg?cacheSeconds=86400) - unstructured streamlines and optimizes the data processing workflow for LLMs, ingesting and pre-processing images and text documents, such as PDFs, HTML, Word docs, and many more. \n* [ZenML](https://github.com/zenml-io/zenml) ![](https://img.shields.io/github/stars/zenml-io/zenml.svg?cacheSeconds=86400) - ZenML is an extensible, open-source MLOps framework to create reproducible ML pipelines with a focus on automated metadata tracking, caching, and many integrations to other tools.\n\n## Data Science Notebook\n* [Apache Zeppelin](https://github.com/apache/zeppelin) ![](https://img.shields.io/github/stars/apache/zeppelin.svg?cacheSeconds=86400) - Web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala and more.\n* [Deepnote](https://github.com/deepnote/deepnote) ![](https://img.shields.io/github/stars/deepnote/deepnote.svg?cacheSeconds=86400) - Deepnote is a drop-in replacement for Jupyter with an AI-first design, sleek UI, new blocks, and native data integrations. Use Python, R, and SQL locally in your favorite IDE, then scale to Deepnote cloud for real-time collaboration, Deepnote agent, and deployable data apps.\n* [Jupyter Notebooks](https://github.com/jupyter/notebook) ![](https://img.shields.io/github/stars/jupyter/notebook.svg?cacheSeconds=86400) - Web interface python sandbox environments for reproducible development\n* [Marimo](https://github.com/marimo-team/marimo) ![](https://img.shields.io/github/stars/marimo-team/marimo.svg?cacheSeconds=86400) - Reactive Python notebook — run reproducible experiments, execute as a script, deploy as an app, and version with git.\n* [Papermill](https://github.com/nteract/papermill) ![](https://img.shields.io/github/stars/nteract/papermill.svg?cacheSeconds=86400) - Papermill is a library for parameterizing notebooks and executing them like Python scripts.\n* [Polynote](https://github.com/polynote/polynote) ![](https://img.shields.io/github/stars/polynote/polynote.svg?cacheSeconds=86400) - Polynote is an experimental polyglot notebook environment. Currently, it supports Scala and Python (with or without Spark), SQL, and Vega.\n* [RMarkdown](https://github.com/rstudio/rmarkdown) ![](https://img.shields.io/github/stars/rstudio/rmarkdown.svg?cacheSeconds=86400) - The rmarkdown package is a next generation implementation of R Markdown based on Pandoc.\n* [Stencila](https://github.com/stencila/stencila) ![](https://img.shields.io/github/stars/stencila/stencila.svg?cacheSeconds=86400) - Stencila is a platform for creating, collaborating on, and sharing data driven content. Content that is transparent and reproducible.\n* [Voilà](https://github.com/voila-dashboards/voila) ![](https://img.shields.io/github/stars/voila-dashboards/voila.svg?cacheSeconds=86400) - Voilà turns Jupyter notebooks into standalone web applications that can e.g. be used as dashboards.\n* [.NET Interactive](https://github.com/dotnet/interactive) ![](https://img.shields.io/github/stars/dotnet/interactive.svg?cacheSeconds=86400) - .NET Interactive takes the power of .NET and embeds it into your interactive experiences.\n\n## Data Storage Optimisation\n* [AIStore](https://github.com/NVIDIA/aistore) ![](https://img.shields.io/github/stars/NVIDIA/aistore.svg?cacheSeconds=86400) - AIStore is a lightweight object storage system with the capability to linearly scale out with each added storage node and a special focus on petascale deep learning.\n* [Alluxio](https://github.com/Alluxio/alluxio) ![](https://img.shields.io/github/stars/Alluxio/alluxio.svg?cacheSeconds=86400) - A virtual distributed storage system that bridges the gab between computation frameworks and storage systems.\n* [Apache Arrow](https://github.com/apache/arrow) ![](https://img.shields.io/github/stars/apache/arrow.svg?cacheSeconds=86400) - In-memory columnar representation of data compatible with Pandas, Hadoop-based systems, etc..\n* [Apache Druid](https://github.com/apache/druid) ![](https://img.shields.io/github/stars/apache/druid.svg?cacheSeconds=86400) - A high performance real-time analytics database. Check this [article](https://towardsdatascience.com/introduction-to-druid-4bf285b92b5a) for introduction.\n* [Apache Hudi](https://github.com/apache/hudi) ![](https://img.shields.io/github/stars/apache/hudi.svg?cacheSeconds=86400) - Hudi is a transactional data lake platform that brings core warehouse and database functionality directly to a data lake. Hudi is great for streaming workloads, and also allows creation of efficient incremental batch pipelines. Supports popular query engines including Spark, Flink, Presto, Trino, Hive, etc. More info [here](https://hudi.apache.org/).\n* [Apache Iceberg](https://github.com/apache/iceberg) ![](https://img.shields.io/github/stars/apache/iceberg.svg?cacheSeconds=86400) - Iceberg is an ACID-compliant, high-performance format built for huge analytic tables (containing tens of petabytes of data), and it brings the reliability and simplicity of SQL tables to big data, while making it possible for engines like Spark, Trino, Flink, Presto, Hive and Impala to safely work with the same tables, at the same time. More info [here](https://iceberg.apache.org/).\n* [Apache Ignite](https://github.com/apache/ignite) ![](https://img.shields.io/github/stars/apache/ignite.svg?cacheSeconds=86400) - A memory-centric distributed database, caching, and processing platform for transactional, analytical, and streaming workloads delivering in-memory speeds at petabyte scale - [Demo](https://www.youtube.com/watch?v=Xt4PWQ__YPw).\n* [Apache Parquet](https://github.com/apache/parquet-java) ![](https://img.shields.io/github/stars/apache/parquet-java.svg?cacheSeconds=86400) - On-disk columnar representation of data compatible with Pandas, Hadoop-based systems, etc..\n* [Apache Pinot](https://github.com/apache/pinot) ![](https://img.shields.io/github/stars/apache/pinot.svg?cacheSeconds=86400) - A realtime distributed OLAP datastore. Comparison of the open source OLAP systems for big data: ClickHouse, Druid, and Pinot is found [here](https://medium.com/@leventov/comparison-of-the-open-source-olap-systems-for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7).\n* [Casibase](https://github.com/casibase/casibase) ![](https://img.shields.io/github/stars/casibase/casibase.svg?cacheSeconds=86400) - Casibase is a LangChain-like RAG (Retrieval-Augmented Generation) knowledge database with web UI and Enterprise SSO.\n* [Chroma](https://github.com/chroma-core/chroma) ![](https://img.shields.io/github/stars/chroma-core/chroma.svg?cacheSeconds=86400) - Chroma is an open-source embedding database.\n* [ClickHouse](https://github.com/ClickHouse/ClickHouse) ![](https://img.shields.io/github/stars/ClickHouse/ClickHouse.svg?cacheSeconds=86400) - ClickHouse is an open source column oriented database management system.\n* [Delta Lake](https://github.com/delta-io/delta) ![](https://img.shields.io/github/stars/delta-io/delta.svg?cacheSeconds=86400) - Delta Lake is a storage layer that brings scalable, ACID transactions to Apache Spark and other big-data engines.\n* [EdgeDB](https://github.com/geldata/gel) ![](https://img.shields.io/github/stars/geldata/gel.svg?cacheSeconds=86400) - Gel supercharges Postgres with a modern data model, graph queries, Auth & AI solutions, and much more.\n* [GPTCache](https://github.com/zilliztech/GPTCache) ![](https://img.shields.io/github/stars/zilliztech/GPTCache.svg?cacheSeconds=86400) - GPTCache is a library for creating semantic cache for large language model queries.\n* [InfluxDB](https://github.com/influxdata/influxdb) ![](https://img.shields.io/github/stars/influxdata/influxdb.svg?cacheSeconds=86400) Scalable datastore for metrics, events, and real-time analytics.\n* [Milvus](https://github.com/milvus-io/milvus) ![](https://img.shields.io/github/stars/milvus-io/milvus.svg?cacheSeconds=86400) Milvus is a cloud-native, open-source vector database built to manage embedding vectors generated by machine learning models and neural networks.\n* [Marqo](https://github.com/marqo-ai/marqo) ![](https://img.shields.io/github/stars/marqo-ai/marqo.svg?cacheSeconds=86400) Marqo is an end-to-end vector search engine.\n* [pgvector](https://github.com/pgvector/pgvector) ![](https://img.shields.io/github/stars/pgvector/pgvector.svg?cacheSeconds=86400) pgvector helps with vector similarity search for Postgres.\n* [PostgresML](https://github.com/postgresml/postgresml) ![](https://img.shields.io/github/stars/postgresml/postgresml.svg?cacheSeconds=86400) PostgresML is a machine learning extension for PostgreSQL that enables you to perform training and inference on text and tabular data using SQL queries.\n* [Safetensors](https://github.com/huggingface/safetensors) ![](https://img.shields.io/github/stars/huggingface/safetensors.svg?cacheSeconds=86400) Simple, safe way to store and distribute tensors.\n* [TimescaleDB](https://github.com/timescale/timescaledb) ![](https://img.shields.io/github/stars/timescale/timescaledb.svg?cacheSeconds=86400) An open-source time-series SQL database optimized for fast ingest and complex queries packaged as a PostgreSQL extension - [(Video)](https://www.youtube.com/watch?v=zbjub8BQPyE).\n* [Weaviate](https://github.com/weaviate/weaviate) ![](https://img.shields.io/github/stars/weaviate/weaviate.svg?cacheSeconds=86400) - A low-latency vector search engine (GraphQL, RESTful) with out-of-the-box support for different media types. Modules include Semantic Search, Q&A, Classification, Customizable Models (PyTorch/TensorFlow/Keras), and more.\n* [Zarr](https://github.com/zarr-developers/zarr-python) ![](https://img.shields.io/github/stars/zarr-developers/zarr-python.svg?cacheSeconds=86400) - Python implementation of chunked, compressed, N-dimensional arrays designed for use in parallel computing.\n\n## Data Stream Processing\n* [Apache Beam](https://github.com/apache/beam) ![](https://img.shields.io/github/stars/apache/beam.svg?cacheSeconds=86400) Apache Beam is a unified programming model for Batch and Streaming.\n* [Apache Flink](https://github.com/apache/flink) ![](https://img.shields.io/github/stars/apache/flink.svg?cacheSeconds=86400) - Open source stream processing framework with powerful stream and batch processing capabilities.\n* [Apache Kafka](https://github.com/apache/kafka) ![](https://img.shields.io/github/stars/apache/kafka.svg?cacheSeconds=86400) - Kafka client library for building applications and microservices where the input and output are stored in kafka clusters.\n* [Apache Samza](https://github.com/apache/samza) ![](https://img.shields.io/github/stars/apache/samza.svg?cacheSeconds=86400) - Distributed stream processing framework. It uses Apache Kafka for messaging, and Apache Hadoop YARN to provide fault tolerance, processor isolation, security, and resource management.\n* [Apache Spark](https://github.com/apache/spark) ![](https://img.shields.io/github/stars/apache/spark.svg?cacheSeconds=86400) - Micro-batch processing for streams using the apache spark framework as a backend supporting stateful exactly-once semantics.\n* [Bytewax](https://github.com/bytewax/bytewax) ![](https://img.shields.io/github/stars/bytewax/bytewax.svg?cacheSeconds=86400) - Flexible Python-centric stateful stream processing framework built on top of Rust engine.\n* [FastStream](https://github.com/airtai/faststream) ![](https://img.shields.io/github/stars/airtai/faststream.svg?cacheSeconds=86400) - A modern broker-agnostic streaming Python framework supporting Apache Kafka, RabbitMQ and NATS protocols, inspired by FastAPI and easily integratable with other web frameworks.\n* [MOA](https://github.com/Waikato/moa) ![](https://img.shields.io/github/stars/Waikato/moa.svg?cacheSeconds=86400) - MOA (Massive Online Analysis) is an open source framework for Big Data stream mining.\n* [MosaicML Streaming](https://github.com/mosaicml/streaming) ![](https://img.shields.io/github/stars/mosaicml/streaming.svg?cacheSeconds=86400) - Fast, deterministic streaming of large datasets from cloud storage for distributed model training.\n* [RisingWave](https://github.com/risingwavelabs/risingwave) ![](https://img.shields.io/github/stars/risingwavelabs/risingwave.svg?cacheSeconds=86400) - A distributed SQL streaming database that unifies stream processing and low-latency serving, ideal for building and serving features for online machine learning.\n* [TensorStore](https://github.com/google/tensorstore) ![](https://img.shields.io/github/stars/google/tensorstore.svg?cacheSeconds=86400) - Library for reading and writing large multi-dimensional arrays.\n\n\n## Deployment and Serving\n* [Agenta](https://github.com/Agenta-AI/agenta) ![](https://img.shields.io/github/stars/Agenta-AI/agenta.svg?cacheSeconds=86400) - Agenta provides end-to-end tools for the entire LLMOps workflow: building (LLM playground, evaluation), deploying (prompt and configuration management), and  (LLM observability and tracing).\n* [AirLLM](https://github.com/lyogavin/airllm) ![](https://img.shields.io/github/stars/lyogavin/airllm.svg?cacheSeconds=86400) - AirLLM optimizes inference memory usage, allowing 70B large language models to run inference on a single 4GB GPU card without quantization, distillation and pruning.\n* [AITemplate](https://github.com/facebookincubator/AITemplate) ![](https://img.shields.io/github/stars/facebookincubator/AITemplate.svg?cacheSeconds=86400) - AITemplate (AIT) is a Python framework that transforms deep neural networks into CUDA (NVIDIA GPU) / HIP (AMD GPU) C++ code for lightning-fast inference serving.\n* [BentoML](https://github.com/bentoml/BentoML) ![](https://img.shields.io/github/stars/bentoml/BentoML.svg?cacheSeconds=86400) - BentoML is an open source framework for high performance ML model serving.\n* [BISHENG](https://github.com/dataelement/bisheng) ![](https://img.shields.io/github/stars/dataelement/bisheng.svg?cacheSeconds=86400) - BISHENG is an open LLM application devops platform, focusing on enterprise scenarios.\n* [DeepDetect](https://github.com/jolibrain/deepdetect) ![](https://img.shields.io/github/stars/jolibrain/deepdetect.svg?cacheSeconds=86400) - Machine Learning production server for TensorFlow, XGBoost and Cafe models written in C++ and maintained by Jolibrain.\n* [Dynamo](https://github.com/ai-dynamo/dynamo) ![](https://img.shields.io/github/stars/ai-dynamo/dynamo.svg?cacheSeconds=86400) - NVIDIA Dynamo is a high-throughput, low-latency inference framework designed for serving generative AI and reasoning models in multi-node distributed environments.\n* [exo](https://github.com/exo-explore/exo) ![](https://img.shields.io/github/stars/exo-explore/exo.svg?cacheSeconds=86400) - exo helps you run your AI cluster at home with everyday devices.\n* [Genkit](https://github.com/firebase/genkit) ![](https://img.shields.io/github/stars/firebase/genkit.svg?cacheSeconds=86400) - Genkit is an open source framework for building AI-powered apps with familiar code-centric patterns. Genkit makes it easy to develop, integrate, and test AI features with observability and evaluations.\n* [Inference](https://github.com/roboflow/inference) ![](https://img.shields.io/github/stars/roboflow/inference.svg?cacheSeconds=86400) - A fast, production-ready inference server for computer vision supporting deployment of many popular model architectures and fine-tuned models. With Inference, you can deploy models such as YOLOv5, YOLOv8, CLIP, SAM, and CogVLM on your own hardware using Docker.\n* [Infinity](https://github.com/michaelfeil/infinity) ![](https://img.shields.io/github/stars/michaelfeil/infinity.svg?cacheSeconds=86400) - Infinity is a high-throughput, low-latency REST API for serving text-embeddings, reranking models and clip. \n* [IPEX-LLM](https://github.com/intel/ipex-llm) ![](https://img.shields.io/github/stars/intel/ipex-llm.svg?cacheSeconds=86400) - IPEX-LLM is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very low latency.\n* [LiteLLM](https://github.com/BerriAI/litellm) ![](https://img.shields.io/github/stars/BerriAI/litellm.svg?cacheSeconds=86400) - LiteLLM is a Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq.\n* [Jina-serve](https://github.com/jina-ai/serve) ![](https://img.shields.io/github/stars/jina-ai/serve.svg?cacheSeconds=86400) - Jina-serve is a framework for building and deploying AI services that communicate via gRPC, HTTP and WebSockets.\n* [Kiln](https://github.com/kiln-ai/kiln) ![](https://img.shields.io/github/stars/kiln-ai/kiln.svg?cacheSeconds=86400) - Kiln is an OSS tool for fine-tuning LLM models, synthetic data generation, and collaborating on datasets.\n* [KServe](https://github.com/kserve/kserve) ![](https://img.shields.io/github/stars/kserve/kserve.svg?cacheSeconds=86400) - KServe provides a Kubernetes Custom Resource Definition for serving predictive and generative ML.\n* [KTransformers](https://github.com/kvcache-ai/ktransformers) ![](https://img.shields.io/github/stars/kvcache-ai/ktransformers.svg?cacheSeconds=86400) - KTransformers is a flexible framework for experiencing cutting-edge LLM inference optimizations.\n* [Langtrace](https://github.com/Scale3-Labs/langtrace) ![](https://img.shields.io/github/stars/Scale3-Labs/langtrace.svg?cacheSeconds=86400) - Langtrace is an open-source, Open Telemetry based end-to-end observability tool for LLM applications, providing real-time tracing, evaluations and metrics for popular LLMs, LLM frameworks, vectorDBs and more.\n* [Lepton AI](https://github.com/leptonai/leptonai) ![](https://img.shields.io/github/stars/leptonai/leptonai.svg?cacheSeconds=86400) - LeptonAI Python library allows you to build an AI service from Python code with ease.\n* [LightLLM](https://github.com/ModelTC/lightllm) ![](https://img.shields.io/github/stars/ModelTC/lightllm.svg?cacheSeconds=86400) - LightLLM is a Python-based LLM (Large Language Model) inference and serving framework, notable for its * [llama.cpp](https://github.com/ggml-org/llama.cpp) ![](https://img.shields.io/github/stars/ggml-org/llama.cpp.svg?cacheSeconds=86400) - llama.cpp is an open source software library that performs inference on various large language models such as Llama.\n* [LMDeploy](https://github.com/InternLM/lmdeploy) ![](https://img.shields.io/github/stars/InternLM/lmdeploy.svg?cacheSeconds=86400) - LMDeploy is a toolkit for compressing, deploying, and serving LLM.\n* [LM Studio](https://github.com/lmstudio-ai/lms) ![](https://img.shields.io/github/stars/lmstudio-ai/lms.svg?cacheSeconds=86400) - LM Studio is a tool for deploying LLM models locally on the computer, even on a relatively modest machine, provided it meets the minimum requirements.\n* [LocalAI](https://github.com/mudler/LocalAI) ![](https://img.shields.io/github/stars/mudler/LocalAI.svg?cacheSeconds=86400) - LocalAI is a drop-in replacement REST API that''s compatible with OpenAI API specifications for local inferencing.\n* [MindsDB](https://github.com/mindsdb/mindsdb) ![](https://img.shields.io/github/stars/mindsdb/mindsdb.svg?cacheSeconds=86400) - MindsDB is the platform to create, serve, and fine-tune models in real-time from your database, vector store, and application data.\n* [MLRun](https://github.com/mlrun/mlrun)![](https://img.shields.io/github/stars/mlrun/mlrun.svg?cacheSeconds=86400)- MLRun is an open MLOps framework for quickly building and managing continuous ML and generative AI applications across their lifecycle.\n* [MLServer](https://github.com/SeldonIO/mlserver) ![](https://img.shields.io/github/stars/SeldonIO/mlserver.svg?cacheSeconds=86400) - An inference server for your machine learning models, including support for multiple frameworks, multi-model serving and more.\n* [Mosec](https://github.com/mosecorg/mosec) ![](https://img.shields.io/github/stars/mosecorg/mosec.svg?cacheSeconds=86400) - A rust-powered and multi-stage pipelined model server which offers dynamic batching and more. Super easy to implement and deploy as micro-services.\n* [nndeploy](https://github.com/nndeploy/nndeploy) ![](https://img.shields.io/github/stars/nndeploy/nndeploy.svg?cacheSeconds=86400) - An Easy-to-Use and High-Performance AI deployment framework.\n* [Nuclio](https://github.com/nuclio/nuclio) ![](https://img.shields.io/github/stars/nuclio/nuclio.svg?cacheSeconds=86400) - A high-performance "serverless" framework focused on data, I/O, and compute-intensive workloads. It is well integrated with popular data science tools, such as Jupyter and Kubeflow; supports a variety of data and streaming sources; and supports execution over CPUs and GPUs.\n* [OpenLLM](https://github.com/bentoml/OpenLLM) ![](https://img.shields.io/github/stars/bentoml/OpenLLM.svg?cacheSeconds=86400) - OpenLLM allows developers to run any open-source LLMs (Llama 3.1, Qwen2, Phi3 and more) or custom models as OpenAI-compatible APIs with a single command.\n* [OpenVINO](https://github.com/openvinotoolkit/openvino) ![](https://img.shields.io/github/stars/openvinotoolkit/openvino.svg?cacheSeconds=86400) - OpenVINO is an open-source toolkit for optimizing and deploying AI inference.\n* [Open WebUI](https://github.com/open-webui/open-webui) ![](https://img.shields.io/github/stars/open-webui/open-webui.svg?cacheSeconds=86400) - Open WebUI is an extensible, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline. It supports various LLM runners like Ollama and OpenAI-compatible APIs, with built-in inference engine for RAG, making it a powerful AI deployment solution.\n* [OptiLLM](https://github.com/algorithmicsuperintelligence/optillm) ![](https://img.shields.io/github/stars/algorithmicsuperintelligence/optillm.svg?cacheSeconds=86400) - OptiLLM is an OpenAI API-compatible optimizing inference proxy that implements 20+ state-of-the-art techniques to dramatically improve LLM accuracy and performance on reasoning tasks - without requiring any model training or fine-tuning.\n* [PowerInfer](https://github.com/SJTU-IPADS/PowerInfer) ![](https://img.shields.io/github/stars/SJTU-IPADS/PowerInfer.svg?cacheSeconds=86400) - PowerInfer is a CPU/GPU LLM inference engine leveraging activation locality for your device.\n* [Prompt2Model](https://github.com/neulab/prompt2model) ![](https://img.shields.io/github/stars/neulab/prompt2model.svg?cacheSeconds=86400) - Prompt2Model is a system that takes a natural language task description (like the prompts used for LLMs such as ChatGPT) to train a small special-purpose model that is conducive for deployment.\n* [Seldon Core](https://github.com/SeldonIO/seldon-core) ![](https://img.shields.io/github/stars/SeldonIO/seldon-core.svg?cacheSeconds=86400) - Open source platform for deploying and  machine learning models in Kubernetes - [(Video)](https://www.youtube.com/watch?v=pDlapGtecbY).\n* [SGLang](https://github.com/sgl-project/sglang) ![](https://img.shields.io/github/stars/sgl-project/sglang.svg?cacheSeconds=86400) - SGLang is a fast serving framework for large language models and vision language models.\n* [SkyPilot](https://github.com/skypilot-org/skypilot) ![](https://img.shields.io/github/stars/skypilot-org/skypilot.svg?cacheSeconds=86400) - SkyPilot is a framework for running LLMs, AI, and batch jobs on any cloud, offering maximum cost savings, highest GPU availability, and managed execution.\n* [Tensorflow Serving](https://github.com/tensorflow/serving) ![](https://img.shields.io/github/stars/tensorflow/serving.svg?cacheSeconds=86400) - High-performant framework to serve Tensorflow models via grpc protocol able to handle 100k requests per second per core.\n* [text-generation-inference](https://github.com/huggingface/text-generation-inference) ![](https://img.shields.io/github/stars/huggingface/text-generation-inference.svg?cacheSeconds=86400) - Large Language Model Text Generation Inference.\n* [TorchServe](https://github.com/pytorch/serve) ![](https://img.shields.io/github/stars/pytorch/serve.svg?cacheSeconds=86400) - TorchServe is a flexible and easy to use tool for serving PyTorch models.\n* [torchtune](https://github.com/meta-pytorch/torchtune) ![](https://img.shields.io/github/stars/meta-pytorch/torchtune.svg?cacheSeconds=86400) - torchtune is a PyTorch library for easily authoring, post-training, and experimenting with LLMs.\n* [Transformer Lab](https://github.com/transformerlab/transformerlab-app) ![](https://img.shields.io/github/stars/transformerlab/transformerlab-app.svg?cacheSeconds=86400) - Transformer Lab is an open-source LLM workspace for finetuning, evaluating, exporting, and testing models locally across inference engines and platforms.\n* [Triton Inference Server](https://github.com/triton-inference-server/server) ![](https://img.shields.io/github/stars/triton-inference-server/server.svg?cacheSeconds=86400) - Triton is a high performance open source serving software to deploy AI models from any framework on GPU & CPU while maximizing utilization.\n* [Vercel AI](https://github.com/vercel/ai) ![](https://img.shields.io/github/stars/vercel/ai.svg?cacheSeconds=86400) - Vercel AI is a TypeScript toolkit designed to help you build AI-powered applications using popular frameworks like Next.js, React, Svelte, Vue and runtimes like Node.js.\n* [Vespa](https://github.com/vespa-engine/vespa) ![](https://img.shields.io/github/stars/vespa-engine/vespa.svg?cacheSeconds=86400) - Search, make inferences in and organize vectors, tensors, text and structured data, at serving time and any scale.\n* [vLLM](https://github.com/vllm-project/vllm) ![](https://img.shields.io/github/stars/vllm-project/vllm.svg?cacheSeconds=86400) - vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\n\n\n## Evaluation and Monitoring\n* [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval) ![](https://img.shields.io/github/stars/tatsu-lab/alpaca_eval.svg?cacheSeconds=86400) - AlpacaEval is an automatic evaluator for instruction-following language models.\n* [ANN-Benchmarks](https://github.com/erikbern/ann-benchmarks) ![](https://img.shields.io/github/stars/erikbern/ann-benchmarks.svg?cacheSeconds=86400) - ANN-Benchmarks is a benchmarking environment for approximate nearest neighbor algorithms search.\n* [ARES](https://github.com/stanford-futuredata/ARES) ![](https://img.shields.io/github/stars/stanford-futuredata/ARES.svg?cacheSeconds=86400) - ARES is a framework for automatically evaluating Retrieval-Augmented Generation (RAG) models.\n* [BEIR](https://github.com/beir-cellar/beir) ![](https://img.shields.io/github/stars/beir-cellar/beir.svg?cacheSeconds=86400) - BEIR is a heterogeneous benchmark containing diverse IR tasks. It also provides a common and easy framework for evaluation of your NLP-based retrieval models within the benchmark.\n* [Code Generation LM Evaluation Harness](https://github.com/bigcode-project/bigcode-evaluation-harness) ![](https://img.shields.io/github/stars/bigcode-project/bigcode-evaluation-harness.svg?cacheSeconds=86400) - Code Generation LM Evaluation Harness is a framework for the evaluation of code generation models.\n* [COMET](https://github.com/Unbabel/COMET) ![](https://img.shields.io/github/stars/Unbabel/COMET.svg?cacheSeconds=86400) - COMET is an open-source framework for machine learning evaluation.\n* [C-Eval](https://github.com/hkust-nlp/ceval) ![](https://img.shields.io/github/stars/hkust-nlp/ceval.svg?cacheSeconds=86400) - C-Eval is a comprehensive Chinese evaluation suite for foundation models.\n* [Deepchecks](https://github.com/deepchecks/deepchecks) ![](https://img.shields.io/github/stars/deepchecks/deepchecks.svg?cacheSeconds=86400) - Deepchecks is a holistic open-source solution for all of your AI & ML validation needs, enabling you to test your data and models from research to production thoroughly.\n* [DeepEval](https://github.com/confident-ai/deepeval) ![](https://img.shields.io/github/stars/confident-ai/deepeval.svg?cacheSeconds=86400) - DeepEval is a simple-to-use, open-source evaluation framework for LLM applications.\n* [DomainBed](https://github.com/facebookresearch/DomainBed) ![](https://img.shields.io/github/stars/facebookresearch/DomainBed.svg?cacheSeconds=86400) - DomainBed is a test suite containing benchmark datasets and algorithms for domain generalization\n* [EvalAI](https://github.com/Cloud-CV/EvalAI) ![](https://img.shields.io/github/stars/Cloud-CV/EvalAI.svg?cacheSeconds=86400) - EvalAI is an open-source platform for evaluating and comparing AI algorithms at scale.\n* [Evalchemy](https://github.com/mlfoundations/evalchemy) ![](https://img.shields.io/github/stars/mlfoundations/evalchemy.svg?cacheSeconds=86400) - Evalchemy is a unified and easy-to-use toolkit for evaluating post-trained language models.\n* [EvalPlus](https://github.com/evalplus/evalplus) ![](https://img.shields.io/github/stars/evalplus/evalplus.svg?cacheSeconds=86400) - EvalPlus is a robust evaluation framework for LLM4Code, featuring expanded HumanEval+ and MBPP+ benchmarks, efficiency assessment (EvalPerf), and a secure, extensible evaluation toolkit.\n* [Evals](https://github.com/openai/evals) ![](https://img.shields.io/github/stars/openai/evals.svg?cacheSeconds=86400) - Evals is a framework for evaluating OpenAI models and an open-source registry of benchmarks.\n* [EvalScope](https://github.com/modelscope/evalscope) ![](https://img.shields.io/github/stars/modelscope/evalscope.svg?cacheSeconds=86400) - EvalScope is a streamlined and customizable framework for efficient large model evaluation and performance benchmarking.\n* [Evaluate](https://github.com/huggingface/evaluate) ![](https://img.shields.io/github/stars/huggingface/evaluate.svg?cacheSeconds=86400) - Evaluate is a library that makes evaluating and comparing models and reporting their performance easier and more standardized.\n* [Evidently](https://github.com/evidentlyai/evidently) ![](https://img.shields.io/github/stars/evidentlyai/evidently.svg?cacheSeconds=86400) - Evidently is an open-source framework to evaluate, test and monitor ML and LLM-powered systems.\n* [GAOKAO-Bench](https://github.com/OpenLMLab/GAOKAO-Bench) ![](https://img.shields.io/github/stars/OpenLMLab/GAOKAO-Bench.svg?cacheSeconds=86400) - GAOKAO-Bench is an evaluation framework that uses Chinese National College Entrance Examination (GAOKAO) questions as a dataset to assess large models'' language comprehension and logical reasoning abilities.\n* [Giskard](https://github.com/Giskard-AI/giskard)![](https://img.shields.io/github/stars/Giskard-AI/giskard.svg?cacheSeconds=86400) - Giskard is an open-source Python library that automatically detects performance, bias & security issues in AI applications.\n* [HumanEval](https://github.com/openai/human-eval)![](https://img.shields.io/github/stars/openai/human-eval.svg?cacheSeconds=86400) - HumanEval is a benchmark for evaluating the functional correctness of code generation models using Python programming problems with unit tests.\n* [Helicone](https://github.com/Helicone/helicone) ![](https://img.shields.io/github/stars/Helicone/helicone.svg?cacheSeconds=86400) - Helicone is the all-in-one, open-source LLM developer platform.\n* [HELM](https://github.com/stanford-crfm/helm) ![](https://img.shields.io/github/stars/stanford-crfm/helm.svg?cacheSeconds=86400) - HELM (Holistic Evaluation of Language Models) provides tools for the holistic evaluation of language models, including standardized datasets, a unified API for various models, diverse metrics, r, and fairness perturbations, a prompt construction framework, and a proxy server for unified model access.\n* [Inspect](https://github.com/UKGovernmentBEIS/inspect_ai) ![](https://img.shields.io/github/stars/UKGovernmentBEIS/inspect_ai.svg?cacheSeconds=86400) - Inspect is a framework for large language model evaluations.\n* [JiWER](https://github.com/jitsi/jiwer) ![](https://img.shields.io/github/stars/jitsi/jiwer.svg?cacheSeconds=86400) - JiWER is a simple and fast python package to evaluate an automatic speech recognition system. \n* [Laminar](https://github.com/lmnr-ai/lmnr) ![](https://img.shields.io/github/stars/lmnr-ai/lmnr.svg?cacheSeconds=86400) - Laminar is an open-source platform to trace, evaluate, label, and analyze LLM data for AI products.\n* [Langfuse](https://github.com/langfuse/langfuse) ![](https://img.shields.io/github/stars/langfuse/langfuse.svg?cacheSeconds=86400) - Langfuse is an observability & analytics solution for LLM-based applications.\n* [LangTest](https://github.com/JohnSnowLabs/langtest) ![](https://img.shields.io/github/stars/JohnSnowLabs/langtest.svg?cacheSeconds=86400) - LangTest is a comprehensive evaluation toolkit for NLP models.\n* [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) ![](https://img.shields.io/github/stars/EleutherAI/lm-evaluation-harness.svg?cacheSeconds=86400) - Language Model Evaluation Harness is a framework to test generative language models on a large number of different evaluation tasks.\n* [LangWatch](https://github.com/langwatch/langwatch) ![](https://img.shields.io/github/stars/langwatch/langwatch.svg?cacheSeconds=86400) - LangWatch is a visual interface for DSPy and a complete LLM Ops platform for monitoring, experimenting, measuring and improving LLM pipelines, with a fair-code distribution model.\n* [LightEval](https://github.com/huggingface/lighteval) ![](https://img.shields.io/github/stars/huggingface/lighteval.svg?cacheSeconds=86400) - LightEval is a lightweight LLM evaluation suite.\n* [LLMonitor](https://github.com/lunary-ai/lunary) ![](https://img.shields.io/github/stars/lunary-ai/lunary.svg?cacheSeconds=86400) - LLMonitor is an observability & analytics for AI apps and agents.\n* [LLMPerf](https://github.com/ray-project/llmperf) ![](https://img.shields.io/github/stars/ray-project/llmperf.svg?cacheSeconds=86400) - LLMPerf is a tool for evaluating the performance of LLM APIs.\n* [lmms-eval](https://github.com/EvolvingLMMs-Lab/lmms-eval) ![](https://img.shields.io/github/stars/EvolvingLMMs-Lab/lmms-eval.svg?cacheSeconds=86400) - lmms-eval is an evaluation framework meticulously crafted for consistent and efficient evaluation of LMM.\n* [Melting Pot](https://github.com/google-deepmind/meltingpot) ![](https://img.shields.io/github/stars/google-deepmind/meltingpot.svg?cacheSeconds=86400) - Melting Pot is a suite of test scenarios for multi-agent reinforcement learning.\n* [Meta-World](https://github.com/Farama-Foundation/Metaworld) ![](https://img.shields.io/github/stars/Farama-Foundation/Metaworld.svg?cacheSeconds=86400) - Meta-World is an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks.\n* [mir_eval](https://github.com/mir-evaluation/mir_eval) ![](https://img.shields.io/github/stars/mir-evaluation/mir_eval.svg?cacheSeconds=86400) - mir_eval is a Python library which provides a transparent, standardized, and straightforward way to evaluate Music Information Retrieval systems.\n* [MLPerf Inference](https://github.com/mlcommons/inference) ![](https://img.shields.io/github/stars/mlcommons/inference.svg?cacheSeconds=86400) - MLPerf Inference is a benchmark suite for measuring how fast systems can run models in a variety of deployment scenarios.\n* [Massive Text Embedding Benchmark](https://github.com/mlcommons/inference) ![](https://img.shields.io/github/stars/mlcommons/inference.svg?cacheSeconds=86400) - Massive Text Embedding Benchmark (MTEB) is a comprehensive evaluation framework that assesses the performance of text embedding models across diverse tasks and languages, encompassing 8 embedding tasks, 58 datasets, and 112 languages.\n* [NannyML](https://github.com/NannyML/nannyml) ![](https://img.shields.io/github/stars/NannyML/nannyml.svg?cacheSeconds=86400) - NannyML is a library that allows you to estimate post-deployment model performance (without access to targets), detect data drift, and intelligently link data drift alerts back to changes in model performance.\n* [OGB](https://github.com/snap-stanford/ogb) ![](https://img.shields.io/github/stars/snap-stanford/ogb.svg?cacheSeconds=86400) - The Open Graph Benchmark (OGB) is a collection of benchmark datasets, data loaders, and evaluators for graph machine learning.\n* [Ollama Grid Search](https://github.com/dezoito/ollama-grid-search) ![](https://img.shields.io/github/stars/dezoito/ollama-grid-search.svg?cacheSeconds=86400) - Ollama Grid Search automates the process of selecting the best models, prompts, or inference parameters for a given use-case, allowing you to iterate over their combinations and to visually inspect the results.\n* [OpenCompass](https://github.com/open-compass/OpenCompass) ![](https://img.shields.io/github/stars/open-compass/OpenCompass.svg?cacheSeconds=86400) - OpenCompass is an LLM evaluation platform, supporting a wide range of models (LLaMA, LLaMa2, ChatGLM2, ChatGPT, Claude, etc) over 50+ datasets.\n* [OpenLIT](https://github.com/openlit/openlit) ![](https://img.shields.io/github/stars/openlit/openlit.svg?cacheSeconds=86400) - OpenLIT is an open-source AI engineering platform that simplifies LLM workflows with observability, monitoring, guardrails, evaluations, and seamless integrations. \n* [OpenLLMetry](https://github.com/traceloop/openllmetry) ![](https://img.shields.io/github/stars/traceloop/openllmetry.svg?cacheSeconds=86400) - OpenLLMetry provides developers with deep visibility into Large Language Model applications through performance monitoring, execution tracing, and debugging capabilities.\n* [Opik](https://github.com/comet-ml/opik) ![](https://img.shields.io/github/stars/comet-ml/opik.svg?cacheSeconds=86400) - Opik is an open-source platform for evaluating, testing and monitoring LLM applications.\n* [Overcooked-AI](https://github.com/HumanCompatibleAI/overcooked_ai) ![](https://img.shields.io/github/stars/HumanCompatibleAI/overcooked_ai.svg?cacheSeconds=86400) - Overcooked-AI is a benchmark environment for fully cooperative human-AI task performance, based on the wildly popular video game Overcooked.\n* [Phoenix](https://github.com/Arize-ai/phoenix) ![](https://img.shields.io/github/stars/Arize-ai/phoenix.svg?cacheSeconds=86400) - Phoenix is an open-source AI observability platform designed for experimentation, evaluation, and troubleshooting.\n* [PromptBench](https://github.com/microsoft/promptbench) ![](https://img.shields.io/github/stars/microsoft/promptbench.svg?cacheSeconds=86400) - PromptBench is a unified evaluation framework for large language models\n* [Promptfoo](https://github.com/promptfoo/promptfoo) ![](https://img.shields.io/github/stars/promptfoo/promptfoo.svg?cacheSeconds=86400) - Promptfoo is a developer-friendly local tool for testing LLM applications. \n* [Prometheus-Eval](https://github.com/prometheus-eval/prometheus-eval) ![](https://img.shields.io/github/stars/prometheus-eval/prometheus-eval.svg?cacheSeconds=86400) - RagaAI Catalyst is a comprehensive platform designed to enhance the management and optimization of LLM projects. \n* [RagaAI Catalyst](https://github.com/raga-ai-hub/RagaAI-Catalyst) ![](https://img.shields.io/github/stars/raga-ai-hub/RagaAI-Catalyst.svg?cacheSeconds=86400) - Prometheus-Eval is a collection of tools for training, evaluating, and using language models specialized in evaluating other language models.\n* [Ragas](https://github.com/explodinggradients/ragas) ![](https://img.shields.io/github/stars/explodinggradients/ragas.svg?cacheSeconds=86400) - Ragas is a framework to evaluate RAG pipelines.\n* [RAGChecker](https://github.com/amazon-science/RAGChecker) ![](https://img.shields.io/github/stars/amazon-science/RAGChecker.svg?cacheSeconds=86400) - RAGChecker is an advanced automatic evaluation framework designed to assess and diagnose Retrieval-Augmented Generation (RAG) systems.\n* [RewardBench](https://github.com/allenai/reward-bench) ![](https://img.shields.io/github/stars/allenai/reward-bench.svg?cacheSeconds=86400) - RewardBench is a benchmark designed to evaluate the capabilities and safety of reward models.\n* [RLBench](https://github.com/stepjam/RLBench) ![](https://img.shields.io/github/stars/stepjam/RLBench.svg?cacheSeconds=86400) - RLBench is an ambitious large-scale benchmark and learning environment designed to facilitate research in a number of vision-guided manipulation research areas, including: reinforcement learning, imitation learning, multi-task learning, geometric computer vision, and in particular, few-shot learning.\n* [SimplerEnv](https://github.com/simpler-env/SimplerEnv) ![](https://img.shields.io/github/stars/simpler-env/SimplerEnv.svg?cacheSeconds=86400) - SimplerEnv is a simulated manipulation policy evaluation environments for real robot setups.\n* [SwanLab](https://github.com/SwanHubX/SwanLab) ![](https://img.shields.io/github/stars/SwanHubX/SwanLab.svg?cacheSeconds=86400) - SwanLab is an AI training tracking and visualization tool.\n* [Speech-to-Text Benchmark](https://github.com/Picovoice/speech-to-text-benchmark) ![](https://img.shields.io/github/stars/Picovoice/speech-to-text-benchmark.svg?cacheSeconds=86400) - Speech-to-Text Benchmark is a minimalist and extensible framework for benchmarking different speech-to-text engines.\n* [TensorFlow Model Analysis](https://github.com/tensorflow/model-analysis) ![](https://img.shields.io/github/stars/tensorflow/model-analysis.svg?cacheSeconds=86400) - TensorFlow Model Analysis (TFMA) is a library for evaluating TensorFlow models on large amounts of data in a distributed manner, using the same metrics defined in their trainer.\n* [TorchBench](https://github.com/pytorch/benchmark) ![](https://img.shields.io/github/stars/pytorch/benchmark.svg?cacheSeconds=86400) - TorchBench is a collection of open source benchmarks used to evaluate PyTorch performance.\n* [TruLens](https://github.com/truera/trulens) ![](https://img.shields.io/github/stars/truera/trulens.svg?cacheSeconds=86400) - TruLens provides a set of tools for evaluating and tracking LLM experiments.\n* [TrustLLM](https://github.com/HowieHwong/TrustLLM) ![](https://img.shields.io/github/stars/HowieHwong/TrustLLM.svg?cacheSeconds=86400) - TrustLLM is a comprehensive framework to evaluate the trustworthiness of large language models, which includes principles, surveys, and benchmarks.\n* [VBench](https://github.com/Vchitect/VBench) ![](https://img.shields.io/github/stars/Vchitect/VBench.svg?cacheSeconds=86400) - VBench is a comprehensive benchmark suite for video generative models.\n* [VLMEvalKit](https://github.com/open-compass/VLMEvalKit) ![](https://img.shields.io/github/stars/open-compass/VLMEvalKit.svg?cacheSeconds=86400) - VLMEvalKit is an open-source evaluation toolkit of large vision-language models (LVLMs).\n\n## Explainability and Fairness\n* [Aequitas](https://github.com/dssg/aequitas) ![](https://img.shields.io/github/stars/dssg/aequitas.svg?cacheSeconds=86400) - An open-source bias audit toolkit for data scientists, machine learning researchers, and policymakers to audit machine learning models for discrimination and bias, and to make informed and equitable decisions around developing and deploying predictive risk-assessment tools.\n* [AI Explainability 360](https://github.com/Trusted-AI/AIX360) ![](https://img.shields.io/github/stars/Trusted-AI/AIX360.svg?cacheSeconds=86400) - Interpretability and explainability of data and machine learning models including a comprehensive set of algorithms that cover different dimensions of explanations along with proxy explainability metrics.\n* [AI Fairness 360](https://github.com/Trusted-AI/AIF360) ![](https://img.shields.io/github/stars/Trusted-AI/AIF360.svg?cacheSeconds=86400) - A comprehensive set of fairness metrics for datasets and machine learning models, explanations for these metrics, and algorithms to mitigate bias in datasets and models.\n* [Alibi](https://github.com/SeldonIO/alibi) ![](https://img.shields.io/github/stars/SeldonIO/alibi.svg?cacheSeconds=86400) - Alibi is an open source Python library aimed at machine learning model inspection and interpretation. The initial focus on the library is on black-box, instance based model explanations.\n* [captum](https://github.com/pytorch/captum) ![](https://img.shields.io/github/stars/pytorch/captum.svg?cacheSeconds=86400) - model interpretability and understanding library for PyTorch developed by Facebook. It contains general purpose implementations of integrated gradients, saliency maps, smoothgrad, vargrad and others for PyTorch models.\n* [Fairlearn](https://github.com/fairlearn/fairlearn) ![](https://img.shields.io/github/stars/fairlearn/fairlearn.svg?cacheSeconds=86400) - Fairlearn is a python toolkit to assess and mitigate unfairness in machine learning models.\n* [InterpretML](https://github.com/interpretml/interpret) ![](https://img.shields.io/github/stars/interpretml/interpret.svg?cacheSeconds=86400) - InterpretML is an open-source package for training interpretable models and explaining blackbox systems.\n* [Lightly](https://github.com/lightly-ai/lightly) ![](https://img.shields.io/github/stars/lightly-ai/lightly.svg?cacheSeconds=86400) - A python framework for self-supervised learning on images. The learned representations can be used to analyze the distribution in unlabeled data and rebalance datasets.\n* [LOFO Importance](https://github.com/aerdem4/lofo-importance) ![](https://img.shields.io/github/stars/aerdem4/lofo-importance.svg?cacheSeconds=86400) - LOFO (Leave One Feature Out) Importance calculates the importances of a set of features based on a metric of choice, for a model of choice, by iteratively removing each feature from the set, and evaluating the performance of the model, with a validation scheme of choice, based on the chosen metric.\n* [mljar-supervised](https://github.com/mljar/mljar-supervised) ![](https://img.shields.io/github/stars/mljar/mljar-supervised.svg?cacheSeconds=86400) - A Python package for AutoML on tabular data with feature engineering, hyper-parameters tuning, explanations and automatic documentation.\n* [Quantus](https://github.com/understandable-machine-intelligence-lab/Quantus) ![](https://img.shields.io/github/stars/understandable-machine-intelligence-lab/Quantus.svg?cacheSeconds=86400) - Quantus is an eXplainable AI toolkit for responsible evaluation of neural network explanations\n* [SHAP](https://github.com/shap/shap) ![](https://img.shields.io/github/stars/shap/shap.svg?cacheSeconds=86400) - SHapley Additive exPlanations is a unified approach to explain the output of any machine learning model.\n* [SHAPash](https://github.com/MAIF/shapash) ![](https://img.shields.io/github/stars/MAIF/shapash.svg?cacheSeconds=86400) - Shapash is a Python library that provides several types of visualization that display explicit labels that everyone can understand.\n* [WhatIf](https://github.com/pair-code/what-if-tool) ![](https://img.shields.io/github/stars/pair-code/what-if-tool.svg?cacheSeconds=86400) - An easy-to-use interface for expanding understanding of a black-box classification or regression ML model.\n\n## Feature Store\n* [FEAST](https://github.com/feast-dev/feast)  ![](https://img.shields.io/github/stars/feast-dev/feast.svg?cacheSeconds=86400) - Feast (Feature Store) is an open source feature store for machine learning. Feast is the fastest path to manage existing infrastructure to productionize analytic data for model training and online inference.\n* [Featureform](https://github.com/featureform/featureform) ![](https://img.shields.io/github/stars/featureform/featureform.svg?cacheSeconds=86400) - A virtual featurestore. Plug-&-play with your existing infra. Data Scientist approved. Discovery, Governance, Lineage, & Collaboration just a pip install away. Supports pandas, Python, spark, SQL + integrations with major cloud vendors. \n* [Hopsworks Feature Store](https://github.com/logicalclocks/hopsworks) ![](https://img.shields.io/github/stars/logicalclocks/hopsworks.svg?cacheSeconds=86400) - Offline/Online Feature Store for ML [(Video)](https://www.youtube.com/watch?v=N1BjPk1smdg).\n\n## Industry-strength Anomaly Detection\n* [Alibi Detect](https://github.com/SeldonIO/alibi-detect) ![](https://img.shields.io/github/stars/SeldonIO/alibi-detect.svg?cacheSeconds=86400) - alibi-detect is a Python package focused on outlier, adversarial and concept drift detection.\n* [Darts](https://github.com/unit8co/darts) ![](https://img.shields.io/github/stars/unit8co/darts.svg?cacheSeconds=86400) - Darts is a library for user-friendly forecasting and anomaly detection on time series.\n* [Deequ](https://github.com/awslabs/deequ) ![](https://img.shields.io/github/stars/awslabs/deequ.svg?cacheSeconds=86400) - A library built on top of Apache Spark for defining "unit tests for data", which measure data quality in large datasets.\n* [PyOD](https://github.com/yzhao062/pyod) ![](https://img.shields.io/github/stars/yzhao062/pyod.svg?cacheSeconds=86400) - A Python Toolbox for Scalable Outlier Detection (Anomaly Detection).\n* [TFDV](https://github.com/tensorflow/data-validation) ![](https://img.shields.io/github/stars/tensorflow/data-validation.svg?cacheSeconds=86400) - TFDV (Tensorflow Data Validation) is a library for exploring and validating machine learning data.\n\n## Industry Strength Computer Vision\n* [Deep Lake](https://github.com/activeloopai/deeplake) ![](https://img.shields.io/github/stars/activeloopai/deeplake.svg?cacheSeconds=86400) - Deep Lake is a data infrastructure optimized for computer vision.\n* [Detectron2](https://github.com/facebookresearch/detectron2) ![](https://img.shields.io/github/stars/facebookresearch/detectron2.svg?cacheSeconds=86400) - Detectron2 is Facebook AI Research''s next generation library that provides state-of-the-art detection and segmentation algorithms.\n* [KerasCV](https://github.com/keras-team/keras-cv) ![](https://img.shields.io/github/stars/keras-team/keras-cv.svg?cacheSeconds=86400) - KerasCV is a library of modular computer vision oriented Keras components.\n* [Kornia](https://github.com/kornia/kornia) ![](https://img.shields.io/github/stars/kornia/kornia.svg?cacheSeconds=86400) - Kornia is a differentiable computer vision library built on PyTorch that provides a rich set of differentiable image processing and geometric vision algorithms.\n* [LAVIS](https://github.com/salesforce/LAVIS) ![](https://img.shields.io/github/stars/salesforce/LAVIS.svg?cacheSeconds=86400) - LAVIS is a deep learning library for LAnguage-and-VISion intelligence research and applications.\n* [libcom](https://github.com/bcmi/libcom) ![](https://img.shields.io/github/stars/bcmi/libcom.svg?cacheSeconds=86400) - libcom is an image composition toolbox.\n* [LightlyTrain](https://github.com/lightly-ai/lightly-train) ![](https://img.shields.io/github/stars/lightly-ai/lightly-train.svg?cacheSeconds=86400) - Pretrain computer vision models on unlabeled data for industrial applications.\n* [MMCV](https://github.com/open-mmlab/mmcv) ![](https://img.shields.io/github/stars/open-mmlab/mmcv.svg?cacheSeconds=86400) - MMCV is a foundational computer vision library from OpenMMLab that provides essential functionalities like image and video processing, data transformation and augmentation, CNN architectures, and optimized CUDA operations.\n* [SuperGradients](https://github.com/Deci-AI/super-gradients) ![](https://img.shields.io/github/stars/Deci-AI/super-gradients.svg?cacheSeconds=86400) - SuperGradients is an open-source library for training PyTorch-based computer vision models.\n* [supervision](https://github.com/roboflow/supervision) ![](https://img.shields.io/github/stars/roboflow/supervision.svg?cacheSeconds=86400) - Supervision is a Python library designed for efficient computer vision pipeline management, providing tools for annotation, visualization, and monitoring of models.\n* [VideoSys](https://github.com/NUS-HPC-AI-Lab/VideoSys) ![](https://img.shields.io/github/stars/NUS-HPC-AI-Lab/VideoSys.svg?cacheSeconds=86400) - VideoSys supports many diffusion models with our various acceleration techniques, enabling these models to run faster and consume less memory.\n\n## Industry Strength Information Retrieval\n* [AutoRAG](https://github.com/Marker-Inc-Korea/AutoRAG) ![](https://img.shields.io/github/stars/Marker-Inc-Korea/AutoRAG.svg?cacheSeconds=86400) - AutoRAG is a RAG AutoML tool for automatically finds an optimal RAG pipeline for your data.\n* [BGE](https://github.com/FlagOpen/FlagEmbedding) ![](https://img.shields.io/github/stars/FlagOpen/FlagEmbedding.svg?cacheSeconds=86400) - BGE builds one-stop retrieval toolkit for search and RAG.\n* [Cognita](https://github.com/truefoundry/cognita) ![](https://img.shields.io/github/stars/truefoundry/cognita.svg?cacheSeconds=86400) - Cognita is a RAG framework for building modular and production-ready applications.\n* [DocArray](https://github.com/docarray/docarray) ![](https://img.shields.io/github/stars/docarray/docarray.svg?cacheSeconds=86400) - DocArray is a library for nested, unstructured, multimodal data in transit, including text, image, audio, video, 3D mesh, etc. It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer multimodal data with a Pythonic API.\n* [Faiss](https://github.com/facebookresearch/faiss) ![](https://img.shields.io/github/stars/facebookresearch/faiss.svg?cacheSeconds=86400) - Faiss is a library for efficient similarity search and clustering of dense vectors.\n* [fastRAG](https://github.com/IntelLabs/fastRAG) ![](https://img.shields.io/github/stars/IntelLabs/fastRAG.svg?cacheSeconds=86400) - fastRAG is a research framework for efficient and optimized retrieval augmented generative pipelines, incorporating state-of-the-art LLMs and Information Retrieval.\n* [GraphRAG](https://github.com/microsoft/graphrag) ![](https://img.shields.io/github/stars/microsoft/graphrag.svg?cacheSeconds=86400) - GraphRAG is a data pipeline and transformation suite that is designed to extract meaningful, structured data from unstructured text using the power of LLMs.\n* [HippoRAG](https://github.com/OSU-NLP-Group/HippoRAG) ![](https://img.shields.io/github/stars/OSU-NLP-Group/HippoRAG.svg?cacheSeconds=86400) - HippoRAG is a novel retrieval augmented generation (RAG) framework inspired by the neurobiology of human long-term memory that enables LLMs to continuously integrate knowledge across external documents.\n* [JamAI Base](https://github.com/EmbeddedLLM/JamAIBase) ![](https://img.shields.io/github/stars/EmbeddedLLM/JamAIBase.svg?cacheSeconds=86400) - JamAI Base is an open-source RAG (Retrieval-Augmented Generation) backend platform that integrates an embedded database (SQLite) and an embedded vector database (LanceDB) with managed memory and RAG capabilities. It features built-in LLM, vector embeddings, and reranker orchestration and management, all accessible through a convenient, intuitive, spreadsheet-like UI and a simple REST API.\n* [LangExtract](https://github.com/google/langextract) ![](https://img.shields.io/github/stars/google/langextract.svg?cacheSeconds=86400) - LangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.\n* [LightRAG](https://github.com/HKUDS/LightRAG) ![](https://img.shields.io/github/stars/HKUDS/LightRAG.svg?cacheSeconds=86400) - A simple and fast retrieval-augmented generation framework.\n* [llmware](https://github.com/llmware-ai/llmware) ![](https://img.shields.io/github/stars/llmware-ai/llmware.svg?cacheSeconds=86400) - llmware provides a unified framework for building LLM-based applications (e.g, RAG, Agents), using small, specialized models that can be deployed privately, integrated with enterprise knowledge sources safely and securely, and cost-effectively tuned and adapted for any business process.\n* [Mem0](https://github.com/mem0ai/mem0) ![](https://img.shields.io/github/stars/mem0ai/mem0.svg?cacheSeconds=86400) - Mem0 enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions.\n* [NGT](https://github.com/yahoojapan/NGT) ![](https://img.shields.io/github/stars/yahoojapan/NGT.svg?cacheSeconds=86400) - NGT provides commands and a library for performing high-speed approximate nearest neighbor searches against a large volume of data in high dimensional vector data space.\n* [NMSLIB](https://github.com/nmslib/nmslib) ![](https://img.shields.io/github/stars/nmslib/nmslib.svg?cacheSeconds=86400) - Non-Metric Space Library (NMSLIB): An efficient similarity search library and a toolkit for evaluation of k-NN methods for generic non-metric spaces.\n* [Qdrant](https://github.com/qdrant/qdrant) ![](https://img.shields.io/github/stars/qdrant/qdrant.svg?cacheSeconds=86400) - An open source vector similarity search engine with extended filtering support.\n* [R2R](https://github.com/SciPhi-AI/R2R) ![](https://img.shields.io/github/stars/SciPhi-AI/R2R.svg?cacheSeconds=86400) - R2R (RAG to Riches) is a comprehensive platform for building, deploying, and scaling RAG applications with hybrid search, multimodal support, and advanced observability.\n* [RAGFlow](https://github.com/infiniflow/ragflow) ![](https://img.shields.io/github/stars/infiniflow/ragflow.svg?cacheSeconds=86400) - RAGFlow is a RAG engine based on deep document understanding.\n* [RAGxplorer](https://github.com/gabrielchua/RAGxplorer) ![](https://img.shields.io/github/stars/gabrielchua/RAGxplorer.svg?cacheSeconds=86400) - RAGxplorer is a tool to build RAG visualisations.\n* [RAG-FiT](https://github.com/IntelLabs/RAG-FiT) ![](https://img.shields.io/github/stars/IntelLabs/RAG-FiT.svg?cacheSeconds=86400) - RAG-FiT is a library designed to improve LLMs ability to use external information by fine-tuning models on specially created RAG-augmented datasets.\n* [TextWorld](https://github.com/microsoft/TextWorld) ![](https://img.shields.io/github/stars/microsoft/TextWorld.svg?cacheSeconds=86400) - TextWorld is a text-based game generator and extensible sandbox learning environment for training and testing reinforcement learning (RL) agents.\n* [Vanna](https://github.com/vanna-ai/vanna) ![](https://img.shields.io/github/stars/vanna-ai/vanna.svg?cacheSeconds=86400) - Vanna is a RAG framework for SQL generation and related functionality.\n\n## Industry Strength Natural Language Processing\n* [aisuite](https://github.com/andrewyng/aisuite) ![](https://img.shields.io/github/stars/andrewyng/aisuite.svg?cacheSeconds=86400) - aisuite is a simple, unified interface to multiple generative AI providers.\n* [Align-Anything](https://github.com/PKU-Alignment/align-anything) ![](https://img.shields.io/github/stars/PKU-Alignment/align-anything.svg?cacheSeconds=86400) - Align-Anything aims to align any modality large models (any-to-any models), including LLMs, VLMs, and others, with human intentions and values\n* [BERTopic](https://github.com/MaartenGr/BERTopic) ![](https://img.shields.io/github/stars/MaartenGr/BERTopic.svg?cacheSeconds=86400) - BERTopic is a topic modeling technique that leverages transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions.\n* [Burr](https://github.com/dagworks-inc/burr) ![](https://img.shields.io/github/stars/dagworks-inc/burr.svg?cacheSeconds=86400) - Burr helps you develop applications that make decisions (chatbot, agent, simulation). It comes with production-ready features (telemetry, persistence, deployment, etc.) and the open-source, free, and local-first Burr UI.\n* [CodeTF](https://github.com/salesforce/CodeTF) ![](https://img.shields.io/github/stars/salesforce/CodeTF.svg?cacheSeconds=86400) - CodeTF is a one-stop Python transformer-based library for code large language models (Code LLMs) and code intelligence, provides a seamless interface for training and inferencing on code intelligence tasks like code summarization, translation, code generation and so on. \n* [Dify](https://github.com/langgenius/dify) ![](https://img.shields.io/github/stars/langgenius/dify.svg?cacheSeconds=86400) - Dify is an open-source LLM app development platform whose intuitive interface combines agentic AI workflow, RAG pipeline, agent capabilities, model management, observability features and more, letting you quickly go from prototype to production.\n* [dspy](https://github.com/stanfordnlp/dspy) ![](https://img.shields.io/github/stars/stanfordnlp/dspy.svg?cacheSeconds=86400) - A framework for programming with foundation models.\n* [Dust](https://github.com/dust-tt/dust) ![](https://img.shields.io/github/stars/dust-tt/dust.svg?cacheSeconds=86400) - Dust assists in the design and deployment of large language model apps.\n* [ESPnet](https://github.com/espnet/espnet) ![](https://img.shields.io/github/stars/espnet/espnet.svg?cacheSeconds=86400) - ESPnet is an end-to-end speech processing toolkit.\n* [FastChat](https://github.com/lm-sys/FastChat) ![](https://img.shields.io/github/stars/lm-sys/FastChat.svg?cacheSeconds=86400) - FastChat is an open platform for training, serving, and evaluating large language model based chatbots.\n* [Flair](https://github.com/flairNLP/flair) ![](https://img.shields.io/github/stars/flairNLP/flair.svg?cacheSeconds=86400) - Simple framework for state-of-the-art NLP developed by Zalando which builds directly on PyTorch.\n* [Gensim](https://github.com/piskvorky/gensim) ![](https://img.shields.io/github/stars/piskvorky/gensim.svg?cacheSeconds=86400) - Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora.\n* [gpt-fast](https://github.com/meta-pytorch/gpt-fast) ![](https://img.shields.io/github/stars/meta-pytorch/gpt-fast.svg?cacheSeconds=86400) - Simple and efficient pytorch-native transformer text generation.\n* [h2oGPT](https://github.com/h2oai/h2ogpt) ![](https://img.shields.io/github/stars/h2oai/h2ogpt.svg?cacheSeconds=86400) - h2oGPT is an open source generative AI, gives organizations like yours the power to own large language models while preserving your data ownership.\n* [Haystack](https://github.com/deepset-ai/haystack) ![](https://img.shields.io/github/stars/deepset-ai/haystack.svg?cacheSeconds=86400) - Haystack is an open source NLP framework to interact with your data using Transformer models and LLMs (GPT-3 and alike). Haystack offers production-ready tools to quickly build ChatGPT-like question answering, semantic search, text generation, and more.\n* [Interactive Composition Explorer](https://github.com/oughtinc/ice) ![](https://img.shields.io/github/stars/oughtinc/ice.svg?cacheSeconds=86400) - ICE is a Python library and trace visualizer for language model programs.\n* [Lamini](https://github.com/lamini-ai/lamini) ![](https://img.shields.io/github/stars/lamini-ai/lamini.svg?cacheSeconds=86400) - Lamini is an LLM engine for rapidly customizing models.\n* [LangChain](https://github.com/langchain-ai/langchain) ![](https://img.shields.io/github/stars/langchain-ai/langchain.svg?cacheSeconds=86400) - LangChain assists in building applications with LLMs through composability.\n* [LlamaIndex](https://github.com/run-llama/llama_index) ![](https://img.shields.io/github/stars/run-llama/llama_index.svg?cacheSeconds=86400) - LlamaIndex (GPT Index) is a data framework for your LLM application.\n* [LLaMA](https://github.com/meta-llama/llama) ![](https://img.shields.io/github/stars/meta-llama/llama.svg?cacheSeconds=86400) - LLaMA is intended as a minimal, hackable and readable example to load LLaMA (arXiv) models and run inference.\n* [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) ![](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory.svg?cacheSeconds=86400) - LLaMA-Factory makes it easy to fine-tunes 100+ large language models with zero-code CLI and Web UI\n* [LLMBox](https://github.com/Alpha-VLLM/LLaMA2-Accessory) ![](https://img.shields.io/github/stars/Alpha-VLLM/LLaMA2-Accessory.svg?cacheSeconds=86400) - LLMBox is a comprehensive library for implementing LLMs, including a unified training pipeline and comprehensive model evaluation.\n* [LLaMA2-Accessory](https://github.com/RUCAIBox/LLMBox) ![](https://img.shields.io/github/stars/RUCAIBox/LLMBox.svg?cacheSeconds=86400) - LLaMA2-Accessory is an open-source toolkit for pretraining, finetuning and deployment of Large Language Models (LLMs) and multimodal LLMs.\n* [LMFlow](https://github.com/OptimalScale/LMFlow) ![](https://img.shields.io/github/stars/OptimalScale/LMFlow.svg?cacheSeconds=86400) - LMFlow is an extensible, convenient, and efficient toolbox for finetuning large machine learning models.\n* [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) ![](https://img.shields.io/github/stars/NVIDIA/Megatron-LM.svg?cacheSeconds=86400) - Megatron-LM is a highly optimized and efficient library for training large language models.\n* [MindNLP](https://github.com/mindspore-lab/mindnlp) ![](https://img.shields.io/github/stars/mindspore-lab/mindnlp.svg?cacheSeconds=86400) - MindNLP is an easy-to-use and high-performance NLP and LLM framework based on MindSpore, compatible with models and datasets of Huggingface.\n* [MLC LLM](https://github.com/mlc-ai/mlc-llm) ![](https://img.shields.io/github/stars/mlc-ai/mlc-llm.svg?cacheSeconds=86400) - MLC LLM is a universal solution that allows any language models to be deployed natively on a diverse set of hardware backends and native applications, plus a productive framework for everyone to further optimize model performance for their own use cases.\n* [Ollam', '{"language":null,"stars":19709,"forks":2479,"watchers":19709,"open_issues":5,"topics":["awesome","awesome-list","data-mining","deep-learning","explainability","interpretability","large-scale-machine-learning","large-scale-ml","machine-learning","machine-learning-operations","ml-operations","ml-ops","mlops","privacy-preserving","privacy-preserving-machine-learning","privacy-preserving-ml","production-machine-learning","production-ml","responsible-ai"],"default_branch":"master","size_kb":2865,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:EthicalML:awesome-production-machine-learning","source_url":"https://github.com/EthicalML/awesome-production-machine-learning"},{"type":"has_code","target_id":"github:EthicalML:awesome-production-machine-learning","source_url":"https://github.com/EthicalML/awesome-production-machine-learning"},{"type":"has_code","target_id":"github:EthicalML:awesome-production-genai","source_url":"https://github.com/EthicalML/awesome-production-genai"},{"type":"has_code","target_id":"github:EthicalML:awesome-production-genai","source_url":"https://github.com/EthicalML/awesome-production-genai"},{"type":"has_code","target_id":"github:autogluon:autogluon","source_url":"https://github.com/autogluon/autogluon"},{"type":"has_code","target_id":"github:keras-team:autokeras","source_url":"https://github.com/keras-team/autokeras"},{"type":"has_code","target_id":"github:automl:auto-sklearn","source_url":"https://github.com/automl/auto-sklearn"},{"type":"has_code","target_id":"github:facebook:Ax","source_url":"https://github.com/facebook/Ax"},{"type":"has_code","target_id":"github:meta-pytorch:botorch","source_url":"https://github.com/meta-pytorch/botorch"},{"type":"has_code","target_id":"github:alteryx:evalml","source_url":"https://github.com/alteryx/evalml"},{"type":"has_code","target_id":"github:feature-engine:feature_engine","source_url":"https://github.com/feature-engine/feature_engine"},{"type":"has_code","target_id":"github:alteryx:featuretools","source_url":"https://github.com/alteryx/featuretools"},{"type":"has_code","target_id":"github:microsoft:FLAML","source_url":"https://github.com/microsoft/FLAML"},{"type":"has_code","target_id":"github:huawei-noah:HEBO","source_url":"https://github.com/huawei-noah/HEBO"},{"type":"has_code","target_id":"github:kubeflow:katib","source_url":"https://github.com/kubeflow/katib"},{"type":"has_code","target_id":"github:keras-team:keras-tuner","source_url":"https://github.com/keras-team/keras-tuner"},{"type":"has_code","target_id":"github:optuna:optuna","source_url":"https://github.com/optuna/optuna"},{"type":"has_code","target_id":"github:google:vizier","source_url":"https://github.com/google/vizier"},{"type":"has_code","target_id":"github:epistasislab:tpot","source_url":"https://github.com/epistasislab/tpot"},{"type":"has_code","target_id":"github:blue-yonder:tsfresh","source_url":"https://github.com/blue-yonder/tsfresh"},{"type":"has_code","target_id":"github:huggingface:accelerate","source_url":"https://github.com/huggingface/accelerate"},{"type":"has_code","target_id":"github:adapter-hub:adapters","source_url":"https://github.com/adapter-hub/adapters"},{"type":"has_code","target_id":"github:microsoft:BitBLAS","source_url":"https://github.com/microsoft/BitBLAS"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:mosaicml:composer","source_url":"https://github.com/mosaicml/composer"},{"type":"has_code","target_id":"github:rapidsai:cudf","source_url":"https://github.com/rapidsai/cudf"},{"type":"has_code","target_id":"github:rapidsai:cuml","source_url":"https://github.com/rapidsai/cuml"},{"type":"has_code","target_id":"github:cupy:cupy","source_url":"https://github.com/cupy/cupy"},{"type":"has_code","target_id":"github:DEAP:deap","source_url":"https://github.com/DEAP/deap"},{"type":"has_code","target_id":"github:deepseek-ai:DeepEP","source_url":"https://github.com/deepseek-ai/DeepEP"},{"type":"has_code","target_id":"github:dmlc:dgl","source_url":"https://github.com/dmlc/dgl"},{"type":"has_code","target_id":"github:intelligent-machine-learning:dlrover","source_url":"https://github.com/intelligent-machine-learning/dlrover"},{"type":"has_code","target_id":"github:dask:dask","source_url":"https://github.com/dask/dask"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:FlagOpen:FlagGems","source_url":"https://github.com/FlagOpen/FlagGems"},{"type":"has_code","target_id":"github:flashlight:flashlight","source_url":"https://github.com/flashlight/flashlight"},{"type":"has_code","target_id":"github:google:flax","source_url":"https://github.com/google/flax"},{"type":"has_code","target_id":"github:gpustack:gpustack","source_url":"https://github.com/gpustack/gpustack"},{"type":"has_code","target_id":"github:learning-at-home:hivemind","source_url":"https://github.com/learning-at-home/hivemind"},{"type":"has_code","target_id":"github:horovod:horovod","source_url":"https://github.com/horovod/horovod"},{"type":"has_code","target_id":"github:jax-ml:jax","source_url":"https://github.com/jax-ml/jax"},{"type":"has_code","target_id":"github:lava-nc:lava","source_url":"https://github.com/lava-nc/lava"},{"type":"has_code","target_id":"github:KomputeProject:kompute","source_url":"https://github.com/KomputeProject/kompute"},{"type":"has_code","target_id":"github:linkedin:Liger-Kernel","source_url":"https://github.com/linkedin/Liger-Kernel"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:ml-explore:mlx","source_url":"https://github.com/ml-explore/mlx"},{"type":"has_code","target_id":"github:modin-project:modin","source_url":"https://github.com/modin-project/modin"},{"type":"has_code","target_id":"github:NVIDIA:TensorRT","source_url":"https://github.com/NVIDIA/TensorRT"},{"type":"has_code","target_id":"github:facebookresearch:nevergrad","source_url":"https://github.com/facebookresearch/nevergrad"},{"type":"has_code","target_id":"github:norse:norse","source_url":"https://github.com/norse/norse"},{"type":"has_code","target_id":"github:numba:numba","source_url":"https://github.com/numba/numba"},{"type":"has_code","target_id":"github:huggingface:optimum","source_url":"https://github.com/huggingface/optimum"},{"type":"has_code","target_id":"github:huggingface:peft","source_url":"https://github.com/huggingface/peft"},{"type":"has_code","target_id":"github:PaddlePaddle:Paddle","source_url":"https://github.com/PaddlePaddle/Paddle"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:Lightning-AI:pytorch-lightning","source_url":"https://github.com/Lightning-AI/pytorch-lightning"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:ray-project:ray","source_url":"https://github.com/ray-project/ray"},{"type":"has_code","target_id":"github:huggingface:setfit","source_url":"https://github.com/huggingface/setfit"},{"type":"has_code","target_id":"github:google-deepmind:sonnet","source_url":"https://github.com/google-deepmind/sonnet"},{"type":"has_code","target_id":"github:mosaicml:streaming","source_url":"https://github.com/mosaicml/streaming"},{"type":"has_code","target_id":"github:tensorflow:tensorflow","source_url":"https://github.com/tensorflow/tensorflow"},{"type":"has_code","target_id":"github:HazyResearch:ThunderKittens","source_url":"https://github.com/HazyResearch/ThunderKittens"},{"type":"has_code","target_id":"github:metaopt:torchopt","source_url":"https://github.com/metaopt/torchopt"},{"type":"has_code","target_id":"github:triton-lang:triton","source_url":"https://github.com/triton-lang/triton"},{"type":"has_code","target_id":"github:vaexio:vaex","source_url":"https://github.com/vaexio/vaex"},{"type":"has_code","target_id":"github:VowpalWabbit:vowpal_wabbit","source_url":"https://github.com/VowpalWabbit/vowpal_wabbit"},{"type":"has_code","target_id":"github:dmlc:xgboost","source_url":"https://github.com/dmlc/xgboost"},{"type":"has_code","target_id":"github:google:yggdrasil-decision-forests","source_url":"https://github.com/google/yggdrasil-decision-forests"},{"type":"has_code","target_id":"github:bitsandbytes-foundation:bitsandbytes","source_url":"https://github.com/bitsandbytes-foundation/bitsandbytes"},{"type":"has_code","target_id":"github:arogozhnikov:einops","source_url":"https://github.com/arogozhnikov/einops"},{"type":"has_code","target_id":"github:scikit-learn:scikit-learn","source_url":"https://github.com/scikit-learn/scikit-learn"},{"type":"has_code","target_id":"github:jeshraghian:snntorch","source_url":"https://github.com/jeshraghian/snntorch"},{"type":"has_code","target_id":"github:yoshitomo-matsubara:torchdistill","source_url":"https://github.com/yoshitomo-matsubara/torchdistill"},{"type":"has_code","target_id":"github:lyhue1991:torchkeras","source_url":"https://github.com/lyhue1991/torchkeras?tab=readme-ov-file"},{"type":"has_code","target_id":"github:volcengine:veScale","source_url":"https://github.com/volcengine/veScale"},{"type":"has_code","target_id":"github:DistrictDataLabs:yellowbrick","source_url":"https://github.com/DistrictDataLabs/yellowbrick"},{"type":"has_code","target_id":"github:argilla-io:argilla","source_url":"https://github.com/argilla-io/argilla"},{"type":"has_code","target_id":"github:cleanlab:cleanlab","source_url":"https://github.com/cleanlab/cleanlab"},{"type":"has_code","target_id":"github:jsbroks:coco-annotator","source_url":"https://github.com/jsbroks/coco-annotator"},{"type":"has_code","target_id":"github:cvat-ai:cvat","source_url":"https://github.com/cvat-ai/cvat"},{"type":"has_code","target_id":"github:doccano:doccano","source_url":"https://github.com/doccano/doccano"},{"type":"has_code","target_id":"github:gretelai:gretel-synthetics","source_url":"https://github.com/gretelai/gretel-synthetics"},{"type":"has_code","target_id":"github:HumanSignal:label-studio","source_url":"https://github.com/HumanSignal/label-studio"},{"type":"has_code","target_id":"github:NVIDIA:NeMo-Curator","source_url":"https://github.com/NVIDIA/NeMo-Curator"},{"type":"has_code","target_id":"github:code-kern-ai:refinery","source_url":"https://github.com/code-kern-ai/refinery"},{"type":"has_code","target_id":"github:sdv-dev:SDV","source_url":"https://github.com/sdv-dev/SDV"},{"type":"has_code","target_id":"github:Hitachi-Automotive-And-Industry-Lab:semantic-segmentation-editor","source_url":"https://github.com/Hitachi-Automotive-And-Industry-Lab/semantic-segmentation-editor"},{"type":"has_code","target_id":"github:vanderschaarlab:synthcity","source_url":"https://github.com/vanderschaarlab/synthcity"},{"type":"has_code","target_id":"github:nv-tlabs:vipe","source_url":"https://github.com/nv-tlabs/vipe"},{"type":"has_code","target_id":"github:ydataai:ydata-synthetic","source_url":"https://github.com/ydataai/ydata-synthetic"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:nifi","source_url":"https://github.com/apache/nifi"},{"type":"has_code","target_id":"github:apache:oozie","source_url":"https://github.com/apache/oozie"},{"type":"has_code","target_id":"github:argoproj:argo-workflows","source_url":"https://github.com/argoproj/argo-workflows"},{"type":"has_code","target_id":"github:couler-proj:couler","source_url":"https://github.com/couler-proj/couler"},{"type":"has_code","target_id":"github:huggingface:datatrove","source_url":"https://github.com/huggingface/datatrove"},{"type":"has_code","target_id":"github:dagster-io:dagster","source_url":"https://github.com/dagster-io/dagster"},{"type":"has_code","target_id":"github:dbt-labs:dbt-core","source_url":"https://github.com/dbt-labs/dbt-core"},{"type":"has_code","target_id":"github:flyteorg:flyte","source_url":"https://github.com/flyteorg/flyte"},{"type":"has_code","target_id":"github:Netflix:genie","source_url":"https://github.com/Netflix/genie"},{"type":"has_code","target_id":"github:dagworks-inc:hamilton","source_url":"https://github.com/dagworks-inc/hamilton"},{"type":"has_code","target_id":"github:instill-ai:instill-core","source_url":"https://github.com/instill-ai/instill-core"},{"type":"has_code","target_id":"github:instructor-ai:instructor","source_url":"https://github.com/instructor-ai/instructor"},{"type":"has_code","target_id":"github:kedro-org:kedro","source_url":"https://github.com/kedro-org/kedro"},{"type":"has_code","target_id":"github:spotify:luigi","source_url":"https://github.com/spotify/luigi"},{"type":"has_code","target_id":"github:Netflix:metaflow","source_url":"https://github.com/Netflix/metaflow"},{"type":"has_code","target_id":"github:pachyderm:pachyderm","source_url":"https://github.com/pachyderm/pachyderm"},{"type":"has_code","target_id":"github:ploomber:ploomber","source_url":"https://github.com/ploomber/ploomber"},{"type":"has_code","target_id":"github:pixeltable:pixeltable","source_url":"https://github.com/pixeltable/pixeltable"},{"type":"has_code","target_id":"github:PrefectHQ:prefect","source_url":"https://github.com/PrefectHQ/prefect"},{"type":"has_code","target_id":"github:google:seqio","source_url":"https://github.com/google/seqio"},{"type":"has_code","target_id":"github:snakemake:snakemake","source_url":"https://github.com/snakemake/snakemake"},{"type":"has_code","target_id":"github:towhee-io:towhee","source_url":"https://github.com/towhee-io/towhee"},{"type":"has_code","target_id":"github:Unstructured-IO:unstructured","source_url":"https://github.com/Unstructured-IO/unstructured"},{"type":"has_code","target_id":"github:zenml-io:zenml","source_url":"https://github.com/zenml-io/zenml"},{"type":"has_code","target_id":"github:apache:zeppelin","source_url":"https://github.com/apache/zeppelin"},{"type":"has_code","target_id":"github:deepnote:deepnote","source_url":"https://github.com/deepnote/deepnote"},{"type":"has_code","target_id":"github:jupyter:notebook","source_url":"https://github.com/jupyter/notebook"},{"type":"has_code","target_id":"github:marimo-team:marimo","source_url":"https://github.com/marimo-team/marimo"},{"type":"has_code","target_id":"github:nteract:papermill","source_url":"https://github.com/nteract/papermill"},{"type":"has_code","target_id":"github:polynote:polynote","source_url":"https://github.com/polynote/polynote"},{"type":"has_code","target_id":"github:rstudio:rmarkdown","source_url":"https://github.com/rstudio/rmarkdown"},{"type":"has_code","target_id":"github:stencila:stencila","source_url":"https://github.com/stencila/stencila"},{"type":"has_code","target_id":"github:voila-dashboards:voila","source_url":"https://github.com/voila-dashboards/voila"},{"type":"has_code","target_id":"github:dotnet:interactive","source_url":"https://github.com/dotnet/interactive"},{"type":"has_code","target_id":"github:NVIDIA:aistore","source_url":"https://github.com/NVIDIA/aistore"},{"type":"has_code","target_id":"github:Alluxio:alluxio","source_url":"https://github.com/Alluxio/alluxio"},{"type":"has_code","target_id":"github:apache:arrow","source_url":"https://github.com/apache/arrow"},{"type":"has_code","target_id":"github:apache:druid","source_url":"https://github.com/apache/druid"},{"type":"has_code","target_id":"github:apache:hudi","source_url":"https://github.com/apache/hudi"},{"type":"has_code","target_id":"github:apache:iceberg","source_url":"https://github.com/apache/iceberg"},{"type":"has_code","target_id":"github:apache:ignite","source_url":"https://github.com/apache/ignite"},{"type":"has_code","target_id":"github:apache:parquet-java","source_url":"https://github.com/apache/parquet-java"},{"type":"has_code","target_id":"github:apache:pinot","source_url":"https://github.com/apache/pinot"},{"type":"has_code","target_id":"github:casibase:casibase","source_url":"https://github.com/casibase/casibase"},{"type":"has_code","target_id":"github:chroma-core:chroma","source_url":"https://github.com/chroma-core/chroma"},{"type":"has_code","target_id":"github:ClickHouse:ClickHouse","source_url":"https://github.com/ClickHouse/ClickHouse"},{"type":"has_code","target_id":"github:delta-io:delta","source_url":"https://github.com/delta-io/delta"},{"type":"has_code","target_id":"github:geldata:gel","source_url":"https://github.com/geldata/gel"},{"type":"has_code","target_id":"github:zilliztech:GPTCache","source_url":"https://github.com/zilliztech/GPTCache"},{"type":"has_code","target_id":"github:influxdata:influxdb","source_url":"https://github.com/influxdata/influxdb"},{"type":"has_code","target_id":"github:milvus-io:milvus","source_url":"https://github.com/milvus-io/milvus"},{"type":"has_code","target_id":"github:marqo-ai:marqo","source_url":"https://github.com/marqo-ai/marqo"},{"type":"has_code","target_id":"github:pgvector:pgvector","source_url":"https://github.com/pgvector/pgvector"},{"type":"has_code","target_id":"github:postgresml:postgresml","source_url":"https://github.com/postgresml/postgresml"},{"type":"has_code","target_id":"github:huggingface:safetensors","source_url":"https://github.com/huggingface/safetensors"},{"type":"has_code","target_id":"github:timescale:timescaledb","source_url":"https://github.com/timescale/timescaledb"},{"type":"has_code","target_id":"github:weaviate:weaviate","source_url":"https://github.com/weaviate/weaviate"},{"type":"has_code","target_id":"github:zarr-developers:zarr-python","source_url":"https://github.com/zarr-developers/zarr-python"},{"type":"has_code","target_id":"github:apache:beam","source_url":"https://github.com/apache/beam"},{"type":"has_code","target_id":"github:apache:flink","source_url":"https://github.com/apache/flink"},{"type":"has_code","target_id":"github:apache:kafka","source_url":"https://github.com/apache/kafka"},{"type":"has_code","target_id":"github:apache:samza","source_url":"https://github.com/apache/samza"},{"type":"has_code","target_id":"github:apache:spark","source_url":"https://github.com/apache/spark"},{"type":"has_code","target_id":"github:bytewax:bytewax","source_url":"https://github.com/bytewax/bytewax"},{"type":"has_code","target_id":"github:airtai:faststream","source_url":"https://github.com/airtai/faststream"},{"type":"has_code","target_id":"github:Waikato:moa","source_url":"https://github.com/Waikato/moa"},{"type":"has_code","target_id":"github:mosaicml:streaming","source_url":"https://github.com/mosaicml/streaming"},{"type":"has_code","target_id":"github:risingwavelabs:risingwave","source_url":"https://github.com/risingwavelabs/risingwave"},{"type":"has_code","target_id":"github:google:tensorstore","source_url":"https://github.com/google/tensorstore"},{"type":"has_code","target_id":"github:Agenta-AI:agenta","source_url":"https://github.com/Agenta-AI/agenta"},{"type":"has_code","target_id":"github:lyogavin:airllm","source_url":"https://github.com/lyogavin/airllm"},{"type":"has_code","target_id":"github:facebookincubator:AITemplate","source_url":"https://github.com/facebookincubator/AITemplate"},{"type":"has_code","target_id":"github:bentoml:BentoML","source_url":"https://github.com/bentoml/BentoML"},{"type":"has_code","target_id":"github:dataelement:bisheng","source_url":"https://github.com/dataelement/bisheng"},{"type":"has_code","target_id":"github:jolibrain:deepdetect","source_url":"https://github.com/jolibrain/deepdetect"},{"type":"has_code","target_id":"github:ai-dynamo:dynamo","source_url":"https://github.com/ai-dynamo/dynamo"},{"type":"has_code","target_id":"github:exo-explore:exo","source_url":"https://github.com/exo-explore/exo"},{"type":"has_code","target_id":"github:firebase:genkit","source_url":"https://github.com/firebase/genkit"},{"type":"has_code","target_id":"github:roboflow:inference","source_url":"https://github.com/roboflow/inference"},{"type":"has_code","target_id":"github:michaelfeil:infinity","source_url":"https://github.com/michaelfeil/infinity"},{"type":"has_code","target_id":"github:intel:ipex-llm","source_url":"https://github.com/intel/ipex-llm"},{"type":"has_code","target_id":"github:BerriAI:litellm","source_url":"https://github.com/BerriAI/litellm"},{"type":"has_code","target_id":"github:jina-ai:serve","source_url":"https://github.com/jina-ai/serve"},{"type":"has_code","target_id":"github:kiln-ai:kiln","source_url":"https://github.com/kiln-ai/kiln"},{"type":"has_code","target_id":"github:kserve:kserve","source_url":"https://github.com/kserve/kserve"},{"type":"has_code","target_id":"github:kvcache-ai:ktransformers","source_url":"https://github.com/kvcache-ai/ktransformers"},{"type":"has_code","target_id":"github:Scale3-Labs:langtrace","source_url":"https://github.com/Scale3-Labs/langtrace"},{"type":"has_code","target_id":"github:leptonai:leptonai","source_url":"https://github.com/leptonai/leptonai"},{"type":"has_code","target_id":"github:ModelTC:lightllm","source_url":"https://github.com/ModelTC/lightllm"},{"type":"has_code","target_id":"github:ggml-org:llama.cpp","source_url":"https://github.com/ggml-org/llama.cpp"},{"type":"has_code","target_id":"github:InternLM:lmdeploy","source_url":"https://github.com/InternLM/lmdeploy"},{"type":"has_code","target_id":"github:lmstudio-ai:lms","source_url":"https://github.com/lmstudio-ai/lms"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mindsdb:mindsdb","source_url":"https://github.com/mindsdb/mindsdb"},{"type":"has_code","target_id":"github:mlrun:mlrun","source_url":"https://github.com/mlrun/mlrun"},{"type":"has_code","target_id":"github:SeldonIO:mlserver","source_url":"https://github.com/SeldonIO/mlserver"},{"type":"has_code","target_id":"github:mosecorg:mosec","source_url":"https://github.com/mosecorg/mosec"},{"type":"has_code","target_id":"github:nndeploy:nndeploy","source_url":"https://github.com/nndeploy/nndeploy"},{"type":"has_code","target_id":"github:nuclio:nuclio","source_url":"https://github.com/nuclio/nuclio"},{"type":"has_code","target_id":"github:bentoml:OpenLLM","source_url":"https://github.com/bentoml/OpenLLM"},{"type":"has_code","target_id":"github:openvinotoolkit:openvino","source_url":"https://github.com/openvinotoolkit/openvino"},{"type":"has_code","target_id":"github:open-webui:open-webui","source_url":"https://github.com/open-webui/open-webui"},{"type":"has_code","target_id":"github:algorithmicsuperintelligence:optillm","source_url":"https://github.com/algorithmicsuperintelligence/optillm"},{"type":"has_code","target_id":"github:SJTU-IPADS:PowerInfer","source_url":"https://github.com/SJTU-IPADS/PowerInfer"},{"type":"has_code","target_id":"github:neulab:prompt2model","source_url":"https://github.com/neulab/prompt2model"},{"type":"has_code","target_id":"github:SeldonIO:seldon-core","source_url":"https://github.com/SeldonIO/seldon-core"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:skypilot-org:skypilot","source_url":"https://github.com/skypilot-org/skypilot"},{"type":"has_code","target_id":"github:tensorflow:serving","source_url":"https://github.com/tensorflow/serving"},{"type":"has_code","target_id":"github:huggingface:text-generation-inference","source_url":"https://github.com/huggingface/text-generation-inference"},{"type":"has_code","target_id":"github:pytorch:serve","source_url":"https://github.com/pytorch/serve"},{"type":"has_code","target_id":"github:meta-pytorch:torchtune","source_url":"https://github.com/meta-pytorch/torchtune"},{"type":"has_code","target_id":"github:transformerlab:transformerlab-app","source_url":"https://github.com/transformerlab/transformerlab-app"},{"type":"has_code","target_id":"github:triton-inference-server:server","source_url":"https://github.com/triton-inference-server/server"},{"type":"has_code","target_id":"github:vercel:ai","source_url":"https://github.com/vercel/ai"},{"type":"has_code","target_id":"github:vespa-engine:vespa","source_url":"https://github.com/vespa-engine/vespa"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:tatsu-lab:alpaca_eval","source_url":"https://github.com/tatsu-lab/alpaca_eval"},{"type":"has_code","target_id":"github:erikbern:ann-benchmarks","source_url":"https://github.com/erikbern/ann-benchmarks"},{"type":"has_code","target_id":"github:stanford-futuredata:ARES","source_url":"https://github.com/stanford-futuredata/ARES"},{"type":"has_code","target_id":"github:beir-cellar:beir","source_url":"https://github.com/beir-cellar/beir"},{"type":"has_code","target_id":"github:bigcode-project:bigcode-evaluation-harness","source_url":"https://github.com/bigcode-project/bigcode-evaluation-harness"},{"type":"has_code","target_id":"github:Unbabel:COMET","source_url":"https://github.com/Unbabel/COMET"},{"type":"has_code","target_id":"github:hkust-nlp:ceval","source_url":"https://github.com/hkust-nlp/ceval"},{"type":"has_code","target_id":"github:deepchecks:deepchecks","source_url":"https://github.com/deepchecks/deepchecks"},{"type":"has_code","target_id":"github:confident-ai:deepeval","source_url":"https://github.com/confident-ai/deepeval"},{"type":"has_code","target_id":"github:facebookresearch:DomainBed","source_url":"https://github.com/facebookresearch/DomainBed"},{"type":"has_code","target_id":"github:Cloud-CV:EvalAI","source_url":"https://github.com/Cloud-CV/EvalAI"},{"type":"has_code","target_id":"github:mlfoundations:evalchemy","source_url":"https://github.com/mlfoundations/evalchemy"},{"type":"has_code","target_id":"github:evalplus:evalplus","source_url":"https://github.com/evalplus/evalplus"},{"type":"has_code","target_id":"github:openai:evals","source_url":"https://github.com/openai/evals"},{"type":"has_code","target_id":"github:modelscope:evalscope","source_url":"https://github.com/modelscope/evalscope"},{"type":"has_code","target_id":"github:huggingface:evaluate","source_url":"https://github.com/huggingface/evaluate"},{"type":"has_code","target_id":"github:evidentlyai:evidently","source_url":"https://github.com/evidentlyai/evidently"},{"type":"has_code","target_id":"github:OpenLMLab:GAOKAO-Bench","source_url":"https://github.com/OpenLMLab/GAOKAO-Bench"},{"type":"has_code","target_id":"github:Giskard-AI:giskard","source_url":"https://github.com/Giskard-AI/giskard"},{"type":"has_code","target_id":"github:openai:human-eval","source_url":"https://github.com/openai/human-eval"},{"type":"has_code","target_id":"github:Helicone:helicone","source_url":"https://github.com/Helicone/helicone"},{"type":"has_code","target_id":"github:stanford-crfm:helm","source_url":"https://github.com/stanford-crfm/helm"},{"type":"has_code","target_id":"github:UKGovernmentBEIS:inspect_ai","source_url":"https://github.com/UKGovernmentBEIS/inspect_ai"},{"type":"has_code","target_id":"github:jitsi:jiwer","source_url":"https://github.com/jitsi/jiwer"},{"type":"has_code","target_id":"github:lmnr-ai:lmnr","source_url":"https://github.com/lmnr-ai/lmnr"},{"type":"has_code","target_id":"github:langfuse:langfuse","source_url":"https://github.com/langfuse/langfuse"},{"type":"has_code","target_id":"github:JohnSnowLabs:langtest","source_url":"https://github.com/JohnSnowLabs/langtest"},{"type":"has_code","target_id":"github:EleutherAI:lm-evaluation-harness","source_url":"https://github.com/EleutherAI/lm-evaluation-harness"},{"type":"has_code","target_id":"github:langwatch:langwatch","source_url":"https://github.com/langwatch/langwatch"},{"type":"has_code","target_id":"github:huggingface:lighteval","source_url":"https://github.com/huggingface/lighteval"},{"type":"has_code","target_id":"github:lunary-ai:lunary","source_url":"https://github.com/lunary-ai/lunary"},{"type":"has_code","target_id":"github:ray-project:llmperf","source_url":"https://github.com/ray-project/llmperf"},{"type":"has_code","target_id":"github:EvolvingLMMs-Lab:lmms-eval","source_url":"https://github.com/EvolvingLMMs-Lab/lmms-eval"},{"type":"has_code","target_id":"github:google-deepmind:meltingpot","source_url":"https://github.com/google-deepmind/meltingpot"},{"type":"has_code","target_id":"github:Farama-Foundation:Metaworld","source_url":"https://github.com/Farama-Foundation/Metaworld"},{"type":"has_code","target_id":"github:mir-evaluation:mir_eval","source_url":"https://github.com/mir-evaluation/mir_eval"},{"type":"has_code","target_id":"github:mlcommons:inference","source_url":"https://github.com/mlcommons/inference"},{"type":"has_code","target_id":"github:mlcommons:inference","source_url":"https://github.com/mlcommons/inference"},{"type":"has_code","target_id":"github:NannyML:nannyml","source_url":"https://github.com/NannyML/nannyml"},{"type":"has_code","target_id":"github:snap-stanford:ogb","source_url":"https://github.com/snap-stanford/ogb"},{"type":"has_code","target_id":"github:dezoito:ollama-grid-search","source_url":"https://github.com/dezoito/ollama-grid-search"},{"type":"has_code","target_id":"github:open-compass:OpenCompass","source_url":"https://github.com/open-compass/OpenCompass"},{"type":"has_code","target_id":"github:openlit:openlit","source_url":"https://github.com/openlit/openlit"},{"type":"has_code","target_id":"github:traceloop:openllmetry","source_url":"https://github.com/traceloop/openllmetry"},{"type":"has_code","target_id":"github:comet-ml:opik","source_url":"https://github.com/comet-ml/opik"},{"type":"has_code","target_id":"github:HumanCompatibleAI:overcooked_ai","source_url":"https://github.com/HumanCompatibleAI/overcooked_ai"},{"type":"has_code","target_id":"github:Arize-ai:phoenix","source_url":"https://github.com/Arize-ai/phoenix"},{"type":"has_code","target_id":"github:microsoft:promptbench","source_url":"https://github.com/microsoft/promptbench"},{"type":"has_code","target_id":"github:promptfoo:promptfoo","source_url":"https://github.com/promptfoo/promptfoo"},{"type":"has_code","target_id":"github:prometheus-eval:prometheus-eval","source_url":"https://github.com/prometheus-eval/prometheus-eval"},{"type":"has_code","target_id":"github:raga-ai-hub:RagaAI-Catalyst","source_url":"https://github.com/raga-ai-hub/RagaAI-Catalyst"},{"type":"has_code","target_id":"github:explodinggradients:ragas","source_url":"https://github.com/explodinggradients/ragas"},{"type":"has_code","target_id":"github:amazon-science:RAGChecker","source_url":"https://github.com/amazon-science/RAGChecker"},{"type":"has_code","target_id":"github:allenai:reward-bench","source_url":"https://github.com/allenai/reward-bench"},{"type":"has_code","target_id":"github:stepjam:RLBench","source_url":"https://github.com/stepjam/RLBench"},{"type":"has_code","target_id":"github:simpler-env:SimplerEnv","source_url":"https://github.com/simpler-env/SimplerEnv"},{"type":"has_code","target_id":"github:SwanHubX:SwanLab","source_url":"https://github.com/SwanHubX/SwanLab"},{"type":"has_code","target_id":"github:Picovoice:speech-to-text-benchmark","source_url":"https://github.com/Picovoice/speech-to-text-benchmark"},{"type":"has_code","target_id":"github:tensorflow:model-analysis","source_url":"https://github.com/tensorflow/model-analysis"},{"type":"has_code","target_id":"github:pytorch:benchmark","source_url":"https://github.com/pytorch/benchmark"},{"type":"has_code","target_id":"github:truera:trulens","source_url":"https://github.com/truera/trulens"},{"type":"has_code","target_id":"github:HowieHwong:TrustLLM","source_url":"https://github.com/HowieHwong/TrustLLM"},{"type":"has_code","target_id":"github:Vchitect:VBench","source_url":"https://github.com/Vchitect/VBench"},{"type":"has_code","target_id":"github:open-compass:VLMEvalKit","source_url":"https://github.com/open-compass/VLMEvalKit"},{"type":"has_code","target_id":"github:dssg:aequitas","source_url":"https://github.com/dssg/aequitas"},{"type":"has_code","target_id":"github:Trusted-AI:AIX360","source_url":"https://github.com/Trusted-AI/AIX360"},{"type":"has_code","target_id":"github:Trusted-AI:AIF360","source_url":"https://github.com/Trusted-AI/AIF360"},{"type":"has_code","target_id":"github:SeldonIO:alibi","source_url":"https://github.com/SeldonIO/alibi"},{"type":"has_code","target_id":"github:pytorch:captum","source_url":"https://github.com/pytorch/captum"},{"type":"has_code","target_id":"github:fairlearn:fairlearn","source_url":"https://github.com/fairlearn/fairlearn"},{"type":"has_code","target_id":"github:interpretml:interpret","source_url":"https://github.com/interpretml/interpret"},{"type":"has_code","target_id":"github:lightly-ai:lightly","source_url":"https://github.com/lightly-ai/lightly"},{"type":"has_code","target_id":"github:aerdem4:lofo-importance","source_url":"https://github.com/aerdem4/lofo-importance"},{"type":"has_code","target_id":"github:mljar:mljar-supervised","source_url":"https://github.com/mljar/mljar-supervised"},{"type":"has_code","target_id":"github:understandable-machine-intelligence-lab:Quantus","source_url":"https://github.com/understandable-machine-intelligence-lab/Quantus"},{"type":"has_code","target_id":"github:shap:shap","source_url":"https://github.com/shap/shap"},{"type":"has_code","target_id":"github:MAIF:shapash","source_url":"https://github.com/MAIF/shapash"},{"type":"has_code","target_id":"github:pair-code:what-if-tool","source_url":"https://github.com/pair-code/what-if-tool"},{"type":"has_code","target_id":"github:feast-dev:feast","source_url":"https://github.com/feast-dev/feast"},{"type":"has_code","target_id":"github:featureform:featureform","source_url":"https://github.com/featureform/featureform"},{"type":"has_code","target_id":"github:logicalclocks:hopsworks","source_url":"https://github.com/logicalclocks/hopsworks"},{"type":"has_code","target_id":"github:SeldonIO:alibi-detect","source_url":"https://github.com/SeldonIO/alibi-detect"},{"type":"has_code","target_id":"github:unit8co:darts","source_url":"https://github.com/unit8co/darts"},{"type":"has_code","target_id":"github:awslabs:deequ","source_url":"https://github.com/awslabs/deequ"},{"type":"has_code","target_id":"github:yzhao062:pyod","source_url":"https://github.com/yzhao062/pyod"},{"type":"has_code","target_id":"github:tensorflow:data-validation","source_url":"https://github.com/tensorflow/data-validation"},{"type":"has_code","target_id":"github:activeloopai:deeplake","source_url":"https://github.com/activeloopai/deeplake"},{"type":"has_code","target_id":"github:facebookresearch:detectron2","source_url":"https://github.com/facebookresearch/detectron2"},{"type":"has_code","target_id":"github:keras-team:keras-cv","source_url":"https://github.com/keras-team/keras-cv"},{"type":"has_code","target_id":"github:kornia:kornia","source_url":"https://github.com/kornia/kornia"},{"type":"has_code","target_id":"github:salesforce:LAVIS","source_url":"https://github.com/salesforce/LAVIS"},{"type":"has_code","target_id":"github:bcmi:libcom","source_url":"https://github.com/bcmi/libcom"},{"type":"has_code","target_id":"github:lightly-ai:lightly-train","source_url":"https://github.com/lightly-ai/lightly-train"},{"type":"has_code","target_id":"github:open-mmlab:mmcv","source_url":"https://github.com/open-mmlab/mmcv"},{"type":"has_code","target_id":"github:Deci-AI:super-gradients","source_url":"https://github.com/Deci-AI/super-gradients"},{"type":"has_code","target_id":"github:roboflow:supervision","source_url":"https://github.com/roboflow/supervision"},{"type":"has_code","target_id":"github:NUS-HPC-AI-Lab:VideoSys","source_url":"https://github.com/NUS-HPC-AI-Lab/VideoSys"},{"type":"has_code","target_id":"github:Marker-Inc-Korea:AutoRAG","source_url":"https://github.com/Marker-Inc-Korea/AutoRAG"},{"type":"has_code","target_id":"github:FlagOpen:FlagEmbedding","source_url":"https://github.com/FlagOpen/FlagEmbedding"},{"type":"has_code","target_id":"github:truefoundry:cognita","source_url":"https://github.com/truefoundry/cognita"},{"type":"has_code","target_id":"github:docarray:docarray","source_url":"https://github.com/docarray/docarray"},{"type":"has_code","target_id":"github:facebookresearch:faiss","source_url":"https://github.com/facebookresearch/faiss"},{"type":"has_code","target_id":"github:IntelLabs:fastRAG","source_url":"https://github.com/IntelLabs/fastRAG"},{"type":"has_code","target_id":"github:microsoft:graphrag","source_url":"https://github.com/microsoft/graphrag"},{"type":"has_code","target_id":"github:OSU-NLP-Group:HippoRAG","source_url":"https://github.com/OSU-NLP-Group/HippoRAG"},{"type":"has_code","target_id":"github:EmbeddedLLM:JamAIBase","source_url":"https://github.com/EmbeddedLLM/JamAIBase"},{"type":"has_code","target_id":"github:google:langextract","source_url":"https://github.com/google/langextract"},{"type":"has_code","target_id":"github:HKUDS:LightRAG","source_url":"https://github.com/HKUDS/LightRAG"},{"type":"has_code","target_id":"github:llmware-ai:llmware","source_url":"https://github.com/llmware-ai/llmware"},{"type":"has_code","target_id":"github:mem0ai:mem0","source_url":"https://github.com/mem0ai/mem0"},{"type":"has_code","target_id":"github:yahoojapan:NGT","source_url":"https://github.com/yahoojapan/NGT"},{"type":"has_code","target_id":"github:nmslib:nmslib","source_url":"https://github.com/nmslib/nmslib"},{"type":"has_code","target_id":"github:qdrant:qdrant","source_url":"https://github.com/qdrant/qdrant"},{"type":"has_code","target_id":"github:SciPhi-AI:R2R","source_url":"https://github.com/SciPhi-AI/R2R"},{"type":"has_code","target_id":"github:infiniflow:ragflow","source_url":"https://github.com/infiniflow/ragflow"},{"type":"has_code","target_id":"github:gabrielchua:RAGxplorer","source_url":"https://github.com/gabrielchua/RAGxplorer"},{"type":"has_code","target_id":"github:IntelLabs:RAG-FiT","source_url":"https://github.com/IntelLabs/RAG-FiT"},{"type":"has_code","target_id":"github:microsoft:TextWorld","source_url":"https://github.com/microsoft/TextWorld"},{"type":"has_code","target_id":"github:vanna-ai:vanna","source_url":"https://github.com/vanna-ai/vanna"},{"type":"has_code","target_id":"github:andrewyng:aisuite","source_url":"https://github.com/andrewyng/aisuite"},{"type":"has_code","target_id":"github:PKU-Alignment:align-anything","source_url":"https://github.com/PKU-Alignment/align-anything"},{"type":"has_code","target_id":"github:MaartenGr:BERTopic","source_url":"https://github.com/MaartenGr/BERTopic"},{"type":"has_code","target_id":"github:dagworks-inc:burr","source_url":"https://github.com/dagworks-inc/burr"},{"type":"has_code","target_id":"github:salesforce:CodeTF","source_url":"https://github.com/salesforce/CodeTF"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:stanfordnlp:dspy","source_url":"https://github.com/stanfordnlp/dspy"},{"type":"has_code","target_id":"github:dust-tt:dust","source_url":"https://github.com/dust-tt/dust"},{"type":"has_code","target_id":"github:espnet:espnet","source_url":"https://github.com/espnet/espnet"},{"type":"has_code","target_id":"github:lm-sys:FastChat","source_url":"https://github.com/lm-sys/FastChat"},{"type":"has_code","target_id":"github:flairNLP:flair","source_url":"https://github.com/flairNLP/flair"},{"type":"has_code","target_id":"github:piskvorky:gensim","source_url":"https://github.com/piskvorky/gensim"},{"type":"has_code","target_id":"github:meta-pytorch:gpt-fast","source_url":"https://github.com/meta-pytorch/gpt-fast"},{"type":"has_code","target_id":"github:h2oai:h2ogpt","source_url":"https://github.com/h2oai/h2ogpt"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:oughtinc:ice","source_url":"https://github.com/oughtinc/ice"},{"type":"has_code","target_id":"github:lamini-ai:lamini","source_url":"https://github.com/lamini-ai/lamini"},{"type":"has_code","target_id":"github:langchain-ai:langchain","source_url":"https://github.com/langchain-ai/langchain"},{"type":"has_code","target_id":"github:run-llama:llama_index","source_url":"https://github.com/run-llama/llama_index"},{"type":"has_code","target_id":"github:meta-llama:llama","source_url":"https://github.com/meta-llama/llama"},{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory","source_url":"https://github.com/hiyouga/LLaMA-Factory"},{"type":"has_code","target_id":"github:Alpha-VLLM:LLaMA2-Accessory","source_url":"https://github.com/Alpha-VLLM/LLaMA2-Accessory"},{"type":"has_code","target_id":"github:RUCAIBox:LLMBox","source_url":"https://github.com/RUCAIBox/LLMBox"},{"type":"has_code","target_id":"github:OptimalScale:LMFlow","source_url":"https://github.com/OptimalScale/LMFlow"},{"type":"has_code","target_id":"github:NVIDIA:Megatron-LM","source_url":"https://github.com/NVIDIA/Megatron-LM"},{"type":"has_code","target_id":"github:mindspore-lab:mindnlp","source_url":"https://github.com/mindspore-lab/mindnlp"},{"type":"has_code","target_id":"github:mlc-ai:mlc-llm","source_url":"https://github.com/mlc-ai/mlc-llm"},{"type":"has_code","target_id":"github:ollama:ollama","source_url":"https://github.com/ollama/ollama"}]', NULL, 'MIT', 'approved', 80, 'b520f9adc848bae9a418f5e23d0c4535', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-EthicalML-awesome-production-machine-learning from https://github.com/EthicalML.png
Image converted to WebP: data/images/github-EthicalML-awesome-production-machine-learning.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-tensorflow-tfjs', 'github--tensorflow--tfjs', 'tfjs', 'tensorflow', 'TensorFlow.js is an open-source hardware-accelerated JavaScript library for training and deploying machine learning models. **Develop ML in the Browser** <br/> Use flexible and intuitive APIs to build models from scratch using the low-level JavaScript linear algebra library or the high-level layers API. **Develop ML in Node.js** <br/> Execute native TensorFlow with the same TensorFlow.js API under the Node.js runtime. **Run Existing models** <br/> Use TensorFlow.js model converters to run pre...', '["deep-learning","deep-neural-network","gpu-acceleration","javascript","machine-learning","neural-network","typescript","wasm","web-assembly","webgl","typescript"]', 'other', 19033, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/tensorflow/tfjs","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '# TensorFlow.js\n\nTensorFlow.js is an open-source hardware-accelerated JavaScript library for\ntraining and deploying machine learning models.\n\n\n**Develop ML in the Browser** <br/>\nUse flexible and intuitive APIs to build models from scratch using the low-level\nJavaScript linear algebra library or the high-level layers API.\n\n**Develop ML in Node.js** <br/>\nExecute native TensorFlow with the same TensorFlow.js API under the Node.js\nruntime.\n\n**Run Existing models** <br/>\nUse TensorFlow.js model converters to run pre-existing TensorFlow models right\nin the browser.\n\n**Retrain Existing models** <br/>\nRetrain pre-existing ML models using sensor data connected to the browser or\nother client-side data.\n\n## About this repo\n\nThis repository contains the logic and scripts that combine\nseveral packages.\n\nAPIs:\n- [TensorFlow.js Core](/tfjs-core),\n  a flexible low-level API for neural networks and numerical computation.\n- [TensorFlow.js Layers](/tfjs-layers),\n  a high-level API which implements functionality similar to\n  [Keras](https://keras.io/).\n- [TensorFlow.js Data](/tfjs-data),\n  a simple API to load and prepare data analogous to\n  [tf.data](https://www.tensorflow.org/guide/datasets).\n- [TensorFlow.js Converter](/tfjs-converter),\n  tools to import a TensorFlow SavedModel to TensorFlow.js\n- [TensorFlow.js Vis](/tfjs-vis),\n  in-browser visualization for TensorFlow.js models\n- [TensorFlow.js AutoML](/tfjs-automl),\n  Set of APIs to load and run models produced by\n  [AutoML Edge](https://cloud.google.com/vision/automl/docs/edge-quickstart).\n\n\nBackends/Platforms:\n- [TensorFlow.js CPU Backend](/tfjs-backend-cpu), pure-JS backend for Node.js and the browser.\n- [TensorFlow.js WebGL Backend](/tfjs-backend-webgl), WebGL backend for the browser.\n- [TensorFlow.js WASM Backend](/tfjs-backend-wasm), WebAssembly backend for the browser.\n- [TensorFlow.js WebGPU](/tfjs-backend-webgpu), WebGPU backend for the browser.\n- [TensorFlow.js Node](/tfjs-node), Node.js platform via TensorFlow C++ adapter.\n- [TensorFlow.js React Native](/tfjs-react-native), React Native platform via expo-gl adapter.\n\nIf you care about bundle size, you can import those packages individually.\n\nIf you are looking for Node.js support, check out the [TensorFlow.js Node directory](/tfjs-node).\n\n## Examples\n\nCheck out our\n[examples repository](https://github.com/tensorflow/tfjs-examples)\nand our [tutorials](https://js.tensorflow.org/tutorials/).\n\n## Gallery\n\nBe sure to check out [the gallery](GALLERY.md) of all projects related to TensorFlow.js.\n\n## Pre-trained models\n\nBe sure to also check out our [models repository](https://github.com/tensorflow/tfjs-models) where we host pre-trained models\non NPM.\n\n## Benchmarks\n\n* [Local benchmark tool](https://tfjs-benchmarks.web.app/). Use this webpage tool to collect the performance related metrics (speed, memory, etc) of TensorFlow.js models and kernels **on your local device** with CPU, WebGL or WASM backends. You can benchmark custom models by following this [guide](https://github.com/tensorflow/tfjs/blob/master/e2e/benchmarks/local-benchmark/README.md).\n* [Multi-device benchmark tool](https://github.com/tensorflow/tfjs/tree/master/e2e/benchmarks/browserstack-benchmark/README.md). Use this tool to collect the same performance related metrics **on a collection of remote devices**.\n\n## Getting started\n\nThere are two main ways to get TensorFlow.js in your JavaScript project:\nvia <a href="https://developer.mozilla.org/en-US/docs/Learn/HTML/Howto/Use_JavaScript_within_a_webpage" target="_blank">script tags</a> <strong>or</strong> by installing it from <a href="https://www.npmjs.com/" target="_blank">NPM</a>\nand using a build tool like <a href="https://parceljs.org/" target="_blank">Parcel</a>,\n<a href="https://webpack.js.org/" target="_blank">WebPack</a>, or <a href="https://rollupjs.org/guide/en" target="_blank">Rollup</a>.\n\n### via Script Tag\n\nAdd the following code to an HTML file:\n\n```html\n<html>\n  <head>\n    <!-- Load TensorFlow.js -->\n    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"> </script>\n\n\n    <!-- Place your code in the script tag below. You can also use an external .js file -->\n    <script>\n      // Notice there is no ''import'' statement. ''tf'' is available on the index-page\n      // because of the script tag above.\n\n      // Define a model for linear regression.\n      const model = tf.sequential();\n      model.add(tf.layers.dense({units: 1, inputShape: [1]}));\n\n      // Prepare the model for training: Specify the loss and the optimizer.\n      model.compile({loss: ''meanSquaredError'', optimizer: ''sgd''});\n\n      // Generate some synthetic data for training.\n      const xs = tf.tensor2d([1, 2, 3, 4], [4, 1]);\n      const ys = tf.tensor2d([1, 3, 5, 7], [4, 1]);\n\n      // Train the model using the data.\n      model.fit(xs, ys).then(() => {\n        // Use the model to do inference on a data point the model hasn''t seen before:\n        // Open the browser devtools to see the output\n        model.predict(tf.tensor2d([5], [1, 1])).print();\n      });\n    </script>\n  </head>\n\n  <body>\n  </body>\n</html>\n```\n\nOpen up that HTML file in your browser, and the code should run!\n\n### via NPM\n\nAdd TensorFlow.js to your project using <a href="https://yarnpkg.com/en/" target="_blank">yarn</a> <em>or</em> <a href="https://docs.npmjs.com/cli/npm" target="_blank">npm</a>. <b>Note:</b> Because\nwe use ES2017 syntax (such as `import`), this workflow assumes you are using a modern browser or a bundler/transpiler\nto convert your code to something older browsers understand. See our\n<a href=''https://github.com/tensorflow/tfjs-examples'' target="_blank">examples</a>\nto see how we use <a href="https://parceljs.org/" target="_blank">Parcel</a> to build\nour code. However, you are free to use any build tool that you prefer.\n\n\n\n```js\nimport * as tf from ''@tensorflow/tfjs'';\n\n// Define a model for linear regression.\nconst model = tf.sequential();\nmodel.add(tf.layers.dense({units: 1, inputShape: [1]}));\n\n// Prepare the model for training: Specify the loss and the optimizer.\nmodel.compile({loss: ''meanSquaredError'', optimizer: ''sgd''});\n\n// Generate some synthetic data for training.\nconst xs = tf.tensor2d([1, 2, 3, 4], [4, 1]);\nconst ys = tf.tensor2d([1, 3, 5, 7], [4, 1]);\n\n// Train the model using the data.\nmodel.fit(xs, ys).then(() => {\n  // Use the model to do inference on a data point the model hasn''t seen before:\n  model.predict(tf.tensor2d([5], [1, 1])).print();\n});\n```\n\nSee our <a href="https://js.tensorflow.org/tutorials/" target="_blank">tutorials</a>, <a href="https://github.com/tensorflow/tfjs-examples" target="_blank">examples</a>\nand <a href="https://js.tensorflow.org/api/latest/">documentation</a> for more details.\n\n## Importing pre-trained models\n\nWe support porting pre-trained models from:\n- [TensorFlow SavedModel](https://www.tensorflow.org/js/tutorials/conversion/import_saved_model)\n- [Keras](https://js.tensorflow.org/tutorials/import-keras.html)\n\n## Various ops supported in different backends\n\nPlease refer below :\n- [TFJS Ops Matrix](https://docs.google.com/spreadsheets/d/1D25XtWaBrmUEErbGQB0QmNhH-xtwHo9LDl59w0TbxrI/edit#gid=0)\n\n## Find out more\n\n[TensorFlow.js](https://js.tensorflow.org) is a part of the\n[TensorFlow](https://www.tensorflow.org) ecosystem. For more info:\n- For help from the community, use the `tfjs` tag on the [TensorFlow Forum](https://discuss.tensorflow.org/tag/tfjs).\n- [TensorFlow.js Website](https://js.tensorflow.org)\n- [Tutorials](https://js.tensorflow.org/tutorials)\n- [API reference](https://js.tensorflow.org/api/latest/)\n- [TensorFlow.js Blog](https://blog.tensorflow.org/search?label=TensorFlow.js)\n\nThanks, <a href="https://www.browserstack.com/">BrowserStack</a>, for providing testing support.\n', '{"language":"TypeScript","stars":19033,"forks":2013,"watchers":19033,"open_issues":615,"topics":["deep-learning","deep-neural-network","gpu-acceleration","javascript","machine-learning","neural-network","typescript","wasm","web-assembly","webgl"],"default_branch":"master","size_kb":174153,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:tensorflow:tfjs-examples","source_url":"https://github.com/tensorflow/tfjs-examples"},{"type":"has_code","target_id":"github:tensorflow:tfjs-models","source_url":"https://github.com/tensorflow/tfjs-models"},{"type":"has_code","target_id":"github:tensorflow:tfjs","source_url":"https://github.com/tensorflow/tfjs"},{"type":"has_code","target_id":"github:tensorflow:tfjs","source_url":"https://github.com/tensorflow/tfjs"},{"type":"has_code","target_id":"github:tensorflow:tfjs-examples''","source_url":"https://github.com/tensorflow/tfjs-examples''"},{"type":"has_code","target_id":"github:tensorflow:tfjs-examples\"","source_url":"https://github.com/tensorflow/tfjs-examples\""}]', NULL, 'Apache-2.0', 'approved', 65, '2a1e9f2e877aba4f08d3ab75492016b4', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-tensorflow-tfjs from https://github.com/tensorflow.png
Image converted to WebP: data/images/github-tensorflow-tfjs.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-Unity-Technologies-ml-agents', 'github--unity-technologies--ml-agents', 'ml-agents', 'Unity-Technologies', '(latest release) (all releases) **The Unity Machine Learning Agents Toolkit** (ML-Agents) is an open-source project that enables games and simulations to serve as environments for training intelligent agents. We provide implementations (based on PyTorch) of state-of-the-art algorithms to enable game developers and hobbyists to easily train intelligent agents for 2D, 3D and VR/AR games. Researchers can also use the provided simple-to-use Python API to train Agents using reinforcement learning,...', '["deep-learning","deep-reinforcement-learning","machine-learning","neural-networks","reinforcement-learning","unity","unity3d","c#"]', 'other', 18919, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/Unity-Technologies/ml-agents","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Unity ML-Agents Toolkit\n\n[![docs badge](https://img.shields.io/badge/docs-reference-blue.svg)](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest)\n\n[![license badge](https://img.shields.io/badge/license-Apache--2.0-green.svg)](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/LICENSE.md)\n\n([latest release](https://github.com/Unity-Technologies/ml-agents/releases/tag/latest_release)) ([all releases](https://github.com/Unity-Technologies/ml-agents/releases))\n\n**The Unity Machine Learning Agents Toolkit** (ML-Agents) is an open-source project that enables games and simulations to serve as environments for training intelligent agents. We provide implementations (based on PyTorch) of state-of-the-art algorithms to enable game developers and hobbyists to easily train intelligent agents for 2D, 3D and VR/AR games. Researchers can also use the provided simple-to-use Python API to train Agents using reinforcement learning, imitation learning, neuroevolution, or any other methods. These trained agents can be used for multiple purposes, including controlling NPC behavior (in a variety of settings such as multi-agent and adversarial), automated testing of game builds and evaluating different game design decisions pre-release. The ML-Agents Toolkit is mutually beneficial for both game developers and AI researchers as it provides a central platform where advances in AI can be evaluated on Unity’s rich environments and then made accessible to the wider research and game developer communities.\n\n## Features\n- 17+ [example Unity environments](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Learning-Environment-Examples.html)\n- Support for multiple environment configurations and training scenarios\n- Flexible Unity SDK that can be integrated into your game or custom Unity scene\n- Support for training single-agent, multi-agent cooperative, and multi-agent competitive scenarios via several Deep Reinforcement Learning algorithms (PPO, SAC, MA-POCA, self-play).\n- Support for learning from demonstrations through two Imitation Learning algorithms (BC and GAIL).\n- Quickly and easily add your own [custom training algorithm](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-Custom-Trainer-Plugin.html) and/or components.\n- Easily definable Curriculum Learning scenarios for complex tasks\n- Train robust agents using environment randomization\n- Flexible agent control with On Demand Decision Making\n- Train using multiple concurrent Unity environment instances\n- Utilizes the [Inference Engine](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Inference-Engine.html) to provide native cross-platform support\n- Unity environment [control from Python](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-LLAPI.html)\n- Wrap Unity learning environments as a [gym](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-Gym-API.html) environment\n- Wrap Unity learning environments as a [PettingZoo](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-PettingZoo-API.html) environment\n\n## Releases & Documentation\n\n> **⚠️ Documentation Migration Notice**\n> We have moved to [Unity Package documentation](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest) as the **primary developer documentation** and have **deprecated** the maintenance of [web docs](https://unity-technologies.github.io/ml-agents/). Please use the Unity Package documentation for the most up-to-date information.\n\nThe table below shows our latest release, including our `develop` branch which is under active development and may be unstable. A few helpful guidelines:\n\n- The [Versioning page](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Versioning.html) overviews how we manage our GitHub releases and the versioning process for each of the ML-Agents components.\n- The [Releases page](https://github.com/Unity-Technologies/ml-agents/releases) contains details of the changes between releases.\n- The [Migration page](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Migrating.html) contains details on how to upgrade from earlier releases of the ML-Agents Toolkit.\n- The `com.unity.ml-agents` package is [verified](https://docs.unity3d.com/2020.1/Documentation/Manual/pack-safe.html) for Unity 2020.1 and later. Verified packages releases are numbered 1.0.x.\n\n|      **Version**       |  **Release Date**   |                                  **Source**                                   |                                                 **Documentation**                                                  |                                      **Download**                                      |                  **Python Package**                   |                                   **Unity Package**                                   |\n|:----------------------:|:-------------------:|:-----------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------------:|:-----------------------------------------------------:|:-------------------------------------------------------------------------------------:|\n|     **Release 23**     | **August 28, 2025** | **[source](https://github.com/Unity-Technologies/ml-agents/tree/release_23)** |              **[docs](https://docs.unity3d.com/Packages/com.unity.ml-agents@4.0/manual/index.html)**               | **[download](https://github.com/Unity-Technologies/ml-agents/archive/release_23.zip)** | **[1.1.0](https://pypi.org/project/mlagents/1.1.0/)** |                                       **4.0.0**                                       |\n| **develop (unstable)** |         --          |    [source](https://github.com/Unity-Technologies/ml-agents/tree/develop)     | [docs](https://github.com/Unity-Technologies/ml-agents/tree/develop/com.unity.ml-agents/Documentation~/index.md)   |    [download](https://github.com/Unity-Technologies/ml-agents/archive/develop.zip)     |                         --                            |                                          --                                           |\n\n\n\nIf you are a researcher interested in a discussion of Unity as an AI platform, see a pre-print of our [reference paper on Unity and the ML-Agents Toolkit](https://arxiv.org/abs/1809.02627).\n\nIf you use Unity or the ML-Agents Toolkit to conduct research, we ask that you cite the following paper as a reference:\n\n```\n@article{juliani2020,\n  title={Unity: A general platform for intelligent agents},\n  author={Juliani, Arthur and Berges, Vincent-Pierre and Teng, Ervin and Cohen, Andrew and Harper, Jonathan and Elion, Chris and Goy, Chris and Gao, Yuan and Henry, Hunter and Mattar, Marwan and Lange, Danny},\n  journal={arXiv preprint arXiv:1809.02627},\n  url={https://arxiv.org/pdf/1809.02627.pdf},\n  year={2020}\n}\n```\n\nAdditionally, if you use the MA-POCA trainer in your research, we ask that you cite the following paper as a reference:\n\n```\n@article{cohen2022,\n  title={On the Use and Misuse of Absorbing States in Multi-agent Reinforcement Learning},\n  author={Cohen, Andrew and Teng, Ervin and Berges, Vincent-Pierre and Dong, Ruo-Ping and Henry, Hunter and Mattar, Marwan and Zook, Alexander and Ganguly, Sujoy},\n  journal={RL in Games Workshop AAAI 2022},\n  url={http://aaai-rlg.mlanctot.info/papers/AAAI22-RLG_paper_32.pdf},\n  year={2022}\n}\n```\n\n\n## Additional Resources\n\n* [Unity Discussions](https://discussions.unity.com/tag/ml-agents)\n* [ML-Agents tutorials by CodeMonkeyUnity](https://www.youtube.com/playlist?list=PLzDRvYVwl53vehwiN_odYJkPBzcqFw110)\n* [Introduction to ML-Agents by Huggingface](https://huggingface.co/learn/deep-rl-course/en/unit5/introduction)\n* [Community created ML-Agents projects](https://discussions.unity.com/t/post-your-ml-agents-project/816756)\n* [ML-Agents models on Huggingface](https://huggingface.co/models?library=ml-agents)\n* [Blog posts](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Blog-posts.html)\n* [Discord](https://discord.com/channels/489222168727519232/1202574086115557446)\n\n## Community and Feedback\n\nThe ML-Agents Toolkit is an open-source project and we encourage and welcome contributions. If you wish to contribute, be sure to review our [contribution guidelines](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/CONTRIBUTING.html) and [code of conduct](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/CODE_OF_CONDUCT.md).\n\nFor problems with the installation and setup of the ML-Agents Toolkit, or discussions about how to best setup or train your agents, please create a new thread on the [Unity ML-Agents discussion forum](https://discussions.unity.com/tag/ml-agents). Be sure to include as many details as possible to help others assist you effectively. If you run into any other problems using the ML-Agents Toolkit or have a specific feature request, please [submit a GitHub issue](https://github.com/Unity-Technologies/ml-agents/issues).\n\nPlease tell us which samples you would like to see shipped with the ML-Agents Unity package by replying to [this discussion thread](https://discussions.unity.com/t/help-shape-the-future-of-ml-agents/1661019).\n\n## Privacy\n\nIn order to improve the developer experience for Unity ML-Agents Toolkit, we have added in-editor analytics. Please refer to "Information that is passively collected by Unity" in the [Unity Privacy Policy](https://unity3d.com/legal/privacy-policy).\n', '{"language":"C#","stars":18919,"forks":4397,"watchers":18919,"open_issues":54,"topics":["deep-learning","deep-reinforcement-learning","machine-learning","neural-networks","reinforcement-learning","unity","unity3d"],"default_branch":"develop","size_kb":3019543,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"}]', NULL, 'NOASSERTION', 'approved', 65, 'e1cfb29ac478ac460f6686612807e35f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-Unity-Technologies-ml-agents from https://github.com/Unity-Technologies.png
Image converted to WebP: data/images/github-Unity-Technologies-ml-agents.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-iperov-DeepFaceLab', 'github--iperov--deepfacelab', 'DeepFaceLab', 'iperov', '<table align="center" border="0"> <tr><td colspan=2 align="center"> <a href="https://arxiv.org/abs/2005.05535"> <img src="https://static.arxiv.org/static/browse/0.3.0/images/icons/favicon.ico" width=14></img> https://arxiv.org/abs/2005.05535</a> </td></tr> <tr><td colspan=2 align="center"> <p align="center"> </p> DeepFaceLab is used by such popular youtube channels as | deeptomcruise| 1facerussia| arnoldschwarzneggar |---|---|---| | mariahcareyathome?| diepnep| mr__heisenberg| deepcaprio |---...', '["arxiv","creating-deepfakes","deep-face-swap","deep-learning","deep-neural-networks","deepface","deepfacelab","deepfakes","deeplearning","face-swap","faceswap","fakeapp","machine-learning","neural-nets","neural-networks","python"]', 'other', 18868, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/iperov/DeepFaceLab","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<table align="center" border="0">\n\n<tr><td colspan=2 align="center">\n\n# DeepFaceLab  \n\n<a href="https://arxiv.org/abs/2005.05535">\n\n<img src="https://static.arxiv.org/static/browse/0.3.0/images/icons/favicon.ico" width=14></img>\nhttps://arxiv.org/abs/2005.05535</a>\n\n</td></tr>\n<tr><td colspan=2 align="center">\n\n<p align="center">\n\n![](doc/logo_tensorflow.png)\n![](doc/logo_cuda.png)\n![](doc/logo_directx.png)\n\n</p>\n\nDeepFaceLab is used by such popular youtube channels as\n\n|![](doc/tiktok_icon.png) [deeptomcruise](https://www.tiktok.com/@deeptomcruise)|![](doc/tiktok_icon.png) [1facerussia](https://www.tiktok.com/@1facerussia)|![](doc/tiktok_icon.png) [arnoldschwarzneggar](https://www.tiktok.com/@arnoldschwarzneggar)\n|---|---|---|\n\n|![](doc/tiktok_icon.png) [mariahcareyathome?](https://www.tiktok.com/@mariahcareyathome?)|![](doc/tiktok_icon.png) [diepnep](https://www.tiktok.com/@diepnep)|![](doc/tiktok_icon.png) [mr__heisenberg](https://www.tiktok.com/@mr__heisenberg)|![](doc/tiktok_icon.png) [deepcaprio](https://www.tiktok.com/@deepcaprio)\n|---|---|---|---|\n\n|![](doc/youtube_icon.png) [VFXChris Ume](https://www.youtube.com/channel/UCGf4OlX_aTt8DlrgiH3jN3g/videos)|![](doc/youtube_icon.png) [Sham00k](https://www.youtube.com/channel/UCZXbWcv7fSZFTAZV4beckyw/videos)|\n|---|---|\n\n|![](doc/youtube_icon.png) [Collider videos](https://www.youtube.com/watch?v=A91P2qtPT54&list=PLayt6616lBclvOprvrC8qKGCO-mAhPRux)|![](doc/youtube_icon.png) [iFake](https://www.youtube.com/channel/UCC0lK2Zo2BMXX-k1Ks0r7dg/videos)|![](doc/youtube_icon.png) [NextFace](https://www.youtube.com/channel/UCFh3gL0a8BS21g-DHvXZEeQ/videos)|\n|---|---|---|\n\n|![](doc/youtube_icon.png) [Futuring Machine](https://www.youtube.com/channel/UCC5BbFxqLQgfnWPhprmQLVg)|![](doc/youtube_icon.png) [RepresentUS](https://www.youtube.com/channel/UCRzgK52MmetD9aG8pDOID3g)|![](doc/youtube_icon.png) [Corridor Crew](https://www.youtube.com/c/corridorcrew/videos)|\n|---|---|---|\n\n|![](doc/youtube_icon.png) [DeepFaker](https://www.youtube.com/channel/UCkHecfDTcSazNZSKPEhtPVQ)|![](doc/youtube_icon.png) [DeepFakes in movie](https://www.youtube.com/c/DeepFakesinmovie/videos)|\n|---|---|\n\n|![](doc/youtube_icon.png) [DeepFakeCreator](https://www.youtube.com/channel/UCkNFhcYNLQ5hr6A6lZ56mKA)|![](doc/youtube_icon.png) [Jarkan](https://www.youtube.com/user/Jarkancio/videos)|\n|---|---|\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\n# What can I do using DeepFaceLab?\n\n</td></tr>\n<tr><td colspan=2 align="center">\n\n## Replace the face\n\n<img src="doc/replace_the_face.jpg" align="center">\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\n## De-age the face\n\n</td></tr>\n\n<tr><td align="center" width="50%">\n\n<img src="doc/deage_0_1.jpg" align="center">\n\n</td>\n<td align="center" width="50%">\n\n<img src="doc/deage_0_2.jpg" align="center">\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\n![](doc/youtube_icon.png) https://www.youtube.com/watch?v=Ddx5B-84ebo\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\n## Replace the head\n\n</td></tr>\n\n<tr><td align="center" width="50%">\n\n<img src="doc/head_replace_1_1.jpg" align="center">\n\n</td>\n<td align="center" width="50%">\n\n<img src="doc/head_replace_1_2.jpg" align="center">\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\n![](doc/youtube_icon.png) https://www.youtube.com/watch?v=RTjgkhMugVw\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\n# Native resolution progress\n\n</td></tr>\n<tr><td colspan=2 align="center">\n\n<img src="doc/deepfake_progress.png" align="center">\n\n</td></tr>\n<tr><td colspan=2 align="center">\n\n<img src="doc/make_everything_ok.png" align="center">\n\nUnfortunately, there is no "make everything ok" button in DeepFaceLab. You should spend time studying the workflow and growing your skills. A skill in programs such as *AfterEffects* or *Davinci Resolve* is also desirable.\n\n</td></tr>\n<tr><td colspan=2 align="center">\n\n## Mini tutorial\n\n<a href="https://www.youtube.com/watch?v=kOIMXt8KK8M">\n\n<img src="doc/mini_tutorial.jpg" align="center">\n\n</a>\n\n</td></tr>\n<tr><td colspan=2 align="center">\n\n## Releases\n\n</td></tr>\n\n<tr><td align="right">\n<a href="https://tinyurl.com/2p9cvt25">Windows (magnet link)</a>\n</td><td align="center">Last release. Use torrent client to download.</td></tr>\n\n<tr><td align="right">\n<a href="https://mega.nz/folder/Po0nGQrA#dbbttiNWojCt8jzD4xYaPw">Windows (Mega.nz)</a>\n</td><td align="center">Contains new and prev releases.</td></tr>\n\n<tr><td align="right">\n<a href="https://disk.yandex.ru/d/7i5XTKIKVg5UUg">Windows (yandex.ru)</a>\n</td><td align="center">Contains new and prev releases.</td></tr>\n\n<tr><td align="right">\n<a href="https://github.com/nagadit/DeepFaceLab_Linux">Linux (github)</a>\n</td><td align="center">by @nagadit</td></tr>\n\n<tr><td align="right">\n<a href="https://github.com/elemantalcode/dfl">CentOS Linux (github)</a>\n</td><td align="center">May be outdated. By @elemantalcode</td></tr>\n\n</table>\n\n<table align="center" border="0">\n\n<tr><td colspan=2 align="center">\n\n### Communication groups\n\n</td></tr>\n\n<tr><td align="right">\n<a href="https://discord.gg/rxa7h9M6rH">Discord</a>\n</td><td align="center">Official discord channel. English / Russian.</td></tr>\n\n<tr><td colspan=2 align="center">\n\n## Related works\n\n</td></tr>\n\n<tr><td align="right">\n<a href="https://github.com/iperov/DeepFaceLive">DeepFaceLive</a>\n</td><td align="center">Real-time face swap for PC streaming or video calls</td></tr>\n\n</td></tr>\n</table>\n\n<table align="center" border="0">\n\n<tr><td colspan=2 align="center">\n\n## How I can help the project?\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\n### Star this repo\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\nRegister github account and push "Star" button.\n\n</td></tr>\n\n</table>\n\n<table align="center" border="0">\n<tr><td colspan=2 align="center">\n\n## Meme zone\n\n</td></tr>\n\n<tr><td align="center" width="50%">\n\n<img src="doc/meme1.jpg" align="center">\n\n</td>\n\n<td align="center" width="50%">\n\n<img src="doc/meme2.jpg" align="center">\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\n<sub>#deepfacelab #faceswap #face-swap #deep-learning #deeplearning #deep-neural-networks #deepface #deep-face-swap #neural-networks #neural-nets #tensorflow #cuda #nvidia</sub>\n\n</td></tr>\n\n\n\n</table>\n', '{"language":"Python","stars":18868,"forks":785,"watchers":18868,"open_issues":538,"topics":["arxiv","creating-deepfakes","deep-face-swap","deep-learning","deep-neural-networks","deepface","deepfacelab","deepfakes","deeplearning","face-swap","faceswap","fakeapp","machine-learning","neural-nets","neural-networks"],"default_branch":"master","size_kb":848594,"archived":true,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:nagadit:DeepFaceLab_Linux\">Linux","source_url":"https://github.com/nagadit/DeepFaceLab_Linux\">Linux"},{"type":"has_code","target_id":"github:elemantalcode:dfl\">CentOS","source_url":"https://github.com/elemantalcode/dfl\">CentOS"},{"type":"has_code","target_id":"github:iperov:DeepFaceLive\">DeepFaceLive<","source_url":"https://github.com/iperov/DeepFaceLive\">DeepFaceLive<"}]', NULL, 'GPL-3.0', 'approved', 65, '1e7526b64e2c0b3aa06217762eaf6d43', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-iperov-DeepFaceLab from https://github.com/iperov.png
Image converted to WebP: data/images/github-iperov-DeepFaceLab.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-amark-gun', 'github--amark--gun', 'gun', 'amark', '<p id="readme"><a href="https://gun.eco/"><img width="40%" src="https://cldup.com/TEy9yGh45l.svg"/></a><img width="50%" align="right" vspace="25" src="https://gun.eco/see/demo.gif"/></p> !Build **GUN** is an ecosystem of **tools** that let you build community run and encrypted applications - like an Open Source Firebase or a Decentralized Dropbox. The Internet Archive and 100s of other apps run GUN in-production. + Multiplayer by default with realtime p2p state synchronization! + Graph data l...', '["artificial-intelligence","big-data","blockchain","crdt","crypto","cryptography","dapp","database","decentralized","dweb","encryption","end-to-end","graph","machine-learning","metaverse","offline-first","p2p","protocol","realtime","web3","javascript"]', 'other', 18785, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/amark/gun","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p id="readme"><a href="https://gun.eco/"><img width="40%" src="https://cldup.com/TEy9yGh45l.svg"/></a><img width="50%" align="right" vspace="25" src="https://gun.eco/see/demo.gif"/></p>\n\n[![](https://data.jsdelivr.com/v1/package/npm/gun/badge)](https://www.jsdelivr.com/package/npm/gun)\n![Build](https://github.com/amark/gun/actions/workflows/ci.yml/badge.svg)\n[![Gitter](https://img.shields.io/gitter/room/amark/gun.js.svg)](http://chat.gun.eco)\n\n**GUN** is an [ecosystem](https://gun.eco/docs/Ecosystem) of **tools** that let you build [community run](https://www.nbcnews.com/tech/tech-news/these-technologists-think-internet-broken-so-they-re-building-another-n1030136) and [encrypted applications](https://gun.eco/docs/Cartoon-Cryptography) - like an Open Source Firebase or a Decentralized Dropbox.\n\nThe [Internet Archive](https://news.ycombinator.com/item?id=17685682) and [100s of other apps](https://github.com/amark/gun/wiki/awesome-gun) run GUN in-production.\n\n + Multiplayer by default with realtime p2p state synchronization!\n + Graph data lets you use key/value, tables, documents, videos, & more!\n + Local-first, offline, and decentralized with end-to-end encryption.\n\nDecentralized alternatives to [Zoom](https://www.zdnet.com/article/era-hatches-meething-an-open-source-browser-based-video-conferencing-system/), [Reddit](https://notabug.io/t/whatever/comments/36588a16b9008da4e3f15663c2225e949eca4a15/gpu-bot-test), [Instagram](https://iris.to/), [Slack](https://iris.to/), [YouTube](https://d.tube/), [Stripe](https://twitter.com/marknadal/status/1422717427427647489), [Wikipedia](https://news.ycombinator.com/item?id=17685682), Facebook [Horizon](https://twitter.com/marknadal/status/1424476179189305347) and more have already pushed terabytes of daily P2P traffic on GUN. We are a [friendly community](http://chat.gun.eco/) creating a [free fun future for freedom](https://youtu.be/1HJdrBk3BlE):\n\n<table>\n<tr>\n<a href="https://youtu.be/s_m16-w6bBI"><img width="31%" src="https://gun.eco/see/3dvr.gif" title="3D VR"/></a>\n<a href="https://github.com/cstefanache/cstefanache.github.io/blob/06697003449e4fc531fd32ee068bab532976f47b/_posts/2016-08-02-gun-db-artificial-knowledge-sharing.md"><img width="31%" src="https://gun.eco/see/aiml.gif" title="AI/ML"/></a>\n<a href="http://gps.gunDB.io/"><img width="31%" src="https://gun.eco/see/gps.gif" title="GPS"/></a>\n</tr>\n<tr>\n<a href="https://github.com/lmangani/gun-scape#gun-scape"><img width="31%" src="https://gun.eco/see/dataviz.gif" title="Data Viz"/></a>\n<a href="https://github.com/amark/gun/wiki/Auth"><img width="31%" src="https://gun.eco/see/p2p.gif" title="P2P"/></a>\n<a href="https://github.com/Stefdv/gun-ui-lcd#okay-what-about-gundb-"><img width="31%" src="https://gun.eco/see/iot.gif" title="IoT"/></a>\n</tr>\n<tr>\n<a href="http://chat.gun.eco"><img width="31%" src="https://gun.eco/see/vr-world.gif" title="VR World"/></a>\n<a href="https://youtu.be/1ASrmQ-CwX4"><img width="31%" src="https://gun.eco/see/ar.gif" title="AR"/></a>\n<a href="https://meething.space/"><img width="31%" src="https://gun.eco/see/video-conf.gif" title="Video Confernece"/></a>\n</tr>\n</table>\n\n## Quickstart\n\nGUN is *super easy* to get started with:\n\n - Try the [interactive tutorial](https://gun.eco/docs/Todo-Dapp) in the browser (**5min** ~ average developer).\n - Or `npm install gun` and run the examples with `cd node_modules/gun && npm start` (**5min** ~ average developer).\n\n> **Note:** If you don''t have [node](http://nodejs.org/) or [npm](https://www.npmjs.com/), read [this](https://github.com/amark/gun/blob/master/examples/install.sh) first.\n> If the `npm` command line didn''t work, you may need to `mkdir node_modules` first or use `sudo`.\n\n- An online demo of the examples are available here: http://try.axe.eco/\n- Or write a quick app: ([try now in a playground](https://jsbin.com/kadobamevo/edit?js,console))\n```html\n<script src="https://cdn.jsdelivr.net/npm/gun/gun.js"></script>\n<script>\n// import GUN from ''gun''; // in ESM\n// GUN = require(''gun''); // in NodeJS\n// GUN = require(''gun/gun''); // in React\ngun = GUN();\n\ngun.get(''mark'').put({\n  name: "Mark",\n  email: "mark@gun.eco",\n});\n\ngun.get(''mark'').on((data, key) => {\n  console.log("realtime updates:", data);\n});\n\nsetInterval(() => { gun.get(''mark'').get(''live'').put(Math.random()) }, 9);\n</script>\n```\n- Or try something **mind blowing**, like saving circular references to a table of documents! ([play](http://jsbin.com/wefozepume/edit?js,console))\n```javascript\ncat = {name: "Fluffy", species: "kitty"};\nmark = {boss: cat};\ncat.slave = mark;\n\n// partial updates merge with existing data!\ngun.get(''mark'').put(mark);\n\n// access the data as if it is a document.\ngun.get(''mark'').get(''boss'').get(''name'').once(function(data, key){\n  // `once` grabs the data once, no subscriptions.\n  console.log("Mark''s boss is", data);\n});\n\n// traverse a graph of circular references!\ngun.get(''mark'').get(''boss'').get(''slave'').once(function(data, key){\n  console.log("Mark is the cat''s slave!", data);\n});\n\n// add both of them to a table!\ngun.get(''list'').set(gun.get(''mark'').get(''boss''));\ngun.get(''list'').set(gun.get(''mark''));\n\n// grab each item once from the table, continuously:\ngun.get(''list'').map().once(function(data, key){\n  console.log("Item:", data);\n});\n\n// live update the table!\ngun.get(''list'').set({type: "cucumber", goal: "jumping cat"});\n```\n\nWant to keep building more? **Jump to [THE DOCUMENTATION](#documentation)!**\n\n# About\nFirst & foremost, GUN is **a community of the nicest and most helpful people** out there. So [I want to invite you](http://chat.gun.eco) to come tell us about what **you** are working on & wanting to build (new or old school alike! Just be nice as well.) and ask us your questions directly. :)\n\n<p align="center"><a href="https://www.youtube.com/watch?v=oTQXzhm8w_8"><img width="250" src="https://img.youtube.com/vi/oTQXzhm8w_8/0.jpg"><br/>Watch the 100 second intro!</a></p>\n\nThe GUN ecosystem stack is a collection of independent and modular tools covering everything from [CRDT](https://crdt.tech/) [conflict resolution](https://gun.eco/distributed/matters.html), [cryptographic security](https://gun.eco/docs/Cartoon-Cryptography) & [encryption](https://gun.eco/docs/SEA), [radix storage serialization](https://gun.eco/docs/RAD), [mesh networking](https://gun.eco/docs/DAM) & [routing algorithms](https://gun.eco/docs/Routing), to distributed systems [correctness & load testing](https://github.com/gundb/panic-server), CPU scheduled [JSON parser](https://github.com/amark/gun/blob/master/lib/yson.js) to prevent UI lag, and more!\n\n<div><img width="48%" src="https://gun.eco/see/stack.png"/>\n<img width="48%" align="right" src="https://gun.eco/see/layers.png"/></div>\n\nOn that note, let''s get some official shout outs covered first:\n\n### Support\n\n<p align="center">\nThanks to:\n\n<table>\n<tr>\n<td vlign="center"><a href="https://mozilla.org/builders"><img height="100" src="https://user-images.githubusercontent.com/1423657/81992335-85346480-9643-11ea-8754-8275e98e06bc.png"></a></td>\n<td vlign="center"><a href="http://unstoppabledomains.com/"><img src="https://gun.eco/img/unstoppable.png"></a></td>\n<td vlign="center"><a href="https://mask.io/"><img src="https://dimensiondev.github.io/Mask-VI/assets/Logo/MB--Logo--CombH-Circle--Blue.svg" width="250"></a></td>\n</tr>\n<tr>\n<td vlign="center">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.ajar.org/"><img src="https://www.ajar.org/logo.png" height="120"></a></td>\n<td vlign="center"><a href="https://wallie.io/"><img src="https://raw.githubusercontent.com/gundb/gun-site/master/img/wallie.png" width="250"></a></td>\n<td vlign="center">&nbsp;&nbsp;<a href="https://ghostdrive.com/"><img src="https://gun.eco/img/ghostdrive.png" height="120"></a></td>\n</tr>\n</table>\n\n<a href="https://github.com/robertheessels">Robert Heessels</a>,\n<a href="http://qxip.net/">Lorenzo Mangani</a>,\n<a href="https://nlnet.nl/">NLnet Foundation</a>,\n<a href="http://github.com/samliu">Sam Liu</a>,\n<a href="http://github.com/ddombrow">Daniel Dombrowsky</a>,\n<a href="http://github.com/vincentwoo">Vincent Woo</a>,\n<a href="http://github.com/coolaj86">AJ ONeal</a>,\n<a href="http://github.com/ottman">Bill Ottman</a>,\n<a href="http://github.com/mikewlange">Mike Lange</a>,\n<a href="http://github.com/ctrlplusb">Sean Matheson</a>,\n<a href="http://github.com/alanmimms">Alan Mimms</a>,\n<a href="https://github.com/dfreire">Dário Freire</a>,\n<a href="http://github.com/velua">John Williamson</a>,\n<a href="http://github.com/finwo">Robin Bron</a>,\n<a href="http://github.com/ElieMakhoul">Elie Makhoul</a>,\n<a href="http://github.com/mikestaub">Mike Staub</a>,\n<a href="http://github.com/bmatusiak">Bradley Matusiak</a>,\n<a href="https://github.com/sjuxax">Jeff Cook</a>,\n<a href="https://github.com/nmauersberg">Nico</a>,\n<a href="https://github.com/ajartille">Aaron Artille</a>,\n<a href="https://github.com/timjrobinson">Tim Robinson</a>,\n<a href="https://github.com/hibas123">Fabian Stamm</a>,\n<a href="https://twitter.com/mikestaub">Mike Staub</a>,\n<a href="https://hunterowens.com/">Hunter Owens</a>,\n<a href="https://github.com/JacobMillner">Jacob Millner</a>,\n<a href="https://github.com/b-lack">Gerrit Balindt</a>,\n<a href="https://github.com/gabriellemon">Gabriel Lemon</a>,\n<a href="https://github.com/murageyun">Murage Martin</a>,\n<a href="https://github.com/octalmage">Jason Stallings</a>\n</p>\n\n - Join others in sponsoring code: https://www.patreon.com/gunDB !\n - Ask questions: http://stackoverflow.com/questions/tagged/gun ?\n - Found a bug? Report at: https://github.com/amark/gun/issues ;\n - **Need help**? Chat with us: http://chat.gun.eco .\n\n### History\n\n[GUN](https://gun.eco) was created by [Mark Nadal](https://twitter.com/marknadal) in 2014 after he had spent 4 years trying to get his collaborative web app to scale up with traditional databases.\n\n<img width="250px" src="https://gun.eco/see/problem.png" align="left" title="pain point" style="margin: 0 1em 1em 0"> After he realized [Master-Slave database architecture causes one big bottleneck](https://gun.eco/distributed/matters.html), he (as a complete newbie outsider) naively decided **to question the status quo** and shake things up with controversial, heretical, and contrarian experiments:\n\n**The NoDB** - no master, no servers, no "single source of truth", not built with a real programming language or real hardware, no DevOps, no locking, not *just* SQL or NoSQL but both (**all** - graphs, documents, tables, key/value).\n\nThe goal was to build a P2P database that could survive living inside **any** browser, and could correctly sync data between **any** device after assuming **any** offline-first activity.\n\n<img src="https://gun.eco/see/compare.png" title="comparison table">\n\nTechnically, **GUN is a graph synchronization protocol** with a *lightweight embedded engine*, capable of doing *[20M+ API ops/sec](https://gun.eco/docs/Performance)* in **just ~9KB gzipped size**.\n\n## Documentation\n\n<table>\n  <tr>\n    <td style="border: 0;"><h3><a href="https://gun.eco/docs/API">API reference</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://gun.eco/docs/Todo-Dapp">Tutorials</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://github.com/amark/gun/tree/master/examples">Examples</a></h3></td>\n  </tr>\n  <tr>\n    <td style="border: 0;"><h3><a href="https://github.com/brysgo/graphql-gun">GraphQL</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://github.com/PenguinMan98/electrontest">Electron</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://gun.eco/docs/React-Native">React & Native</a></h3></td>\n  </tr>\n  <tr>\n    <td style="border: 0;"><h3><a href="https://github.com/sjones6/vue-gun">Vue</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://gun.eco/docs/Svelte">Svelte</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://github.com/Stefdv/gun-ui-lcd#syncing">Webcomponents</a></h3></td>\n  </tr>\n  <tr>\n    <td style="border: 0;"><h3><a href="https://gun.eco/docs/CAP-Theorem">CAP Theorem Tradeoffs</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://gun.eco/distributed/matters.html">How Data Sync Works</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://gun.eco/docs/Porting-GUN">How GUN is Built</a></h3></td>\n  </tr>\n  <tr>\n    <td style="border: 0;"><h3><a href="https://gun.eco/docs/Auth">Crypto Auth</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://github.com/amark/gun/wiki/Awesome-GUN">Modules</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://gun.eco/docs/Roadmap">Roadmap</a></h3></td>\n  </tr>\n</table>\n\nThis would not be possible without **community contributors**, big shout out to:\n\n**[ajmeyghani](https://github.com/ajmeyghani) ([Learn GUN Basics with Diagrams](https://medium.com/@ajmeyghani/gundb-a-graph-database-in-javascript-3860a08d873c))**; **[anywhichway](https://github.com/anywhichway) ([Block Storage](https://github.com/anywhichway/gun-block))**; **[beebase](https://github.com/beebase) ([Quasar](https://github.com/beebase/gun-vuex-quasar))**; **[BrockAtkinson](https://github.com/BrockAtkinson) ([brunch config](https://github.com/BrockAtkinson/brunch-gun))**; **[Brysgo](https://github.com/brysgo) ([GraphQL](https://github.com/brysgo/graphql-gun))**; **[d3x0r](https://github.com/d3x0r) ([SQLite](https://github.com/d3x0r/gun-db))**; **[forrestjt](https://github.com/forrestjt) ([file.js](https://github.com/amark/gun/blob/master/lib/file.js))**; **[hillct](https://github.com/hillct) (Docker)**; **[JosePedroDias](https://github.com/josepedrodias) ([graph visualizer](http://acor.sl.pt:9966))**; **[JuniperChicago](https://github.com/JuniperChicago) ([cycle.js bindings](https://github.com/JuniperChicago/cycle-gun))**; **[jveres](https://github.com/jveres) ([todoMVC](https://github.com/jveres/todomvc))**; **[kristianmandrup](https://github.com/kristianmandrup) ([edge](https://github.com/kristianmandrup/gun-edge))**; **[Lightnet](https://github.com/Lightnet)** ([Awesome Vue User Examples](https://glitch.com/edit/#!/jsvuegunui?path=README.md:1:0) & [User Kitchen Sink Playground](https://gdb-auth-vue-node.glitch.me/)); **[lmangani](https://github.com/lmangani) ([Cytoscape Visualizer](https://github.com/lmangani/gun-scape), [Cassandra](https://github.com/lmangani/gun-cassandra), [Fastify](https://github.com/lmangani/fastify-gundb), [LetsEncrypt](https://github.com/lmangani/polyGun-letsencrypt))**; **[mhelander](https://github.com/mhelander) ([SEA](https://github.com/amark/gun/blob/master/sea.js))**; [omarzion](https://github.com/omarzion) ([Sticky Note App](https://github.com/omarzion/stickies)); [PsychoLlama](https://github.com/PsychoLlama) ([LevelDB](https://github.com/PsychoLlama/gun-level)); **[RangerMauve](https://github.com/RangerMauve) ([schema](https://github.com/gundb/gun-schema))**; **[robertheessels](https://github.com/swifty) ([gun-p2p-auth](https://github.com/swifty/gun-p2p-auth))**; **[rogowski](https://github.com/rogowski) (AXE)**; [sbeleidy](https://github.com/sbeleidy); **[sbiaudet](https://github.com/sbiaudet) ([C# Port](https://github.com/sbiaudet/cs-gun))**; **[Sean Matheson](https://github.com/ctrlplusb) ([Observable/RxJS/Most.js bindings](https://github.com/ctrlplusb/gun-most))**; **[Shadyzpop](https://github.com/Shadyzpop) ([React Native example](https://github.com/amark/gun/tree/master/examples/react-native))**; **[sjones6](https://github.com/sjones6) ([Flint](https://github.com/sjones6/gun-flint))**; RIP **[Stefdv](https://github.com/stefdv) (Polymer/web components)**; **[zrrrzzt](https://github.com/zrrrzzt) ([JWT Auth](https://gist.github.com/zrrrzzt/6f88dc3cedee4ee18588236756d2cfce))**; **[xmonader](https://github.com/xmonader) ([Python Port](https://github.com/xmonader/pygundb))**; \n\nI am missing many others, apologies, will be adding them soon! This list is infinitely old & way out of date, if you want to be listed in it please make a PR! :)\n\n## Testing\n\nYou will need to `npm install -g mocha` first. Then in the gun root folder run `npm test`. Tests will trigger persistent writes to the DB, so subsequent runs of the test will fail. You must clear the DB before running the tests again. This can be done by running `rm -rf *data*` command in the project directory.\n\n## Shims\n\n > These are only needed for NodeJS & React Native, they shim the native Browser WebCrypto API.\n\nIf you want to use [SEA](https://gun.eco/docs/SEA) for `User` auth and security, you will need to install:\n\n`npm install @peculiar/webcrypto --save`\n\nPlease see [our React Native docs](https://gun.eco/docs/React-Native) for installation instructions!\n\nThen you can require [SEA](https://gun.eco/docs/SEA) without an error:\n\n```javascript\nGUN = require(''gun/gun'');\nSEA = require(''gun/sea'');\n```\n\n## Deploy\n\n > Note: The default examples that get auto-deployed on `npm start` CDN-ify all GUN files, modules, & storage.\n \n > Note: Moving forward, AXE will start to automatically cluster your peer into a shared DHT. You may want to disable this to run an isolated network.\n \n > Note: When deploying a web application using GUN on a cloud provider, you may have to set `CI=false` in your `.env`. This prevents GUN-specific warnings from being treated as errors when deploying your app. You may also resolve this by modifying your webpack config to not try to build the GUN dependencies.\n\nTo quickly spin up a GUN relay peer for your development team, utilize [Heroku](http://heroku.com), [Docker](http://docker.com), or any others listed below. Or some variant thereof [Dokku](http://dokku.viewdocs.io/dokku/), K8s, etc. ! Or use all of them so your relays are decentralized too!\n\n### Linux\n\n`SSH` into the home directory of a clean OS install with `sudo` ability. Set any environment variables you need (see below), then do:\n\n```bash\ncurl -o- https://raw.githubusercontent.com/amark/gun/master/examples/install.sh | bash\n```\n\n > Read [install.sh](https://github.com/amark/gun/blob/master/examples/install.sh) first!\n > If `curl` is not found, *copy&paste* the contents of install.sh into your ssh.\n\nYou can now safely `CTRL+A+D` to escape without stopping the peer. To stop everything `killall screen` or `killall node`.\n\nEnvironment variables may need to be set like `export HTTPS_CERT=~/cert.pem HTTPS_KEY=~/key.pem PORT=443`. You can also look at a sample [nginx](https://gun.eco/docs/nginx) config. For production deployments, you probably will want to use something like `pm2` or better to keep the peer alive after machine reboots.\n\n### [Dome](https://www.trydome.io/)\n[Deploy GUN in one-click](https://app.trydome.io/signup?package=gun) with [Dome](https://trydome.io) and receive a free trial:\n\n[![Deploy to Dome](https://trydome.io/button.svg)](https://app.trydome.io/signup?package=gun)\n\n### [Heroku](https://www.heroku.com/)\n\n[![Deploy](https://www.herokucdn.com/deploy/button.svg)](https://heroku.com/deploy?template=https://github.com/amark/gun)\n\n > Heroku deletes your data every 15 minutes, one way to fix this is by adding [cheap storage](https://gun.eco/docs/Using-Amazon-S3-for-Storage).\n\nOr:\n\n```bash\ngit clone https://github.com/amark/gun.git\ncd gun\nheroku create\ngit push -f heroku HEAD:master\n```\n\nThen visit the URL in the output of the ''heroku create'' step, in a browser. Make sure to set any environment config vars in the settings tab.\n\n### [Zeet.co](https://www.zeet.co/)\n\n[![Deploy](https://deploy.zeet.co/gun.svg)](https://deploy.zeet.co/?url=https://github.com/amark/gun)\n\nThen visit the URL in the output of the ''now --npm'' step, in your browser.\n\n### [Docker](https://www.docker.com/)\n\n > Warning: Docker image is community contributed and may be old with missing security updates, please check version numbers to compare.\n\n[![Docker Automated build](https://img.shields.io/docker/automated/gundb/gun.svg)](https://hub.docker.com/r/gundb/gun/) [![](https://images.microbadger.com/badges/image/gundb/gun.svg)](https://microbadger.com/images/gundb/gun "Get your own image badge on microbadger.com") [![Docker Pulls](https://img.shields.io/docker/pulls/gundb/gun.svg)](https://hub.docker.com/r/gundb/gun/) [![Docker Stars](https://img.shields.io/docker/stars/gundb/gun.svg)](https://hub.docker.com/r/gundb/gun/)\n\nPull from the [Docker Hub](https://hub.docker.com/r/gundb/gun/) [![](https://images.microbadger.com/badges/commit/gundb/gun.svg)](https://microbadger.com/images/gundb/gun). Or:\n\n```bash\ndocker run -p 8765:8765 gundb/gun\n```\n\nOr build the [Docker](https://docs.docker.com/engine/installation/) image locally:\n\n```bash\ngit clone https://github.com/amark/gun.git\ncd gun\ndocker build -t myrepo/gundb:v1 .\ndocker run -p 8765:8765 myrepo/gundb:v1\n```\n\nOr, if you prefer your Docker image with metadata labels (Linux/Mac only):\n\n```bash\nnpm run docker\ndocker run -p 8765:8765 username/gun:git\n```\n\nThen visit [http://localhost:8765](http://localhost:8765) in your browser.\n\n## License\n\nDesigned with ♥ by Mark Nadal, the GUN team, and many amazing contributors.\n\nOpenly licensed under [Zlib / MIT / Apache 2.0](https://github.com/amark/gun/blob/master/LICENSE.md).\n\n[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bhttps%3A%2F%2Fgithub.com%2Famark%2Fgun.svg?size=large)](https://app.fossa.io/projects/git%2Bhttps%3A%2F%2Fgithub.com%2Famark%2Fgun?ref=badge_large)\n\n[YouTube](https://www.youtube.com/channel/UCQAtpf-zi9Pp4__2nToOM8g) . [Twitter](https://twitter.com/marknadal)\n', '{"language":"JavaScript","stars":18785,"forks":1221,"watchers":18785,"open_issues":312,"topics":["artificial-intelligence","big-data","blockchain","crdt","crypto","cryptography","dapp","database","decentralized","dweb","encryption","end-to-end","graph","machine-learning","metaverse","offline-first","p2p","protocol","realtime","web3"],"default_branch":"master","size_kb":32905,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:cstefanache:cstefanache.github.io","source_url":"https://github.com/cstefanache/cstefanache.github.io"},{"type":"has_code","target_id":"github:lmangani:gun-scape","source_url":"https://github.com/lmangani/gun-scape#gun-scape\"><img"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:Stefdv:gun-ui-lcd","source_url":"https://github.com/Stefdv/gun-ui-lcd#okay-what-about-gundb-\"><img"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:gundb:panic-server","source_url":"https://github.com/gundb/panic-server"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:nmauersberg\">Nico<:a>,","source_url":"https://github.com/nmauersberg\">Nico</a>,"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:brysgo:graphql-gun\">GraphQL<","source_url":"https://github.com/brysgo/graphql-gun\">GraphQL<"},{"type":"has_code","target_id":"github:PenguinMan98:electrontest\">Electron<","source_url":"https://github.com/PenguinMan98/electrontest\">Electron<"},{"type":"has_code","target_id":"github:sjones6:vue-gun\">Vue<","source_url":"https://github.com/sjones6/vue-gun\">Vue<"},{"type":"has_code","target_id":"github:Stefdv:gun-ui-lcd","source_url":"https://github.com/Stefdv/gun-ui-lcd#syncing\">Webcomponents<"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:anywhichway:gun-block","source_url":"https://github.com/anywhichway/gun-block"},{"type":"has_code","target_id":"github:beebase:gun-vuex-quasar","source_url":"https://github.com/beebase/gun-vuex-quasar"},{"type":"has_code","target_id":"github:BrockAtkinson:brunch-gun","source_url":"https://github.com/BrockAtkinson/brunch-gun"},{"type":"has_code","target_id":"github:brysgo:graphql-gun","source_url":"https://github.com/brysgo/graphql-gun"},{"type":"has_code","target_id":"github:d3x0r:gun-db","source_url":"https://github.com/d3x0r/gun-db"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:JuniperChicago:cycle-gun","source_url":"https://github.com/JuniperChicago/cycle-gun"},{"type":"has_code","target_id":"github:jveres:todomvc","source_url":"https://github.com/jveres/todomvc"},{"type":"has_code","target_id":"github:kristianmandrup:gun-edge","source_url":"https://github.com/kristianmandrup/gun-edge"},{"type":"has_code","target_id":"github:lmangani:gun-scape","source_url":"https://github.com/lmangani/gun-scape"},{"type":"has_code","target_id":"github:lmangani:gun-cassandra","source_url":"https://github.com/lmangani/gun-cassandra"},{"type":"has_code","target_id":"github:lmangani:fastify-gundb","source_url":"https://github.com/lmangani/fastify-gundb"},{"type":"has_code","target_id":"github:lmangani:polyGun-letsencrypt","source_url":"https://github.com/lmangani/polyGun-letsencrypt"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:omarzion:stickies","source_url":"https://github.com/omarzion/stickies"},{"type":"has_code","target_id":"github:PsychoLlama:gun-level","source_url":"https://github.com/PsychoLlama/gun-level"},{"type":"has_code","target_id":"github:gundb:gun-schema","source_url":"https://github.com/gundb/gun-schema"},{"type":"has_code","target_id":"github:swifty:gun-p2p-auth","source_url":"https://github.com/swifty/gun-p2p-auth"},{"type":"has_code","target_id":"github:sbiaudet:cs-gun","source_url":"https://github.com/sbiaudet/cs-gun"},{"type":"has_code","target_id":"github:ctrlplusb:gun-most","source_url":"https://github.com/ctrlplusb/gun-most"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:sjones6:gun-flint","source_url":"https://github.com/sjones6/gun-flint"},{"type":"has_code","target_id":"github:xmonader:pygundb","source_url":"https://github.com/xmonader/pygundb"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:amark:gun.git","source_url":"https://github.com/amark/gun.git"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:amark:gun.git","source_url":"https://github.com/amark/gun.git"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"}]', NULL, 'NOASSERTION', 'approved', 80, 'c175fb4df68a242905c9453f8af1695f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-amark-gun from https://github.com/amark.png
Image converted to WebP: data/images/github-amark-gun.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-afshinea-stanford-cs-229-machine-learning', 'github--afshinea--stanford-cs-229-machine-learning', 'stanford-cs-229-machine-learning', 'afshinea', 'Available in العربية - English - Español - فارسی - Français - 한국어 - Português - Türkçe - Tiếng Việt - 简中 - 繁中 This repository aims at summing up in the same place all the important notions that are covered in Stanford''s CS 229 Machine Learning course, and include: - **Refreshers** in related topics that highlight the key points of the **prerequisites of the course**. - **Cheatsheets for each machine learning field**, as well as another dedicated to tips and tricks to have in mind when trainin...', '["cheatsheet","cs229","data-science","deep-learning","machine-learning","ml-cheatsheet","supervised-learning","unsupervised-learning"]', 'other', 18764, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Machine Learning cheatsheets for Stanford''s CS 229\n\nAvailable in [العربية](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/ar) -  [English](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/en) -  [Español](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/es) -  [فارسی](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/fa) -  [Français](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/fr) -  [한국어](https://stanford.edu/~shervine/l/ko/teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks) -  [Português](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/pt) -  [Türkçe](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/tr) - [Tiếng Việt](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/vi) -  [简中](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/zh) -  [繁中](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/zh-tw)\n\n## Goal\nThis repository aims at summing up in the same place all the important notions that are covered in Stanford''s CS 229 Machine Learning course, and include:\n- **Refreshers** in related topics that highlight the key points of the **prerequisites of the course**.\n- **Cheatsheets for each machine learning field**, as well as another dedicated to tips and tricks to have in mind when training a model.\n- All elements of the above combined in an **ultimate compilation of concepts**, to have with you at all times!\n\n## Content\n#### VIP Cheatsheets\n|<a href="https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/cheatsheet-supervised-learning.pdf"><img src="https://stanford.edu/~shervine/teaching/cs-229/illustrations/cover/en-001.png?" alt="Illustration" width="220px"/></a>|<a href="https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/cheatsheet-unsupervised-learning.pdf"><img src="https://stanford.edu/~shervine/teaching/cs-229/illustrations/cover/en-002.png" alt="Illustration" width="220px"/></a>|<a href="https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/cheatsheet-deep-learning.pdf"><img src="https://stanford.edu/~shervine/teaching/cs-229/illustrations/cover/en-003.png" alt="Illustration" width="220px"/></a>|<a href="https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/cheatsheet-machine-learning-tips-and-tricks.pdf"><img src="https://stanford.edu/~shervine/teaching/cs-229/illustrations/cover/en-004.png" alt="Illustration" width="220px"/></a>|\n|:--:|:--:|:--:|:--:|\n|Supervised Learning|Unsupervised Learning|Deep Learning|Tips and tricks|\n\n#### VIP Refreshers\n|<a href="https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/refresher-probabilities-statistics.pdf"><img src="https://stanford.edu/~shervine/teaching/cs-229/illustrations/cover/en-005.png" alt="Illustration" width="220px"/></a>|<a href="https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/refresher-algebra-calculus.pdf"><img src="https://stanford.edu/~shervine/teaching/cs-229/illustrations/cover/en-006.png#1" alt="Illustration" width="220px"/></a>|\n|:--:|:--:|\n|Probabilities and Statistics|Algebra and Calculus|\n\n\n#### Super VIP Cheatsheet\n|<a href="https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/super-cheatsheet-machine-learning.pdf"><img src="https://stanford.edu/~shervine/teaching/cs-229/illustrations/cover/en-007.png" alt="Illustration" width="400px"/></a>|\n|:--:|\n|All the above gathered in one place|\n\n## Website\nThis material is also available on a dedicated [website](https://stanford.edu/~shervine/teaching/cs-229), so that you can enjoy reading it from any device.\n\n## Translation\nWould you like to see these cheatsheets in your native language? You can help us translating them on [this dedicated repo](https://github.com/shervinea/cheatsheet-translation)!\n\n## Authors\n[Afshine Amidi](https://twitter.com/afshinea) (Ecole Centrale Paris, MIT) and [Shervine Amidi](https://twitter.com/shervinea) (Ecole Centrale Paris, Stanford University)\n', '{"language":null,"stars":18764,"forks":4115,"watchers":18764,"open_issues":20,"topics":["cheatsheet","cs229","data-science","deep-learning","machine-learning","ml-cheatsheet","supervised-learning","unsupervised-learning"],"default_branch":"master","size_kb":38339,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:shervinea:cheatsheet-translation","source_url":"https://github.com/shervinea/cheatsheet-translation"}]', NULL, 'MIT', 'approved', 65, 'ef9bb54ad7e4eadb52330d57ee5edb61', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-afshinea-stanford-cs-229-machine-learning from https://github.com/afshinea.png
Image converted to WebP: data/images/github-afshinea-stanford-cs-229-machine-learning.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-microsoft-onnxruntime', 'github--microsoft--onnxruntime', 'onnxruntime', 'microsoft', '<p align="center"><img width="50%" src="docs/images/ONNX_Runtime_logo_dark.png" /></p> **ONNX Runtime is a cross-platform inference and training machine-learning accelerator**. **ONNX Runtime inference** can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, driver...', '["ai-framework","deep-learning","hardware-acceleration","machine-learning","neural-networks","onnx","pytorch","scikit-learn","tensorflow","c++"]', 'other', 18600, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/microsoft/onnxruntime","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center"><img width="50%" src="docs/images/ONNX_Runtime_logo_dark.png" /></p>\n\n**ONNX Runtime is a cross-platform inference and training machine-learning accelerator**.\n\n**ONNX Runtime inference** can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, drivers, and operating systems, and provides optimal performance by leveraging hardware accelerators where applicable alongside graph optimizations and transforms. [Learn more &rarr;](https://www.onnxruntime.ai/docs/#onnx-runtime-for-inferencing)\n\n**ONNX Runtime training** can accelerate the model training time on multi-node NVIDIA GPUs for transformer models with a one-line addition for existing PyTorch training scripts. [Learn more &rarr;](https://www.onnxruntime.ai/docs/#onnx-runtime-for-training)\n\n## Get Started & Resources\n\n* **General Information**: [onnxruntime.ai](https://onnxruntime.ai)\n\n* **Usage documentation and tutorials**: [onnxruntime.ai/docs](https://onnxruntime.ai/docs)\n\n* **YouTube video tutorials**: [youtube.com/@ONNXRuntime](https://www.youtube.com/@ONNXRuntime)\n\n* [**Upcoming Release Roadmap**](https://onnxruntime.ai/roadmap)\n\n* **Companion sample repositories**:\n  - ONNX Runtime Inferencing: [microsoft/onnxruntime-inference-examples](https://github.com/microsoft/onnxruntime-inference-examples)\n  - ONNX Runtime Training: [microsoft/onnxruntime-training-examples](https://github.com/microsoft/onnxruntime-training-examples)\n\n## Releases\n\nThe current release and past releases can be found here: https://github.com/microsoft/onnxruntime/releases.\n\nFor details on the upcoming release, including release dates, announcements, features, and guidance on submitting feature requests, please visit the release roadmap: https://onnxruntime.ai/roadmap.\n\n## Data/Telemetry\n\nWindows distributions of this project may collect usage data and send it to Microsoft to help improve our products and services. See the [privacy statement](docs/Privacy.md) for more details.\n\n## Contributions and Feedback\n\nWe welcome contributions! Please see the [contribution guidelines](CONTRIBUTING.md).\n\nFor feature requests or bug reports, please file a [GitHub Issue](https://github.com/Microsoft/onnxruntime/issues).\n\nFor general discussion or questions, please use [GitHub Discussions](https://github.com/microsoft/onnxruntime/discussions).\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE).\n', '{"language":"C++","stars":18600,"forks":3588,"watchers":18600,"open_issues":1204,"topics":["ai-framework","deep-learning","hardware-acceleration","machine-learning","neural-networks","onnx","pytorch","scikit-learn","tensorflow"],"default_branch":"main","size_kb":1401142,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:microsoft:onnxruntime-inference-examples","source_url":"https://github.com/microsoft/onnxruntime-inference-examples"},{"type":"has_code","target_id":"github:microsoft:onnxruntime-training-examples","source_url":"https://github.com/microsoft/onnxruntime-training-examples"},{"type":"has_code","target_id":"github:microsoft:onnxruntime","source_url":"https://github.com/microsoft/onnxruntime"},{"type":"has_code","target_id":"github:Microsoft:onnxruntime","source_url":"https://github.com/Microsoft/onnxruntime"},{"type":"has_code","target_id":"github:microsoft:onnxruntime","source_url":"https://github.com/microsoft/onnxruntime"}]', NULL, 'MIT', 'approved', 65, 'c14f78b6f6bf7a387071c76da43965ca', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-microsoft-onnxruntime from https://github.com/microsoft.png
Image converted to WebP: data/images/github-microsoft-onnxruntime.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-AI4Finance-Foundation-FinGPT', 'github--ai4finance-foundation--fingpt', 'FinGPT', 'AI4Finance-Foundation', '<div align="center"> <img align="center" width="30%" alt="image" src="https://github.com/AI4Finance-Foundation/FinGPT/assets/31713746/e0371951-1ce1-488e-aa25-0992dafcc139"> </div> !License <div align="center"> <img align="center" src=figs/logo_transparent_background.png width="40%"/> </div> Let us not expect Wall Street to open-source LLMs or open APIs, due to FinTech institutes'' internal regulations and policies. Blueprint of FinGPT <https://huggingface.co/FinGPT> !Visitors - [Model Release]...', '["chatgpt","finance","fingpt","fintech","large-language-models","machine-learning","nlp","prompt-engineering","pytorch","reinforcement-learning","robo-advisor","sentiment-analysis","technical-analysis","jupyter notebook"]', 'other', 18173, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/AI4Finance-Foundation/FinGPT","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<div align="center">\n<img align="center" width="30%" alt="image" src="https://github.com/AI4Finance-Foundation/FinGPT/assets/31713746/e0371951-1ce1-488e-aa25-0992dafcc139">\n</div>\n\n# FinGPT: Open-Source Financial Large Language Models\n[![Downloads](https://static.pepy.tech/badge/fingpt)](https://pepy.tech/project/fingpt)\n[![Downloads](https://static.pepy.tech/badge/fingpt/week)](https://pepy.tech/project/fingpt)\n[![Join Discord](https://img.shields.io/badge/Discord-Join-blue)](https://discord.gg/trsr8SXpW5)\n[![Python 3.8](https://img.shields.io/badge/python-3.6-blue.svg)](https://www.python.org/downloads/release/python-360/)\n[![PyPI](https://img.shields.io/pypi/v/fingpt.svg)](https://pypi.org/project/fingpt/)\n![License](https://img.shields.io/github/license/AI4Finance-Foundation/fingpt.svg?color=brightgreen)\n![](https://img.shields.io/github/issues-raw/AI4Finance-Foundation/fingpt?label=Issues)\n![](https://img.shields.io/github/issues-closed-raw/AI4Finance-Foundation/fingpt?label=Closed+Issues)\n![](https://img.shields.io/github/issues-pr-raw/AI4Finance-Foundation/fingpt?label=Open+PRs)\n![](https://img.shields.io/github/issues-pr-closed-raw/AI4Finance-Foundation/fingpt?label=Closed+PRs)\n\n<div align="center">\n<img align="center" src=figs/logo_transparent_background.png width="40%"/>\n</div>\n\nLet us not expect Wall Street to open-source LLMs or open APIs, due to FinTech institutes'' internal regulations and policies.\n\n[Blueprint of FinGPT](https://arxiv.org/abs/2306.06031)\n\n<https://huggingface.co/FinGPT>\n\n![Visitors](https://api.visitorbadge.io/api/VisitorHit?user=AI4Finance-Foundation&repo=FinGPT&countColor=%23B17A)\n[![](https://dcbadge.limes.pink/api/server/trsr8SXpW5?cb=1)](https://discord.gg/trsr8SXpW5)\n\n\n## What''s New:\n - [Model Release] Nov, 2023: We release [FinGPT-Forecaster](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Forecaster)!  🔥[Demo](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster), [Medium Blog](https://medium.datadriveninvestor.com/introducing-fingpt-forecaster-the-future-of-robo-advisory-services-50add34e3d3c) & [Model](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) are available on Huggingface🤗!\n - [Paper Acceptance] Oct, 2023: ["FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets"](https://arxiv.org/abs/2310.04793) is accepted🎉 by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023 \n - [Paper Acceptance] Oct, 2023: ["FinGPT: Democratizing Internet-scale Data for Financial Large Language Models"](https://arxiv.org/abs/2307.10485) is accepted🎉 by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023\n - [Model Release] Oct, 2023: We release the [financial multi-task LLMs](https://huggingface.co/FinGPT) 🔥 produced when evaluating base-LLMs on [FinGPT-Benchmark](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)\n - [Paper Acceptance] Sep, 2023: ["Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models"](https://arxiv.org/abs/2310.04027) is accepted🎉 by [ACM International Conference on AI in Finance (ICAIF-23)](https://ai-finance.org/icaif-23-accepted-papers/)\n - [Model Release] Aug, 2023: We release the [financial sentiment analysis model](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) 🔥\n - [Paper Acceptance] Jul, 2023: ["Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models"](https://arxiv.org/abs/2306.12659) is accepted🎉 by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023\n - [Paper Acceptance] Jul, 2023: ["FinGPT: Open-Source Financial Large Language Models"](https://arxiv.org/abs/2306.06031) is accepted🎉 by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023\n - [Medium Blog] Jun 2023: [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8)\n\n## Why FinGPT?\n\n1). Finance is highly dynamic. [BloombergGPT](https://arxiv.org/abs/2303.17564) trained an LLM using a mixture of finance data and general-purpose data, which took about 53 days, at a cost of around **$3M**). It is costly to retrain an LLM model like BloombergGPT every month or every week, thus lightweight adaptation is highly favorable. FinGPT can be fine-tuned swiftly to incorporate new data (the cost falls significantly, less than **$300 per fine-tuning**).\n\n2). Democratizing Internet-scale financial data is critical, say allowing timely updates of the model (monthly or weekly updates) using an automatic data curation pipeline.  BloombergGPT has privileged data access and APIs, while FinGPT presents a more accessible alternative. It prioritizes lightweight adaptation, leveraging the best available open-source LLMs.\n\n3). The key technology is "RLHF (Reinforcement learning from human feedback)", which is missing in BloombergGPT. RLHF enables an LLM model to learn individual preferences (risk-aversion level, investing habits, personalized robo-advisor, etc.), which is the "secret" ingredient of ChatGPT and GPT4.\n\n\n### Milestone of AI Robo-Advisor: FinGPT-Forecaster\n\nTry the latest released FinGPT-Forecaster demo at our [HuggingFace Space](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster)\n\nThe dataset for FinGPT-Forecaster: https://huggingface.co/datasets/FinGPT/fingpt-forecaster-dow30-202305-202405\n\n![demo_interface](fingpt/FinGPT_Forecaster/figs/interface.png)\n\nEnter the following inputs:\n\n1) ticker symbol (e.g. AAPL, MSFT, NVDA)\n2) the day from which you want the prediction to happen (yyyy-mm-dd)\n3) the number of past weeks where market news are retrieved\n4) whether to add the latest basic financials as additional information\n\nClick Submit！ And you''ll be responded with a well-rounded analysis of the company and a prediction for next week''s stock price movement!\n\nFor detailed and more customized implementation, please refer to [FinGPT-Forecaster](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Forecaster)\n\n\n## FinGPT Demos: \n\n### Current State-of-the-arts for Financial Sentiment Analysis\n\n* [FinGPT V3 (Updated on 10/12/2023)](./fingpt)\n  \n  * What''s new: **Best trainable and inferable FinGPT for sentiment analysis on a single RTX 3090, which is even better than GPT-4 and ChatGPT Finetuning.**\n  \n  * [FinGPT v3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) series are LLMs finetuned with the LoRA method on the News and Tweets sentiment analysis dataset which achieve the best scores on most of the financial sentiment analysis datasets with low cost.\n  \n  * FinGPT v3.3 use llama2-13b as base model; FinGPT v3.2 uses llama2-7b as base model; FinGPT v3.1 uses chatglm2-6B as base model.\n  \n  * Benchmark Results:\n  \n  * | Weighted F1                                                  |    FPB    |  FiQA-SA  |   TFNS    |   NWGI    |      Devices       |    Time     |      Cost      |\n    | ------------------------------------------------------------ | :-------: | :-------: | :-------: | :-------: | :----------------: | :---------: | :------------: |\n    | [FinGPT v3.3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora)| **0.882** |   0.874   | **0.903** | **0.643** |    1 × RTX 3090    | 17.25 hours |     $17.25     |\n    | FinGPT v3.2|   0.850   |   0.860   |   0.894   |   0.636   |      1 × A100      |  5.5 hours  |    $ 22.55     |\n    | FinGPT v3.1|   0.855   |   0.850   |   0.875   |   0.642   |      1 × A100      |  5.5 hours  |    $ 22.55     |\n    | FinGPT (8bit)                                                |   0.855   |   0.847   |   0.879   |   0.632   |    1 × RTX 3090    | 6.47 hours  |     $ 6.47     |\n    | FinGPT (QLoRA)                                               |   0.777   |   0.752   |   0.828   |   0.583   |    1 × RTX 3090    | 4.15 hours  |     $ 4.15     |\n    | OpenAI Fine-tune                                             |   0.878   | **0.887** |   0.883   |     -     |         -          |      -      |       -        |\n    | GPT-4                                                        |   0.833   |   0.630   |   0.808   |     -     |         -          |      -      |       -        |\n    | FinBERT                                                      |   0.880   |   0.596   |   0.733   |   0.538   | 4 × NVIDIA K80 GPU |      -      |       -        |\n    | Llama2-7B                                                    |   0.390   |   0.800   |   0.296   |   0.503   |    2048 × A100     |   21 days   | $ 4.23 million |\n    | BloombergGPT                                                 |   0.511   |   0.751   |     -     |     -     |     512 × A100     |   53 days   | $ 2.67 million |\n  \n    **Cost per GPU hour.** For **A100 GPUs**, the AWS p4d.24xlarge instance, equipped with 8 A100 GPUs is used as a benchmark to estimate the costs. Note that BloombergGPT also used p4d.24xlarge As of July 11, 2023, the hourly rate for this instance stands at $32.773. Consequently, the estimated cost per GPU hour comes to $32.77 divided by 8, resulting in approximately **$4.10**. With this value as the reference unit price (1 GPU hour). **BloombergGPT estimated cost= 512 x 53 x 24 = 651,264 GPU hours x $4.10 = $2,670,182.40**. For **RTX 3090**, we assume its cost per hour is approximately **$1.0**, which is actually much higher than available GPUs from platforms like vast.ai.\n  \n  * Reproduce the results by running [benchmarks](./fingpt/FinGPT_Sentiment_Analysis_v3/benchmark/benchmarks.ipynb), and the detailed tutorial is on the way.\n  * Finetune your own FinGPT v3 model with the LoRA method on only an RTX 3090 with this [notebook](./fingpt/FinGPT_Sentiment_Analysis_v3/training_8bit/train_Llama2_13B.ipynb) in 8bit or this [notebook](./fingpt/FinGPT_Sentiment_Analysis_v3/training_int4/train.ipynb) in int4 (QLoRA)\n  \n* [FinGPT V1](./fingpt)\n  + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**\n \n## Instruction Tuning Datasets and Models\nThe datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>\n\n[Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)\n  \n  | Datasets | Train Rows |  Test Rows |Description  |\n  | --------- | ----------------- | ------------ | --------------------- |\n  | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |\n  | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |\n  | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|\n  | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|\n  | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|\n  | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|\n\n  Multi-task financial LLMs Models:\n```python\n  demo_tasks = [\n      ''Financial Sentiment Analysis'',\n      ''Financial Relation Extraction'',\n      ''Financial Headline Classification'',\n      ''Financial Named Entity Recognition'',]\n  demo_inputs = [\n      "Glaxo''s ViiV Healthcare Signs China Manufacturing Deal With Desano",\n      "Apple Inc. Chief Executive Steve Jobs sought to soothe investor concerns about his health on Monday, saying his weight loss was caused by a hormone imbalance that is relatively simple to treat.",\n      ''gold trades in red in early trade; eyes near-term range at rs 28,300-28,600'',\n      ''This LOAN AND SECURITY AGREEMENT dated January 27 , 1999 , between SILICON VALLEY BANK (" Bank "), a California - chartered bank with its principal place of business at 3003 Tasman Drive , Santa Clara , California 95054 with a loan production office located at 40 William St ., Ste .'',]\n  demo_instructions = [\n      ''What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.'',\n      ''Given phrases that describe the relationship between two words/phrases as options, extract the word/phrase pair and the corresponding lexical relationship between them from the input text. The output format should be "relation1: word1, word2; relation2: word3, word4". Options: product/material produced, manufacturer, distributed by, industry, position held, original broadcaster, owned by, founded by, distribution format, headquarters location, stock exchange, currency, parent organization, chief executive officer, director/manager, owner of, operator, member of, employer, chairperson, platform, subsidiary, legal form, publisher, developer, brand, business division, location of formation, creator.'',\n      ''Does the news headline talk about price going up? Please choose an answer from {Yes/No}.'',\n      ''Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.'',]\n```\n\n  | Models | Description  | Function |\n  | --------- | --------------------- |---------------- |\n  | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |\n  | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |\n  | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |\n  | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |\n  | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |\n  | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |\n  | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |\n  | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |\n\n  \n## Tutorials\n[[Training] Beginner’s Guide to FinGPT: Training with LoRA and ChatGLM2–6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)\n\n## Understanding FinGPT: An Educational Blog Series\n+ [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications\n](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8)\n+ [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance\n](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca)\n+ [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models\n](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)\n\n\n## FinGPT Ecosystem\n### FinGPT embraces a full-stack framework for FinLLMs with five layers:\n1. **Data source layer**: This layer assures comprehensive market coverage, addressing the temporal sensitivity of financial data through real-time information capture.\n2. **Data engineering layer**: Primed for real-time NLP data processing, this layer tackles the inherent challenges of high temporal sensitivity and low signal-to-noise ratio in financial data.\n3. **LLMs layer**: Focusing on a range of fine-tuning methodologies such as LoRA, this layer mitigates the highly dynamic nature of financial data, ensuring the model’s relevance and accuracy.\n4. **Task layer**: This layer is responsible for executing fundamental tasks. These tasks serve as the benchmarks for performance evaluations and cross-comparisons in the realm of FinLLMs\n5. **Application layer**: Showcasing practical applications and demos, this layer highlights the potential capability of FinGPT in the financial sector.\n\n* FinGPT Framework: Open-Source Financial Large Language Models\n\n<div align="center">\n<img align="center" src=figs/FinGPT_framework_20240301.png>\n</div>\n\n* [FinGPT-RAG](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_RAG): We present a retrieval-augmented large language model framework specifically designed for financial sentiment analysis, optimizing information depth and context through external knowledge retrieval, thereby ensuring nuanced predictions.\n\n<div align="center">\n<img align="center" src=figs/FinGPT_RAG_framework.png>\n</div>\n\n* [FinGPT-FinNLP](https://github.com/AI4Finance-Foundation/FinNLP): FinNLP provides a playground for all people interested in LLMs and NLP in Finance. Here we provide full pipelines for LLM training and finetuning in the field of finance. The full architecture is shown in the following picture. Detail codes and introductions can be found [here](https://github.com/AI4Finance-Foundation/FinNLP). Or you may refer to the [wiki](https://ai4finance-foundation.github.io/FinNLP/)\n\n<div align="center">\n<img align="center" src=figs/FinGPT_FinNLP_data_source.png>\n</div>\n\n* [FinGPT-Benchmark](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark): We introduce a novel Instruction Tuning paradigm optimized for open-source Large Language Models (LLMs) in finance, enhancing their adaptability to diverse financial datasets while also facilitating cost-effective, systematic benchmarking from task-specific, multi-task, and zero-shot instruction tuning tasks. \n\n\n<div align="center">\n<img align="center" src=figs/FinGPT_Benchmark_20231110.png>\n</div>\n\n\n\n## Open-Source Base Model used in the LLMs layer of FinGPT\n* Feel free to contribute more open-source base models tailored for various language-specific financial markets.\n\n| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications |\n|  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  |\n| [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor |\n| [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis |\n| [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis |\n| [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World’s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis |\n| [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary |\n| [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis|\n| [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |\n\n* Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):\n  | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |\n  | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |\n  | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |\n  | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |\n  | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|\n  | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|\n\n### All Thanks To Our Contributors :\n<a href="https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=AI4Finance-Foundation/FinGPT" />\n</a>\n\n## News\n\n+ [Columbia Perspectives on ChatGPT](https://datascience.columbia.edu/news/2023/columbia-perspectives-on-chatgpt/?utm_source=sendinblue&utm_campaign=DSI%20Newsletter%20April%202023&utm_medium=email)\n+ [MIT Technology Review] [ChatGPT is about to revolutionize the economy. We need to decide what that looks like](https://www.technologyreview.com/2023/03/25/1070275/chatgpt-revolutionize-economy-decide-what-looks-like/)\n+ [BloombergGPT] [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564)\n+ [Finextra] [ChatGPT and Bing AI to sit as panellists at fintech conference](https://www.finextra.com/newsarticle/41973/chatgpt-and-bing-ai-to-sit-as-panellists-at-fintech-conference)\n\n## ChatGPT at AI4Finance\n\n+ [YouTube video] [I Built a Trading Bot with ChatGPT](https://www.youtube.com/watch?v=fhBw3j_O9LE), combining ChatGPT and FinRL.\n+ [Hey, ChatGPT! Explain FinRL code to me!](https://medium.com/@ai4finance/hey-chatgpt-explain-finrl-code-to-me-6a91d612296f)\n\n## Introductory\n\n+ [Sparks of artificial general intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712)\n+ [GPT-4] [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)\n+ [InstructGPT] [Training language models to follow instructions with human feedback](https://openreview.net/forum?id=TG8KACxEON) NeurIPS 2022.\n\n[The Journey of Open AI GPT models](https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2).  GPT models explained. Open AI''s GPT-1, GPT-2, GPT-3.\n\n+ [GPT-3] [Language models are few-shot learners](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html) NeurIPS 2020.\n+ [GPT-2] [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n+ [GPT-1] [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n+ [Transformer] [Attention is All you Need](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) NeurIPS 2017.\n\n## (Financial) Big Data\n\n+ [BloombergGPT] [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564)\n\n+ [WHAT’S IN MY AI?](https://lifearchitect.ai/whats-in-my-ai/) A Comprehensive Analysis of Datasets Used to Train GPT-1, GPT-2, GPT-3, GPT-NeoX-20B, Megatron-11B, MT-NLG, and Gopher\n\n+ [FinRL-Meta Repo](https://github.com/AI4Finance-Foundation/FinRL-Meta) and paper [FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/0bf54b80686d2c4dc0808c2e98d430f7-Abstract-Datasets_and_Benchmarks.html). Advances in Neural Information Processing Systems, 2022.\n\n+ [AI4Finance] [FinNLP](https://github.com/AI4Finance-Foundation/FinNLP) Democratizing Internet-scale financial data.\n\n## Interesting Demos\n\n+ [GPT-3 Creative Fiction](https://gwern.net/gpt-3#prompts-as-programming) Creative writing by OpenAI’s GPT-3 model, demonstrating poetry, dialogue, puns, literary parodies, and storytelling. Plus advice on effective GPT-3 prompt programming & avoiding common errors.\n\n## ChatGPT for FinTech\n\n**ChatGPT Trading Bot**\n+ [YouTube video] [ChatGPT Trading strategy 20097% returns](https://www.youtube.com/watch?v=unsa_gXPAJ4)\n+ [YouTube video] [ChatGPT Coding - Make A Profitable Trading Strategy In Five Minutes!](https://www.youtube.com/watch?v=4SG2884RcDY)\n+ [YouTube video] [Easy Automated Live Trading using ChatGPT (+9660.3% hands free)](https://www.youtube.com/watch?v=dIEZVPVOZPQ)\n+ [YouTube video] [ChatGPT Trading Strategy 893% Returns](https://www.youtube.com/watch?v=YxjvjK5AD2M)\n+ [YouTube video] [ChatGPT 10 Million Trading Strategy](https://www.youtube.com/watch?v=9VPfd08uU4Q)\n+ [YouTube video] [ChatGPT: Your Crypto Assistant](https://www.youtube.com/watch?v=LpzeshX6s2w)\n+ [YouTube video] [Generate Insane Trading Returns with ChatGPT and TradingView](https://www.youtube.com/watch?v=ekz6ugJE1h0&t=3s)\n\n<!--- \n**(Fast and accurate) Sentiment Analysis**\n\n   GPT-3 can help study customer surveys, social media tweets from customers/users.\n\n   Tweets\n+ [Tweet Classifier](https://platform.openai.com/playground/p/default-tweet-classifier?model=text-davinci-003)\n+ [Advanced Tweet Classifier](https://platform.openai.com/playground/p/default-adv-tweet-classifier?model=text-davinci-003)\n\n  Financial News\n+ [Algorithmic Trading using Sentiment Analysis on News Articles](https://towardsdatascience.com/https-towardsdatascience-com-algorithmic-trading-using-sentiment-analysis-on-news-articles-83db77966704)\n+ [Accessing Historical Financial News Headlines with Python](https://python.plainenglish.io/access-historical-financial-news-headlines-with-python-be1b8faaea9f)\n\n**PromptNet** Analogy to ImageNet and WordNet, it is critical to build a PromptNet.\n\n+ [Awesome_Prompting_Papers_in_Computer_Vision](https://github.com/ttengwang/Awesome_Prompting_Papers_in_Computer_Vision)\n+ [OpenPrompt](https://github.com/thunlp/OpenPrompt)\n+ [promptsource](https://github.com/bigscience-workshop/promptsource)\n\n**Robo-advisor**\n\n**Coding-tutor**\n\n+ [Hey, ChatGPT! Explain FinRL code to me!](https://medium.com/@ai4finance/hey-chatgpt-explain-finrl-code-to-me-6a91d612296f)\n\n**Blogs about ChatGPT for FinTech**\n\n## ChatGPT APIs\n\nPrompting as a new programming paradigm!\n+ [Towards Data Science] [GPT-3: Creative Potential of NLP](https://towardsdatascience.com/gpt-3-creative-potential-of-nlp-d5ccae16c1ab)\n+ [YouTube video] [OpenAI GPT-3 - Prompt Engineering For Financial NLP](https://www.youtube.com/watch?v=Nl2Cdbao5Ws)\n\n+ [OpenAI API for GPT-3](https://platform.openai.com/docs/models/gpt-3)\n+ [ChatGPT-wrapper: python and shell](https://github.com/mmabrouk/chatgpt-wrapper)\n+ [OpenAI Examples Library](https://platform.openai.com/examples)\n+ [GPT-3 Sandbox (Github)](https://github.com/shreyashankar/gpt3-sandbox) Enable users to create cool web demos using OpenAI GPT-3 API.\n+ [Exploring the Capabilities of the ChatGPT API: A Beginner’s Guide](https://levelup.gitconnected.com/exploring-the-capabilities-of-the-chatgpt-api-a-beginners-guide-e9089d49961f)\n+ [Reverse engineered ChatGPT API](https://github.com/acheong08/ChatGPT)\n\n**Prompting programming**\n\n## ChatGPT relatives: \n\n[A Release Timeline](https://github.com/osanseviero/ml_timeline) of many LLMs.\n\n[PaLM](https://arxiv.org/abs/2204.02311)\n\n[Chincella](https://arxiv.org/abs/2203.15556)\n\nInteresting evaluations:\n+ [RLHF for pretraining](https://arxiv.org/abs/2302.08582)\n\n+ [Compare ChatGPT with GPT3.5](https://arxiv.org/pdf/2302.06476.pdf)\n\n+ [Is ChatGPT A Good Translator? A Preliminary Study](https://arxiv.org/pdf/2301.08745.pdf)\n\n+ [A Multitask, Multilingual, Multimodal Evaluation of ChatGPT\non Reasoning, Hallucination, and Interactivity](https://arxiv.org/pdf/2302.04023.pdf)\n\n[YouTube video] [Physics Solution: ChatGPT vs. Google](https://www.youtube.com/watch?v=x4dIx9VYQoM)\n---> \n\n## Citing FinGPT\n```\n@article{yang2023fingpt,\n  title={FinGPT: Open-Source Financial Large Language Models},\n  author={Yang, Hongyang and Liu, Xiao-Yang and Wang, Christina Dan},\n  journal={FinLLM Symposium at IJCAI 2023},\n  year={2023}\n}\n@article{zhang2023instructfingpt,\n      title={Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models}, \n      author={Boyu Zhang and Hongyang Yang and Xiao-Yang Liu},\n      journal={FinLLM Symposium at IJCAI 2023},\n      year={2023}\n}\n@article{zhang2023fingptrag,\n  title={Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models},\n  author={Zhang, Boyu and Yang, Hongyang and Zhou, tianyu and Babar, Ali and Liu, Xiao-Yang},\n journal = {ACM International Conference on AI in Finance (ICAIF)},\n  year={2023}\n}\n\n@article{wang2023fingptbenchmark,\n  title={FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets},\n  author={Wang, Neng and Yang, Hongyang and Wang, Christina Dan},\n  journal={NeurIPS Workshop on Instruction Tuning and Instruction Following},\n  year={2023}\n}\n@article{2023finnlp,\n  title={Data-centric FinGPT: Democratizing Internet-scale Data for Financial Large Language Models},\n  author={Liu, Xiao-Yang and Wang, Guoxuan and Yang, Hongyang and Zha, Daochen},\n  journal={NeurIPS Workshop on Instruction Tuning and Instruction Following},\n  year={2023}\n}\n\n```\n\n<div align="center">\n<a href="https://finllm.github.io/workshop/#/fcb" target="_blank">\n<img align="center" src=figs/fingpt_best_presentation.png width="65%">\n</div>\n\n\n## LICENSE\n\nMIT License\n\n**Disclaimer: We are sharing codes for academic purposes under the MIT education license. Nothing herein is financial advice, and NOT a recommendation to trade real money. Please use common sense and always first consult a professional before trading or investing.**\n\n', '{"language":"Jupyter Notebook","stars":18173,"forks":2561,"watchers":18173,"open_issues":83,"topics":["chatgpt","finance","fingpt","fintech","large-language-models","machine-learning","nlp","prompt-engineering","pytorch","reinforcement-learning","robo-advisor","sentiment-analysis","technical-analysis"],"default_branch":"master","size_kb":12579,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinGPT","source_url":"https://github.com/AI4Finance-Foundation/FinGPT"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinGPT","source_url":"https://github.com/AI4Finance-Foundation/FinGPT"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinGPT","source_url":"https://github.com/AI4Finance-Foundation/FinGPT"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinGPT","source_url":"https://github.com/AI4Finance-Foundation/FinGPT"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinGPT","source_url":"https://github.com/AI4Finance-Foundation/FinGPT"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinGPT","source_url":"https://github.com/AI4Finance-Foundation/FinGPT"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinNLP","source_url":"https://github.com/AI4Finance-Foundation/FinNLP"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinNLP","source_url":"https://github.com/AI4Finance-Foundation/FinNLP"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinGPT","source_url":"https://github.com/AI4Finance-Foundation/FinGPT"},{"type":"has_code","target_id":"github:facebookresearch:llama","source_url":"https://github.com/facebookresearch/llama"},{"type":"has_code","target_id":"github:falconry:falcon","source_url":"https://github.com/falconry/falcon"},{"type":"has_code","target_id":"github:mosaicml:llm-foundry","source_url":"https://github.com/mosaicml/llm-foundry"},{"type":"has_code","target_id":"github:bigscience-workshop:bigscience","source_url":"https://github.com/bigscience-workshop/bigscience"},{"type":"has_code","target_id":"github:THUDM:ChatGLM2-6B","source_url":"https://github.com/THUDM/ChatGLM2-6B"},{"type":"has_code","target_id":"github:QwenLM:Qwen-7B","source_url":"https://github.com/QwenLM/Qwen-7B"},{"type":"has_code","target_id":"github:InternLM:InternLM","source_url":"https://github.com/InternLM/InternLM"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinGPT","source_url":"https://github.com/AI4Finance-Foundation/FinGPT"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinRL-Meta","source_url":"https://github.com/AI4Finance-Foundation/FinRL-Meta"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinNLP","source_url":"https://github.com/AI4Finance-Foundation/FinNLP"},{"type":"has_code","target_id":"github:ttengwang:Awesome_Prompting_Papers_in_Computer_Vision","source_url":"https://github.com/ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"},{"type":"has_code","target_id":"github:thunlp:OpenPrompt","source_url":"https://github.com/thunlp/OpenPrompt"},{"type":"has_code","target_id":"github:bigscience-workshop:promptsource","source_url":"https://github.com/bigscience-workshop/promptsource"},{"type":"has_code","target_id":"github:mmabrouk:chatgpt-wrapper","source_url":"https://github.com/mmabrouk/chatgpt-wrapper"},{"type":"has_code","target_id":"github:shreyashankar:gpt3-sandbox","source_url":"https://github.com/shreyashankar/gpt3-sandbox"},{"type":"has_code","target_id":"github:acheong08:ChatGPT","source_url":"https://github.com/acheong08/ChatGPT"},{"type":"has_code","target_id":"github:osanseviero:ml_timeline","source_url":"https://github.com/osanseviero/ml_timeline"}]', NULL, 'MIT', 'approved', 80, 'f9f857e8b6c8ccb83103417c6368281f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-AI4Finance-Foundation-FinGPT from https://github.com/AI4Finance-Foundation.png
Image converted to WebP: data/images/github-AI4Finance-Foundation-FinGPT.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-meta-llama-llama-cookbook', 'github--meta-llama--llama-cookbook', 'llama-cookbook', 'meta-llama', '<h1 align="center"> Llama Cookbook </h1> <p align="center"> <a href="https://llama.developer.meta.com/join_waitlist?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main"><img src="https://img.shields.io/badge/Llama_API-Join_Waitlist-brightgreen?logo=meta" /></a> <a href="https://llama.developer.meta.com/docs?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main"><img src="https://img.shields.io/badge/Llama_API-Documentation-4BA9FE?logo=meta" /></a> </p> <p align="center"> <a ...', '["ai","finetuning","langchain","llama","llama2","llm","machine-learning","python","pytorch","vllm","jupyter notebook"]', 'other', 18067, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/meta-llama/llama-cookbook","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<h1 align="center"> Llama Cookbook </h1>\n<p align="center">\n	<a href="https://llama.developer.meta.com/join_waitlist?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main"><img src="https://img.shields.io/badge/Llama_API-Join_Waitlist-brightgreen?logo=meta" /></a>\n	<a href="https://llama.developer.meta.com/docs?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main"><img src="https://img.shields.io/badge/Llama_API-Documentation-4BA9FE?logo=meta" /></a>\n\n</p>\n<p align="center">\n	<a href="https://github.com/meta-llama/llama-models/blob/main/models/?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main"><img alt="Llama Model cards" src="https://img.shields.io/badge/Llama_OSS-Model_cards-green?logo=meta" /></a>\n	<a href="https://www.llama.com/docs/overview/?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main"><img alt="Llama Documentation" src="https://img.shields.io/badge/Llama_OSS-Documentation-4BA9FE?logo=meta" /></a>\n	<a href="https://huggingface.co/meta-llama"><img alt="Hugging Face meta-llama" src="https://img.shields.io/badge/Hugging_Face-meta--llama-yellow?logo=huggingface" /></a>\n\n</p>\n<p align="center">\n	<a href="https://github.com/meta-llama/synthetic-data-kit"><img alt="Llama Tools Syntethic Data Kit" src="https://img.shields.io/badge/Llama_Tools-synthetic--data--kit-orange?logo=meta" /></a>\n	<a href="https://github.com/meta-llama/llama-prompt-ops"><img alt="Llama Tools Syntethic Data Kit" src="https://img.shields.io/badge/Llama_Tools-llama--prompt--ops-orange?logo=meta" /></a>\n</p>\n<h2> Official Guide to building with Llama </h2>\n\n\n\nWelcome to the official repository for helping you get started with [inference](https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/inference/), [fine-tuning](https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/finetuning) and [end-to-end use-cases](https://github.com/meta-llama/llama-cookbook/tree/main/end-to-end-use-cases) of building with the Llama Model family.\n\nThis repository covers the most popular community approaches, use-cases and the latest recipes for Llama Text and Vision models.\n\n## Latest Llama 4 recipes\n\n* [Get started](./getting-started/build_with_llama_api.ipynb) with [Llama API](https://bit.ly/llama-api-main)\n* Integrate [Llama API](https://bit.ly/llama-api-main) with [WhatsApp](./end-to-end-use-cases/whatsapp_llama_4_bot/README.md)\n* 5M long context using [Llama 4 Scout](./getting-started/build_with_llama_4.ipynb)\n* Analyze research papers with [Llama 4 Maverick](./end-to-end-use-cases/research_paper_analyzer/README.md)\n* Create a character mind map from a book using [Llama 4 Maverick](./end-to-end-use-cases/book-character-mindmap/README.md)\n\n## Repository Structure:\n\n- [3P Integrations](https://github.com/meta-llama/llama-cookbook/tree/main/3p-integrations): Getting Started Recipes and End to End Use-Cases from various Llama providers\n- [End to End Use Cases](https://github.com/meta-llama/llama-cookbook/tree/main/end-to-end-use-cases): As the name suggests, spanning various domains and applications\n- [Getting Started](https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/): Reference for inferencing, fine-tuning and RAG examples\n- [src](https://github.com/meta-llama/llama-cookbook/tree/main/src/): Contains the src for the original llama-recipes library along with some FAQs for fine-tuning.\n\n> Note: We recently did a refactor of the repo, [archive-main](https://github.com/meta-llama/llama-cookbook/tree/archive-main) is a snapshot branch from before the refactor\n\n## FAQ:\n\n- **Q:** What happened to llama-recipes?\n  **A:** We recently renamed llama-recipes to llama-cookbook.\n\n- **Q:** I have some questions for Fine-Tuning, is there a section to address these?\n  **A:** Check out the Fine-Tuning FAQ [here](https://github.com/meta-llama/llama-cookbook/tree/main/src/docs/).\n\n- **Q:** Some links are broken/folders are missing:\n  **A:** We recently did a refactor of the repo, [archive-main](https://github.com/meta-llama/llama-cookbook/tree/archive-main) is a snapshot branch from before the refactor.\n\n- **Q:** Where can we find details about the latest models?\n  **A:** Official [Llama models website](https://www.llama.com).\n\n## Contributing\n\nPlease read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests to us.\n\n## License\n<!-- markdown-link-check-disable -->\nSee the License file for Meta Llama 4 [here](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE) and Acceptable Use Policy [here](https://github.com/meta-llama/llama-models/blob/main/models/llama4/USE_POLICY.md)\n\nSee the License file for Meta Llama 3.3 [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE) and Acceptable Use Policy [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/USE_POLICY.md)\n\nSee the License file for Meta Llama 3.2 [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) and Acceptable Use Policy [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/USE_POLICY.md)\n\nSee the License file for Meta Llama 3.1 [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE) and Acceptable Use Policy [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/USE_POLICY.md)\n\nSee the License file for Meta Llama 3 [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3/LICENSE) and Acceptable Use Policy [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3/USE_POLICY.md)\n\nSee the License file for Meta Llama 2 [here](https://github.com/meta-llama/llama-models/blob/main/models/llama2/LICENSE) and Acceptable Use Policy [here](https://github.com/meta-llama/llama-models/blob/main/models/llama2/USE_POLICY.md)\n<!-- markdown-link-check-enable -->\n', '{"language":"Jupyter Notebook","stars":18067,"forks":2656,"watchers":18067,"open_issues":61,"topics":["ai","finetuning","langchain","llama","llama2","llm","machine-learning","python","pytorch","vllm"],"default_branch":"main","size_kb":279925,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:synthetic-data-kit\"><img","source_url":"https://github.com/meta-llama/synthetic-data-kit\"><img"},{"type":"has_code","target_id":"github:meta-llama:llama-prompt-ops\"><img","source_url":"https://github.com/meta-llama/llama-prompt-ops\"><img"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"}]', NULL, 'MIT', 'approved', 65, 'fa1d80b2fdbe0393fe2c52b13cf3bf0c', NULL, NULL, CURRENT_TIMESTAMP);
