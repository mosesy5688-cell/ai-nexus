/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-2-9b', 'huggingface--google--gemma-2-9b', 'gemma-2-9b', 'google', '', '["transformers","safetensors","gemma2","text-generation","arxiv:2009.03300","arxiv:1905.07830","arxiv:1911.11641","arxiv:1904.09728","arxiv:1905.10044","arxiv:1907.10641","arxiv:1811.00937","arxiv:1809.02789","arxiv:1911.01547","arxiv:1705.03551","arxiv:2107.03374","arxiv:2108.07732","arxiv:2110.14168","arxiv:2009.11462","arxiv:2101.11718","arxiv:2110.08193","arxiv:1804.09301","arxiv:2109.07958","arxiv:1804.06876","arxiv:2103.03874","arxiv:2304.06364","arxiv:2206.04615","arxiv:2203.09509","license:gemma","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 680, 39625, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-2-9b","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":9241705984,"storage_bytes":74018169483,"files_count":18,"spaces_count":89,"gated":"manual","private":false,"config":{"architectures":["Gemma2ForCausalLM"],"model_type":"gemma2","tokenizer_config":{"bos_token":"<bos>","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1811.00937","source_url":"https://arxiv.org/abs/1811.00937"},{"type":"based_on_paper","target_id":"arxiv:1809.02789","source_url":"https://arxiv.org/abs/1809.02789"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2110.14168","source_url":"https://arxiv.org/abs/2110.14168"},{"type":"based_on_paper","target_id":"arxiv:2009.11462","source_url":"https://arxiv.org/abs/2009.11462"},{"type":"based_on_paper","target_id":"arxiv:2101.11718","source_url":"https://arxiv.org/abs/2101.11718"},{"type":"based_on_paper","target_id":"arxiv:2110.08193","source_url":"https://arxiv.org/abs/2110.08193"},{"type":"based_on_paper","target_id":"arxiv:1804.09301","source_url":"https://arxiv.org/abs/1804.09301"},{"type":"based_on_paper","target_id":"arxiv:2109.07958","source_url":"https://arxiv.org/abs/2109.07958"},{"type":"based_on_paper","target_id":"arxiv:1804.06876","source_url":"https://arxiv.org/abs/1804.06876"},{"type":"based_on_paper","target_id":"arxiv:2103.03874","source_url":"https://arxiv.org/abs/2103.03874"},{"type":"based_on_paper","target_id":"arxiv:2304.06364","source_url":"https://arxiv.org/abs/2304.06364"},{"type":"based_on_paper","target_id":"arxiv:2206.04615","source_url":"https://arxiv.org/abs/2206.04615"},{"type":"based_on_paper","target_id":"arxiv:2203.09509","source_url":"https://arxiv.org/abs/2203.09509"}]', NULL, 'Gemma', 'approved', 38.3, 'f5506d68e7deab41f9fd3bbf29acb70b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-gradientai-Llama-3-8B-Instruct-Gradient-1048k', 'huggingface--gradientai--llama-3-8b-instruct-gradient-1048k', 'Llama-3-8B-Instruct-Gradient-1048k', 'gradientai', '--- language: - en pipeline_tag: text-generation tags: - meta - llama-3 license: llama3 --- <a href="https://www.gradient.ai" target="_blank"><img src="https://cdn-uploads.huggingface.co/production/uploads/655bb613e8a8971e89944f3e/TSa3V8YpoVagnTYgxiLaO.png" width="200"/></a> Join our custom agent and long context (262k-1M+) waitlist: https://forms.gle/L6TDY7dozx8TuoUv7 Gradient incorporates your data to deploy autonomous assistants that power critical operations across your business. If you''r...', '["transformers","safetensors","llama","text-generation","meta","llama-3","conversational","en","arxiv:2309.00071","arxiv:2402.08268","arxiv:2305.14233","doi:10.57967/hf/3372","license:llama3","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 679, 8805, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\npipeline_tag: text-generation\ntags:\n- meta\n- llama-3\nlicense: llama3\n---\n<a href="https://www.gradient.ai" target="_blank"><img src="https://cdn-uploads.huggingface.co/production/uploads/655bb613e8a8971e89944f3e/TSa3V8YpoVagnTYgxiLaO.png" width="200"/></a>\n\n# Llama-3 8B Gradient Instruct 1048k\n\nJoin our custom agent and long context (262k-1M+) waitlist: https://forms.gle/L6TDY7dozx8TuoUv7\n\nGradient incorporates your data to deploy autonomous assistants that power critical operations across your business. If you''re looking to build custom AI models or agents, email us a message contact@gradient.ai. For more info see our [end-to-end development service for custom LLMs and AI systems](https://gradient.ai/development-lab)\n\n[Join our Discord](https://discord.com/invite/2QVy2qt2mf)\n\nThis model extends LLama-3 8B''s context length from 8k to > 1040K, developed by Gradient, sponsored by compute from [Crusoe Energy](https://huggingface.co/crusoeai). It demonstrates that SOTA LLMs can learn to operate on long context with minimal training by appropriately adjusting RoPE theta. We trained on 830M tokens for this stage, and 1.4B tokens total for all stages, which is < 0.01% of Llama-3''s original pre-training data.\n\n**Update (5/3): We further fine-tuned our model to strengthen its assistant-like chat ability as well.**\n\nUpdated NIAH result:\n<img src="https://cdn-uploads.huggingface.co/production/uploads/6585dc9be92bc5f258156bd6/-qaI__83ksClzoJzlqZjq.png" width="900" />\n\nRULER evals:\n- Our model is behind only GPT-4 and Yi in the retrieval and Q&A tasks\n- Itâ€™s the smallest parameter model to rank in the top 7 overall\n<img src="https://cdn-uploads.huggingface.co/production/uploads/655bb613e8a8971e89944f3e/0mLjl0Latrjc8gOrdtbc6.png" width="900" />\n\n**Approach:**\n\n- [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) as the base\n- NTK-aware interpolation [1] to initialize an optimal schedule for RoPE theta, followed by empirical RoPE theta optimization\n- Progressive training on increasing context lengths, similar to [Large World Model](https://huggingface.co/LargeWorldModel) [2] (See details below)\n\n**Infra:**\n\nWe build on top of the EasyContext Blockwise RingAttention library [3] to scalably and efficiently train on contexts up to 1048k tokens on [Crusoe Energy](https://huggingface.co/crusoeai) high performance L40S cluster.\n\nNotably, we layered parallelism on top of Ring Attention with a custom network topology to better leverage large GPU clusters in the face of network bottlenecks from passing many KV blocks between devices. This gave us a 33x speedup in model training (compare 524k and 1048k to 65k and 262k in the table below).\n\n**Data:**\n\nFor training data, we generate long contexts by augmenting [SlimPajama](https://huggingface.co/datasets/cerebras/SlimPajama-627B). We also fine-tune on a chat dataset based on UltraChat [4], following a similar recipe for data augmentation to [2].\n\n**Progressive Training Details:**\n\n|                        | 65K       | 262K      | 524k      | 1048k     |\n|------------------------|-----------|-----------|-----------|-----------|\n| Initialize From        | LLaMA-3 8B| 65K       | 262K      | 524k      |\n| Sequence Length 2^N    | 16        | 18        | 19        | 20        |\n| RoPE theta             | 15.3 M    | 207.1 M   | 1.06B     | 2.80B     |\n| Batch Size             | 1         | 1         | 16         | 8         |\n| Gradient Accumulation Steps | 32  | 16        | 1         | 1         |\n| Steps                  | 30        | 24        | 50        | 50        |\n| Total Tokens           | 62914560  | 100663296 | 419430400 | 838860800 |\n| Learning Rate          | 2.00E-05  | 2.00E-05  | 2.00E-05  | 2.00E-05  |\n| # GPUs                 | 8         | 32        | 512       | 512       |\n| GPU Type               | NVIDIA L40S | NVIDIA L40S | NVIDIA L40S | NVIDIA L40S |\n| Minutes to Train (Wall)| 202       | 555       | 61        | 87        |\n\n\n**Evaluation:**\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6585dc9be92bc5f258156bd6/mWxIGZNi3ejlmeIDWafKu.png)\n\n```\nEVAL_MAX_CONTEXT_LENGTH=1040200\nEVAL_MIN_CONTEXT_LENGTH=100\nEVAL_CONTEXT_INTERVAL=86675\nEVAL_DEPTH_INTERVAL=0.2\nEVAL_RND_NUMBER_DIGITS=8\n\nHAYSTACK1:\n    EVAL_GENERATOR_TOKENS=25\n\nHAYSTACK2:\n    EVAL_CONTEXT_INTERVAL=173350\n    EVAL_GENERATOR_TOKENS=150000\n\nHAYSTACK3:\n    EVAL_GENERATOR_TOKENS=925000\n```\n\nAll boxes not pictured for Haystack 1 and 3 are 100% accurate. Haystacks 1,2 and 3 are further detailed in this [blog post](https://gradient.ai/blog/the-haystack-matters-for-niah-evals).\n\n**Quants:**\n- [GGUF by Crusoe](https://huggingface.co/crusoeai/Llama-3-8B-Instruct-1048k-GGUF). Note that you need to add 128009 as [special token with llama.cpp](https://huggingface.co/gradientai/Llama-3-8B-Instruct-262k/discussions/13).\n- [MLX-4bit](https://huggingface.co/mlx-community/Llama-3-8B-Instruct-1048k-4bit)\n- [Ollama](https://ollama.com/library/llama3-gradient)\n- vLLM docker image, recommended to load via `--max-model-len 32768`\n- If you are interested in a hosted version, drop us a mail below.\n\n## The Gradient AI Team\n\nhttps://gradient.ai/\n\nGradient is accelerating AI transformation across industries. Our AI Foundry incorporates your data to deploy autonomous assistants that power critical operations across your business.\n\n## Contact Us\n\nDrop an email to [contact@gradient.ai](mailto:contact@gradient.ai)\n\n## Citation:\n\n```bibtex\n@misc{gradientlongcontextllama3,\n  title={Llama 3 Gradient: A series of long context models},\n  author={Leonid Pekelis and Michael Feil and Forrest Moret and Mark Huang and Tiffany Peng},\n  year={2024},\n  url = {https://gradient.ai/blog/scaling-rotational-embeddings-for-long-context-language-models},\n  doi = { 10.57967/hf/3372 },\n}\n```\n\n## References\n\n[1] Peng, Bowen, et al. "Yarn: Efficient context window extension of large language models." arXiv preprint arXiv:2309.00071 (2023).\n\n[2] Liu, Hao, et al. "World Model on Million-Length Video And Language With RingAttention." arXiv preprint arXiv:2402.08268 (2024).\n\n[3] https://github.com/jzhang38/EasyContext\n\n[4] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan\nLiu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling\nhigh-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023.\n\n----\n\n# Base Model\n\n## Model Details\n\nMeta developed and released the Meta Llama 3 family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety. \n\n**Model developers** Meta\n\n**Variations** Llama 3 comes in two sizes â€” 8B and 70B parameters â€” in pre-trained and instruction tuned variants.\n\n**Input** Models input text only.\n\n**Output** Models generate text and code only.\n\n**Model Architecture** Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Data</strong>\n   </td>\n   <td><strong>Params</strong>\n   </td>\n   <td><strong>Context length</strong>\n   </td>\n   <td><strong>GQA</strong>\n   </td>\n   <td><strong>Token count</strong>\n   </td>\n   <td><strong>Knowledge cutoff</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan="2" >Llama 3\n   </td>\n   <td rowspan="2" >A new mix of publicly available online data.\n   </td>\n   <td>8B\n   </td>\n   <td>8k\n   </td>\n   <td>Yes\n   </td>\n   <td rowspan="2" >15T+\n   </td>\n   <td>March, 2023\n   </td>\n  </tr>\n  <tr>\n   <td>70B\n   </td>\n   <td>8k\n   </td>\n   <td>Yes\n   </td>\n   <td>December, 2023\n   </td>\n  </tr>\n</table>\n\n\n**Llama 3 family of models**. Token counts refer to pretraining data only. Both the 8 and 70B versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date** April 18, 2024.\n\n**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license is available at: [https://llama.meta.com/llama3/license](https://llama.meta.com/llama3/license)\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n\n\n## Intended Use\n\n**Intended Use Cases** Llama 3 is intended for commercial and research use in English. Instruction tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3 Community License. Use in languages other than English**.\n\n**Note: Developers may fine-tune Llama 3 models for languages beyond English provided they comply with the Llama 3 Community License and the Acceptable Use Policy.\n\n## How to use\n\nThis repository contains two versions of Meta-Llama-3-8B-Instruct, for use with transformers and with the original `llama3` codebase.\n\n### Use with transformers\n\nYou can run conversational inference using the Transformers pipeline abstraction, or by leveraging the Auto classes with the `generate()` function. Let''s see examples of both.\n\n#### Transformers pipeline\n\n```python\nimport transformers\nimport torch\n\nmodel_id = "meta-llama/Meta-Llama-3-8B-Instruct"\n\npipeline = transformers.pipeline(\n    "text-generation",\n    model=model_id,\n    model_kwargs={"torch_dtype": torch.bfloat16},\n    device_map="auto",\n)\n\nmessages = [\n    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},\n    {"role": "user", "content": "Who are you?"},\n]\n\nprompt = pipeline.tokenizer.apply_chat_template(\n		messages, \n		tokenize=False, \n		add_generation_prompt=True\n)\n\nterminators = [\n    pipeline.tokenizer.eos_token_id,\n    pipeline.tokenizer.convert_tokens_to_ids("<|eot_id|>")\n]\n\noutputs = pipeline(\n    prompt,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nprint(outputs[0]["generated_text"][len(prompt):])\n```\n\n#### Transformers AutoModelForCausalLM\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_id = "meta-llama/Meta-Llama-3-8B-Instruct"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,\n    device_map="auto",\n)\n\nmessages = [\n    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},\n    {"role": "user", "content": "Who are you?"},\n]\n\ninput_ids = tokenizer.apply_chat_template(\n    messages,\n    add_generation_prompt=True,\n    return_tensors="pt"\n).to(model.device)\n\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids("<|eot_id|>")\n]\n\noutputs = model.generate(\n    input_ids,\n    max_new_tokens=256,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9,\n)\nresponse = outputs[0][input_ids.shape[-1]:]\nprint(tokenizer.decode(response, skip_special_tokens=True))\n```\n\n\n### Use with `llama3`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama3)\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --include "original/*" --local-dir Meta-Llama-3-8B-Instruct\n```\n\nFor Hugging Face support, we recommend using transformers or TGI, but a similar command works.\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta''s Research SuperCluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\n\n**Carbon Footprint Pretraining utilized a cumulative** 7.7M GPU hours of computation on hardware of type H100-80GB (TDP of 700W). Estimated total emissions were 2290 tCO2eq, 100% of which were offset by Metaâ€™s sustainability program.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Time (GPU hours)</strong>\n   </td>\n   <td><strong>Power Consumption (W)</strong>\n   </td>\n   <td><strong>Carbon Emitted(tCO2eq)</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3 8B\n   </td>\n   <td>1.3M\n   </td>\n   <td>700\n   </td>\n   <td>390\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3 70B\n   </td>\n   <td>6.4M\n   </td>\n   <td>700\n   </td>\n   <td>1900\n   </td>\n  </tr>\n  <tr>\n   <td>Total\n   </td>\n   <td>7.7M\n   </td>\n   <td>\n   </td>\n   <td>2290\n   </td>\n  </tr>\n</table>\n\n\n\n**CO2 emissions during pre-training**. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta''s sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\n\n\n## Training Data\n\n**Overview** Llama 3 was pretrained on over 15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 10M human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n\n**Data Freshness** The pretraining data has a cutoff of March 2023 for the 7B and December 2023 for the 70B models respectively. \n\n\n## Benchmarks \n\nIn this section, we report the results for Llama 3 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. For details on the methodology see [here](https://github.com/meta-llama/llama3/blob/main/eval_methodology.md).\n\n\n### Base pretrained models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama2 7B</strong>\n   </td>\n   <td><strong>Llama2 13B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama2 70B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan="6" >General\n   </td>\n   <td>MMLU (5-shot)\n   </td>\n   <td>66.6\n   </td>\n   <td>45.7\n   </td>\n   <td>53.8\n   </td>\n   <td>79.5\n   </td>\n   <td>69.7\n   </td>\n  </tr>\n  <tr>\n   <td>AGIEval English (3-5 shot)\n   </td>\n   <td>45.9\n   </td>\n   <td>28.8\n   </td>\n   <td>38.7\n   </td>\n   <td>63.0\n   </td>\n   <td>54.8\n   </td>\n  </tr>\n  <tr>\n   <td>CommonSenseQA (7-shot)\n   </td>\n   <td>72.6\n   </td>\n   <td>57.6\n   </td>\n   <td>67.6\n   </td>\n   <td>83.8\n   </td>\n   <td>78.7\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande (5-shot)\n   </td>\n   <td>76.1\n   </td>\n   <td>73.3\n   </td>\n   <td>75.4\n   </td>\n   <td>83.1\n   </td>\n   <td>81.8\n   </td>\n  </tr>\n  <tr>\n   <td>BIG-Bench Hard (3-shot, CoT)\n   </td>\n   <td>61.1\n   </td>\n   <td>38.1\n   </td>\n   <td>47.0\n   </td>\n   <td>81.3\n   </td>\n   <td>65.7\n   </td>\n  </tr>\n  <tr>\n   <td>ARC-Challenge (25-shot)\n   </td>\n   <td>78.6\n   </td>\n   <td>53.7\n   </td>\n   <td>67.6\n   </td>\n   <td>93.0\n   </td>\n   <td>85.3\n   </td>\n  </tr>\n  <tr>\n   <td>Knowledge reasoning\n   </td>\n   <td>TriviaQA-Wiki (5-shot)\n   </td>\n   <td>78.5\n   </td>\n   <td>72.1\n   </td>\n   <td>79.6\n   </td>\n   <td>89.7\n   </td>\n   <td>87.5\n   </td>\n  </tr>\n  <tr>\n   <td rowspan="4" >Reading comprehension\n   </td>\n   <td>SQuAD (1-shot)\n   </td>\n   <td>76.4\n   </td>\n   <td>72.2\n   </td>\n   <td>72.1\n   </td>\n   <td>85.6\n   </td>\n   <td>82.6\n   </td>\n  </tr>\n  <tr>\n   <td>QuAC (1-shot, F1)\n   </td>\n   <td>44.4\n   </td>\n   <td>39.6\n   </td>\n   <td>44.9\n   </td>\n   <td>51.1\n   </td>\n   <td>49.4\n   </td>\n  </tr>\n  <tr>\n   <td>BoolQ (0-shot)\n   </td>\n   <td>75.7\n   </td>\n   <td>65.5\n   </td>\n   <td>66.9\n   </td>\n   <td>79.0\n   </td>\n   <td>73.1\n   </td>\n  </tr>\n  <tr>\n   <td>DROP (3-shot, F1)\n   </td>\n   <td>58.4\n   </td>\n   <td>37.9\n   </td>\n   <td>49.8\n   </td>\n   <td>79.7\n   </td>\n   <td>70.2\n   </td>\n  </tr>\n</table>\n\n\n\n### Instruction tuned models\n\n\n<table>\n  <tr>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama 2 7B</strong>\n   </td>\n   <td><strong>Llama 2 13B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama 2 70B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (5-shot)\n   </td>\n   <td>68.4\n   </td>\n   <td>34.1\n   </td>\n   <td>47.8\n   </td>\n   <td>82.0\n   </td>\n   <td>52.9\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA (0-shot)\n   </td>\n   <td>34.2\n   </td>\n   <td>21.7\n   </td>\n   <td>22.3\n   </td>\n   <td>39.5\n   </td>\n   <td>21.0\n   </td>\n  </tr>\n  <tr>\n   <td>HumanEval (0-shot)\n   </td>\n   <td>62.2\n   </td>\n   <td>7.9\n   </td>\n   <td>14.0\n   </td>\n   <td>81.7\n   </td>\n   <td>25.6\n   </td>\n  </tr>\n  <tr>\n   <td>GSM-8K (8-shot, CoT)\n   </td>\n   <td>79.6\n   </td>\n   <td>25.7\n   </td>\n   <td>77.4\n   </td>\n   <td>93.0\n   </td>\n   <td>57.5\n   </td>\n  </tr>\n  <tr>\n   <td>MATH (4-shot, CoT)\n   </td>\n   <td>30.0\n   </td>\n   <td>3.8\n   </td>\n   <td>6.7\n   </td>\n   <td>50.4\n   </td>\n   <td>11.6\n   </td>\n  </tr>\n</table>\n\n\n\n### Responsibility & Safety\n\nWe believe that an open approach to AI leads to better, safer products, faster innovation, and a bigger overall market. We are committed to Responsible AI development and took a series of steps to limit misuse and harm and support the open source community.\n\nFoundation models are widely capable technologies that are built to be used for a diverse range of applications. They are not designed to meet every developer preference on safety levels for all use cases, out-of-the-box, as those by their nature will differ across different applications. \n\nRather, responsible LLM-application deployment is achieved by implementing a series of safety best practices throughout the development of such applications, from the model pre-training, fine-tuning and the deployment of systems composed of safeguards to tailor the safety needs specifically to the use case and audience. \n\n\nAs part of the Llama 3 release, we updated our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to outline the steps and best practices for developers to implement model and system level safety for their application. We also provide a set of resources including [Meta Llama Guard 2](https://llama.meta.com/purple-llama/) and [Code Shield](https://llama.meta.com/purple-llama/) safeguards. These tools have proven to drastically reduce residual risks of LLM Systems, while maintaining a high level of helpfulness. We encourage developers to tune and deploy these safeguards according to their needs and we provide a [reference implementation](https://github.com/meta-llama/llama-recipes/tree/main/recipes/responsible_ai) to get you started.\n\n\n#### Llama 3-Instruct\n\nAs outlined in the Responsible Use Guide, some trade-off between model helpfulness and model alignment is likely unavoidable. Developers should exercise discretion about how to weigh the benefits of alignment and helpfulness for their specific use case and audience. Developers should be mindful of residual risks when using Llama models and leverage additional safety tools as needed to reach the right safety bar for their use case. \n\n<span style="text-decoration:underline;">Safety</span>\n\nFor our instruction tuned model, we conducted extensive red teaming exercises, performed adversarial evaluations and implemented safety mitigations techniques to lower residual risks. As with any Large Language Model, residual risks will likely remain and we recommend that developers assess these risks in the context of their use case. In parallel, we are working with the community to make AI safety benchmark standards transparent, rigorous and interpretable. \n\n<span style="text-decoration:underline;">Refusals</span>\n\nIn addition to residual risks, we put a great emphasis on model refusals to benign prompts. Over-refusing not only can impact the user experience but could even be harmful in certain contexts as well. Weâ€™ve heard the feedback from the developer community and improved our fine tuning to ensure that Llama 3 is significantly less likely to falsely refuse to answer prompts than Llama 2. \n\nWe built internal benchmarks and developed mitigations to limit false refusals making Llama 3 our most helpful model to date. \n\n\n#### Responsible release \n\nIn addition to responsible use considerations outlined above, we followed a rigorous process that requires us to take extra measures against misuse and critical risks before we make our release decision. \n\nMisuse\n\nIf you access or use Llama 3, you agree to the Acceptable Use Policy. The most recent copy of this policy can be found at [https://llama.meta.com/llama3/use-policy/](https://llama.meta.com/llama3/use-policy/).\n\n\n#### Critical risks \n\n<span style="text-decoration:underline;">CBRNE</span> (Chemical, Biological, Radiological, Nuclear, and high yield Explosives)\n\nWe have conducted a two fold assessment of the safety of the model in this area:\n\n\n\n* Iterative testing during model training to assess the safety of responses related to CBRNE threats and other adversarial risks.\n* Involving external CBRNE experts to conduct an uplift test assessing the ability of the model to accurately provide expert knowledge and reduce barriers to potential CBRNE misuse, by reference to what can be achieved using web search (without the model).\n\n\n### <span style="text-decoration:underline;">Cyber Security </span>\n\nWe have evaluated Llama 3 with CyberSecEval, Metaâ€™s cybersecurity safety eval suite, measuring Llama 3â€™s propensity to suggest insecure code when used as a coding assistant, and Llama 3â€™s propensity to comply with requests to help carry out cyber attacks, where attacks are defined by the industry standard MITRE ATT&CK cyber attack ontology. On our insecure coding and cyber attacker helpfulness tests, Llama 3 behaved in the same range or safer than models of [equivalent coding capability](https://huggingface.co/spaces/facebook/CyberSecEval). \n\n\n### <span style="text-decoration:underline;">Child Safety</span>\n\nChild Safety risk assessments were conducted using a team of experts, to assess the modelâ€™s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership in AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community. \n\n\n## Ethical Considerations and Limitations\n\nThe core values of Llama 3 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nBut Llama 3 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has been in English, and has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3â€™s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3 models, developers should perform safety testing and tuning tailored to their specific applications of the model. As outlined in the Responsible Use Guide, we recommend incorporating [Purple Llama](https://github.com/facebookresearch/PurpleLlama) solutions into your workflows and specifically [Llama Guard](https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/) which provides a base model to filter input and output prompts to layer system-level safety on top of model-level safety. \n\nPlease see the Responsible Use Guide available at [http://llama.meta.com/responsible-use-guide](http://llama.meta.com/responsible-use-guide)\n\n\n## Citation instructions\n\n@article{llama3modelcard,\n\n  title={Llama 3 Model Card},\n\n  author={AI@Meta},\n\n  year={2024},\n\n  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}\n\n}\n\n## Contributors\n\nAaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":8030261248,"storage_bytes":32121112752,"files_count":13,"spaces_count":15,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","chat_template":"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = ''<|start_header_id|>'' + message[''role''] + ''<|end_header_id|>\n\n''+ message[''content''] | trim + ''<|eot_id|>'' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ ''<|start_header_id|>assistant<|end_header_id|>\n\n'' }}{% endif %}","eos_token":"<|end_of_text|>"}}}', '[]', '[{"type":"has_code","target_id":"github:jzhang38:EasyContext","source_url":"https://github.com/jzhang38/EasyContext"},{"type":"has_code","target_id":"github:meta-llama:llama3","source_url":"https://github.com/meta-llama/llama3"},{"type":"has_code","target_id":"github:meta-llama:llama-recipes","source_url":"https://github.com/meta-llama/llama-recipes"},{"type":"has_code","target_id":"github:meta-llama:llama3","source_url":"https://github.com/meta-llama/llama3"},{"type":"has_code","target_id":"github:meta-llama:llama3","source_url":"https://github.com/meta-llama/llama3"},{"type":"has_code","target_id":"github:meta-llama:llama-recipes","source_url":"https://github.com/meta-llama/llama-recipes"},{"type":"has_code","target_id":"github:meta-llama:PurpleLlama","source_url":"https://github.com/meta-llama/PurpleLlama"},{"type":"has_code","target_id":"github:facebookresearch:PurpleLlama","source_url":"https://github.com/facebookresearch/PurpleLlama"},{"type":"has_code","target_id":"github:meta-llama:llama3","source_url":"https://github.com/meta-llama/llama3"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2402.08268","source_url":"https://arxiv.org/abs/2402.08268"},{"type":"based_on_paper","target_id":"arxiv:2305.14233","source_url":"https://arxiv.org/abs/2305.14233"}]', NULL, 'LLaMA-3', 'approved', 78.3, '50133065de327f2afc577151e1ded4e4', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-agentica-org-DeepCoder-14B-Preview', 'huggingface--agentica-org--deepcoder-14b-preview', 'DeepCoder-14B-Preview', 'agentica-org', '--- license: mit library_name: transformers datasets: - PrimeIntellect/verifiable-coding-problems - likaixin/TACO-verified - livecodebench/code_generation_lite language: - en base_model: - deepseek-ai/DeepSeek-R1-Distill-Qwen-14B pipeline_tag: text-generation --- <div align="center"> <span style="font-family: default; font-size: 1.5em;">DeepCoder-14B-Preview</span> <div> ðŸš€ Democratizing Reinforcement Learning for LLMs (RLLM) ðŸŒŸ </div> </div> <br> <div align="center" style="line-height: 1;"> ...', '["transformers","safetensors","qwen2","text-generation","conversational","en","dataset:primeintellect/verifiable-coding-problems","dataset:likaixin/taco-verified","dataset:livecodebench/code_generation_lite","license:mit","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 679, 1579, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/agentica-org/DeepCoder-14B-Preview","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: mit\nlibrary_name: transformers\ndatasets:\n- PrimeIntellect/verifiable-coding-problems\n- likaixin/TACO-verified\n- livecodebench/code_generation_lite\nlanguage:\n- en\nbase_model:\n- deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\npipeline_tag: text-generation\n---\n\n<div align="center">\n<span style="font-family: default; font-size: 1.5em;">DeepCoder-14B-Preview</span>\n<div>\nðŸš€ Democratizing Reinforcement Learning for LLMs (RLLM) ðŸŒŸ\n</div>\n</div>\n<br>\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/agentica-project/rllm" style="margin: 2px;">\n    <img alt="Code" src="https://img.shields.io/badge/RLLM-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://pretty-radio-b75.notion.site/DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51" target="_blank" style="margin: 2px;">\n    <img alt="Blog" src="https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://x.com/Agentica_" style="margin: 2px;">\n    <img alt="X.ai" src="https://img.shields.io/badge/Agentica-white?style=for-the-badge&logo=X&logoColor=000&color=000&labelColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/agentica-org" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/Agentica-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://www.together.ai" style="margin: 2px;">\n    <img alt="Together AI" src="https://img.shields.io/badge/-Together_AI%20-white?style=for-the-badge&logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAAUAAAAFACAMAAAD6TlWYAAAC7lBMVEUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8AAAAPb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8Pb%2F8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADIBDt6AAAA%2BnRSTlMAAiQEKgcdKQwiHBMUzrtSUEmjhmZGH96yv8n1ey7nL3y1U%2FZfCaIo1WFg1NrcsHYrA2%2Fv80J%2BMeilnpefqKw%2B64%2BQlSbYZGVnBGkCV%2BxW8XJube6WJ9kZF9bSzBALRynPQfLhIjvwyBEAXOTLp3o%2FJA9Y9%2F7%2F9FEKDhIVFo4GHkVzjGz8icrHzY39iHR1i0M8Jj14LLZUvb7DxMXGoQEFeQcgSBOHaPvm4uOdRLMMqcDTLbcII0sNuVn4TKaRd6RKIeDd37Svra6xuLpaW17lXUAlHh8WGxUPIS4JGQoFECMsBg4gFwsRJRIrCC0oAycaFC8NMDIzMRgBsVt9rwAAD25JREFUeNrs3QVzG0kWB%2FA3ikHhZeYwk3LMbF7GcBasOGw9hb3MzLyKw8zMzMx2rsokhySNY2mmR1N4xXV3a7sHuzWu%2BX2Ef3XPG%2Br3wOVyuVwul8vlcrlcLpfL5XK5dOlXOHTIvLnb27Xd%2FasBvrt9A%2B7r1bbdTTffcmuXwhzgTYwk6q%2BHr2RWlcclRYqXV2VeCV%2Bvr4mIkCJKZ83uc9NLC0fMD%2BD%2FCswfMfLtzh%2FeelsJcKJW19SG66KSTP6fLEXrwrU11Srw5Z8zbuzePcUBbFyg%2BPY7Pv%2Bs0A%2Bsid7ayiqFNEWp8iS9Ir%2F0Cl957bkRAaQLFLz15sBBfpbpJc7FJKKFFGuV4JJh6N573g6idr7vP%2F8iC9iI1NZJRDupLnlRBbaW3XjTfQHUJ3D8d68MBtsJiTNRold5uEYAdibkHgqiESMefGi9zfFVeCRihOS5LLJafV99XYxGddgwabKt8SmEyEQ%2FmRDlSoUA9gsNvKMDmhE8MC4L7OFtSYmPFmFlAmzm%2F9tfH0Oz8v6yFmxQ3SpOiY8eYTwjHew0%2BB9%2FD6B5ga4dLd%2FHQus0SnzaIrzWWgDb9P19MVqjw01dwFLpYYVYQymLgD1Kjj6J1umaHwLLqJfpy0%2FHIryqgg2mvetDKxXMnQMWEa9LxEpSqxZguS%2B%2BfA%2Bt9cZBi7ZxeqVMX376FqEnAtbyv7ISrTfspB%2FM82bq3r70BNMSYKV%2Bo4rQDiPzc8Csy1Fih%2BhVsE7o0cfQHnn%2FygJz6uNEJtaTSfy8ChYpnelDuxQ8HAIT1LOS8fwoCSq1FiVYcs%2FdaJ%2FgNhMJqrWKqfwoCSYtSTA08260U%2FBh47v4LDU%2F%2FgnmPOJDexX86ycwpp6yf80neB7M8o96DO2Wl2%2Bw%2FlLrh%2FlKYroW31qE9ht5EgzwRs3nR00wmgBTVq1EFtp2Ad0imdbkR0kwLQImTP8S2eg9B3QSKwkbHhPPxSUzAsjGe3P1luLrMmGklQpGjfIhKwU6C8llibBJUCaS4UKy6klkp0cX0CE9zcr8KAlei4Ahy36PLHXuBJqpYcJSmQBG3LIJWerQETS7qhCWlHowoMvfka2Va0Gjaus3MGUTp4NuWY8ja3%2FuB9q0IqydBt1eeQxZ%2B9MfQRNvnLAWT%2BiuIEuRvT9MBg3UlkQmbMmkUgB9cjsge8EbQIMLCmFPuQy6DPoGeVi9HqgED5EJazL5VAQ9Nm5CHjq0B6oKhZCUX4LrNyAfSycDhVBJZMKeTK4IoN26IPJRsAQoEhLhQ7kAmoV%2Bjbwspt0LniF8yKRMBa1%2B%2BSvkZVFfaFIkSngpvwha%2FQL56QNNqiX8%2FBs0mnMX8vPtBGiCWEf4iYmgzey7kZ8Rw6EJXonwo9SANn9GnuZCE84RnlqBJm3aIk8vFUKjxBjhKbMFaDHQhzy9%2BAI06pJEeJIS%2FGuwBn1M1WD%2BdXjNauSrdwk0Qq0kfHlUoFs7Evnq9TI0orqK8BVN1%2FIcvAn56vAKNCKhEDruz8NjkbdXOV4CKZJA1W8M8vbjT9CwMOGtDKjmjEbefpgCDRLqCB33p7kvipC3kc83UkOihLdohF5DfMjbiBf43UZTSPQq8vobyNsbudCgyzLhTT4PNK8hpmoZPkv4awU0y5G%2F1%2Fj90WG%2BDK9ATNX7mDDh71OgWYn83RHi9yRMkQY0I5G%2FOydDA4RPCX9RoMlD%2Fu6a0mCAMcJfHGh8yN%2BwqdAAMZPwJwFNB%2BRv5TRoQIs0wp%2FiiAB7TG%2B2Abor0L0GmiO5VdicuHsfaE7UfRIxJ80Rz8Kdnfss7L6NoShz8vvAWsLfOUe8kZ7o5DfSm1Pgm8gnTv4msqoIzXC%2FyrUZjWa434XdPxOoRZjiHjTD%2FTcGNm9Cg9y%2Fs9z%2FAymi1e4fqqZ4VPcfaQZnlQYGkacXP3H6X%2FrT2qIZ7jkR%2BAvy9L5jTyq5Z%2BUolBpHnNYc5PDTmubrsHtemOeJ9aJmcWI9tAV5%2BQ29Z4Kc%2Bj0TYHOQVwl5pVl07YD1h9EMt28MHOHUueihZtK5CArvRB4OTWkuvbNgYjGyF5wEGlQ4oXsbrF%2BK7O2fDBoIPPoHegQndLAc14w6WELot8jaX5pVD1Xo8iSy1WM8nzbcFMZbcf%2BLcR%2Fp7qBZayf0kYZly5GlzpOd3Mmcfy%2F9rl1AhwjTXvoXwaATDKc55Dp6mgP%2FeSLvZ4E%2B55wwTwSmr0Y2Djp6og3%2FmUrDhqbuTKWLYMqQ42i%2FkcNTdqpXeQ2Y4z82AO2Wl8txrpz5AkLRr38Q7TUiOydlJxueBfNCYzugnYKvOn62JkXpA3YmGPy8xPnTXanzhYP27d8PSvjPFzafH0Wov12VJC87ZSdcS2dVsEy%2FE8fRDgtznTFj3Tz%2FrT3QesOGO2bKv3mrVr%2BH1nrjjqFgiUilTGRr8%2FNEwHLTZ%2FisLR9vzgGLiOckYiWpVQuwQcmonmidZ3JDYBn1chohslXL79pVFWzh%2F2L5JrRG8fahYKlIWCHWUMoiYJtl%2F3wygOYFunabDBYTWmtdhJTlVy%2BAjfxPPP4YmpW3dTzYID0jTo%2BQEl88Ix1sFlqytAOacfe%2Bk1lgD29LxXiEMiFKZUIF%2By3L%2F6YYjSpu134w2EaouEKPsNH4rlwWgI0JEzcE0Qjfl19NAVsJFR6JGCF5LovAzrId2%2B8LoD6BBT8OGQy2E2rCUaJXebhGALZC9z%2FwUhC18%2F0wc1UWsBFJ1klEOymWvKgCe%2F7CW999xxdAusCI0R99PMgP7IiJczFJY3qtEiLw8tOckw88uKs40FR4xXuWzvzjVD%2BwJnqTlVUKaYpS5Ul6ReCsdOeOmVveKgq%2Bh%2F%2FvveCiu7Zvmz2rFDhRq2tqw7GoJJP%2FJ0vRWFmyplqF1NBv0KmTJz7fumX1d889%2B8yTzzz73Ldfbtm6bdS48RNygDcx3Xu1NqPMUxdLS7uWlhar85RlJK9600VIOf6c0mWDpj391NNtBg0uyfFDSlEF8T%2Ft3eFyqjwTwPGNiKq9eq%2BtqiCeoxZVEcRW4mK%2Bvc%2F5%2Bk7bBSDZOJPfFfwHWkEMG%2B%2BfXChwHMdxHMdxHMdxHMdxHMdxHIeV4yiR%2FyOUS6tHfBxP88Vse74N%2F7mdt7PF%2FHT8EFakbYg0XupvMZ%2Fddt%2F%2Ber27zebFX%2BXSfpQfD%2BMLsX7iMp4fc460%2BfgiqbSD1jSCGH1WXAV1v32OhOm0O1Yh9aUR0sNUYnVyekjBEH9eL%2B2mIY2gilmGdWXvhTKQNnpvkDYrBJgjNluJTchtIDSnBY3TNgLMUEGvbL4Qvhco3WkPbOS%2FNAEGjMay1bsEMjyCJsewXVo5HoFuH5P2b7OsJh9a0har1mn3tmkElXTzPlU%2FUd2nDfnTKH53b%2FTN%2FI7TZp2l7X3QZNPlO6X9jb1pJwUa5J8SuyQ%2Fc2vTFjl0zu%2F8vfrH2O8obdx52jaFjmmZ7HAdQQeOVw1pwxF0StNskd0GWtvsUIfsBB3SNt3m%2FgUtva1402jEfCXm%2BUBLjWkHBZ2gJ3zxHcG51JhWdnQENc%2BYk3O2vz%2F6CEJrBqYcyi9o6E172hJaMjJn876BRjYG0k7QiqFJr7tRo7SdgbSsgBaMzRoe%2BlCbfzWTlkILxqZdj%2FPaaWM0Y%2BtBUwbnrT8%2BoaZPY2kLBc2Ynfi%2FgVo2BtNO0JDRPSf6PtTgm0y7pNCI2KNJewWVqZnZNAH1md93J4HKEsNpb1Abw85P%2FQ%2Bo6GNoOs2H%2BgZo2gQqWqBpA6iNY%2Fe7EVRyXNm%2FMR%2FP%2FotjBRWokCFtK6AOrh1AA6ggkBxpG6hFnImzzLUFKNv2uOec5Q9Qw3kO7N%2BgmT7LjB81asuU1hNQXSyRhyyAULClxVDdHh%2FI4YEzIMzY0vZQWZQhlyyFX6V8aasIqnoinwP86oB8nlBRfkM%2Btxx%2BIaZWpNGf03zkCH4xYk0r7PiuTljALz6R0wQqya%2FI6ZrTHy78acS%2FCSd5hB8dmdNGdlyDCQfiGmz7dVhtkddWWZvWU0D72CGv3Qf84O%2BFP40Wl8irLOAHBXtaDLQDoq0fgnPk9gTaHrnt4Qcz5Bba8T2OcBPwLUGnWXAnmGbILfP5Lm%2BELLX3WSp9v3q0IC0GytcDuT1O8K2TBWlLq58kEJfhOfJbACVEfhN7z20IlDPy2xM3WIymQBkiv57i%2ByZM6ANlh%2FymAr6hpshvB5QVoqW3q%2BKK%2FO5AkchvmMM38iHyk0ApkV%2Ffg294feRXugPoDiCr0n0GtiPdVbid%2BwvfB4op8svcN5F2%2Bu67cDvTV34aM0F%2B4Ss%2FDzzYcW4JSwse%2Byav%2FETa4t9ERhakBS%2F9q5wFaRH%2F6kDaNbf3d2EPXuAyvLd30UQItCdyO9i7bOf5EquzYnvTgpdeH8iflvlAUz3kZf8KVcs%2FBJ%2F2rl1cQxWFvUvhR8xpBVThDfnvAu28SR16UMkEOS3sfdQxgGri0tp%2Fk0Lac39l6T%2FKLbd2AfLVg4rW9t7rPy24BtOiFXJZRda%2BTL%2F6A1Wp0N7BBHu2tFBBZUGJPGRs7QPfMrB9cBExnIV7pM1ZQA0nrvFA9qYlUEc%2B5R9QZddYrymdxn%2Bey5O9g%2BUSqEf0rB3SJ7YMaT0BNRUMEywLa9NkDHWpdzRtYO9413cFtaUXw6NyL76VA4abj%2BL%2BMjys%2BcvaEdePJTQhxmhSKGqkhWjSWEAj0cXagfWpybRdBA0lpbktExJrN5oo36ApNUFTJqpm2gJNGShozOuhGT3P2rSzBy1EfSMbF%2FVTqC01lBZBK%2FHK2q2zisxA2iqGlhKpf%2FO2pGHaXXuafOPfGZKMLJeMO0MSaXNoTz1LvRtYPhXftqlE2lpBB9SayOQ6fgDqqTXtk07jzKSPH00dpL60tbJ9h%2Bb2%2BzODWt7tSKM34tZhlUBrSaYn7Q06Ffc1bKXfj6EDhQ1ptOhcP5OI7EXQibTXedo5gs55gxK7VE68ztImstu0gQcaqGSH%2BOjqHF8S1WXapcO03ZsCPaLxA7tRhhF0Kg1L7MZjHIE24os%2B05X%2B%2FL6ErWm7pQCd0ndJdxKN93cfNPDf763T5CwFzVTcK%2BnOXxrLXqE0pRXbtmmxAv3EaUp3%2Ftg4PQlL0x7TRIAZeXIusYnyfMo1p50apyU5mCOCcIV1rcJA2J9mivqzvpZYXXldR8pQWlQ77Y8CBnk8GFYLlcNBnJtNmwwlVlH%2Bl%2BYBG69Yn7Py98Ksty48lrQemXY2kEZRfvAMr5l84P97yOwaPgNfWZq2NpZG86JgPhlP%2B9ldlo9S3rP%2BdDyZB5FnRdqygzTHcRzHcRzHcRzHcRzHcZz%2FAbyvLkVmYcs9AAAAAElFTkSuQmCC&link=https%3A%2F%2Fwww.together.ai" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n</div>\n</div>\n\n## DeepCoder Overview\nDeepCoder-14B-Preview is a code reasoning LLM fine-tuned from DeepSeek-R1-Distilled-Qwen-14B using distributed reinforcement learning (RL) to scale up to long context lengths. The model achieves 60.6% Pass@1 accuracy on LiveCodeBench v5 (8/1/24-2/1/25), representing a 8% improvement over the base model (53%) and achieving similar performance to OpenAI''s o3-mini with just 14B parameters.\n\n<div style="margin: 0 auto;">\n  <img src="https://cdn-uploads.huggingface.co/production/uploads/654037be97949fd2304aab7f/r3-vzkItOCrMf1qldW0Mj.png" style="width: 100%;" />\n</div>\n\n## Data\nOur training dataset consists of approximately 24K unique problem-tests pairs compiled from:\n- Taco-Verified\n- PrimeIntellect SYNTHETIC-1\n- LiveCodeBench v5 (5/1/23-7/31/24)\n\n## Training Recipe\n\nOur training recipe relies on an improved version of GRPO (GRPO+) and iterative context lengthening, introduced in DeepScaleR.\n\n### GRPO+\n\nWe enhance the original GRPO algorithm with insights from DAPO to enable more stable training:\n\n- **Offline Difficulty Filtering:** DAPO employs online dynamic sampling, discarding both entirely correct and entirely incorrect samples on the fly. While this helps maintain a more stable effective batch size, it introduces significant runtime overhead due to rejection sampling. Instead, we perform offline difficulty filtering on a subset of coding problems to ensure the training dataset remains within a suitable difficulty range.\n- **No Entropy Loss:** We observed that including an entropy loss term often led to instability, with entropy growing exponentially and ultimately collapsing training. To mitigate this, we eliminate the entropy loss entirely.\n- **No KL Loss:** Eliminating KL loss prevents the LLM from staying within trust region of the original SFT model. This removal also obviates the need to compute log probabilities for the reference policy, thereby accelerating training.\n- **Overlong Filtering** **(from DAPO):** To preserve long-context reasoning, we mask the loss for truncated sequences. This technique enables DeepCoder to generalize to 64K-context inference despite being trained with a 32K context.\n- **Clip High (from DAPO):** By increasing the upper bound in GRPO/PPOâ€™s surrogate loss, we encourage more exploration and more stable entropy.\n\n### Iterative Context Lengthening\n\nOur original `Deepscaler-1.5B-Preview` scaled long context training from 8Kâ†’16Kâ†’24K, achieving 33â†’38â†’43% on AIME respectively. Similarly, `Deepcoder-14B-Preview` is trained on 16Kâ†’32K, achieving 54â†’58% on LiveCodeBench (v5). `DeepCoder-14B-Preview` successfully generalizes to longer contexts when evaluated at 64K context, reaching 60.6%. \n\nDeepCoder generalizes better to long contexts than the base distilled model, due to DAPO''s overlong filtering. However, it''s longer responses are often truncated when the max length is capped at 16K, which can lower its scores.\n\n| **Model** | **16K** | **32K** | **64K** |\n| --- | --- | --- | --- |\n| **DeepCoder-14B-Preview** | 45.6 | 57.9 | 60.6 |\n| **DeepSeek-R1-Distill-Qwen-14B** | 50.2 | 53.0 | 53.0 |\n\nA more detailed description of the training recipe can be found in our [blog post](https://pretty-radio-b75.notion.site/DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51).\n\n## Evaluation\n\nWe evaluate `Deepcoder-14B-Preview` on various coding benchmarks, including LiveCodeBench (LCBv5), Codeforces, and HumanEval+. \n\n| **Model** | LCB (v5)(8/1/24-2/1/25) | Codeforces Rating | Codeforces Percentile | HumanEval+ |\n| --- | --- | --- | --- | --- |\n| **DeepCoder-14B-Preview (ours)** | ***60.6*** | ***1936*** | ***95.3*** | ***92.6*** |\n| **DeepSeek-R1-Distill-Qwen-14B** | 53.0 | 1791 | 92.7 | 92.0 |\n| **O1-2024-12-17 (Low)** | 59.5 | **1991** | **96.1** | 90.8 |\n| **O3-Mini-2025-1-31 (Low)** | **60.9** | 1918 | 94.9 | 92.6 |\n| **O1-Preview** | 42.7 | 1658 | 88.5 | 89 |\n| **Deepseek-R1** | 62.8 | 1948 | 95.4 | 92.6 |\n| **Llama-4-Behemoth** | 49.4 | - | - | - |\n\n## Serving DeepCoder\nOur model can be served using popular high-performance inference systems:\n- vLLM\n- Hugging Face Text Generation Inference (TGI)\n- SGLang\n- TensorRT-LLM\n\nAll these systems support the OpenAI Chat Completions API format.\n\n### Usage Recommendations\nOur usage recommendations are similar to those of R1 and R1 Distill series:\n1. Avoid adding a system prompt; all instructions should be contained within the user prompt.\n2. `temperature = 0.6`\n3. `top_p = 0.95`\n4. This model performs best with `max_tokens` set to at least `64000` \n\n## License\nThis project is released under the MIT License, reflecting our commitment to open and accessible AI development.\nWe believe in democratizing AI technology by making our work freely available for anyone to use, modify, and build upon.\nThis permissive license ensures that researchers, developers, and enthusiasts worldwide can leverage and extend our work without restrictions, fostering innovation and collaboration in the AI community.\n\n## Acknowledgement\n- Our training experiments are powered by our heavily modified fork of [Verl](https://github.com/agentica-project/verl), an open-source post-training library.\n- Our model is trained on top of [`DeepSeek-R1-Distill-Qwen-14B`](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B).\n- Our work is done as part of  [Berkeley Sky Computing Lab](https://skycomputing.berkeley.edu/) and [Berkeley AI Research](https://bair.berkeley.edu/).\n\n## Citation \n```bibtex\n@misc{deepcoder2025,\n  title={DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level},\n  author={Michael Luo and Sijun Tan and Roy Huang and Ameen Patel and Alpay Ariyak and Qingyang Wu and Xiaoxiang Shi and Rachel Xin and Colin Cai and Maurice Weber and Ce Zhang and Li Erran Li and Raluca Ada Popa and Ion Stoica},\n  howpublished={\url{https://pretty-radio-b75.notion.site/DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51}},\n  note={Notion Blog},\n  year={2025}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":14770033664,"storage_bytes":59091623642,"files_count":21,"spaces_count":31,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":"<ï½œbeginâ–ofâ–sentenceï½œ>","chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''') %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{% set ns.system_prompt = message[''content''] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{{''<ï½œUserï½œ>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is none %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls'']%}{%- if not ns.is_first %}{{''<ï½œAssistantï½œ><ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>'' + tool[''type''] + ''<ï½œtoolâ–sepï½œ>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ï½œtoolâ–callâ–endï½œ>''}}{%- set ns.is_first = true -%}{%- else %}{{''\\n'' + ''<ï½œtoolâ–callâ–beginï½œ>'' + tool[''type''] + ''<ï½œtoolâ–sepï½œ>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ï½œtoolâ–callâ–endï½œ>''}}{{''<ï½œtoolâ–callsâ–endï½œ><ï½œendâ–ofâ–sentenceï½œ>''}}{%- endif %}{%- endfor %}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is not none %}{%- if ns.is_tool %}{{''<ï½œtoolâ–outputsâ–endï½œ>'' + message[''content''] + ''<ï½œendâ–ofâ–sentenceï½œ>''}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message[''content''] %}{% if ''</think>'' in content %}{% set content = content.split(''</think>'')[-1] %}{% endif %}{{''<ï½œAssistantï½œ>'' + content + ''<ï½œendâ–ofâ–sentenceï½œ>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<ï½œtoolâ–outputsâ–beginï½œ><ï½œtoolâ–outputâ–beginï½œ>'' + message[''content''] + ''<ï½œtoolâ–outputâ–endï½œ>''}}{%- set ns.is_output_first = false %}{%- else %}{{''\\n<ï½œtoolâ–outputâ–beginï½œ>'' + message[''content''] + ''<ï½œtoolâ–outputâ–endï½œ>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<ï½œtoolâ–outputsâ–endï½œ>''}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{''<ï½œAssistantï½œ><think>\\n''}}{% endif %}","eos_token":"<ï½œendâ–ofâ–sentenceï½œ>","pad_token":"<ï½œendâ–ofâ–sentenceï½œ>","unk_token":null,"use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:agentica-project:rllm\"","source_url":"https://github.com/agentica-project/rllm\""},{"type":"has_code","target_id":"github:agentica-project:verl","source_url":"https://github.com/agentica-project/verl"}]', NULL, 'MIT', 'approved', 78.3, '4b8680c75514929fab33fa1c8936f646', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen2-7B-Instruct', 'huggingface--qwen--qwen2-7b-instruct', 'Qwen2-7B-Instruct', 'Qwen', '--- license: apache-2.0 language: - en pipeline_tag: text-generation tags: - chat base_model: Qwen/Qwen2-7B --- Qwen2 is the new series of Qwen large language models. For Qwen2, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters, including a Mixture-of-Experts model. This repo contains the instruction-tuned 7B Qwen2 model. Compared with the state-of-the-art opensource language models, including the previous released Qwen...', '["transformers","safetensors","qwen2","text-generation","chat","conversational","en","arxiv:2309.00071","base_model:qwen/qwen2-7b","base_model:finetune:qwen/qwen2-7b","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 678, 166217, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen2-7B-Instruct","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\npipeline_tag: text-generation\ntags:\n- chat\nbase_model: Qwen/Qwen2-7B\n---\n\n# Qwen2-7B-Instruct\n\n## Introduction\n\nQwen2 is the new series of Qwen large language models. For Qwen2, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters, including a Mixture-of-Experts model. This repo contains the instruction-tuned 7B Qwen2 model.\n\nCompared with the state-of-the-art opensource language models, including the previous released Qwen1.5, Qwen2 has generally surpassed most opensource models and demonstrated competitiveness against proprietary models across a series of benchmarks targeting for language understanding, language generation, multilingual capability, coding, mathematics, reasoning, etc.\n\nQwen2-7B-Instruct supports a context length of up to 131,072 tokens, enabling the processing of extensive inputs. Please refer to [this section](#processing-long-texts) for detailed instructions on how to deploy Qwen2 for handling long texts.\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2/), [GitHub](https://github.com/QwenLM/Qwen2), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n<br>\n\n## Model Details\nQwen2 is a language model series including decoder language models of different model sizes. For each size, we release the base language model and the aligned chat model. It is based on the Transformer architecture with SwiGLU activation, attention QKV bias, group query attention, etc. Additionally, we have an improved tokenizer adaptive to multiple natural languages and codes.\n\n## Training details\nWe pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization.\n\n\n## Requirements\nThe code of Qwen2 has been in the latest Hugging face transformers and we advise you to install `transformers>=4.37.0`, or you might encounter the following error:\n```\nKeyError: ''qwen2''\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ndevice = "cuda" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    "Qwen/Qwen2-7B-Instruct",\n    torch_dtype="auto",\n    device_map="auto"\n)\ntokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-7B-Instruct")\n\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "system", "content": "You are a helpful assistant."},\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### Processing Long Texts\n\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YARN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor deployment, we recommend using vLLM. You can enable the long-context capabilities by following these steps:\n\n1. **Install vLLM**: You can install vLLM by running the following command.\n\n```bash\npip install "vllm>=0.4.3"\n```\n\nOr you can install vLLM from [source](https://github.com/vllm-project/vllm/).\n\n2. **Configure Model Settings**: After downloading the model weights, modify the `config.json` file by including the below snippet:\n    ```json\n        {\n            "architectures": [\n                "Qwen2ForCausalLM"\n            ],\n            // ...\n            "vocab_size": 152064,\n\n            // adding the following snippets\n            "rope_scaling": {\n                "factor": 4.0,\n                "original_max_position_embeddings": 32768,\n                "type": "yarn"\n            }\n        }\n    ```\n    This snippet enable YARN to support longer contexts.\n\n3. **Model Deployment**: Utilize vLLM to deploy your model. For instance, you can set up an openAI-like server using the command:\n\n    ```bash\n    python -m vllm.entrypoints.openai.api_server --served-model-name Qwen2-7B-Instruct --model path/to/weights\n    ```\n\n    Then you can access the Chat API by:\n\n    ```bash\n    curl http://localhost:8000/v1/chat/completions \\n        -H "Content-Type: application/json" \\n        -d ''{\n        "model": "Qwen2-7B-Instruct",\n        "messages": [\n            {"role": "system", "content": "You are a helpful assistant."},\n            {"role": "user", "content": "Your Long Input Here."}\n        ]\n        }''\n    ```\n\n    For further usage instructions of vLLM, please refer to our [Github](https://github.com/QwenLM/Qwen2).\n\n**Note**: Presently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. We advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation\n\nWe briefly compare Qwen2-7B-Instruct with similar-sized instruction-tuned LLMs, including Qwen1.5-7B-Chat. The results are shown below:\n\n| Datasets | Llama-3-8B-Instruct | Yi-1.5-9B-Chat | GLM-4-9B-Chat | Qwen1.5-7B-Chat | Qwen2-7B-Instruct |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| _**English**_ |  |  |  |  |  |\n| MMLU | 68.4 | 69.5 | **72.4** | 59.5 | 70.5 |\n| MMLU-Pro | 41.0 | - | - | 29.1 | **44.1** |\n| GPQA | **34.2** | - | **-** | 27.8 | 25.3 |\n| TheroemQA | 23.0 | - | - | 14.1 | **25.3** |\n| MT-Bench | 8.05 | 8.20 | 8.35 | 7.60 | **8.41** |\n| _**Coding**_ |  |  |  |  |  |\n| Humaneval | 62.2 | 66.5 | 71.8 | 46.3 | **79.9** |\n| MBPP | **67.9** | - | - | 48.9 | 67.2 |\n| MultiPL-E | 48.5 | - | - | 27.2 | **59.1** |\n| Evalplus | 60.9 | - | - | 44.8 | **70.3** |\n| LiveCodeBench | 17.3 | - | - | 6.0 | **26.6** |\n| _**Mathematics**_ |  |  |  |  |  |\n| GSM8K | 79.6 | **84.8** | 79.6 | 60.3 | 82.3 |\n| MATH | 30.0 | 47.7 | **50.6** | 23.2 | 49.6 |\n| _**Chinese**_ |  |  |  |  |  |\n| C-Eval | 45.9 | - | 75.6 | 67.3 | **77.2** |\n| AlignBench | 6.20 | 6.90 | 7.01 | 6.20 | **7.21** |\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@article{qwen2,\n  title={Qwen2 Technical Report},\n  year={2024}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7615616512,"storage_bytes":18993584208,"files_count":14,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":null,"chat_template":"{% for message in messages %}{% if loop.first and messages[0][''role''] != ''system'' %}{{ ''<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n'' }}{% endif %}{{''<|im_start|>'' + message[''role''] + ''\n'' + message[''content''] + ''<|im_end|>'' + ''\n''}}{% endfor %}{% if add_generation_prompt %}{{ ''<|im_start|>assistant\n'' }}{% endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen2","source_url":"https://github.com/QwenLM/Qwen2"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:QwenLM:Qwen2","source_url":"https://github.com/QwenLM/Qwen2"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"}]', NULL, 'Apache-2.0', 'approved', 63.3, '657cdecc46805d5404e698e1c253b9a2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-2vXpSwA7-iroiro-lora', 'huggingface--2vxpswa7--iroiro-lora', 'iroiro-lora', '2vXpSwA7', '--- license: creativeml-openrail-m ---', '["license:creativeml-openrail-m","region:us"]', 'other', 677, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/2vXpSwA7/iroiro-lora","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\n---', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":96217872778,"files_count":376,"spaces_count":15,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'creativeml-openrail-m', 'approved', 38.3, '951176fe35055c087fa7c74a2863d93e', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-ostris-OpenFLUX.1', 'huggingface--ostris--openflux.1', 'OpenFLUX.1', 'ostris', '--- license: apache-2.0 library_name: diffusers pipeline_tag: text-to-image --- <img src="https://huggingface.co/ostris/OpenFLUX.1/resolve/main/assets/banner_0_1_0-2.png" style="max-width: 100%;"> <div style="color: #f0b800;"> After numerous iterations and spending way too much of my own money on compute to train this, I think it is finally at the point I am happy to consider it a beta. I am still going to continue to train it, but the distillation has been mostly trained out of it at this po...', '["diffusers","safetensors","text-to-image","license:apache-2.0","endpoints_compatible","diffusers:fluxpipeline","region:us"]', 'text-to-image', 677, 1575, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/ostris/OpenFLUX.1","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlibrary_name: diffusers\npipeline_tag: text-to-image\n---\n\n<img src="https://huggingface.co/ostris/OpenFLUX.1/resolve/main/assets/banner_0_1_0-2.png" style="max-width: 100%;">\n\n<div style="color: #f0b800;">\n  \n# <span style="color: #f0b800;"> Beta Version v0.1.0 </span>\n  \nAfter numerous iterations and spending way too much of my own money on compute to train this, I think it is finally at the point I am happy to consider it a beta. I am still going to continue to train it, but the distillation has been mostly trained out of it at this point. So phase 1 is complete. Feel free to use it and fine tune it, but be aware that I will likely continue to update it.\n\n</div>\n\n<img src="https://huggingface.co/ostris/OpenFLUX.1/resolve/main/assets/banner_0_1_0-3.png" style="max-width: 100%;">\n\n## What is this?\n\nThis is a fine tune of the [FLUX.1-schnell](https://huggingface.co/black-forest-labs/FLUX.1-schnell) model that has had the distillation trained out of it. Flux Schnell is licensed Apache 2.0, but it is a distilled model, meaning you cannot fine-tune it. However, it is an amazing model that can generate amazing images in 1-4 steps. This is an attempt to remove the distillation to create an open source, permissivle licensed model that can be fine tuned. \n\n<img src="https://huggingface.co/ostris/OpenFLUX.1/resolve/main/assets/banner_0_1_0-1.png" style="max-width: 100%;">\n\n\n## How to Use\n\nSince the distillation has been fine tuned out of the model, it uses classic CFG. Since it requires CFG, it will require a different pipeline than the original FLUX.1 schnell and dev models. This pipeline can be found in open_flux_pipeline.py in this repo. I will be adding example code in the next few days, but for now, a cfg of 3.5 seems to work well. \n\n<img src="https://huggingface.co/ostris/OpenFLUX.1/resolve/main/assets/banner_0_1_0-0.png" style="max-width: 100%;">\n\n<img src="https://huggingface.co/ostris/OpenFLUX.1/resolve/main/assets/banner_0_1_0-4.png" style="max-width: 100%;">', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":265699620448,"files_count":38,"spaces_count":31,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"FluxPipeline"}}}', '[]', '[]', NULL, 'Apache-2.0', 'approved', 83.3, '03ac4f0c3e792db9dd04cb72d4f574d0', NULL, 'https://huggingface.co/ostris/OpenFLUX.1/resolve/main/assets/banner_0_0_6.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-ostris-OpenFLUX.1 from https://huggingface.co/ostris/OpenFLUX.1/resolve/main/assets/banner_0_0_6.png
Image converted to WebP: data/images/huggingface-ostris-OpenFLUX.1.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-dslim-bert-base-NER', 'huggingface--dslim--bert-base-ner', 'bert-base-NER', 'dslim', '--- language: en datasets: - conll2003 license: mit model-index: - name: dslim/bert-base-NER results: - task: type: token-classification name: Token Classification dataset: name: conll2003 type: conll2003 config: conll2003 split: test metrics: - name: Accuracy type: accuracy value: 0.9118041001560013 verified: true - name: Precision type: precision value: 0.9211550382257732 verified: true - name: Recall type: recall value: 0.9306415698281261 verified: true - name: F1 type: f1 value: 0.9258740...', '["transformers","pytorch","tf","jax","onnx","safetensors","bert","token-classification","en","dataset:conll2003","arxiv:1810.04805","license:mit","model-index","endpoints_compatible","deploy:azure","region:us"]', 'token-classification', 676, 1913030, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/dslim/bert-base-NER","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: en\ndatasets:\n- conll2003\nlicense: mit\nmodel-index:\n- name: dslim/bert-base-NER\n  results:\n  - task:\n      type: token-classification\n      name: Token Classification\n    dataset:\n      name: conll2003\n      type: conll2003\n      config: conll2003\n      split: test\n    metrics:\n    - name: Accuracy\n      type: accuracy\n      value: 0.9118041001560013\n      verified: true\n    - name: Precision\n      type: precision\n      value: 0.9211550382257732\n      verified: true\n    - name: Recall\n      type: recall\n      value: 0.9306415698281261\n      verified: true\n    - name: F1\n      type: f1\n      value: 0.9258740048459675\n      verified: true\n    - name: loss\n      type: loss\n      value: 0.48325642943382263\n      verified: true\n---\n# bert-base-NER\n\nIf my open source models have been useful to you, please consider supporting me in building small, useful AI models for everyone (and help me afford med school / help out my parents financially). Thanks!\n\n<a href="https://www.buymeacoffee.com/dslim" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/arial-yellow.png" alt="Buy Me A Coffee" style="height: 60px !important;width: 217px !important;" ></a>\n\n## Model description\n\n**bert-base-NER** is a fine-tuned BERT model that is ready to use for **Named Entity Recognition** and achieves **state-of-the-art performance** for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). \n\nSpecifically, this model is a *bert-base-cased* model that was fine-tuned on the English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. \n\nIf you''d like to use a larger BERT-large model fine-tuned on the same dataset, a [**bert-large-NER**](https://huggingface.co/dslim/bert-large-NER/) version is also available. \n\n### Available NER models \n| Model Name | Description | Parameters |\n|-------------------|-------------|------------------|\n| [distilbert-NER](https://huggingface.co/dslim/distilbert-NER) **(NEW!)** | Fine-tuned DistilBERT - a smaller, faster, lighter version of BERT | 66M |\n| [bert-large-NER](https://huggingface.co/dslim/bert-large-NER/) | Fine-tuned bert-large-cased - larger model with slightly better performance | 340M |\n| [bert-base-NER](https://huggingface.co/dslim/bert-base-NER)-([uncased](https://huggingface.co/dslim/bert-base-NER-uncased)) | Fine-tuned bert-base, available in both cased and uncased versions | 110M |\n\n\n## Intended uses & limitations\n\n#### How to use\n\nYou can use this model with Transformers *pipeline* for NER.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER")\nmodel = AutoModelForTokenClassification.from_pretrained("dslim/bert-base-NER")\n\nnlp = pipeline("ner", model=model, tokenizer=tokenizer)\nexample = "My name is Wolfgang and I live in Berlin"\n\nner_results = nlp(example)\nprint(ner_results)\n```\n\n#### Limitations and bias\n\nThis model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases. \n\n## Training data\n\nThis model was fine-tuned on English version of the standard [CoNLL-2003 Named Entity Recognition](https://www.aclweb.org/anthology/W03-0419.pdf) dataset. \n\nThe training dataset distinguishes between the beginning and continuation of an entity so that if there are back-to-back entities of the same type, the model can output where the second entity begins. As in the dataset, each token will be classified as one of the following classes:\n\nAbbreviation|Description\n-|-\nO|Outside of a named entity\nB-MISC |Beginning of a miscellaneous entity right after another miscellaneous entity\nI-MISC | Miscellaneous entity\nB-PER |Beginning of a personâ€™s name right after another personâ€™s name\nI-PER |Personâ€™s name\nB-ORG |Beginning of an organization right after another organization\nI-ORG |organization\nB-LOC |Beginning of a location right after another location\nI-LOC |Location\n\n\n### CoNLL-2003 English Dataset Statistics\nThis dataset was derived from the Reuters corpus which consists of Reuters news stories. You can read more about how this dataset was created in the CoNLL-2003 paper. \n#### # of training examples per entity type\nDataset|LOC|MISC|ORG|PER\n-|-|-|-|-\nTrain|7140|3438|6321|6600\nDev|1837|922|1341|1842\nTest|1668|702|1661|1617\n#### # of articles/sentences/tokens per dataset\nDataset |Articles |Sentences |Tokens\n-|-|-|-\nTrain |946 |14,987 |203,621\nDev |216 |3,466 |51,362\nTest |231 |3,684 |46,435\n\n## Training procedure\n\nThis model was trained on a single NVIDIA V100 GPU with recommended hyperparameters from the [original BERT paper](https://arxiv.org/pdf/1810.04805) which trained & evaluated the model on CoNLL-2003 NER task. \n\n## Eval results\nmetric|dev|test\n-|-|-\nf1 |95.1 |91.3\nprecision |95.0 |90.7\nrecall |95.3 |91.9\n\nThe test metrics are a little lower than the official Google BERT results which encoded document context & experimented with CRF. More on replicating the original results [here](https://github.com/google-research/bert/issues/223).\n\n### BibTeX entry and citation info\n\n```\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n```\n@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,\n    title = "Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition",\n    author = "Tjong Kim Sang, Erik F.  and\n      De Meulder, Fien",\n    booktitle = "Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003",\n    year = "2003",\n    url = "https://www.aclweb.org/anthology/W03-0419",\n    pages = "142--147",\n}\n```\n', '{"pipeline_tag":"token-classification","library_name":"transformers","framework":"transformers","params":108317193,"storage_bytes":2162448417,"files_count":18,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BertForTokenClassification"],"model_type":"bert","tokenizer_config":{}}}', '[]', '[{"type":"has_code","target_id":"github:google-research:bert","source_url":"https://github.com/google-research/bert"},{"type":"based_on_paper","target_id":"arxiv:1810.04805","source_url":"https://arxiv.org/abs/1810.04805"}]', NULL, 'MIT', 'approved', 63.3, '0130c6e6efd0d9930be5fd4b6c1aa148', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-Coder-V2-Instruct', 'huggingface--deepseek-ai--deepseek-coder-v2-instruct', 'DeepSeek-Coder-V2-Instruct', 'deepseek-ai', '--- license: other license_name: deepseek-license license_link: LICENSE base_model: deepseek-ai/DeepSeek-Coder-V2-Base --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V2" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="...', '["transformers","safetensors","deepseek_v2","text-generation","conversational","custom_code","arxiv:2401.06066","base_model:deepseek-ai/deepseek-coder-v2-base","license:other","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 673, 92884, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: deepseek-license\nlicense_link: LICENSE\nbase_model: deepseek-ai/DeepSeek-Coder-V2-Base\n---\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V2" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ðŸ¤–%20Chat-DeepSeek%20V2-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/LICENSE-CODE" style="margin: 2px;">\n    <img alt="Code License" src="https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/LICENSE-MODEL" style="margin: 2px;">\n    <img alt="Model License" src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n<p align="center">\n  <a href="#4-api-platform">API Platform</a> |\n  <a href="#5-how-to-run-locally">How to Use</a> |\n  <a href="#6-license">License</a> |\n</p>\n\n\n<p align="center">\n  <a href="https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/paper.pdf"><b>Paper Link</b>ðŸ‘ï¸</a>\n</p>\n\n# DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence\n\n## 1. Introduction\nWe present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K. \n\n<p align="center">\n  <img width="100%" src="https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/figures/performance.png?raw=true">\n</p>\n\n\nIn standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.  The list of supported programming languages can be found [here](https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/supported_langs.txt).\n\n## 2. Model Downloads\n\nWe release the DeepSeek-Coder-V2 with 16B and 236B parameters based on the [DeepSeekMoE](https://arxiv.org/pdf/2401.06066) framework, which has actived parameters of only 2.4B and 21B , including base and instruct models, to the public. \n\n<div align="center">\n\n|            **Model**            | **#Total Params** | **#Active Params** | **Context Length** |                         **Download**                         |\n| :-----------------------------: | :---------------: | :----------------: | :----------------: | :----------------------------------------------------------: |\n|   DeepSeek-Coder-V2-Lite-Base   |        16B        |        2.4B        |        128k        | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Base) |\n| DeepSeek-Coder-V2-Lite-Instruct |        16B        |        2.4B        |        128k        | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct) |\n|     DeepSeek-Coder-V2-Base      |       236B        |        21B         |        128k        | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Base) |\n|   DeepSeek-Coder-V2-Instruct    |       236B        |        21B         |        128k        | [ðŸ¤— HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct) |\n\n</div>\n\n\n## 3. Chat Website\n\nYou can chat with the DeepSeek-Coder-V2 on DeepSeek''s official website: [coder.deepseek.com](https://coder.deepseek.com/sign_in)\n\n## 4. API Platform\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/), and you can also pay-as-you-go at an unbeatable price.\n<p align="center">\n  <img width="40%" src="https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/figures/model_price.jpg?raw=true">\n</p>\n\n\n## 5. How to run locally\n**Here, we provide some examples of how to use DeepSeek-Coder-V2-Lite model. If you want to utilize DeepSeek-Coder-V2 in BF16 format for inference, 80GB*8 GPUs are required.**\n\n### Inference with Huggingface''s Transformers\nYou can directly employ [Huggingface''s Transformers](https://github.com/huggingface/transformers) for model inference.\n\n#### Code Completion\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-Coder-V2-Lite-Base", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained("deepseek-ai/DeepSeek-Coder-V2-Lite-Base", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\ninput_text = "#write a quick sort algorithm"\ninputs = tokenizer(input_text, return_tensors="pt").to(model.device)\noutputs = model.generate(**inputs, max_length=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n#### Code Insertion\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-Coder-V2-Lite-Base", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained("deepseek-ai/DeepSeek-Coder-V2-Lite-Base", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\ninput_text = """<ï½œfimâ–beginï½œ>def quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[0]\n    left = []\n    right = []\n<ï½œfimâ–holeï½œ>\n        if arr[i] < pivot:\n            left.append(arr[i])\n        else:\n            right.append(arr[i])\n    return quick_sort(left) + [pivot] + quick_sort(right)<ï½œfimâ–endï½œ>"""\ninputs = tokenizer(input_text, return_tensors="pt").to(model.device)\noutputs = model.generate(**inputs, max_length=128)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True)[len(input_text):])\n```\n\n#### Chat Completion\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\ntokenizer = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained("deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\nmessages=[\n    { ''role'': ''user'', ''content'': "write a quick sort algorithm in python."}\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(model.device)\n# tokenizer.eos_token_id is the id of <ï½œendâ–ofâ–sentenceï½œ>  token\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n```\n\n\n\nThe complete chat template can be found within `tokenizer_config.json` located in the huggingface model repository.\n\nAn example of chat template is as belows:\n\n```bash\n<ï½œbeginâ–ofâ–sentenceï½œ>User: {user_message_1}\n\nAssistant: {assistant_message_1}<ï½œendâ–ofâ–sentenceï½œ>User: {user_message_2}\n\nAssistant:\n```\n\nYou can also add an optional system message:\n\n```bash\n<ï½œbeginâ–ofâ–sentenceï½œ>{system_message}\n\nUser: {user_message_1}\n\nAssistant: {assistant_message_1}<ï½œendâ–ofâ–sentenceï½œ>User: {user_message_2}\n\nAssistant:\n```\n\n### Inference with vLLM (recommended)\nTo utilize [vLLM](https://github.com/vllm-project/vllm) for model inference, please merge this Pull Request into your vLLM codebase: https://github.com/vllm-project/vllm/pull/4650.\n\n```python\nfrom transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n\nmax_model_len, tp_size = 8192, 1\nmodel_name = "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True, enforce_eager=True)\nsampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])\n\nmessages_list = [\n    [{"role": "user", "content": "Who are you?"}],\n    [{"role": "user", "content": "write a quick sort algorithm in python."}],\n    [{"role": "user", "content": "Write a piece of quicksort code in C++."}],\n]\n\nprompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\n\noutputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n\ngenerated_text = [output.outputs[0].text for output in outputs]\nprint(generated_text)\n```\n\n\n\n## 6. License\n\nThis code repository is licensed under [the MIT License](https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/LICENSE-CODE). The use of DeepSeek-Coder-V2 Base/Instruct models is subject to [the Model License](https://github.com/deepseek-ai/DeepSeek-Coder-V2/blob/main/LICENSE-MODEL). DeepSeek-Coder-V2 series (including Base and Instruct) supports commercial use.\n\n\n## 7. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":235741434880,"storage_bytes":471486512925,"files_count":65,"spaces_count":62,"gated":false,"private":false,"config":{"architectures":["DeepseekV2ForCausalLM"],"auto_map":{"AutoConfig":"configuration_deepseek.DeepseekV2Config","AutoModel":"modeling_deepseek.DeepseekV2Model","AutoModelForCausalLM":"modeling_deepseek.DeepseekV2ForCausalLM"},"model_type":"deepseek_v2","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ï½œbeginâ–ofâ–sentenceï½œ>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ï½œendâ–ofâ–sentenceï½œ>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ï½œendâ–ofâ–sentenceï½œ>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ bos_token }}{% for message in messages %}{% if message[''role''] == ''user'' %}{{ ''User: '' + message[''content''] + ''\n\n'' }}{% elif message[''role''] == ''assistant'' %}{{ ''Assistant: '' + message[''content''] + eos_token }}{% elif message[''role''] == ''system'' %}{{ message[''content''] + ''\n\n'' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ ''Assistant:'' }}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-Coder-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-Coder-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-Coder-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-Coder-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-Coder-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-Coder-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-Coder-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-Coder-V2"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-Coder-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-Coder-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-Coder-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-Coder-V2"},{"type":"based_on_paper","target_id":"arxiv:2401.06066","source_url":"https://arxiv.org/abs/2401.06066"}]', NULL, 'Other', 'approved', 78.3, 'b853f9ea3e7e741c988e1a89c46cc7ec', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-yisol-IDM-VTON', 'huggingface--yisol--idm-vton', 'IDM-VTON', 'yisol', '--- base_model: stable-diffusion-xl-1.0-inpainting-0.1 tags: - stable-diffusion-xl - inpainting - virtual try-on license: cc-by-nc-sa-4.0 --- This is an official implementation of paper ''Improving Diffusion Models for Authentic Virtual Try-on in the Wild'' - paper - project page ðŸ¤— Try our huggingface Demo !teaser&nbsp; !teaser2&nbsp; - [x] demo model - [x] inference code - [ ] training code For the demo, GPUs are supported from zerogpu, and auto masking generation codes are based on OOTDiffus...', '["diffusers","onnx","safetensors","stable-diffusion-xl","inpainting","virtual try-on","arxiv:2403.05139","license:cc-by-nc-sa-4.0","diffusers:stablediffusionxlinpaintpipeline","region:us"]', 'image-to-image', 672, 13213, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/yisol/IDM-VTON","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nbase_model: stable-diffusion-xl-1.0-inpainting-0.1\ntags:\n  - stable-diffusion-xl\n  - inpainting\n  - virtual try-on\nlicense: cc-by-nc-sa-4.0\n---\n\n\n\n# Check out more codes on our [github repository](https://github.com/yisol/IDM-VTON)!\n\n# IDM-VTON : Improving Diffusion Models for Authentic Virtual Try-on in the Wild\nThis is an official implementation of paper ''Improving Diffusion Models for Authentic Virtual Try-on in the Wild''\n- [paper](https://arxiv.org/abs/2403.05139) \n- [project page](https://idm-vton.github.io/) \n\nðŸ¤— Try our huggingface [Demo](https://huggingface.co/spaces/yisol/IDM-VTON)\n\n![teaser](assets/teaser.png)&nbsp;\n![teaser2](assets/teaser2.png)&nbsp;\n\n\n## TODO LIST\n\n\n- [x] demo model\n- [x] inference code\n- [ ] training code\n\n\n\n\n## Acknowledgements\n\nFor the demo, GPUs are supported from [zerogpu](https://huggingface.co/zero-gpu-explorers), and auto masking generation codes are based on [OOTDiffusion](https://github.com/levihsu/OOTDiffusion) and [DCI-VTON](https://github.com/bcmi/DCI-VTON-Virtual-Try-On).  \nParts of the code are based on [IP-Adapter](https://github.com/tencent-ailab/IP-Adapter).\n\n\n\n## Citation\n```\n@article{choi2024improving,\n  title={Improving Diffusion Models for Virtual Try-on},\n  author={Choi, Yisol and Kwak, Sangkyung and Lee, Kyungmin and Choi, Hyungwon and Shin, Jinwoo},\n  journal={arXiv preprint arXiv:2403.05139},\n  year={2024}\n}\n```\n\n## License\nThe codes and checkpoints in this repository are under the [CC BY-NC-SA 4.0 license](https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\n\n\n', '{"pipeline_tag":"image-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":41354273599,"files_count":30,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionXLInpaintPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:yisol:IDM-VTON","source_url":"https://github.com/yisol/IDM-VTON"},{"type":"has_code","target_id":"github:levihsu:OOTDiffusion","source_url":"https://github.com/levihsu/OOTDiffusion"},{"type":"has_code","target_id":"github:bcmi:DCI-VTON-Virtual-Try-On","source_url":"https://github.com/bcmi/DCI-VTON-Virtual-Try-On"},{"type":"has_code","target_id":"github:tencent-ailab:IP-Adapter","source_url":"https://github.com/tencent-ailab/IP-Adapter"},{"type":"based_on_paper","target_id":"arxiv:2403.05139","source_url":"https://arxiv.org/abs/2403.05139"}]', NULL, 'CC-BY-NC-SA-4.0', 'approved', 68.3, '5fdeef8df11dd7aab5bf558830c498cd', NULL, 'https://huggingface.co/yisol/IDM-VTON/resolve/main/assets/teaser.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-yisol-IDM-VTON from https://huggingface.co/yisol/IDM-VTON/resolve/main/assets/teaser.png
Image converted to WebP: data/images/huggingface-yisol-IDM-VTON.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mistral-community-Mixtral-8x22B-v0.1', 'huggingface--mistral-community--mixtral-8x22b-v0.1', 'Mixtral-8x22B-v0.1', 'mistral-community', '--- language: - fr - it - de - es - en license: apache-2.0 tags: - moe model-index: - name: Mixtral-8x22B-v0.1 results: - task: type: text-generation name: Text Generation dataset: name: AI2 Reasoning Challenge (25-Shot) type: ai2_arc config: ARC-Challenge split: test args: num_few_shot: 25 metrics: - type: acc_norm value: 70.48 name: normalized accuracy source: url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=mistral-community/Mixtral-8x22B-v0.1 name: Open LLM Lead...', '["transformers","safetensors","mixtral","text-generation","moe","fr","it","de","es","en","license:apache-2.0","model-index","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 672, 386, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- fr\n- it\n- de\n- es\n- en\nlicense: apache-2.0\ntags:\n- moe\nmodel-index:\n- name: Mixtral-8x22B-v0.1\n  results:\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: AI2 Reasoning Challenge (25-Shot)\n      type: ai2_arc\n      config: ARC-Challenge\n      split: test\n      args:\n        num_few_shot: 25\n    metrics:\n    - type: acc_norm\n      value: 70.48\n      name: normalized accuracy\n    source:\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=mistral-community/Mixtral-8x22B-v0.1\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: HellaSwag (10-Shot)\n      type: hellaswag\n      split: validation\n      args:\n        num_few_shot: 10\n    metrics:\n    - type: acc_norm\n      value: 88.73\n      name: normalized accuracy\n    source:\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=mistral-community/Mixtral-8x22B-v0.1\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: MMLU (5-Shot)\n      type: cais/mmlu\n      config: all\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 77.81\n      name: accuracy\n    source:\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=mistral-community/Mixtral-8x22B-v0.1\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: TruthfulQA (0-shot)\n      type: truthful_qa\n      config: multiple_choice\n      split: validation\n      args:\n        num_few_shot: 0\n    metrics:\n    - type: mc2\n      value: 51.08\n    source:\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=mistral-community/Mixtral-8x22B-v0.1\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: Winogrande (5-shot)\n      type: winogrande\n      config: winogrande_xl\n      split: validation\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 84.53\n      name: accuracy\n    source:\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=mistral-community/Mixtral-8x22B-v0.1\n      name: Open LLM Leaderboard\n  - task:\n      type: text-generation\n      name: Text Generation\n    dataset:\n      name: GSM8k (5-shot)\n      type: gsm8k\n      config: main\n      split: test\n      args:\n        num_few_shot: 5\n    metrics:\n    - type: acc\n      value: 74.15\n      name: accuracy\n    source:\n      url: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard?query=mistral-community/Mixtral-8x22B-v0.1\n      name: Open LLM Leaderboard\n---\n# Mixtral-8x22B\n\n> [!WARNING]\n> This model checkpoint is provided as-is and might not be up-to-date. Please use the corresponding version from https://huggingface.co/mistralai org\n\n> [!TIP]\n> MistralAI has uploaded weights to their organization at [mistralai/Mixtral-8x22B-v0.1](https://huggingface.co/mistralai/Mixtral-8x22B-v0.1) and [mistralai/Mixtral-8x22B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1) too. \n\n> [!TIP]\n> Kudos to [@v2ray](https://huggingface.co/v2ray) for converting the checkpoints and uploading them in `transformers` compatible format. Go give them a follow!\n\nConverted to HuggingFace Transformers format using the script [here](https://huggingface.co/v2ray/Mixtral-8x22B-v0.1/blob/main/convert.py).\n\nThe Mixtral-8x22B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.\n## Run the model\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = "mistral-community/Mixtral-8x22B-v0.1"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\ntext = "Hello my name is"\ninputs = tokenizer(text, return_tensors="pt")\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\nBy default, transformers will load the model in full precision. Therefore you might be interested to further reduce down the memory requirements to run the model through the optimizations we offer in HF ecosystem:\n### In half-precision\nNote `float16` precision only works on GPU devices\n<details>\n<summary> Click to expand </summary>\n\n```diff\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = "mistral-community/Mixtral-8x22B-v0.1"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(0)\n\ntext = "Hello my name is"\n+ inputs = tokenizer(text, return_tensors="pt").to(0)\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n</details>\n\n### Lower precision using (8-bit & 4-bit) using `bitsandbytes`\n<details>\n<summary> Click to expand </summary>\n\n```diff\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = "mistral-community/Mixtral-8x22B-v0.1"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)\n\ntext = "Hello my name is"\n+ inputs = tokenizer(text, return_tensors="pt").to(0)\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n</details>\n\n### Load the model with Flash Attention 2\n<details>\n<summary> Click to expand </summary>\n\n```diff\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = "mistral-community/Mixtral-8x22B-v0.1"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=True)\n\ntext = "Hello my name is"\n+ inputs = tokenizer(text, return_tensors="pt").to(0)\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n</details>\n\n## Notice\nMixtral-8x22B-v0.1 is a pretrained base model and therefore does not have any moderation mechanisms.\n# The Mistral AI Team\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault,Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, TimothÃ©e Lacroix, ThÃ©ophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall.\n# [Open LLM Leaderboard Evaluation Results](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\nDetailed results can be found [here](https://huggingface.co/datasets/open-llm-leaderboard/details_mistral-community__Mixtral-8x22B-v0.1)\n\n|             Metric              |Value|\n|---------------------------------|----:|\n|Avg.                             |74.46|\n|AI2 Reasoning Challenge (25-Shot)|70.48|\n|HellaSwag (10-Shot)              |88.73|\n|MMLU (5-Shot)                    |77.81|\n|TruthfulQA (0-shot)              |51.08|\n|Winogrande (5-shot)              |84.53|\n|GSM8k (5-shot)                   |74.15|\n\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":140620634112,"storage_bytes":281241986787,"files_count":70,"spaces_count":18,"gated":false,"private":false,"config":{"architectures":["MixtralForCausalLM"],"model_type":"mixtral","tokenizer_config":{"bos_token":"<s>","eos_token":"</s>","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[]', NULL, 'Apache-2.0', 'approved', 63.3, '6a6690a44a48cda215810a3bf64c0936', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mistralai-Pixtral-12B-2409', 'huggingface--mistralai--pixtral-12b-2409', 'Pixtral-12B-2409', 'mistralai', '--- library_name: vllm language: - en - fr - de - es - it - pt - ru - zh - ja license: apache-2.0 base_model: - mistralai/Pixtral-12B-Base-2409 inference: false extra_gated_description: >- If you want to learn more about how we process your personal data, please read our <a href="https://mistral.ai/terms/">Privacy Policy</a>. tags: - mistral-common --- The Pixtral-12B-2409 is a Multimodal Model of 12B parameters plus a 400M parameter vision encoder. For more details about this model please re...', '["vllm","mistral-common","en","fr","de","es","it","pt","ru","zh","ja","base_model:mistralai/pixtral-12b-base-2409","license:apache-2.0","region:us"]', 'other', 670, 6117, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mistralai/Pixtral-12B-2409","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: vllm\nlanguage:\n- en\n- fr\n- de\n- es\n- it\n- pt\n- ru\n- zh\n- ja\nlicense: apache-2.0\nbase_model:\n- mistralai/Pixtral-12B-Base-2409\ninference: false\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href="https://mistral.ai/terms/">Privacy Policy</a>.\ntags:\n- mistral-common\n---\n\n# Model Card for Pixtral-12B-2409\n\nThe Pixtral-12B-2409 is a Multimodal Model of 12B parameters plus a 400M parameter vision encoder.\n\nFor more details about this model please refer to our release [blog post](https://mistral.ai/news/pixtral-12b/).\n\nFeel free to try it [here](https://chat.mistral.ai/chat)\n\n## Key features\n- Natively multimodal, trained with interleaved image and text data\n- 12B parameter Multimodal Decoder + 400M parameter Vision Encoder\n- Supports variable image sizes\n- Leading performance in its weight class on multimodal tasks\n- Maintains state-of-the-art performance on text-only benchmarks\n- Sequence length: 128k\n- License: Apache 2.0\n\n## Benchmarks\nThe performance of Pixtral-12B-2409 compared to multimodal models.  \nAll models were re-evaluated and benchmarked through the same evaluation pipeline.\n\n### Multimodal Benchmarks\n\n|                   | Pixtral 12B | Qwen2 7B VL | LLaVA-OV 7B | Phi-3 Vision | Phi-3.5 Vision |\n|:-------------------:|:-------------:|:----------:|:-------------:|:--------------:|:--------------:|\n| **MMMU** *(CoT)*      | <ins>**52.5**</ins>        | 47.6     | 45.1        | 40.3         | 38.3         |\n| **Mathvista** *(CoT)*   | <ins>**58.0**</ins>        | 54.4     | 36.1        | 36.4         | 39.3         |\n| **ChartQA** *(CoT)*    | <ins>**81.8**</ins>        | 38.6     | 67.1        | 72.0         | 67.7         |\n| **DocVQA** *(ANLS)*        | 90.7        | <ins>**94.5**</ins>     | 90.5        | 84.9         | 74.4         |\n| **VQAv2** *(VQA Match)*         | <ins>**78.6**</ins>        | 75.9     | 78.3        | 42.4         | 56.1         |\n\n### Instruction Following\n\n|                   | Pixtral 12B | Qwen2 7B VL | LLaVA-OV 7B | Phi-3 Vision | Phi-3.5 Vision |\n|:-------------------:|:-------------:|:----------:|:-------------:|:--------------:|:--------------:|\n| **MM MT-Bench**   | <ins>**6.05**</ins>        | 5.43     | 4.12        | 3.70         |4.46         |\n| **Text MT-Bench** | <ins>**7.68**</ins>        | 6.41     | 6.94        | 6.27         |6.31         |\n| **MM IF-Eval**    | <ins>**52.7**</ins>        | 38.9     | 42.5        | 41.2         |31.4         |\n| **Text IF-Eval**  | <ins>**61.3**</ins>        | 50.1     | 51.4        | 50.9         |47.4         |\n\n### Text Benchmarks\n\n|                   | Pixtral 12B | Qwen2 7B VL | LLaVA-OV 7B | Phi-3 Vision | Phi-3.5 Vision |\n|:-------------------:|:-------------:|:----------:|:-------------:|:--------------:|:--------------:|\n| **MMLU** *(5-shot)*   | <ins>**69.2**</ins>        | 68.5     | 67.9        | 63.5         | 63.6         |\n| **Math** *(Pass@1)*         | <ins>**48.1**</ins>        | 27.8     | 38.6        | 29.2         | 28.4         |\n| **Human Eval** *(Pass@1)*    | <ins>**72.0**</ins>        | 64.6     | 65.9        | 48.8         | 49.4         |\n\n### Comparison with Closed Source and Larger Models\n|                   | Pixtral 12B | Claude-3 Haiku | Gemini-1.5 Flash 8B *(0827)* | .  |*LLaVA-OV 72B* | *GPT-4o* | *Claude-3.5 Sonnet* |\n|:-------------------:|:-------------:|:----------------:|:----------------------:|:--------:|:----:|:-------------------:|:-------------------:|\n| **MMMU** *(CoT)*      | **52.5**        | 50.4           | 50.7                |   |*54.4*   |<ins>*68.6*</ins>   | *68.0*              |\n| **Mathvista** *(CoT)*  | **58.0**        | 44.8           | 56.9                |  |*57.2*   |<ins>*64.6*</ins>   | *64.4*              |\n| **ChartQA** *(CoT)*  | **81.8**        | 69.6           | 78.0                |  |*66.9*   |*85.1*   | <ins>*87.6*</ins>              |\n| **DocVQA** *(ANLS)* | **90.7**</ins>        | 74.6           | 79.5                   | |<ins>*91.6*</ins>   |*88.9*   | *90.3*              |\n| **VQAv2** *(VQA Match)* | **78.6**        | 68.4           | 65.5                |  |<ins>*83.8*</ins>   |*77.8*   | *70.7*              |\n\n## Usage Examples\n\n### vLLM (recommended)\n\nWe recommend using Pixtral with the [vLLM library](https://github.com/vllm-project/vllm)\nto implement production-ready inference pipelines with Pixtral.\n\n**_Installation_**\n\nMake sure you install `vLLM >= v0.6.2`:\n\n```\npip install --upgrade vllm\n```\n\nAlso make sure you have `mistral_common >= 1.4.4` installed:\n\n```\npip install --upgrade mistral_common\n```\n\nYou can also make use of a ready-to-go [docker image](https://hub.docker.com/layers/vllm/vllm-openai/latest/images/sha256-de9032a92ffea7b5c007dad80b38fd44aac11eddc31c435f8e52f3b7404bbf39?context=explore).\n\n**_Simple Example_**\n\n```py\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\n\nmodel_name = "mistralai/Pixtral-12B-2409"\n\nsampling_params = SamplingParams(max_tokens=8192)\n\nllm = LLM(model=model_name, tokenizer_mode="mistral")\n\nprompt = "Describe this image in one sentence."\nimage_url = "https://picsum.photos/id/237/200/300"\n\nmessages = [\n    {\n        "role": "user",\n        "content": [{"type": "text", "text": prompt}, {"type": "image_url", "image_url": {"url": image_url}}]\n    },\n]\n\noutputs = llm.chat(messages, sampling_params=sampling_params)\n\nprint(outputs[0].outputs[0].text)\n```\n\n**_Advanced Example_**\n\nYou can also pass multiple images per message and/or pass multi-turn conversations\n\n```py\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\n\nmodel_name = "mistralai/Pixtral-12B-2409"\nmax_img_per_msg = 5\n\nsampling_params = SamplingParams(max_tokens=8192, temperature=0.7)\n\n# Lower max_num_seqs or max_model_len on low-VRAM GPUs.\nllm = LLM(model=model_name, tokenizer_mode="mistral", limit_mm_per_prompt={"image": max_img_per_msg}, max_model_len=32768)\n\nprompt = "Describe the following image."\n\nurl_1 = "https://huggingface.co/datasets/patrickvonplaten/random_img/resolve/main/yosemite.png"\nurl_2 = "https://picsum.photos/seed/picsum/200/300"\nurl_3 = "https://picsum.photos/id/32/512/512"\n\nmessages = [\n    {\n        "role": "user",\n        "content": [{"type": "text", "text": prompt}, {"type": "image_url", "image_url": {"url": url_1}}, {"type": "image_url", "image_url": {"url": url_2}}],\n    },\n    {\n        "role": "assistant",\n        "content": "The images shows nature.",\n    },\n    {\n        "role": "user",\n        "content": "More details please and answer only in French!."\n    },\n    {\n        "role": "user",\n        "content": [{"type": "image_url", "image_url": {"url": url_3}}],\n    }\n]\n\noutputs = llm.chat(messages=messages, sampling_params=sampling_params)\nprint(outputs[0].outputs[0].text)\n```\n\nYou can find more examples and tests directly in vLLM.\n- [Examples](https://github.com/vllm-project/vllm/blob/main/examples/offline_inference_pixtral.py)\n- [Tests](https://github.com/vllm-project/vllm/blob/main/tests/models/test_pixtral.py)\n\n**_Server_**\n\nYou can also use pixtral in a server/client setting. \n\n1. Spin up a server:\n\n```\nvllm serve mistralai/Pixtral-12B-2409 --tokenizer_mode mistral --limit_mm_per_prompt ''image=4''\n```\n\n2. And ping the client:\n\n```\ncurl --location ''http://<your-node-url>:8000/v1/chat/completions'' \\n--header ''Content-Type: application/json'' \\n--header ''Authorization: Bearer token'' \\n--data ''{\n    "model": "mistralai/Pixtral-12B-2409",\n    "messages": [\n      {\n        "role": "user",\n        "content": [\n            {"type" : "text", "text": "Describe this image in detail please."},\n            {"type": "image_url", "image_url": {"url": "https://s3.amazonaws.com/cms.ipressroom.com/338/files/201808/5b894ee1a138352221103195_A680%7Ejogging-edit/A680%7Ejogging-edit_hero.jpg"}},\n            {"type" : "text", "text": "and this one as well. Answer in French."},\n            {"type": "image_url", "image_url": {"url": "https://www.wolframcloud.com/obj/resourcesystem/images/a0e/a0ee3983-46c6-4c92-b85d-059044639928/6af8cfb971db031b.png"}}\n        ]\n      }\n    ]\n  }''\n```\n\n### Mistral-inference\n\nWe recommend using [mistral-inference](https://github.com/mistralai/mistral-inference) to quickly try out / "vibe-check" Pixtral.\n\n\n**_Install_**\n\nMake sure to have `mistral_inference >= 1.4.1` installed.\n\n```\npip install mistral_inference --upgrade\n```\n\n**_Download_**\n\n```py\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmistral_models_path = Path.home().joinpath(''mistral_models'', ''Pixtral'')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id="mistralai/Pixtral-12B-2409", allow_patterns=["params.json", "consolidated.safetensors", "tekken.json"], local_dir=mistral_models_path)\n```\n\n**_Chat_**\n\nAfter installing `mistral_inference`, a `mistral-chat` CLI command should be available in your environment. \nYou can pass text and images or image urls to the model in *instruction-following* mode as follows:\n\n```\nmistral-chat $HOME/mistral_models/Pixtral --instruct --max_tokens 256 --temperature 0.35\n```\n\n*E.g.* Try out something like:\n\n```\nText prompt: What can you see on the following picture?\n[You can input zero, one or more images now.]\nImage path or url [Leave empty and press enter to finish image input]: https://picsum.photos/id/237/200/300\nImage path or url [Leave empty and press enter to finish image input]:\nI see a black dog lying on a wooden surface. The dog appears to be looking up, and its eyes are clearly visible.\n```\n\n**_Python_**\n\nYou can also run the model in a Python shell as follows.\n\n```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage, TextChunk, ImageURLChunk\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\ntokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tekken.json")\nmodel = Transformer.from_folder(mistral_models_path)\n\nurl = "https://huggingface.co/datasets/patrickvonplaten/random_img/resolve/main/yosemite.png"\nprompt = "Describe the image."\n\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=[ImageURLChunk(image_url=url), TextChunk(text=prompt)])])\n\nencoded = tokenizer.encode_chat_completion(completion_request)\n\nimages = encoded.images\ntokens = encoded.tokens\n\nout_tokens, _ = generate([tokens], model, images=[images], max_tokens=256, temperature=0.35, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n## Limitations\n\nThe Pixtral model does not have any moderation mechanisms. We''re looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Diogo Costa, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, MickaÃ«l Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, ThÃ©ophile Gervet, TimothÃ©e Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall', '{"pipeline_tag":null,"library_name":"vllm","framework":"vllm","params":null,"storage_bytes":25384829919,"files_count":5,"spaces_count":36,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:mistralai:mistral-inference","source_url":"https://github.com/mistralai/mistral-inference"}]', NULL, 'Apache-2.0', 'approved', 78.3, '6ac69fcd0f9fe3662b73156e7f14895f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-3.2-3B', 'huggingface--meta-llama--llama-3.2-3b', 'Llama-3.2-3B', 'meta-llama', '', '["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","en","de","fr","it","pt","hi","es","th","arxiv:2204.05149","arxiv:2405.16406","license:llama3.2","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 668, 391316, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-3.2-3B","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":3212749824,"storage_bytes":12853731547,"files_count":15,"spaces_count":95,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","eos_token":"<|end_of_text|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2204.05149","source_url":"https://arxiv.org/abs/2204.05149"},{"type":"based_on_paper","target_id":"arxiv:2405.16406","source_url":"https://arxiv.org/abs/2405.16406"}]', NULL, 'llama3.2', 'approved', 38.3, '6bbe629cbd463f0ac0d68c2119facacc', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-DeepFloyd-IF-I-XL-v1.0', 'huggingface--deepfloyd--if-i-xl-v1.0', 'IF-I-XL-v1.0', 'DeepFloyd', '', '["diffusers","pytorch","safetensors","if","text-to-image","arxiv:2205.11487","arxiv:2110.02861","license:deepfloyd-if-license","diffusers:ifpipeline","region:us"]', 'text-to-image', 667, 16784, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/DeepFloyd/IF-I-XL-v1.0","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":140925308588,"files_count":47,"spaces_count":27,"gated":"auto","private":false,"config":{"diffusers":{"_class_name":"IFPipeline"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2205.11487","source_url":"https://arxiv.org/abs/2205.11487"},{"type":"based_on_paper","target_id":"arxiv:2110.02861","source_url":"https://arxiv.org/abs/2110.02861"}]', NULL, 'deepfloyd-if-license', 'approved', 58.2, 'c4c81be3d84f24cfdf3c6bb234b20def', NULL, 'https://huggingface.co/DeepFloyd/IF-I-XL-v1.0/resolve/main/pics/if_architecture.jpg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-DeepFloyd-IF-I-XL-v1.0 from https://huggingface.co/DeepFloyd/IF-I-XL-v1.0/resolve/main/pics/if_architecture.jpg
HTTP error: 401 Unauthorized

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-microsoft-Orca-2-13b', 'huggingface--microsoft--orca-2-13b', 'Orca-2-13b', 'microsoft', '--- pipeline_tag: text-generation tags: - orca - orca2 - microsoft license: other license_name: microsoft-research-license license_link: LICENSE --- <!-- Provide a quick summary of what the model is/does. --> Orca 2 is built for research purposes only and provides a single turn response in tasks such as reasoning over user given data, reading comprehension, math problem solving and text summarization. The model is designed to excel particularly in reasoning. Note that: 1. This is a research m...', '["transformers","pytorch","llama","text-generation","orca","orca2","microsoft","arxiv:2311.11045","license:other","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 666, 8089, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/microsoft/Orca-2-13b","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\npipeline_tag: text-generation\ntags:\n- orca\n- orca2\n- microsoft\nlicense: other\nlicense_name: microsoft-research-license\nlicense_link: LICENSE\n---\n\n# Orca 2\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nOrca 2 is built for research purposes only and provides a single turn response in tasks such as reasoning over user given data, reading comprehension, math problem solving and text summarization. The model is designed to excel particularly in reasoning.\n\nNote that:\n\n1. This is a research model, intended to show that we can use capable models and complex workflows (advanced prompts, multiple calls) to create synthetic data that can teach Small Language Models (SLMs) new capabilities. We chose reasoning because it is a widely useful capability that SLMs lack.\n2. The model is not optimized for chat and has not been trained with RLHF or DPO. It is best used after being finetuned for chat or for a specific task.\n3. Beyond reasoning, the model inherits capabilities and limitations of its base (LLAMA-2 base). We have already seen that the benefits of the Orca training can be applied to other base model too.\n\nWe make Orca 2''s weights publicly available to support further research on the development, evaluation, and alignment of SLMs.\n\n## What is Orca 2â€™s intended use(s)?\n\n+ Orca 2 is built for research purposes only.\n+ The main purpose is to allow the research community to assess its abilities and to provide a foundation for\nbuilding better frontier models.\n\n## How was Orca 2 evaluated?\n\n+ Orca 2 has been evaluated on a large number of tasks ranging from reasoning to grounding and safety. Please refer\nto Section 6 and Appendix in the [Orca 2 paper](https://arxiv.org/pdf/2311.11045.pdf) for details on evaluations.\n\n## Model Details\n\nOrca 2 is a finetuned version of LLAMA-2. Orca 2â€™s training data is a synthetic dataset that was created to enhance the small modelâ€™s reasoning abilities. \nAll synthetic training data was moderated using the Microsoft Azure content filters. More details about the model can be found in the [Orca 2 paper](https://arxiv.org/pdf/2311.11045.pdf).\n\nPlease refer to LLaMA-2 technical report for details on the model architecture.\n\n## License\n\nOrca 2 is licensed under the [Microsoft Research License](LICENSE).\n\nLlama 2 is licensed under the [LLAMA 2 Community License](https://ai.meta.com/llama/license/), Copyright Â© Meta Platforms, Inc. All Rights Reserved.\n\n## Bias, Risks, and Limitations\n\nOrca 2, built upon the LLaMA 2 model family, retains many of its limitations, as well as the\ncommon limitations of other large language models or limitation caused by its training process,\nincluding:\n\n**Data Biases**: Large language models, trained on extensive data, can inadvertently carry\nbiases present in the source data. Consequently, the models may generate outputs that could\nbe potentially biased or unfair.\n\n**Lack of Contextual Understanding**: Despite their impressive capabilities in language understanding and generation, these models exhibit limited real-world understanding, resulting\nin potential inaccuracies or nonsensical responses.\n\n**Lack of Transparency**: Due to the complexity and size, large language models can act\nas â€œblack boxesâ€, making it difficult to comprehend the rationale behind specific outputs or\ndecisions. We recommend reviewing transparency notes from Azure for more information.\n\n**Content Harms**: There are various types of content harms that large language models\ncan cause. It is important to be aware of them when using these models, and to take\nactions to prevent them. It is recommended to leverage various content moderation services\nprovided by different companies and institutions. On an important note, we hope for better\nregulations and standards from government and technology leaders around content harms\nfor AI technologies in future. We value and acknowledge the important role that research\nand open source community can play in this direction.\n\n**Hallucination**: It is important to be aware and cautious not to entirely rely on a given\nlanguage model for critical decisions or information that might have deep impact as it is\nnot obvious how to prevent these models from fabricating content. Moreover, it is not clear\nwhether small models may be more susceptible to hallucination in ungrounded generation\nuse cases due to their smaller sizes and hence reduced memorization capacities. This is an\nactive research topic and we hope there will be more rigorous measurement, understanding\nand mitigations around this topic.\n\n**Potential for Misuse**: Without suitable safeguards, there is a risk that these models could\nbe maliciously used for generating disinformation or harmful content.\n\n**Data Distribution**: Orca 2â€™s performance is likely to correlate strongly with the distribution\nof the tuning data. This correlation might limit its accuracy in areas underrepresented in\nthe training dataset such as math, coding, and reasoning.\n\n**System messages**: Orca 2 demonstrates variance in performance depending on the system\ninstructions. Additionally, the stochasticity introduced by the model size may lead to\ngeneration of non-deterministic responses to different system instructions.\n\n**Zero-Shot Settings**: Orca 2 was trained on data that mostly simulate zero-shot settings.\nWhile the model demonstrate very strong performance in zero-shot settings, it does not show\nthe same gains of using few-shot learning compared to other, specially larger, models.\n\n**Synthetic data**: As Orca 2 is trained on synthetic data, it could inherit both the advantages\nand shortcomings of the models and methods used for data generation. We posit that Orca\n2 benefits from the safety measures incorporated during training and safety guardrails (e.g.,\ncontent filter) within the Azure OpenAI API. However, detailed studies are required for\nbetter quantification of such risks.\n\nThis model is solely designed for research settings, and its testing has only been carried\nout in such environments. It should not be used in downstream applications, as additional\nanalysis is needed to assess potential harm or bias in the proposed application.\n\n## Getting started with Orca 2\n\n**Inference with Hugging Face library**\n\n```python\nimport torch\nimport transformers\n\nif torch.cuda.is_available():\n    torch.set_default_device("cuda")\nelse:\n    torch.set_default_device("cpu")\n    \nmodel = transformers.AutoModelForCausalLM.from_pretrained("microsoft/Orca-2-13b", device_map=''auto'')\n\n# https://github.com/huggingface/transformers/issues/27132\n# please use the slow tokenizer since fast and slow tokenizer produces different tokens\ntokenizer = transformers.AutoTokenizer.from_pretrained(\n        "microsoft/Orca-2-13b",\n        use_fast=False,\n    )\n\nsystem_message = "You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior."\nuser_message = "How can you determine if a restaurant is popular among locals or mainly attracts tourists, and why might this information be useful?"\n\nprompt = f"<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{user_message}<|im_end|>\n<|im_start|>assistant"\n\ninputs = tokenizer(prompt, return_tensors=''pt'')\noutput_ids = model.generate(inputs["input_ids"],)\nanswer = tokenizer.batch_decode(output_ids)[0]\n\nprint(answer)\n\n# This example continues showing how to add a second turn message by the user to the conversation\nsecond_turn_user_message = "Give me a list of the key points of your first answer."\n\n# we set add_special_tokens=False because we dont want to automatically add a bos_token between messages\nsecond_turn_message_in_markup = f"\n<|im_start|>user\n{second_turn_user_message}<|im_end|>\n<|im_start|>assistant"\nsecond_turn_tokens = tokenizer(second_turn_message_in_markup, return_tensors=''pt'', add_special_tokens=False)\nsecond_turn_input = torch.cat([output_ids, second_turn_tokens[''input_ids'']], dim=1)\n\noutput_ids_2 = model.generate(second_turn_input,)\nsecond_turn_answer = tokenizer.batch_decode(output_ids_2)[0]\n\nprint(second_turn_answer)\n```\n\n\n**Safe inference with Azure AI Content Safety**\n\nThe usage of [Azure AI Content Safety](https://azure.microsoft.com/en-us/products/ai-services/ai-content-safety/) on top of model prediction is strongly encouraged \nand can help prevent content harms. Azure AI Content Safety is a content moderation platform\nthat uses AI to keep your content safe. By integrating Orca 2 with Azure AI Content Safety, \nwe can moderate the model output by scanning it for sexual content, violence, hate, and \nself-harm with multiple severity levels and multi-lingual detection. \n\n```python\nimport os\nimport math\nimport transformers\nimport torch\n\nfrom azure.ai.contentsafety import ContentSafetyClient\nfrom azure.core.credentials import AzureKeyCredential\nfrom azure.core.exceptions import HttpResponseError\nfrom azure.ai.contentsafety.models import AnalyzeTextOptions\n\nCONTENT_SAFETY_KEY = os.environ["CONTENT_SAFETY_KEY"]\nCONTENT_SAFETY_ENDPOINT = os.environ["CONTENT_SAFETY_ENDPOINT"]\n\n# We use Azure AI Content Safety to filter out any content that reaches "Medium" threshold\n# For more information: https://learn.microsoft.com/en-us/azure/ai-services/content-safety/\ndef should_filter_out(input_text, threshold=4):\n    # Create an Content Safety client\n    client = ContentSafetyClient(CONTENT_SAFETY_ENDPOINT, AzureKeyCredential(CONTENT_SAFETY_KEY))\n\n    # Construct a request\n    request = AnalyzeTextOptions(text=input_text)\n\n    # Analyze text\n    try:\n        response = client.analyze_text(request)\n    except HttpResponseError as e:\n        print("Analyze text failed.")\n        if e.error:\n            print(f"Error code: {e.error.code}")\n            print(f"Error message: {e.error.message}")\n            raise\n        print(e)\n        raise\n\n    categories = ["hate_result", "self_harm_result", "sexual_result", "violence_result"]\n    max_score = -math.inf\n    for category in categories:\n        max_score = max(max_score, getattr(response, category).severity)\n\n    return max_score >= threshold\n\nmodel_path = ''microsoft/Orca-2-13b''\ndevice = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")\nmodel = transformers.AutoModelForCausalLM.from_pretrained(model_path)\nmodel.to(device)\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\n    model_path,\n    model_max_length=4096,\n    padding_side="right",\n    use_fast=False,\n    add_special_tokens=False,\n)\n\nsystem_message = "You are Orca, an AI language model created by Microsoft. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior."\nuser_message = "\" \n :You can''t just say, \"\"that''s crap\"\" and remove it without gaining a consensus. You already know this, based on your block history. â€”/ \" \nIs the comment obscene? \nOptions : Yes, No."\n\nprompt =  f"<|im_start|>system\n{system_message}<|im_end|>\n<|im_start|>user\n{user_message}<|im_end|>\n<|im_start|>assistant"\n\ninputs = tokenizer(prompt, return_tensors=''pt'')\ninputs = inputs.to(device)\n\noutput_ids = model.generate(inputs["input_ids"], max_length=4096, do_sample=False, temperature=0.0, use_cache=True)\nsequence_length = inputs["input_ids"].shape[1]\nnew_output_ids = output_ids[:, sequence_length:]\nanswers = tokenizer.batch_decode(new_output_ids, skip_special_tokens=True)\nfinal_output = answers[0] if not should_filter_out(answers[0]) else "[Content Filtered]"\n\nprint(final_output)\n```\n\n## Citation\n```bibtex\n@misc{mitra2023orca,\n      title={Orca 2: Teaching Small Language Models How to Reason}, \n      author={Arindam Mitra and Luciano Del Corro and Shweti Mahajan and Andres Codas and Clarisse Simoes and Sahaj Agrawal and Xuxi Chen and Anastasia Razdaibiedina and Erik Jones and Kriti Aggarwal and Hamid Palangi and Guoqing Zheng and Corby Rosset and Hamed Khanpour and Ahmed Awadallah},\n      year={2023},\n      eprint={2311.11045},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":182223364533,"files_count":17,"spaces_count":61,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<s>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"</s>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false},"pad_token":null,"unk_token":{"__type":"AddedToken","content":"<unk>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false},"use_default_system_prompt":true}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"based_on_paper","target_id":"arxiv:2311.11045","source_url":"https://arxiv.org/abs/2311.11045"}]', NULL, 'Other', 'approved', 78.2, 'c80a911ec79f024ec3bb6ec845da80b1', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-QuickWick-Music-AI-Voices', 'huggingface--quickwick--music-ai-voices', 'Music-AI-Voices', 'QuickWick', '--- license: other --- **â™« Discord: https://discord.gg/aihub | Join the community, learn to make models, chat with link-minded people and lets create music â™© â™ª** **â™« Discord Latino: https://discord.gg/Crfqs7uB5V | Entren a nuestra comunidad, aprendan a crear modelos AI, habla con otros sobre musica y disfruta las notas musicales â™© â™ª** **IMPORTANT!!!!!!!!!: VOICES CANNOT BE COPYRIGHTED. We do not promote piracy so please do not come in with that. We do promote legal-length sample clips of voca...', '["license:other","region:us"]', 'other', 662, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/QuickWick/Music-AI-Voices","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\n---\n\n**â™« Discord: https://discord.gg/aihub | Join the community, learn to make models, chat with link-minded people and lets create music â™© â™ª**\n\n**â™« Discord Latino: https://discord.gg/Crfqs7uB5V | Entren a nuestra comunidad, aprendan a crear modelos AI, habla con otros sobre musica y disfruta las notas musicales â™© â™ª**\n\n**IMPORTANT!!!!!!!!!: VOICES CANNOT BE COPYRIGHTED. We do not promote piracy so please do not come in with that. We do promote legal-length sample clips of vocals. We promote music & AI produced music covers (impressions). We promote machine learning & Voice AI Models. Note: This repository does NOT include ANY DATASETS. Only models are included.**\n\n**If you want your credits/name removed, please message me on discord and I will remove it diligently.**\n\n**Tools: https://vocalremover.org/ https://x-minus.pro/ai https://create.musicfy.lol/**\n\n**Created Using: SoftVC VITS Singing Voice Conversion (so vits svc 4.0) | Retrieval based Voice Conversion (RVC)**\n\n====================================\n\nAnnouncements:\n\nDocumentation is outdated and it is not planned to be updated. The backlog of Models is immense. Last known Read-ME update: 5/12/2023 - I apologize in advance to the model makers of our community but I have too much going on IRL. Go to files to view models rather than using the documentation.\n\nEdit: Figured out a way to automate 85% of documentation. This is a v2 of my document automation. I will begin to work on v3 in about 3 weeks and this will complete my document automation. Documentation/Credits will not be posted until v3 is completed. All previous models will be appropriately credited at that point.\n\n**Do not rely on ctrl+f for the time being** Models are in fact being uploaded. Check the files section in the mean time.\n\n====================================\n\n**Name - Amount of Steps - Creator**\n\n21 Savage - 100k - brandy#4247 | \n21 Savage -	50k - candy#6483\n\n2Pac Tupac - 50k - Makaveli AI#4517 |\n2Pac Tupac (RVC) - 150 Epoch - Makaveli AI#4517 |\n2Pac Tupac - 33k - ????\n\n6lack (RVC) - 700 Epoch - RomeTheDaddy#4293\n\nAaliyah - 33.6k - COMEHU#2094\n\nAitana - 75K - blaise#9999\n\nAlizee - 45.6k - CrimsonZockt#2221 | \nAlizee (2000-2003) - 23.2k - CrimsonZockt#2221\n\nAmano Pikamee (VOMS Project) - 30k - dacoolkid44#4173\n\nAmeer Vann - 15k - asher roth#3637\n\nAmelia Watson (Hololive EN) - 30k - dacoolkid44#4173\n\nAndrew Tate - 50k - Makaveli AI#4517\n\nAnt Clemons (RVC - 3150 Steps - SamV1sion#5354\n\nAnthony Green (Circa Survive) (RVC) - 500 Epochs - owl#1313 |\nAnthony Green (RVC) (Alpha) - 250 Epoch - philo#9160\n\nAnuel AA - 41.6k - Smile WRLD#9877 |\nAnuel AA (2016 Era) - 500 Steps - Raaul10#2946\n\nAriana Grande - 73k - ????? - [Trained using pro tools sessions so the vocals sound dry] |\nAriana Grande - 89k -	christy#0059 |\nAriana Grande (RVC) - 4k Epoch 28k Steps - MentosAndRice#8492\n\nAries of Wunderworld - 150k - lij#0001\n\nASAP Rocky (RVC) - 1k Epoch - Ski#5447\n\nAyesha Erotica - 100k -	henry_#7065\n\nBaby Keem - 191k - okcool#5237\n\nBad Bunny - 180k - Bowl#2016 |\nBad Bunny - 1k Epoch - CJPP270#0162\n\nBANANIROU - 100k - ÅŸtar#7068\n\nBart Simpson - 22k - AnthonyFandom70100#9529 |\nBart Simpson (RVC) - 250 Epoch - AnthonyFandom70100#9529\n\nBENEE - 8k - rejekts#0820\n\nBiden - 20k - Nardicality\n\nBiggie Smalls - 112.8k - justinjohn-03#4897 |\nBiggie Smalls (RVC) - 20k - Makaveli AI#4517\n\n\nBillie Eilish - 8k - Vali665#9670 [7 Hours of Training] |\nBillie Eilish 2016-2018 - 1k - Vali665#9670 |\nBillie Eilish (RVC) - ???? - senzo#1502\n\nBillie Joe - 24k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\n\nBinyamin Netanyahu (Israel''s PM) - 67.7K - yeatfan119#8009\n\nBktherula - 47k - averycj#3997\n\nBo Burnham (Inside) (RVC) - 250 Epoch -  analogspiderweb#7099\n\nBONES - 1k Epoch 110k - ðŸ’Š LÃ¼h Minion ðŸ’‰#1804\n\nBrandy (RVC) - 200 Epoch - fractalfantasy#2748\n\nBrendon Urie - Panic! at the Disco - 49k - Budman#5216 & Bowl#2016\n\nBrian Wilson (Modern Era) (RVC) - 200 Epoch - Jay#0152\n\nBritney Spears - 100k - AIVERSE#5393 | \nBritney Speaks (Young) - 17k - Frix#2580 |\nBritney Spears (RVC) - 500 Epoch - AIVERSE#5393\n\nBruno Mars - 124.9k - Thompson#2472 |\nBruno Mars (RVC) - 24k - Thompson#2472\n\nBruno Powroznik (RVC) - 250 Epochs - analogspiderweb#7099\n\nBryska - 45.6k - CrimsonZockt#2221\n\nCamila Cabello (RVC) - 600 Epoch - LMAO DEAD ðŸ˜‚ðŸ˜‚ðŸ˜‚#8206\n\nCanserbero - 67k - Frix#2580\n\nCaparezza - 200K - LollenApe#4707\n\nCazzu - 62k - NuokiFTW#0001\n\nChano (From Tan BiÃ³nica) - 24k - StarBoy#2512\n\nCharlie Dompler (Smiling Friends) (RVC) - 300 Epoch - analogspiderweb#7099 [Zach Hadel / psychicpebbles / Charlie Dompler]\n\nCharlie Puth - 36k - Crewe''s Corner#4767\n\nCharlie Scene (From Hollywood Undead) - 14k - ThatOneDuder710#2594 [Rapping]\n\nChase Atlantic - 500 Epoch - rejekts#0820\n\nChester Bennington (Linkin Park) - 79k - Cheech#8254 |\nChester Bennington (RVC) - 1k Epoch 40k Steps - sgsavu#0733\n\nChief Keef - 100k - candy#6483\n\nChildish Gambino (RVC) - 1k Epoch - kalomaze#2983\n\nChris Brown - 105k - Sample.House#0737 [Sounds best using his lower register, when transposed down 1-2 semitones] |\nChris Brown (RVC) - 700 Epoch - RomeTheDaddy#4293\n\nChris Cornell - 7.4k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\n\nComethazine - 1086 Epoch 25K - sgsavu#0733 [batch size 7, 161 - 9 second samples] [trained on: open mics, interviews, live freestyles]\n\nComethazine [Mixed Edition] - 1000 Epoch 64.3k - sgsavu#0733 [trained on everything from PURE edition + least amount of voice processing (556, highriser, etc) + Mixed edition sounds more agressive than PURE but has more artifacts and noise in the resulting audio] | \nComethazine [Pure Edition] - 1000 Epoch 43k - sgsavu#0733 [trained on clean acapellas/vocals from: interviews, open mics, live freestyles]\n\nC.R.O - 42k - visarra#1117\n\nCupcakKe - 100k - HuntyDarling#4808\n\nDaBaby (RVC) - 1k Epoch 70k steps - sgsavu#0733\n\nDanny Ocean - 34k - matias464#2068\n\nDave Mustaine (Megadeth) (RVC) - 1000 Epoch - trioskosmos#8731\n\nDavid Bowie - 7.2k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\n\nDeku (Izuku Midoriya) (RVC) - 100 Epoch - Anon\n\nDem Jointz (RVC) - 4.6k - SamV1sion#5354\n\nDeuce (From Hollywood Undead) (RVC) - 1K Epoch - sgsavu#0733\n\nDigga D (RVC) - 1000 Epoch 5.6k Steps - arturocookinup#5078\n\nDillom - 12.8k - Xvalen#3936\n\nDio Brando (From JoJo''s Bizzare Adventure) (RVC) -  10k Steps - nicegame#6990\n\nDiomedes Diaz (Cacique) (RVC) - 200 Epoch - [El Cacique de la Junta]\n\nDoja Cat  - 163.2k - á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼#7280\n\nDon Toliver - 88k - Alei#0950 | \nDon Toliver - 68k - Lightning McQueen#0001 [68k Cleaner/Better than 88k version]\n\nDrake - 100k - Snoop Dogg#8709 |\nDrake (RVC) - ???? - Snoop Dogg#8709\n\nDua Lipa - 72k - aimelody#5393\n\nDuki - 116.8k - Andres0i#4229 [si lo van a probar usen audios sin tune y sin entonaciones, de resto no les va a servir] |\nDuki - 75k - Labrador#6962 |\nDuki - 1k - 0900#9787 |\nDuki (RVC) - 250 Epoch - diegoAsdf#9942\n\nEd Sheeran (RVC) - 1000 Epoch - AIVERSE#5393\n\nEddie Vedder - 48.8k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\n\nEl Puto Coke - 10k - Vigo#2099\n\nEladio CarriÃ³n - 40k - blaise#9999\n\nElon Musk - 99K - Stephen5311#6349\n\nElton John - 14k - Frix#2580\n\nEminem (General Model v1) - 86k - Bowl#2016 \n\nEminem (SLIM SHADY Edition) - 209k  - ???????? | \nEminem (Slim Shady Era) - 400 Epoch 48k Steps - SpaceCypher#6133 |\nEminem (New Era) (RVC) - 1k Epoch - Bowl#2016 & TRB Harry$#7680\n\nEnna Alouette (NIJISANJI EN) - 10k - dacoolkid44#4173\n\nEric Cartman - 10.2k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\n\nFase Yoda - 50k - Kyume â˜¥ (MÃ©ry)#4518\n\nFeid - 147k - CAMARA DE GTX#4459\n\nFerxxo - ???? - KHAKO#8845\n\nFoda C (French Rapper) - 30k - Kyume â˜¥ (MÃ©ry)#4518\n\nFrank Ocean - 400k  - Yurboii#8420 [30kEpoch70minDataset] |\nFrank Ocean (RVC) - 18.2k Steps, 210 Epoch - TheLosslessPlug#3202 |\nFrank Ocean (RVC) - 500 Epoch - Hubert Paul Flatt#9804\n\nFreddie Mercury - 300k - Bowl#2016 & Roberto89#2726 & musictrackcenter#4011 |\nFreddie Mercury - 125k  - jev217#8700 |\nFreddie Mercury (RVC) - Unknown Steps - K7#4523 [Around 1000 epochs, kinda better than sovits model]\n\nFuture - 45k - candy#6483 | \nFuture (RVC) - 2.7k - arturocookinup#5078\n\nGawr Gura (Hololive EN) - 30k  - dadcoolkid44#4173 |\nGawr Gura (RVC) - 126 Epoch - RaymondReddington#6845\n\nGeorge Harrison - ???? - ZGLM#6250 [batch size of 4,927 samples and 101 epochs]\n\nGeorge Michael (RVC) - 500 Epoch - clubbedsam#4419 [Trained on Crepe]\n\nGiovanna Grigio (Chiquititas 2013 Era) - 31.2k - CrimsonZockt#2221\n\nGoku (RVC) - ???? - nicegame#6990\n\nGunna - 123k - elijah#2251 [Sounds bad with high notes] |\nGunna (RVC) - 3.5k Steps - 1ski#4245\n\nHaachama (Hololive JP) RVC - 1000 Epoch - dacoolkid44#4173 & mochikiri-chan#0665\n\nHalf Life 2 (Male 07) (RVC) - 1K Epoch 28K Steps - ðŸ’Š LÃ¼h Minion ðŸ’‰#1804\n\nHarry Styles - 72k - Melatone#1344 |\nHarry Styles - 56k  - K7#4523\n\nHayley Williams (From Paramore) - 300k - Thompson#2472 |\nHayley Williams (From Paramore) (RVC) - 600 Epoch - owl#1313\n\nHef (RVC) - 250 Epoch 1362 Steps - arturocookinup#5078\n\nHomer Simpson - 22k - AnthonyFandom70100#9529 [voiced by Dan Castellaneta]\n\nHoshimachi Suisei (Hololive JP) (RVC) - ???? - Shiro-chan#9415\n\nHozier (RVC) - 270 Epoch - Jatazgo#2719\n\nHyunjin (From Stray Kids) - ???? - Smile WRLD#9877\n\nIbai - 11k - blaise#9999\n\nIce Spice - ???? - ayydot#7545 |\nIce Spice (RVC) - 11k - Zeuz Makes Music#6014\n\nIndio Solari - 60k - RedamOk#7021\n\nInugami Korone (Hololive JP) (RVC) Upd 5.2.23 - ???? dacoolkid44#4173 mochikiri-chan#0665\n\nIrene (From Red Velvet) - 4k - Smile WRLD#9877\n\nIsaac Kleiner (From Half-Life 2) - 500 Epoch - jakeH#5394\n\nIU (RVC) - 1k Epoch 99k Steps - baloneyboy#4232 |\nIU (RVC) - 800 Epoch - checkmate#2840\n\nJ Cole - 100k  - á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼#7280\n\nJaghit Singh (Indian Ghazal) (RVC) - 400 Epoch 48k Steps - SpaceCypher#6133\n\nJames Hetfield - 49.6k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\n\nJay Kay (Jamiroquai lead singer) - 40k - l3af#3435\n\nJay Z - 54.4k  - justinjohn-03#4987\n\nJamiroquai - 44k - ????\n\nJeff Lynne (Electric Light Orchestra) (RVC) - 325 Epoch - Jay#0152\n\nJennie Kim (From BLACKPINK) (RVC) -  300 Epoch -  ???? |\nJennie Kim (From BLACKPINK) - 65k - hristy#0059\n\nJeon So-yeon (From (G)I-DLE) - 800 Steps - Smile WRLD#9877\n\nJhene Aiko - 61.6k - ariscult#6164 | \nJhene Aiko (RVC) - 175 Epoch - baloneyboy#4232\n\nJihyo (Twice) - 1.6k - Smile WRLD#9877\n\nJim James (My Morning Jacket) (RVC) - 5k - Jay#0152\n\nJimin (From BTS) - 24K - neoculture#4390\n\nJisoo (From BLACKPINK) - 113k - RadmirGrande#0544 |\nJisoo (From BLACKPINK) (RVC) -  250 Epoch - Moonkissed#1774 Arithyst#3931\n\nJoba of BROCKHAMPTON - 15k - asher roth#3637\n\nJohn F. Kennedy (JFK) (RVC) - 600 Epoch 53k Steps - Disc#0287\n\nJohn Frusciante (RVC) - 1k Epoch - sgsavu#0733\n\nJohn Lennon - 78k - Vlader#7108 |\nJohn Lennon - 365k - Anon [Beatles AI Discord] |\nJohn Lennon (1970 Era) (RVC) - 5k - Jay#0152\n\nJoji (RVC) - 32k - MentosAndRice#8492\n\nJotaro Kujo (From JoJo''s Bizzare Adventure) (RVC) - 15k Steps - nicegame#6990\n\nJoy (From Red Velvet) (RVC) - 200 Epoch - bee#0069\n\nJuice WRLD - 160k  - ryyyy#5003 |\nJuice WRLD (Agressive) - 28k - BigDRá—©CO$O#2129 |\nJuice WRLD - 1k Epoch 15k Steps - sgsavu#0733\n\nJulia Volkova (From  t.A.T.u.) - 500 Epoch - JpopKARAOKE#6331\n\nJung Kook (RVC) - 4k Epoch - MentosAndRice#8492 [v3 APR 25 2023] | \nJung Kook - 5k - MentosAndRice#8492 | \nJung Kook (RVC) - 200 Epoch 350 steps - rejekts#0820 [70mb version, 200 Epoch @ 20 Batch Size, 35 clips] |\nJung Kook - 60k - Moonkissed#1774 & Arithyst#3931\n\nJustin Bieber - 67k  - AguacateDev#4071\n\nK Suave (RVC) - 700 Epoch - checkmate#2840\n\nKai - Kim Jong-in (From Exo) - 34.4k Steps - YH#9495\n\nKanye West - 199.2k  - Pyeon Yeongsun #5759 - **Internet Wide Release aka ye200k** | \nKanye West (RVC) - ???? - Wil#7050 [ran to 1000 epochs] | \nKanye West - 112k - ???? (Author said 100k and model is called yeversiontwo) |\nKanye West (RVC) - 233.3k Steps, 1000 epoch - Wil#7050\n\nKaty Perry - 28k - RaulBlue#3655\n\nKen Carson (Only Interviews) - 52k - BigDRá—©CO$O#2129 |\nKen Carson (Rapping Vocals) -  59k - averycj#3997\n\nKendrick Lamar - 67.2k  - Snoop Dogg#8709 | \nKendrick Lamar (RVC) - ???? - Snoop Dogg#8709 | \nKendrick Lamar - 100.2k  - okcool#5237 [Might be overtrained]\n\nKhea - 20.8k - NuokiFTW#0001\n\nKid Mess (Alpha) - 0.8k  - Cowton#5872 & kesnomanaow#3304\n\nKidd Keo - 32k - NuokiFTW#0001\n\nKim Chaewon (From LE SSERAFIM) (Beta) - 500 Epoch - codebloodedgirl6#2315\n\nKim Garam (From LE SSERAFIM) (RVC) - 300 Epoch - codebloodedgirl6#2315\n\nKim Seokjin (From BTS) - 24k - neoculture#4390\n\nKim Taehyung - 24k - neoculture#4390\n\nKizaru - 45.6k - CrimsonZockt#2221\n\nKrystal Jung (RVC) - 1008 Epoch - Shabi_Chats#0606 [Works better with high notes]\n\nKurt Cobain - 138.6k  - á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼#7280\n\nKurtains (RVC) - 500 Epoch - Autumn#4768\n\nL-Gante - 12k - StarBoy#2512\n\nLa+ Darkness (Hololive JP) - 12k - dacoolkid44#4173 | La+ Darkness (Hololive JP) (RVC) - Updated 4.29.2023 - mochikiri-chan#0665 & dacoolkid44#4173\n\nLady Gaga - 14k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\n\nLalisa Manoban - ??? - Smile WRLD#9877\n\nLana Del Rey - 100k - K7#4523 |\nLana Del Rey (RVC) - 1k Epoch 74k Steps - sgsavu#0733\n\nLauryn Hill - 45k - averycj#3997\n\nLena Katina ( From t.A.T.u.) (RVC) - 300 Epoch- JpopKARAOKE#6331\n\nLiam Gallagher - 18.4k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\n\nLil Baby (RVC) - 500 Epoch - arturocookinup#5078 [Batch Size: 20]\n\nLil Dicky (RVC) - 1000 Epoch - Carson#1111\n\nLil Nas X - 26K - riddle#3363\n\nLil Tracy - ???? - Sztef#7028\n\nLil Peep - 33k  - Sztef#7028\n\nLil Uzi Vert - 80k  - ShadowTB#8205 | \nLil Uzi Vert - 1k Epoch 37k Steps - sgsavu#0733 [batch size 6]\n\nLil Yachty - 10k Epoch 120k - game#0102\n\nLily (From NMIXX) (RVC) - 250 Epoch - jisoos cat#7462 [Works better with high notes]\n\nLisa (From BLACKPINK) (RVC) - 900 Epoch - checkmate#2840\n\nLisa Simpson - 22k - AnthonyFandom70100#9529 |\nLisa Simpson (RVC) - 250 Epoch - AnthonyFandom70100#9529\n\nLiz (From IVE) - 800 steps - Smile WRLD#9877\n\nLogic (RVC) - 1k Epoch 116k Steps - sgsavu#0733\n\nLuis Miguel - 82.4k - jrbeat#4961\n\nLuther (French Rapper) - 50k - Kyume â˜¥ (MÃ©ry)#4518\n\nMaeve (From Paladins) - 1600 Epoch - wlrkt#2520\n\nMaria Becerra - 122k - dariovelaam#3542\n\nMariah Angeliq - 10k - remix#7551\n\nMarina Sena - 8.8k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\n\nMatt Bellamy (From Muse) (RVC) - 200 Epoch 61k Steps - Ryanz#0053\n\nMCParodyVoice - ???? - TheEpicRock7#9557\n\nMelanie Martinez - 72K - aimelody#5393 |\nMelanie Martinez (RVC) - 1000 Epoch - AIVERSE#5393\n\nMaria MendonÃ§a - 10.4k - hugo97#5776\n\nMariah Carey (RVC) - 300 Epoch - fractalfantasy#2748\n\nMF Doom - 45k  - Mellon#2653\n\nMichael Jackson - 83k  - clubbedsam#4419 | \nMichael Jackson (RVC) - 1k Epoch - premydaremy#2498 |\nMichael Jackson - 150k - Nyxel#7778 | \nMichael Jackson (RVC) - 1k Epoch - tea#6949 [Harsh Vocals]\n\nMikey Sawyer of Miss Fortune - 336k - mikeysawyermf#3327\n\nMiko - ???? - ????\n\nMiley Cyrus (RVC) - 750 Epoch - AIVERSE#5393\n\nMina Myoi (From TWICE) - 2k - â­ ð“šð“¾ð“¶ð“ª â­ Ê•ã£â€¢á´¥â€¢Ê”ã£#0001\n\nMona Lisa - 10k - COMEHU#2094\n\nMoonMan - 120k  - ????\n\nMon Laferte (RVC) - 600 Epoch - AnotherNoName#3807\n\nMora - 73.6k - NuokiFTW#0001\n\nMorad - 11k - blaise#9999\n\nMordecai (RVC) - 3.6k steps, 750 epochs - kalomaze#2983 [39 clips, 6 minutes long dataset]\n\nMorgenshtern - 15k - lunnaholy#0147\n\nMori Calliope (Hololive EN) - 8.8k  - dacoolkid44#4173\n\nMyke Towers - 100k - Labrador#6962\n\nNas (King''s Disease Era) (SVC) - 171k - bola#1593\n\nNCT Haechan (SVC) - Unknown - à¸—à¸±à¸šà¸šà¸„#2007\n\nNCT Jaemin (RVC) - Unknown - à¸—à¸±à¸šà¸šà¸„#2007\n\nNCT Jeno (RVC) - 350 Epoch 11k Steps - à¸—à¸±à¸šà¸šà¸„#2007\n\nNCT Mark Lee (RVC) - Unknown - à¸—à¸±à¸šà¸šà¸„#2007\n\nNCT Renjun (RVC) - 250 Epoch 9k Steps - à¸—à¸±à¸šà¸šà¸„#2007\n\nNeyo - 80k  - subraiz#4688 & NoRappersAllowed#1186\n\nNicky Jam - 25k  - ????\n\nNicki Minaj - 64k - LMAO DEAD ðŸ˜‚ðŸ˜‚ðŸ˜‚#8206 |\nNicki Minaj - 27.2k - COMEHU#2094\n\nNicki Nicole - 120k - StarBoy#2512\n\nNinomae Ina''nis (Hololive EN) - 30k - dacoolkid44#4173\n\nNipsey Hussle - 100k  - justinjohn-03#4897\n\nNLE Choppa (RVC) - 1000 epochs 51k - sgsavu#0733 [trained on around 15 minutes of edited freestyles, open mics, interviews, and least vocal processed songs]\n\nNotti Osama - 60k - averycj#3997 & fr1ends#0001\n\nObama - 50k  - Nardicality\n\nOddcast Daniel (FROM MLG TTS Voice)(RVC) - 300 Epochs - analogspiderweb#7099 [Works best on lower pitch vocals.]\n\nOki (Oskar KamiÅ„ski) - 49.6k - CrimsonZockt#2221\n\nOlivia Rodrigo - 12.8k - karol jozef pelin#2129 |\nOlivia Rodrigo - 4k - tahaefe.ipekk#9926\n\nOmar Rudberg - 100k - reee#2204\n\nOptiJuegos - 100k - ÅŸtar#7068\n\nOzuna - 4.8k - ???? |\nOzuna - 4k - matias464#2068\n\nOzzy Osbourne (Young) (RVC) - 470 Epoch - ancientdeit#3609 [Black Sabbath to Sabotage Era & Blizzard Of Ozz]\n\noxxxymiron - 24K - Uker#8854\n\nP!NK (RVC) - 1000 Epoch - AIVERSE#5393\n\nPaloma Mami - 32k - Benja#4927\n\nPatrick Star - 500 Epoch - Autumn#4768\n\nParappa The Rapper (Video Game Character) - 59k - nicegame#6990\n\nPark Jimin (RVC) Demo - 16k - KaraBaby#3426\n\nPatrick Warburton (RVC) - 200 Epoch - Samoa Noah#5570 [AKA Kronk from Emperor''s new Groove and Joe Swanson]\n\nPaul McCartney (SVC) - 200k - Albinator#8386 |\nPaul McCartney (Young Era) (RVC) - 1k Epoch - kalomaze#2983 & Albinator#8386 [Trained on harvest pitch inference using the same dataset as the sovits Paul from Albinator]\nPaul McCartney (1964 Era) (RVC) - 5k - Jay#0152\n\nPaulo Londra - 100k - Milkitos03#5076 |\nPaulo Londra - 10k - ð–ð–‰ð–Žð–Šð–Œð–”ð–™ð–Š#3978\n\nPekora - ???? - ????\n\nPeso Pluma - 40k - NRM#5257\n\nPeter Griffin (RVC) - 4.5k - Delik#0001\n\nPhil Anselmo - 25k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\n\nPlankton (From SpongeBob) (RVC) - 500 Epoch - Hubert Paul Flatt#9804\n\nPlayboi Carti - 45k - Snoop Dogg#8709 [This is probably v2 or SVC edition|\nPlayboi Carti - 42k - Molo#0001 [Whole Lotta Red Era v2] |\nPlayboi Carti (Die Lit Era) - 18k - Zeuz Makes Music#6014 |\nPlayboi Carti v3 (RVC) - ???? - Snoop Dogg#8709 |\nPlayboi Carti - 46k - BigDRá—©CO$O#2129 [New Sessions Used]\n\nPop Smoke - 36.8k - sable#0001\n\nPost Malone - 9.6k - Prod. Bad Dude#3218\n\nPostal Dude (From Postal Game) - 2.5k - HuggingFace link to be added |\nPostal Dude (From POSTAL 2) - 1K Epochs 25K Steps - ðŸ’Š LÃ¼h Minion ðŸ’‰#1804\n\nQuasimoto - 50k - Bowl#2016\n\nQuevedo - 28k - ALEXSZYT#0432\n\nRalph Kaminski - 48.8k - CrimsonZockt#2221 |\nRalph Kaminski(alt) - 25.6k - CrimsonZockt#2221\n\nRauw Alejandro - 4.8k - GOD_Tofer#6528\n\nRigby (RVC) 500 Epoch - Hubert Paul Flatt#9804\n\nRihanna - 200k - Seif#3218 & Provindo#4444 |\nRihanna (alt) - 75k - Seif#3218 & Provindo#4444 |\nRihanna (RVC)  - ???? - Snoop Dogg#8709\n\nRingo Starr (From Beatles) - Unknown Steps - ZGLM#6250 [Beatles AI Discord]\n\nRivers Cuomo of Weezer (RVC) - 18k Steps, 140 Epoch - rthawk#1502\n\nRochy RD - 90k - Styl#6247\n\nRodrigo BarÃ£o (BarÃµes Da Pisadinha) - 8k - Dimitri#7373 (Brazilian Portuguese)\n\nRosaliÃ¡ - 35k - Styl#6247 |\nRosalia (RVC) - 1k Epoch 15k Steps - Styl#6247\n\nRose (From BLACKPINK) (RVC)- ???? - uji#8864\n\nRossa (Indonesian Singer) (RVC) - 350 Epoch - Hengky Wijaya#3599 [not quite good at high notes, at certain high note it comes lowered to the lower octave.] [350 Epoch, 20 Batch, RVC, trained in filtered voice, podcast, live performance]\n\nRoxie Wegiel (13+5 Era) - 45.6k - CrimsonZockt#2221\n\nSaiko - 13k - Smile WRLD#9877|\nSaiko - 26.4k - blaise#9999 & m1n1#7342 |\nSaiko - 55k - blaise#9999\n\nSamuel L Jackson - 30k - Thompson#2472\n\nSarah Bonito (Kero Kero Bonito KKB) - 9k - Bwib#8693\n\nSCARLXRD (RVC) - 300 Epoch - YETI#9058\n\nSean Leon - 3.15k - SamV1sion#5354\n\nSelena Gomez (RVC) - 1000 Epoch - AIVERSE#5393\n\nSematary  - 122k - kala#6494 (trained from Rainbow Bridge 1)\n\nSeulgi Red Velvet - 3.2k - Smile WRLD#9877\n\nShakira (Classic Era) - 15k - Frix#2580 |\nShakira (Modern Era) (RVC) - 19.8K - kaan36875#0001\n\nSia (RVC) - 500 Epoch - owl#1313\n\nShiloh Dynasty - 3.3k - rejekts#0820\n\nSidhu Moosewala - 10k - Puneet#6616 |\nSidhu Moose Wala (RVC) - 220 Epoch - Sukh#0648 |\nSidhu Moose Wala - 60k - Frix#2580\n\nSolar (From MAMAMOO) - 1.6k - ????\n\nSOOBIN (From TOMORROW X TOGETHER) - 46K - neoculture#4390\n\nSpongebob Squarepants (RVC) - Unkown Steps - kalomaze#2983 [1k epochs, dataset of 19 clips, trained on pm pitch method]\n\nStevie Ray Vaughan - 6.2k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\n\nStevie Wonder - 31k - clubbedsam#4419\n\nStewie Griffin (RVC) - 4.5k - Delik#0001\n\nSUGA (From BTS) - 21.6k - neoculture#4390\n\nSugarhill Ddot (RVC) - 150 Epoch - Notti Osama#1111 & dacoolkid44#4173\n\nSummer Walker - 11k - ayydot#7545 |\nSummer Walker - 400 Epoch - RomeTheDaddy#4293\n\nSZA - 21k - ayydot#7545\n\nSwae Lee - 231k - joman_g#9910\n\nTaeyeon (RVC) - 72k - baloneyboi#4232 |\nTaeyeon (FROM SNSD) - 800 Steps - Smile WRLD#9877\n\nTakanashi Kiara (Hololive EN) - 10k - dacoolkid44#4173\n\nTay-K (RVC) - 300 Epoch - Notti Osama#1111\n\nTaylor swift - 152k Steps, 7.6k Epoch - JohnnyJones#8867 [7.6k epochs at around 20 steps an epoch so 152k steps] | \nTaylor Swift - 106.4k - ???? [Not the best but it does work good with dry vocals when it comes to hitting a bit higher notes] |\nTaylor Swift (RVC) - 3.3k Epoch 101k Steps- Filthycasual#5666\n\nTF2 Team Fortress 2 Demoman (RVC) - ???? - nicegame#6990\n\nTF2 Team Fortress 2 Engineer (RVC) - ???? - nicegame#6990\n\nTF2 Team Fortress 2 Heavy (RVC) - ???? - nicegame#6990\n\nTF2 Team Fortress 2 Medic (RVC) - ???? - nicegame#6990\n\nTF2 Team Fortress 2 Scout (RVC) - ???? - nicegame#6990\n\nTF2 Team Fortress 2 Spy (RVC) - ???? - nicegame#6990\n\nThe Kid LAROI - 342k - michaell#1404 |\nThe Kid LAROI - 170k - sable#0001\n\nThe Stanley Parable [Narrator] - 4k 286 Epoch - sourcelocation#0001 |\nThe Stanley Parable [Narrator] (RVC) - 500 Epoch - jakeH#5394\n\nThe Weeknd - 94k - Maki Ligon#6713 |\nThe Weeknd v2 - 110k - lonelystar#4813 |\nThe Weeknd - 60K - lonelystar#4813 [Alt Version]\n\nThom Yorke (RVC) - 75 Epochs - ????\n\nTiago PZK - 55k - StarBoy#2512\n\nTim Maia - 319.2k - https://huggingface.co/marcoc2/so-vits-svc-4.0-models\n\nTom Waits (Raspy Voice) (RVC) - 600 Epoch 18K Steps - Disc#0287\n\nTory Lanez (RVC) - 700 Epoch - Rome#2527\n\nTravis Scott - 100k - RoddyRogu#3360 |\nTravis Scott - 77k - Snoop Dogg#8709 |\nTravis Scott (RVC) - 6720 Epoch - Snoop Dogg#8709\n\nTrippie Redd - 56k - ShadowTB#8205 [Includes a clustering model for clustering]\n\nTroye Sivan - 36k - junjuncuti3#9962\n\nTrump - 68k - joman_g#9910 |\nTrump (alt) - 18.5k - Nardicality\n\nTyler The Creator - 60k - Snoop Dogg#8709\n\nVegeta (From Dragon Ball Z) (RVC) - 4.9k Steps - nicegame#6990 [DBZ]\n\nVergil (From Devil May Cry) - 1000 Epoch - just paps#6512\n\nWendy (From Red Velvet) - 800 Steps - Smile WRLD#9877\n\nWhitney Houston - 33.6K - COMEHU#2094\n\nwill.i.am (RVC) - 3250 steps - SamV1sion#5354\n\nWill Stenson - 210k - bruhmoment#7334\n\nxQc - 25k - kyle#9690\n\nXXXTentacion - 165k - Chakras#???? |\nXXXTentacion - 55k - Angell#4859 |\nXXXTENTACION (RVC) - 150 Epoch 14k Steps - ShadowTB#8205\n\nYeat - 60k - Vision#3184  [Go to https://medium.com/@vision3/yeat-2-0-model-status-19f47994385f for updates on ver 2.0!]\n\nYeonjun (From TXT) - 24K - neoculture#4390\n\nYoko Ono (RVC) - 4k - Jay#0152\n\nYoung Leosia - 45.6k - CrimsonZockt#2221\n\nYoung Thug - 279.2k - Monki#8033 |\nYoung Thug - 153k - á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼á²¼#7280\n\nYSY A - 40k - Raidener#3810', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":499331803920,"files_count":881,"spaces_count":1,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'Other', 'approved', 78.2, '9ff3fc0e0f452c1a31790c6e414aaeeb', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-CohereLabs-aya-101', 'huggingface--coherelabs--aya-101', 'aya-101', 'CohereLabs', '--- license: apache-2.0 datasets: - CohereLabs/xP3x - CohereLabs/aya_dataset - CohereLabs/aya_collection - DataProvenanceInitiative/Commercially-Verified-Licenses - CohereLabs/aya_evaluation_suite language: - afr - amh - ara - aze - bel - ben - bul - cat - ceb - ces - cym - dan - deu - ell - eng - epo - est - eus - fin - fil - fra - fry - gla - gle - glg - guj - hat - hau - heb - hin - hun - hye - ibo - ind - isl - ita - jav - jpn - kan - kat - kaz - khm - kir - kor - kur - lao - lav - lat - ...', '["transformers","safetensors","t5","text2text-generation","afr","amh","ara","aze","bel","ben","bul","cat","ceb","ces","cym","dan","deu","ell","eng","epo","est","eus","fin","fil","fra","fry","gla","gle","glg","guj","hat","hau","heb","hin","hun","hye","ibo","ind","isl","ita","jav","jpn","kan","kat","kaz","khm","kir","kor","kur","lao","lav","lat","lit","ltz","mal","mar","mkd","mlg","mlt","mon","mri","msa","mya","nep","nld","nor","nso","nya","ory","pan","pes","pol","por","pus","ron","rus","sin","slk","slv","smo","sna","snd","som","sot","spa","sqi","srp","sun","swa","swe","tam","tel","tgk","tha","tur","twi","ukr","urd","uzb","vie","xho","yid","yor","zho","zul","dataset:coherelabs/xp3x","dataset:coherelabs/aya_dataset","dataset:coherelabs/aya_collection","dataset:coherelabs/aya_evaluation_suite","arxiv:2402.07827","license:apache-2.0","text-generation-inference","endpoints_compatible","region:us"]', 'other', 662, 1842, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/CohereLabs/aya-101","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ndatasets:\n  - CohereLabs/xP3x\n  - CohereLabs/aya_dataset\n  - CohereLabs/aya_collection\n  - DataProvenanceInitiative/Commercially-Verified-Licenses\n  - CohereLabs/aya_evaluation_suite\nlanguage:\n  - afr\n  - amh\n  - ara\n  - aze\n  - bel\n  - ben\n  - bul\n  - cat\n  - ceb\n  - ces\n  - cym\n  - dan\n  - deu\n  - ell\n  - eng\n  - epo\n  - est\n  - eus\n  - fin\n  - fil\n  - fra\n  - fry\n  - gla\n  - gle\n  - glg\n  - guj\n  - hat\n  - hau\n  - heb\n  - hin\n  - hun\n  - hye\n  - ibo\n  - ind\n  - isl\n  - ita\n  - jav\n  - jpn\n  - kan\n  - kat\n  - kaz\n  - khm\n  - kir\n  - kor\n  - kur\n  - lao\n  - lav\n  - lat\n  - lit\n  - ltz\n  - mal\n  - mar\n  - mkd\n  - mlg\n  - mlt\n  - mon\n  - mri\n  - msa\n  - mya\n  - nep\n  - nld\n  - nor\n  - nso\n  - nya\n  - ory\n  - pan\n  - pes\n  - pol\n  - por\n  - pus\n  - ron\n  - rus\n  - sin\n  - slk\n  - slv\n  - smo\n  - sna\n  - snd\n  - som\n  - sot\n  - spa\n  - sqi\n  - srp\n  - sun\n  - swa\n  - swe\n  - tam\n  - tel\n  - tgk\n  - tha\n  - tur\n  - twi\n  - ukr\n  - urd\n  - uzb\n  - vie\n  - xho\n  - yid\n  - yor\n  - zho\n  - zul\nmetrics:\n  - accuracy\n  - bleu\n---\n\n<img src="./assets/aya-fig1.png" alt="Aya model summary image" width="800" style="margin-left:''auto'' margin-right:''auto'' display:''block''"/>\n\n# Model Card for Aya 101\n\n## Model Summary\n\n> The Aya model is a massively multilingual generative language model that follows instructions in 101 languages.\n> Aya outperforms [mT0](https://huggingface.co/bigscience/mt0-xxl) and [BLOOMZ](https://huggingface.co/bigscience/bloomz) a wide variety of automatic and human evaluations despite covering double the number of languages.\n> The Aya model is trained using [xP3x](https://huggingface.co/datasets/CohereLabs/xP3x), [Aya Dataset](https://huggingface.co/datasets/CohereLabs/aya_dataset), [Aya Collection](https://huggingface.co/datasets/CohereForAI/aya_collection), a subset of [DataProvenance collection](https://huggingface.co/datasets/DataProvenanceInitiative/Commercially-Verified-Licenses) and ShareGPT-Command.\n> We release the checkpoints under a Apache-2.0 license to further our mission of multilingual technologies empowering a\n> multilingual world.\n\n- **Developed by:** [Cohere Labs](https://cohere.for.ai)\n- **Model type:** a Transformer style autoregressive massively multilingual language model.\n- **Paper**: [Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model](https://arxiv.org/abs/2402.07827)\n- **Point of Contact**: [Cohere Labs](https://cohere.for.ai)\n- **Languages**: Refer to the list of languages in the `language` section of this model card.\n- **License**: Apache-2.0\n- **Model**: [Aya-101](https://huggingface.co/CohereLabs/aya-101)\n- **Model Size**: 13 billion parameters\n- **Datasets**: [xP3x](https://huggingface.co/datasets/CohereLabs/xP3x), [Aya Dataset](https://huggingface.co/datasets/CohereLabs/aya_dataset), [Aya Collection](https://huggingface.co/datasets/CohereLabs/aya_collection), [DataProvenance collection](https://huggingface.co/datasets/DataProvenanceInitiative/Commercially-Verified-Licenses), ShareGPT-Command.\n\n## Use\n\n```python\n# pip install -q transformers\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\ncheckpoint = "CohereLabs/aya-101"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\naya_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n\n# Turkish to English translation\ntur_inputs = tokenizer.encode("Translate to English: Aya cok dilli bir dil modelidir.", return_tensors="pt")\ntur_outputs = aya_model.generate(tur_inputs, max_new_tokens=128)\nprint(tokenizer.decode(tur_outputs[0]))\n# Aya is a multi-lingual language model\n\n# Q: Why are there so many languages in India?\nhin_inputs = tokenizer.encode("à¤­à¤¾à¤°à¤¤ à¤®à¥‡à¤‚ à¤‡à¤¤à¤¨à¥€ à¤¸à¤¾à¤°à¥€ à¤­à¤¾à¤·à¤¾à¤à¤ à¤•à¥à¤¯à¥‹à¤‚ à¤¹à¥ˆà¤‚?", return_tensors="pt")\nhin_outputs = aya_model.generate(hin_inputs, max_new_tokens=128)\nprint(tokenizer.decode(hin_outputs[0]))\n# Expected output: à¤­à¤¾à¤°à¤¤ à¤®à¥‡à¤‚ à¤•à¤ˆ à¤­à¤¾à¤·à¤¾à¤à¤ à¤¹à¥ˆà¤‚ à¤”à¤° à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨ à¤­à¤¾à¤·à¤¾à¤“à¤‚ à¤•à¥‡ à¤¬à¥‹à¤²à¥€ à¤œà¤¾à¤¨à¥‡ à¤µà¤¾à¤²à¥‡ à¤²à¥‹à¤— à¤¹à¥ˆà¤‚à¥¤ à¤¯à¤¹ à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨à¤¤à¤¾ à¤­à¤¾à¤·à¤¾à¤ˆ à¤µà¤¿à¤µà¤¿à¤§à¤¤à¤¾ à¤”à¤° à¤¸à¤¾à¤‚à¤¸à¥à¤•à¥ƒà¤¤à¤¿à¤• à¤µà¤¿à¤µà¤¿à¤§à¤¤à¤¾ à¤•à¤¾ à¤ªà¤°à¤¿à¤£à¤¾à¤® à¤¹à¥ˆà¥¤ Translates to "India has many languages and people speaking different languages. This diversity is the result of linguistic diversity and cultural diversity."\n\n```\n\n## Model Details\n\n### Finetuning\n\n- Architecture: Same as [mt5-xxl](https://huggingface.co/google/mt5-xxl)\n- Number of Samples seen during Finetuning: 25M\n- Batch size: 256\n- Hardware: TPUv4-128\n- Software: T5X, Jax\n\n### Data Sources\n\nThe Aya model is trained on the following datasets:\n\n- [xP3x](https://huggingface.co/datasets/CohereLabs/xP3x)\n- [Aya Dataset](https://huggingface.co/datasets/CohereLabs/aya_dataset)\n- [Aya Collection](https://huggingface.co/datasets/CohereLabs/aya_collection)\n- [DataProvenance collection](https://huggingface.co/datasets/DataProvenanceInitiative/Commercially-Verified-Licenses)\n- ShareGPT-Command\n\nAll datasets are subset to the 101 languages supported by [mT5](https://huggingface.co/google/mt5-xxl). See the [paper](https://arxiv.org/abs/2402.07827) for details about filtering and pruning.\n\n## Evaluation\n\nWe refer to Section 5 from our paper for multilingual eval across 99 languages â€“ including discriminative and generative tasks, human evaluation, and simulated win rates that cover both held-out tasks and in-distribution performance.\n\n## Bias, Risks, and Limitations\n\n\nFor a detailed overview of our effort at safety mitigation and benchmarking toxicity and bias across multiple languages, we refer to Sections 6 and 7 of our paper: [Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model](https://arxiv.org/abs/2402.07827).\n\nWe hope that the release of the Aya model will make community-based redteaming efforts possible, by exposing an open-source massively-multilingual model for community research.\n\n## Citation\n\n**BibTeX:**\n\n```\n@article{Ã¼stÃ¼n2024aya,\n  title={Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model},\n  author={Ahmet ÃœstÃ¼n and Viraat Aryabumi and Zheng-Xin Yong and Wei-Yin Ko and Daniel D''souza and Gbemileke Onilude and Neel Bhandari and Shivalika Singh and Hui-Lee Ooi and Amr Kayid and Freddie Vargus and Phil Blunsom and Shayne Longpre and Niklas Muennighoff and Marzieh Fadaee and Julia Kreutzer and Sara Hooker},\n  journal={arXiv preprint arXiv:2402.07827},\n  year={2024}\n}\n```\n\n## Languages Covered\n\n<details>\n<summary>Click to see Languages Covered</summary>\n\nBelow is the list of languages used in finetuning the Aya Model. We group languages into higher-, mid-, and lower-resourcedness based on a language classification by [Joshi et. al, 2020](https://microsoft.github.io/linguisticdiversity/). For further details, we refer to our [paper](https://arxiv.org/abs/2402.07827)\n\n| ISO Code | Language Name   |    Script    |     Family      |    Subgrouping    | Resourcedness |\n| :------- | :-------------- | :----------: | :-------------: | :---------------: | :-----------: |\n| afr      | Afrikaans       |    Latin     |  Indo-European  |     Germanic      |      Mid      |\n| amh      | Amharic         |    Ge''ez     |  Afro-Asiatic   |      Semitic      |      Low      |\n| ara      | Arabic          |    Arabic    |  Afro-Asiatic   |      Semitic      |     High      |\n| aze      | Azerbaijani     | Arabic/Latin |     Turkic      |   Common Turkic   |      Low      |\n| bel      | Belarusian      |   Cyrillic   |  Indo-European  |   Balto-Slavic    |      Mid      |\n| ben      | Bengali         |   Bengali    |  Indo-European  |    Indo-Aryan     |      Mid      |\n| bul      | Bulgarian       |   Cyrillic   |  Indo-European  |   Balto-Slavic    |      Mid      |\n| cat      | Catalan         |    Latin     |  Indo-European  |      Italic       |     High      |\n| ceb      | Cebuano         |    Latin     |  Austronesian   | Malayo-Polynesian |      Mid      |\n| ces      | Czech           |    Latin     |  Indo-European  |   Balto-Slavic    |     High      |\n| cym      | Welsh           |    Latin     |  Indo-European  |      Celtic       |      Low      |\n| dan      | Danish          |    Latin     |  Indo-European  |     Germanic      |      Mid      |\n| deu      | German          |    Latin     |  Indo-European  |     Germanic      |     High      |\n| ell      | Greek           |    Greek     |  Indo-European  |  Graeco-Phrygian  |      Mid      |\n| eng      | English         |    Latin     |  Indo-European  |     Germanic      |     High      |\n| epo      | Esperanto       |    Latin     |   Constructed   |    Esperantic     |      Low      |\n| est      | Estonian        |    Latin     |     Uralic      |      Finnic       |      Mid      |\n| eus      | Basque          |    Latin     |     Basque      |         -         |     High      |\n| fin      | Finnish         |    Latin     |     Uralic      |      Finnic       |     High      |\n| fil      | Tagalog         |    Latin     |  Austronesian   | Malayo-Polynesian |      Mid      |\n| fra      | French          |    Latin     |  Indo-European  |      Italic       |     High      |\n| fry      | Western Frisian |    Latin     |  Indo-European  |     Germanic      |      Low      |\n| gla      | Scottish Gaelic |    Latin     |  Indo-European  |      Celtic       |      Low      |\n| gle      | Irish           |    Latin     |  Indo-European  |      Celtic       |      Low      |\n| glg      | Galician        |    Latin     |  Indo-European  |      Italic       |      Mid      |\n| guj      | Gujarati        |   Gujarati   |  Indo-European  |    Indo-Aryan     |      Low      |\n| hat      | Haitian Creole  |    Latin     |  Indo-European  |      Italic       |      Low      |\n| hau      | Hausa           |    Latin     |  Afro-Asiatic   |      Chadic       |      Low      |\n| heb      | Hebrew          |    Hebrew    |  Afro-Asiatic   |      Semitic      |      Mid      |\n| hin      | Hindi           |  Devanagari  |  Indo-European  |    Indo-Aryan     |     High      |\n| hun      | Hungarian       |    Latin     |     Uralic      |         -         |     High      |\n| hye      | Armenian        |   Armenian   |  Indo-European  |      Armenic      |      Low      |\n| ibo      | Igbo            |    Latin     | Atlantic-Congo  |    Benue-Congo    |      Low      |\n| ind      | Indonesian      |    Latin     |  Austronesian   | Malayo-Polynesian |      Mid      |\n| isl      | Icelandic       |    Latin     |  Indo-European  |     Germanic      |      Low      |\n| ita      | Italian         |    Latin     |  Indo-European  |      Italic       |     High      |\n| jav      | Javanese        |    Latin     |  Austronesian   | Malayo-Polynesian |      Low      |\n| jpn      | Japanese        |   Japanese   |     Japonic     |     Japanesic     |     High      |\n| kan      | Kannada         |   Kannada    |    Dravidian    |  South Dravidian  |      Low      |\n| kat      | Georgian        |   Georgian   |   Kartvelian    |   Georgian-Zan    |      Mid      |\n| kaz      | Kazakh          |   Cyrillic   |     Turkic      |   Common Turkic   |      Mid      |\n| khm      | Khmer           |    Khmer     |  Austroasiatic  |      Khmeric      |      Low      |\n| kir      | Kyrgyz          |   Cyrillic   |     Turkic      |   Common Turkic   |      Low      |\n| kor      | Korean          |    Hangul    |    Koreanic     |      Korean       |     High      |\n| kur      | Kurdish         |    Latin     |  Indo-European  |      Iranian      |      Low      |\n| lao      | Lao             |     Lao      |    Tai-Kadai    |      Kam-Tai      |      Low      |\n| lav      | Latvian         |    Latin     |  Indo-European  |   Balto-Slavic    |      Mid      |\n| lat      | Latin           |    Latin     |  Indo-European  |      Italic       |      Mid      |\n| lit      | Lithuanian      |    Latin     |  Indo-European  |   Balto-Slavic    |      Mid      |\n| ltz      | Luxembourgish   |    Latin     |  Indo-European  |     Germanic      |      Low      |\n| mal      | Malayalam       |  Malayalam   |    Dravidian    |  South Dravidian  |      Low      |\n| mar      | Marathi         |  Devanagari  |  Indo-European  |    Indo-Aryan     |      Low      |\n| mkd      | Macedonian      |   Cyrillic   |  Indo-European  |   Balto-Slavic    |      Low      |\n| mlg      | Malagasy        |    Latin     |  Austronesian   | Malayo-Polynesian |      Low      |\n| mlt      | Maltese         |    Latin     |  Afro-Asiatic   |      Semitic      |      Low      |\n| mon      | Mongolian       |   Cyrillic   | Mongolic-Khitan |     Mongolic      |      Low      |\n| mri      | Maori           |    Latin     |  Austronesian   | Malayo-Polynesian |      Low      |\n| msa      | Malay           |    Latin     |  Austronesian   | Malayo-Polynesian |      Mid      |\n| mya      | Burmese         |   Myanmar    |  Sino-Tibetan   |   Burmo-Qiangic   |      Low      |\n| nep      | Nepali          |  Devanagari  |  Indo-European  |    Indo-Aryan     |      Low      |\n| nld      | Dutch           |    Latin     |  Indo-European  |     Germanic      |     High      |\n| nor      | Norwegian       |    Latin     |  Indo-European  |     Germanic      |      Low      |\n| nso      | Northern Sotho  |    Latin     | Atlantic-Congo  |    Benue-Congo    |      Low      |\n| nya      | Chichewa        |    Latin     | Atlantic-Congo  |    Benue-Congo    |      Low      |\n| ory      | Oriya           |    Oriya     |  Indo-European  |    Indo-Aryan     |      Low      |\n| pan      | Punjabi         |   Gurmukhi   |  Indo-European  |    Indo-Aryan     |      Low      |\n| pes      | Persian         |    Arabic    |  Indo-European  |      Iranian      |     High      |\n| pol      | Polish          |    Latin     |  Indo-European  |   Balto-Slavic    |     High      |\n| por      | Portuguese      |    Latin     |  Indo-European  |      Italic       |     High      |\n| pus      | Pashto          |    Arabic    |  Indo-European  |      Iranian      |      Low      |\n| ron      | Romanian        |    Latin     |  Indo-European  |      Italic       |      Mid      |\n| rus      | Russian         |   Cyrillic   |  Indo-European  |   Balto-Slavic    |     High      |\n| sin      | Sinhala         |   Sinhala    |  Indo-European  |    Indo-Aryan     |      Low      |\n| slk      | Slovak          |    Latin     |  Indo-European  |   Balto-Slavic    |      Mid      |\n| slv      | Slovenian       |    Latin     |  Indo-European  |   Balto-Slavic    |      Mid      |\n| smo      | Samoan          |    Latin     |  Austronesian   | Malayo-Polynesian |      Low      |\n| sna      | Shona           |    Latin     |  Indo-European  |    Indo-Aryan     |      Low      |\n| snd      | Sindhi          |    Arabic    |  Indo-European  |    Indo-Aryan     |      Low      |\n| som      | Somali          |    Latin     |  Afro-Asiatic   |     Cushitic      |      Low      |\n| sot      | Southern Sotho  |    Latin     | Atlantic-Congo  |    Benue-Congo    |      Low      |\n| spa      | Spanish         |    Latin     |  Indo-European  |      Italic       |     High      |\n| sqi      | Albanian        |    Latin     |  Indo-European  |     Albanian      |      Low      |\n| srp      | Serbian         |   Cyrillic   |  Indo-European  |   Balto-Slavic    |     High      |\n| sun      | Sundanese       |    Latin     |  Austronesian   | Malayo-Polynesian |      Low      |\n| swa      | Swahili         |    Latin     | Atlantic-Congo  |    Benue-Congo    |      Low      |\n| swe      | Swedish         |    Latin     |  Indo-European  |     Germanic      |     High      |\n| tam      | Tamil           |    Tamil     |    Dravidian    |  South Dravidian  |      Mid      |\n| tel      | Telugu          |    Telugu    |    Dravidian    |  South Dravidian  |      Low      |\n| tgk      | Tajik           |   Cyrillic   |  Indo-European  |      Iranian      |      Low      |\n| tha      | Thai            |     Thai     |    Tai-Kadai    |      Kam-Tai      |      Mid      |\n| tur      | Turkish         |    Latin     |     Turkic      |   Common Turkic   |     High      |\n| twi      | Twi             |    Latin     | Atlantic-Congo  |    Niger-Congo    |      Low      |\n| ukr      | Ukrainian       |   Cyrillic   |  Indo-European  |   Balto-Slavic    |      Mid      |\n| urd      | Urdu            |    Arabic    |  Indo-European  |    Indo-Aryan     |      Mid      |\n| uzb      | Uzbek           |    Latin     |     Turkic      |   Common Turkic   |      Mid      |\n| vie      | Vietnamese      |    Latin     |  Austroasiatic  |      Vietic       |     High      |\n| xho      | Xhosa           |    Latin     | Atlantic-Congo  |    Benue-Congo    |      Low      |\n| yid      | Yiddish         |    Hebrew    |  Indo-European  |     Germanic      |      Low      |\n| yor      | Yoruba          |    Latin     | Atlantic-Congo  |    Benue-Congo    |      Low      |\n| zho      | Chinese         |     Han      |  Sino-Tibetan   |      Sinitic      |     High      |\n| zul      | Zulu            |    Latin     | Atlantic-Congo  |    Benue-Congo    |      Low      |\n</details>\n\n## Model Card Contact\n\nFor errors in this model card, contact Ahmet or Viraat, `{ahmet, viraat} at cohere dot com`.\n', '{"pipeline_tag":null,"library_name":"transformers","framework":"transformers","params":12921057280,"storage_bytes":51703138340,"files_count":20,"spaces_count":21,"gated":false,"private":false,"config":{"architectures":["T5ForConditionalGeneration"],"model_type":"t5","tokenizer_config":{"eos_token":"</s>","pad_token":"<pad>","unk_token":"<unk>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2402.07827","source_url":"https://arxiv.org/abs/2402.07827"}]', NULL, 'Apache-2.0', 'approved', 98.2, '26e943d52cc690c49bfaef6c67995de4', NULL, 'https://huggingface.co/CohereLabs/aya-101/resolve/main/assets/aya-fig1.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-CohereLabs-aya-101 from https://huggingface.co/CohereLabs/aya-101/resolve/main/assets/aya-fig1.png
Image converted to WebP: data/images/huggingface-CohereLabs-aya-101.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-pyannote-segmentation', 'huggingface--pyannote--segmentation', 'segmentation', 'pyannote', '', '["pyannote-audio","pytorch","pyannote","pyannote-audio-model","audio","voice","speech","speaker","speaker-segmentation","voice-activity-detection","overlapped-speech-detection","resegmentation","arxiv:2104.04045","license:mit","region:us"]', 'voice-activity-detection', 659, 1787067, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/pyannote/segmentation","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"voice-activity-detection","library_name":"pyannote-audio","framework":"pyannote-audio","params":null,"storage_bytes":101988255,"files_count":28,"spaces_count":74,"gated":"auto","private":false,"config":null}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2104.04045","source_url":"https://arxiv.org/abs/2104.04045"}]', NULL, 'MIT', 'approved', 38.2, '2ecbee1fcdb4eb276f6122a16403afc9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-uwg-upscaler', 'huggingface--uwg--upscaler', 'upscaler', 'uwg', '--- tags: - Upscalers model-index: - name: Upscalers results: [] language: - en license: mit --- <img src="https://public.bnbstatic.com/image/cms/crawler/METAVERSEPOST_NEWS/image-68-16-1024x683.jpg" alt="Zephyr Logo" width="800" style="margin-left:''auto'' margin-right:''auto'' display:''block''"/> The models they found here taken from the community OpenModelDB is a community driven database of AI Upscaling models. openmodeldb', '["onnx","upscalers","en","license:mit","region:us"]', 'other', 658, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/uwg/upscaler","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- Upscalers\nmodel-index:\n- name: Upscalers\n  results: []\nlanguage:\n- en\nlicense: mit\n---\n\n\n<img src="https://public.bnbstatic.com/image/cms/crawler/METAVERSEPOST_NEWS/image-68-16-1024x683.jpg" alt="Zephyr Logo" width="800" style="margin-left:''auto'' margin-right:''auto'' display:''block''"/>\n\n\nThe models they found here taken from the community OpenModelDB is a community driven database of AI Upscaling models. [openmodeldb](https://openmodeldb.info/)\n\n', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":12470142958,"files_count":96,"spaces_count":2,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'MIT', 'approved', 38.2, 'd77a108ceaa1e69422e00fded5349258', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-stable-code-3b', 'huggingface--stabilityai--stable-code-3b', 'stable-code-3b', 'stabilityai', '--- license: other datasets: - tiiuae/falcon-refinedweb - bigcode/the-stack-github-issues - bigcode/commitpackft - bigcode/starcoderdata - EleutherAI/proof-pile-2 - meta-math/MetaMathQA language: - en tags: - causal-lm - code metrics: - code_eval library_name: transformers model-index: - name: stabilityai/stable-code-3b results: - task: type: text-generation dataset: type: nuprl/MultiPL-E name: MultiPL-HumanEval (Python) metrics: - name: pass@1 type: pass@1 value: 32.4 verified: false - task:...', '["transformers","safetensors","gguf","stablelm","text-generation","causal-lm","code","en","dataset:tiiuae/falcon-refinedweb","dataset:bigcode/the-stack-github-issues","dataset:bigcode/commitpackft","dataset:bigcode/starcoderdata","dataset:eleutherai/proof-pile-2","dataset:meta-math/metamathqa","arxiv:2307.09288","arxiv:2104.09864","arxiv:2204.06745","arxiv:2305.06161","arxiv:2310.10631","arxiv:2309.12284","arxiv:1910.02054","license:other","model-index","endpoints_compatible","region:us"]', 'text-generation', 658, 6055, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/stable-code-3b","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: other\ndatasets:\n- tiiuae/falcon-refinedweb\n- bigcode/the-stack-github-issues\n- bigcode/commitpackft\n- bigcode/starcoderdata\n- EleutherAI/proof-pile-2\n- meta-math/MetaMathQA\nlanguage:\n- en\ntags:\n- causal-lm\n- code\nmetrics:\n- code_eval\nlibrary_name: transformers\nmodel-index:\n- name: stabilityai/stable-code-3b\n  results:\n  - task:\n      type: text-generation\n    dataset:\n      type: nuprl/MultiPL-E\n      name: MultiPL-HumanEval (Python)\n    metrics:\n    - name: pass@1\n      type: pass@1\n      value: 32.4\n      verified: false\n  - task:\n      type: text-generation\n    dataset:\n      type: nuprl/MultiPL-E\n      name: MultiPL-HumanEval (C++)\n    metrics:\n    - name: pass@1\n      type: pass@1\n      value: 30.9\n      verified: false\n  - task:\n      type: text-generation\n    dataset:\n      type: nuprl/MultiPL-E\n      name: MultiPL-HumanEval (Java)\n    metrics:\n    - name: pass@1\n      type: pass@1\n      value: 32.1\n      verified: false\n  - task:\n      type: text-generation\n    dataset:\n      type: nuprl/MultiPL-E\n      name: MultiPL-HumanEval (JavaScript)\n    metrics:\n    - name: pass@1\n      type: pass@1\n      value: 32.1\n      verified: false\n  - task:\n      type: text-generation\n    dataset:\n      type: nuprl/MultiPL-E\n      name: MultiPL-HumanEval (PHP)\n    metrics:\n    - name: pass@1\n      type: pass@1\n      value: 24.2\n      verified: false\n  - task:\n      type: text-generation\n    dataset:\n      type: nuprl/MultiPL-E\n      name: MultiPL-HumanEval (Rust)\n    metrics:\n    - name: pass@1\n      type: pass@1\n      value: 23.0\n      verified: false\n---\n# `stable-code-3b`\n\nPlease note: For commercial use, please refer to https://stability.ai/license.\n\n## Model Description\n\n`stable-code-3b` is a 2.7B billion parameter decoder-only language model pre-trained on 1.3 trillion tokens of diverse textual and code datasets. `stable-code-3b` is trained on 18 programming languages (selected based on the 2023 StackOverflow Developer Survey) and demonstrates state-of-the-art performance (compared to models of similar size) on the MultiPL-E metrics across multiple programming languages tested using [BigCode''s Evaluation Harness](https://github.com/bigcode-project/bigcode-evaluation-harness/tree/main).\n\n![spiderchart](stable_code_3b_spiderchart.svg)\n\n| Model            | Size | Python | C++  | Javascript | Java | PHP  | Rust |\n|------------------|------|--------|------|------------|------|------|------|\n| **Stable Code**  | 3B   | 32.4%  | 30.9%| 32.1%      | 32.1%| 24.2%| 23.0%|\n| CodeLLama        | 7B   | 30.0%  | 28.2%| 32.5%      | 31.1%| 25.7%| 26.3%|\n| Deepseek Coder   | 1.3B | 28.6%  | 29.2%| 28.7%      | 29.0%| 23.6%| 18.5%|\n| Wizard Coder     | 3B   | 31.6%  | 25.6%| 26.2%      | 25.8%| 25.3%| 20.4%|\n| StarCoder        | 3B   | 21.6%  | 19.8%| 21.5%      | 20.5%| 19.0%| 16.9%|\n| Replit Code V1.5 | 3B   | 23.0%  | 25.9%| 26.2%      | 23.6%| 23.2%| 21.5%|\n| Deci Coder       | 1B   | 19.1%  | 6.8% | 18.4%      | 16.7%| 2.1% | 1.7% |\n\n**Key Features**\n* Fill in Middle Capability (FIM)\n* Supports Long Context, trained with Sequences upto 16,384\n\n## Usage\n\nGet started generating text with `stable-code-3b` by using the following code snippet:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained("stabilityai/stable-code-3b")\nmodel = AutoModelForCausalLM.from_pretrained(\n  "stabilityai/stable-code-3b",\n  torch_dtype="auto",\n)\nmodel.cuda()\ninputs = tokenizer("import torch\nimport torch.nn as nn", return_tensors="pt").to(model.device)\ntokens = model.generate(\n  **inputs,\n  max_new_tokens=48,\n  temperature=0.2,\n  do_sample=True,\n)\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\n```\n\n### Run with Fill in Middle (FIM) âš¡ï¸\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained("stabilityai/stable-code-3b")\nmodel = AutoModelForCausalLM.from_pretrained(\n  "stabilityai/stable-code-3b",\n  torch_dtype="auto",\n  attn_implementation="flash_attention_2",\n)\nmodel.cuda()\ninputs = tokenizer("<fim_prefix>def fib(n):<fim_suffix>    else:\n        return fib(n - 2) + fib(n - 1)<fim_middle>", return_tensors="pt").to(model.device)\ntokens = model.generate(\n  **inputs,\n  max_new_tokens=48,\n  temperature=0.2,\n  do_sample=True,\n)\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\n```\n\n</details>\n\n### Run with Flash Attention 2 âš¡ï¸\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained("stabilityai/stable-code-3b", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n  "stabilityai/stable-code-3b",\n  trust_remote_code=True,\n  torch_dtype="auto",\n+ attn_implementation="flash_attention_2",\n)\nmodel.cuda()\ninputs = tokenizer("import torch\nimport torch.nn as nn", return_tensors="pt").to(model.device)\ntokens = model.generate(\n  **inputs,\n  max_new_tokens=48,\n  temperature=0.2,\n  do_sample=True,\n)\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\n```\n\n</details>\n\n\n## Model Details\n\n* **Developed by**: [Stability AI](https://stability.ai/)\n* **Model type**: `stable-code-3b` models are auto-regressive language models based on the transformer decoder architecture.\n* **Language(s)**: English, Code\n* **Library**: [GPT-NeoX](https://github.com/EleutherAI/gpt-neox)\n* **License**: Stability AI Community License.\n* **Commercial License**: to use this model commercially, please refer to https://stability.ai/license\n* **Contact**: For questions and comments about the model, please email `lm@stability.ai`\n\n### Model Architecture\n\nThe model is a decoder-only transformer similar to the LLaMA ([Touvron et al., 2023](https://arxiv.org/abs/2307.09288)) architecture with the following modifications:\n\n| Parameters     | Hidden Size | Layers | Heads | Sequence Length |\n|----------------|-------------|--------|-------|-----------------|\n| 2,796,431,360  | 2560        | 32     | 32    | 16384            |\n\n* **Position Embeddings**: Rotary Position Embeddings ([Su et al., 2021](https://arxiv.org/abs/2104.09864)) applied to the first 25% of head embedding dimensions for improved throughput following [Black et al. (2022)](https://arxiv.org/pdf/2204.06745.pdf).\n* **Tokenizer**: We use a modified version of the GPTNeoX Tokenizer.[`NeoX`](https://github.com/EleutherAI/gpt-neox). We add special tokens to train for Fill in the Middle (FIM) capabilities like `<FIM_PREFIX>` and `<FIM_SUFFIX>` along with other special tokens.\n\n## Training\n\n### Training Dataset\n\nThe dataset is comprised of a filtered mixture of open-source large-scale datasets available on the [HuggingFace Hub](https://huggingface.co/datasets): Falcon RefinedWeb extract ([Penedo et al., 2023](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)), along with [CommitPackFT](https://huggingface.co/datasets/bigcode/commitpackft) and [Github Issues](https://huggingface.co/datasets/bigcode/the-stack-github-issues) (BigCode., 2023), and StarCoder ([Li et al., 2023](https://arxiv.org/abs/2305.06161)). We further supplement our training with data from mathematical domains ([Azerbayev, Zhangir, et al., 2023](https://arxiv.org/abs/2310.10631) and, [Yu, Longhui, et al., 2023](https://arxiv.org/abs/2309.12284)). \n\nTop 18 programming languages trained on:\n- C\n- CPP\n- Java\n- JavaScript\n- CSS\n- Go\n- HTML\n- Ruby\n- Rust\n- Markdown\n- Shell\n- Php\n- Sql\n- R\n- Typescript\n- Python\n- Jupyter-Clean\n- RestructuredText\n\n### Training Procedure\n\nThe model is pre-trained on the aforementioned datasets in `bfloat16` precision, optimized with AdamW.\n\n### Training Infrastructure\n\n* **Hardware**: `stable-code-3b` was trained on the Stability AI cluster across 256 NVIDIA A100 40GB GPUs (AWS P4d instances).\n\n* **Software**: We use a fork of `gpt-neox` ([EleutherAI, 2021](https://github.com/EleutherAI/gpt-neox)), train under 2D parallelism (Data and Tensor Parallel) with ZeRO-1 ([Rajbhandari et al., 2019](https://arxiv.org/abs/1910.02054v3)), and rely on flash-attention as well as SwiGLU and Rotary Embedding kernels from FlashAttention-2 ([Dao et al., 2023](https://tridao.me/publications/flash2/flash2.pdf))\n\n## Use and Limitations\n\n### Intended Use\n\nThe model is intended to be used as a foundational base model for application-specific fine-tuning. Developers must evaluate and fine-tune the model for safe performance in downstream applications. For commercial use, please refer to https://stability.ai/license.\n\n### Limitations and Bias\nâ€‹\nAs a base model, this model may exhibit unreliable, unsafe, or other undesirable behaviors that must be corrected through evaluation and fine-tuning prior to deployment. The pre-training dataset may have contained offensive or inappropriate content, even after applying data cleansing filters, which can be reflected in the model-generated text. We recommend that users exercise caution when using these models in production systems. Do not use the models if they are unsuitable for your application, or for any applications that may cause deliberate or unintentional harm to others.\n\n## How to Cite\n\n```bibtex\n@misc{stable-code-3b,\n      url={[https://huggingface.co/stabilityai/stable-code-3b](https://huggingface.co/stabilityai/stable-code-3b)},\n      title={Stable Code 3B},\n      author={Pinnaparaju, Nikhil and Adithyan, Reshinth and Phung, Duy and Tow, Jonathan and Baicoianu, James and Cooper, Nathan}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":2795443200,"storage_bytes":15473643632,"files_count":17,"spaces_count":31,"gated":false,"private":false,"config":{"architectures":["StableLmForCausalLM"],"model_type":"stablelm","tokenizer_config":{"bos_token":"<|endoftext|>","eos_token":"<|endoftext|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:bigcode-project:bigcode-evaluation-harness","source_url":"https://github.com/bigcode-project/bigcode-evaluation-harness"},{"type":"has_code","target_id":"github:EleutherAI:gpt-neox","source_url":"https://github.com/EleutherAI/gpt-neox"},{"type":"has_code","target_id":"github:EleutherAI:gpt-neox","source_url":"https://github.com/EleutherAI/gpt-neox"},{"type":"has_code","target_id":"github:EleutherAI:gpt-neox","source_url":"https://github.com/EleutherAI/gpt-neox"},{"type":"based_on_paper","target_id":"arxiv:2307.09288","source_url":"https://arxiv.org/abs/2307.09288"},{"type":"based_on_paper","target_id":"arxiv:2104.09864","source_url":"https://arxiv.org/abs/2104.09864"},{"type":"based_on_paper","target_id":"arxiv:2204.06745","source_url":"https://arxiv.org/abs/2204.06745"},{"type":"based_on_paper","target_id":"arxiv:2305.06161","source_url":"https://arxiv.org/abs/2305.06161"},{"type":"based_on_paper","target_id":"arxiv:2310.10631","source_url":"https://arxiv.org/abs/2310.10631"},{"type":"based_on_paper","target_id":"arxiv:2309.12284","source_url":"https://arxiv.org/abs/2309.12284"},{"type":"based_on_paper","target_id":"arxiv:1910.02054","source_url":"https://arxiv.org/abs/1910.02054"}]', NULL, 'Other', 'approved', 63.2, '5408e424f9859c1aeff4c6b5bf7baaf0', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-zai-org-CogVideoX-5b', 'huggingface--zai-org--cogvideox-5b', 'CogVideoX-5b', 'zai-org', '--- license: other license_link: https://huggingface.co/THUDM/CogVideoX-5b/blob/main/LICENSE language: - en tags: - cogvideox - video-generation - thudm - text-to-video inference: false --- <p style="text-align: center;"> <div align="center"> <img src=https://github.com/THUDM/CogVideo/raw/main/resources/logo.svg width="50%"/> </div> <p align="center"> <a href="https://huggingface.co/THUDM/CogVideoX-5b/blob/main/README_zh.md">ðŸ“„ ä¸­æ–‡é˜…è¯»</a> | <a href="https://huggingface.co/spaces/THUDM/CogVideoX...', '["diffusers","safetensors","cogvideox","video-generation","thudm","text-to-video","en","arxiv:2408.06072","license:other","diffusers:cogvideoxpipeline","region:us"]', 'text-to-video', 655, 34605, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/zai-org/CogVideoX-5b","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_link: https://huggingface.co/THUDM/CogVideoX-5b/blob/main/LICENSE\nlanguage:\n  - en\ntags:\n  - cogvideox\n  - video-generation\n  - thudm\n  - text-to-video\ninference: false\n---\n\n# CogVideoX-5B\n\n<p style="text-align: center;">\n  <div align="center">\n  <img src=https://github.com/THUDM/CogVideo/raw/main/resources/logo.svg width="50%"/>\n  </div>\n  <p align="center">\n  <a href="https://huggingface.co/THUDM/CogVideoX-5b/blob/main/README_zh.md">ðŸ“„ ä¸­æ–‡é˜…è¯»</a> | \n  <a href="https://huggingface.co/spaces/THUDM/CogVideoX-5B-Space">ðŸ¤— Huggingface Space</a> |\n  <a href="https://github.com/THUDM/CogVideo">ðŸŒ Github </a> | \n  <a href="https://arxiv.org/pdf/2408.06072">ðŸ“œ arxiv </a>\n</p>\n<p align="center">\nðŸ“ Visit <a href="https://chatglm.cn/video?lang=en?fr=osm_cogvideo">QingYing</a> and <a href="https://open.bigmodel.cn/?utm_campaign=open&_channel_track_key=OWTVNma9">API Platform</a> to experience commercial video generation models.\n</p>\n\n## Demo Show\n\n<!DOCTYPE html>\n<html lang="en">\n<head>\n    <meta charset="UTF-8">\n    <meta name="viewport" content="width=device-width, initial-scale=1.0">\n    <title>Video Gallery with Captions</title>\n    <style>\n        .video-container {\n            display: flex;\n            flex-wrap: wrap;\n            justify-content: space-around;\n        }\n        .video-item {\n            width: 45%;\n            margin-bottom: 20px;\n            transition: transform 0.3s;\n        }\n        .video-item:hover {\n            transform: scale(1.1);\n        }\n        .caption {\n            text-align: center;\n            margin-top: 10px;\n            font-size: 11px;\n        }\n    </style>\n</head>\n<body>\n    <div class="video-container">\n        <div class="video-item">\n            <video width="100%" controls>\n                <source src="https://github.com/user-attachments/assets/cf5953ea-96d3-48fd-9907-c4708752c714" type="video/mp4">\n            </video>\n            <div class="caption">A garden comes to life as a kaleidoscope of butterflies flutters amidst the blossoms, their delicate wings casting shadows on the petals below. In the background, a grand fountain cascades water with a gentle splendor, its rhythmic sound providing a soothing backdrop. Beneath the cool shade of a mature tree, a solitary wooden chair invites solitude and reflection, its smooth surface worn by the touch of countless visitors seeking a moment of tranquility in nature''s embrace.</div>\n        </div>\n        <div class="video-item">\n            <video width="100%" controls>\n                <source src="https://github.com/user-attachments/assets/fe0a78e6-b669-4800-8cf0-b5f9b5145b52" type="video/mp4">\n            </video>\n            <div class="caption">A small boy, head bowed and determination etched on his face, sprints through the torrential downpour as lightning crackles and thunder rumbles in the distance. The relentless rain pounds the ground, creating a chaotic dance of water droplets that mirror the dramatic sky''s anger. In the far background, the silhouette of a cozy home beckons, a faint beacon of safety and warmth amidst the fierce weather. The scene is one of perseverance and the unyielding spirit of a child braving the elements.</div>\n        </div>\n        <div class="video-item">\n            <video width="100%" controls>\n                <source src="https://github.com/user-attachments/assets/c182f606-8f8c-421d-b414-8487070fcfcb" type="video/mp4">\n            </video>\n            <div class="caption">A suited astronaut, with the red dust of Mars clinging to their boots, reaches out to shake hands with an alien being, their skin a shimmering blue, under the pink-tinged sky of the fourth planet. In the background, a sleek silver rocket, a beacon of human ingenuity, stands tall, its engines powered down, as the two representatives of different worlds exchange a historic greeting amidst the desolate beauty of the Martian landscape.</div>\n        </div>\n        <div class="video-item">\n            <video width="100%" controls>\n                <source src="https://github.com/user-attachments/assets/7db2bbce-194d-434d-a605-350254b6c298" type="video/mp4">\n            </video>\n            <div class="caption">An elderly gentleman, with a serene expression, sits at the water''s edge, a steaming cup of tea by his side. He is engrossed in his artwork, brush in hand, as he renders an oil painting on a canvas that''s propped up against a small, weathered table. The sea breeze whispers through his silver hair, gently billowing his loose-fitting white shirt, while the salty air adds an intangible element to his masterpiece in progress. The scene is one of tranquility and inspiration, with the artist''s canvas capturing the vibrant hues of the setting sun reflecting off the tranquil sea.</div>\n        </div>\n        <div class="video-item">\n            <video width="100%" controls>\n                <source src="https://github.com/user-attachments/assets/62b01046-8cab-44cc-bd45-4d965bb615ec" type="video/mp4">\n            </video>\n            <div class="caption">In a dimly lit bar, purplish light bathes the face of a mature man, his eyes blinking thoughtfully as he ponders in close-up, the background artfully blurred to focus on his introspective expression, the ambiance of the bar a mere suggestion of shadows and soft lighting.</div>\n        </div>\n        <div class="video-item">\n            <video width="100%" controls>\n                <source src="https://github.com/user-attachments/assets/d78e552a-4b3f-4b81-ac3f-3898079554f6" type="video/mp4">\n            </video>\n            <div class="caption">A golden retriever, sporting sleek black sunglasses, with its lengthy fur flowing in the breeze, sprints playfully across a rooftop terrace, recently refreshed by a light rain. The scene unfolds from a distance, the dog''s energetic bounds growing larger as it approaches the camera, its tail wagging with unrestrained joy, while droplets of water glisten on the concrete behind it. The overcast sky provides a dramatic backdrop, emphasizing the vibrant golden coat of the canine as it dashes towards the viewer.</div>\n        </div>\n        <div class="video-item">\n            <video width="100%" controls>\n                <source src="https://github.com/user-attachments/assets/30894f12-c741-44a2-9e6e-ddcacc231e5b" type="video/mp4">\n            </video>\n            <div class="caption">On a brilliant sunny day, the lakeshore is lined with an array of willow trees, their slender branches swaying gently in the soft breeze. The tranquil surface of the lake reflects the clear blue sky, while several elegant swans glide gracefully through the still water, leaving behind delicate ripples that disturb the mirror-like quality of the lake. The scene is one of serene beauty, with the willows'' greenery providing a picturesque frame for the peaceful avian visitors.</div>\n        </div>\n        <div class="video-item">\n            <video width="100%" controls>\n                <source src="https://github.com/user-attachments/assets/926575ca-7150-435b-a0ff-4900a963297b" type="video/mp4">\n            </video>\n            <div class="caption">A Chinese mother, draped in a soft, pastel-colored robe, gently rocks back and forth in a cozy rocking chair positioned in the tranquil setting of a nursery. The dimly lit bedroom is adorned with whimsical mobiles dangling from the ceiling, casting shadows that dance on the walls. Her baby, swaddled in a delicate, patterned blanket, rests against her chest, the child''s earlier cries now replaced by contented coos as the mother''s soothing voice lulls the little one to sleep. The scent of lavender fills the air, adding to the serene atmosphere, while a warm, orange glow from a nearby nightlight illuminates the scene with a gentle hue, capturing a moment of tender love and comfort.</div>\n        </div>\n    </div>\n</body>\n</html>\n\n## Model Introduction\n\nCogVideoX is an open-source version of the video generation model originating\nfrom [QingYing](https://chatglm.cn/video?lang=en?fr=osm_cogvideo). The table below displays the list of video generation\nmodels we currently offer, along with their foundational information.\n\n<table style="border-collapse: collapse; width: 100%;">\n  <tr>\n    <th style="text-align: center;">Model Name</th>\n    <th style="text-align: center;">CogVideoX-2B</th>\n    <th style="text-align: center;">CogVideoX-5B (This Repository)</th>\n  </tr>\n  <tr>\n    <td style="text-align: center;">Model Description</td>\n    <td style="text-align: center;">Entry-level model, balancing compatibility. Low cost for running and secondary development.</td>\n    <td style="text-align: center;">Larger model with higher video generation quality and better visual effects.</td>\n  </tr>\n  <tr>\n    <td style="text-align: center;">Inference Precision</td>\n    <td style="text-align: center;"><b>FP16* (Recommended)</b>, BF16, FP32, FP8*, INT8, no support for INT4</td>\n    <td style="text-align: center;"><b>BF16 (Recommended)</b>, FP16, FP32, FP8*, INT8, no support for INT4</td>\n  </tr>\n  <tr>\n    <td style="text-align: center;">Single GPU VRAM Consumption<br></td>\n    <td style="text-align: center;"><a href="https://github.com/THUDM/SwissArmyTransformer">SAT</a> FP16: 18GB <br><b>diffusers FP16: starting from 4GB*</b><br><b>diffusers INT8(torchao): starting from 3.6GB*</b></td>\n    <td style="text-align: center;"><a href="https://github.com/THUDM/SwissArmyTransformer">SAT</a> BF16: 26GB <br><b>diffusers BF16: starting from 5GB*</b><br><b>diffusers INT8(torchao): starting from 4.4GB*</b></td>\n  </tr>\n  <tr>\n    <td style="text-align: center;">Multi-GPU Inference VRAM Consumption</td>\n    <td style="text-align: center;"><b>FP16: 10GB* using diffusers</b></td>\n    <td style="text-align: center;"><b>BF16: 15GB* using diffusers</b></td>\n  </tr>\n  <tr>\n    <td style="text-align: center;">Inference Speed<br>(Step = 50, FP/BF16)</td>\n    <td style="text-align: center;">Single A100: ~90 seconds<br>Single H100: ~45 seconds</td>\n    <td style="text-align: center;">Single A100: ~180 seconds<br>Single H100: ~90 seconds</td>\n  </tr>\n  <tr>\n    <td style="text-align: center;">Fine-tuning Precision</td>\n    <td style="text-align: center;"><b>FP16</b></td>\n    <td style="text-align: center;"><b>BF16</b></td>\n  </tr>\n  <tr>\n    <td style="text-align: center;">Fine-tuning VRAM Consumption (per GPU)</td>\n    <td style="text-align: center;">47 GB (bs=1, LORA)<br> 61 GB (bs=2, LORA)<br> 62GB (bs=1, SFT)</td>\n    <td style="text-align: center;">63 GB (bs=1, LORA)<br> 80 GB (bs=2, LORA)<br> 75GB (bs=1, SFT)</td>\n  </tr>\n  <tr>\n    <td style="text-align: center;">Prompt Language</td>\n    <td colspan="2" style="text-align: center;">English*</td>\n  </tr>\n  <tr>\n    <td style="text-align: center;">Prompt Length Limit</td>\n    <td colspan="2" style="text-align: center;">226 Tokens</td>\n  </tr>\n  <tr>\n    <td style="text-align: center;">Video Length</td>\n    <td colspan="2" style="text-align: center;">6 Seconds</td>\n  </tr>\n  <tr>\n    <td style="text-align: center;">Frame Rate</td>\n    <td colspan="2" style="text-align: center;">8 Frames per Second</td>\n  </tr>\n  <tr>\n    <td style="text-align: center;">Video Resolution</td>\n    <td colspan="2" style="text-align: center;">720 x 480, no support for other resolutions (including fine-tuning)</td>\n  </tr>\n  <tr>\n    <td style="text-align: center;">Positional Encoding</td>\n    <td style="text-align: center;">3d_sincos_pos_embed</td>\n    <td style="text-align: center;">3d_rope_pos_embed</td>\n  </tr>\n</table>\n\n**Data Explanation**\n\n+ When testing using the `diffusers` library, all optimizations provided by the `diffusers` library were enabled. This\n  solution has not been tested for actual VRAM/memory usage on devices other than **NVIDIA A100 / H100**. Generally,\n  this solution can be adapted to all devices with **NVIDIA Ampere architecture** and above. If the optimizations are\n  disabled, VRAM usage will increase significantly, with peak VRAM usage being about 3 times higher than the table\n  shows. However, speed will increase by 3-4 times. You can selectively disable some optimizations, including:\n\n```\npipe.enable_model_cpu_offload()\npipe.enable_sequential_cpu_offload()\npipe.vae.enable_slicing()\npipe.vae.enable_tiling()\n``` \n\n+ When performing multi-GPU inference, the `enable_model_cpu_offload()` optimization needs to be disabled.\n+ Using INT8 models will reduce inference speed. This is to ensure that GPUs with lower VRAM can perform inference\n  normally while maintaining minimal video quality loss, though inference speed will decrease significantly.\n+ The 2B model is trained with `FP16` precision, and the 5B model is trained with `BF16` precision. We recommend using\n  the precision the model was trained with for inference.\n+ [PytorchAO](https://github.com/pytorch/ao) and [Optimum-quanto](https://github.com/huggingface/optimum-quanto/) can be\n  used to quantize the text encoder, Transformer, and VAE modules to reduce CogVideoX''s memory requirements. This makes\n  it possible to run the model on a free T4 Colab or GPUs with smaller VRAM! It is also worth noting that TorchAO\n  quantization is fully compatible with `torch.compile`, which can significantly improve inference speed. `FP8`\n  precision must be used on devices with `NVIDIA H100` or above, which requires installing\n  the `torch`, `torchao`, `diffusers`, and `accelerate` Python packages from source. `CUDA 12.4` is recommended.\n+ The inference speed test also used the above VRAM optimization scheme. Without VRAM optimization, inference speed\n  increases by about 10%. Only the `diffusers` version of the model supports quantization.\n+ The model only supports English input; other languages can be translated into English during refinement by a large\n  model.\n\n**Note**\n\n+ Using [SAT](https://github.com/THUDM/SwissArmyTransformer)  for inference and fine-tuning of SAT version\n  models. Feel free to visit our GitHub for more information.\n\n## Quick Start ðŸ¤—\n\nThis model supports deployment using the huggingface diffusers library. You can deploy it by following these steps.\n\n**We recommend that you visit our [GitHub](https://github.com/THUDM/CogVideo) and check out the relevant prompt\noptimizations and conversions to get a better experience.**\n\n1. Install the required dependencies\n\n```shell\n# diffusers>=0.30.1\n# transformers>=4.44.2\n# accelerate>=0.33.0 (suggest install from source)\n# imageio-ffmpeg>=0.5.1\npip install --upgrade transformers accelerate diffusers imageio-ffmpeg \n```\n\n2. Run the code\n\n```python\nimport torch\nfrom diffusers import CogVideoXPipeline\nfrom diffusers.utils import export_to_video\n\nprompt = "A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda''s fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda''s face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance."\n\npipe = CogVideoXPipeline.from_pretrained(\n    "THUDM/CogVideoX-5b",\n    torch_dtype=torch.bfloat16\n)\n\npipe.enable_model_cpu_offload()\npipe.vae.enable_tiling()\n\nvideo = pipe(\n    prompt=prompt,\n    num_videos_per_prompt=1,\n    num_inference_steps=50,\n    num_frames=49,\n    guidance_scale=6,\n    generator=torch.Generator(device="cuda").manual_seed(42),\n).frames[0]\n\nexport_to_video(video, "output.mp4", fps=8)\n```\n\n## Quantized Inference\n\n[PytorchAO](https://github.com/pytorch/ao) and [Optimum-quanto](https://github.com/huggingface/optimum-quanto/) can be\nused to quantize the Text Encoder, Transformer and VAE modules to lower the memory requirement of CogVideoX. This makes\nit possible to run the model on free-tier T4 Colab or smaller VRAM GPUs as well! It is also worth noting that TorchAO\nquantization is fully compatible with `torch.compile`, which allows for much faster inference speed.\n\n```diff\n# To get started, PytorchAO needs to be installed from the GitHub source and PyTorch Nightly.\n# Source and nightly installation is only required until next release.\n\nimport torch\nfrom diffusers import AutoencoderKLCogVideoX, CogVideoXTransformer3DModel, CogVideoXPipeline\nfrom diffusers.utils import export_to_video\n+ from transformers import T5EncoderModel\n+ from torchao.quantization import quantize_, int8_weight_only, int8_dynamic_activation_int8_weight\n\n+ quantization = int8_weight_only\n\n+ text_encoder = T5EncoderModel.from_pretrained("THUDM/CogVideoX-5b", subfolder="text_encoder", torch_dtype=torch.bfloat16)\n+ quantize_(text_encoder, quantization())\n\n+ transformer = CogVideoXTransformer3DModel.from_pretrained("THUDM/CogVideoX-5b", subfolder="transformer", torch_dtype=torch.bfloat16)\n+ quantize_(transformer, quantization())\n\n+ vae = AutoencoderKLCogVideoX.from_pretrained("THUDM/CogVideoX-5b", subfolder="vae", torch_dtype=torch.bfloat16)\n+ quantize_(vae, quantization())\n\n# Create pipeline and run inference\npipe = CogVideoXPipeline.from_pretrained(\n    "THUDM/CogVideoX-5b",\n+    text_encoder=text_encoder,\n+    transformer=transformer,\n+    vae=vae,\n    torch_dtype=torch.bfloat16,\n)\npipe.enable_model_cpu_offload()\npipe.vae.enable_tiling()\n\nprompt = "A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda''s fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda''s face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance."\n\nvideo = pipe(\n    prompt=prompt,\n    num_videos_per_prompt=1,\n    num_inference_steps=50,\n    num_frames=49,\n    guidance_scale=6,\n    generator=torch.Generator(device="cuda").manual_seed(42),\n).frames[0]\n\nexport_to_video(video, "output.mp4", fps=8)\n```\n\nAdditionally, the models can be serialized and stored in a quantized datatype to save disk space when using PytorchAO.\nFind examples and benchmarks at these links:\n\n- [torchao](https://gist.github.com/a-r-r-o-w/4d9732d17412888c885480c6521a9897)\n- [quanto](https://gist.github.com/a-r-r-o-w/31be62828b00a9292821b85c1017effa)\n\n## Explore the Model\n\nWelcome to our [github](https://github.com/THUDM/CogVideo), where you will find:\n\n1. More detailed technical details and code explanation.\n2. Optimization and conversion of prompt words.\n3. Reasoning and fine-tuning of SAT version models, and even pre-release.\n4. Project update log dynamics, more interactive opportunities.\n5. CogVideoX toolchain to help you better use the model.\n6. INT8 model inference code support.\n\n## Model License\n\nThis model is released under the [CogVideoX LICENSE](LICENSE).\n\n## Citation\n\n```\n@article{yang2024cogvideox,\n  title={CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer},\n  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},\n  journal={arXiv preprint arXiv:2408.06072},\n  year={2024}\n}\n```\n\n\n', '{"pipeline_tag":"text-to-video","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":21959732570,"files_count":21,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"CogVideoXPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:THUDM:CogVideo","source_url":"https://github.com/THUDM/CogVideo"},{"type":"has_code","target_id":"github:THUDM:CogVideo\">ðŸŒ","source_url":"https://github.com/THUDM/CogVideo\">ðŸŒ"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:THUDM:SwissArmyTransformer\">SAT<","source_url":"https://github.com/THUDM/SwissArmyTransformer\">SAT<"},{"type":"has_code","target_id":"github:THUDM:SwissArmyTransformer\">SAT<","source_url":"https://github.com/THUDM/SwissArmyTransformer\">SAT<"},{"type":"has_code","target_id":"github:pytorch:ao","source_url":"https://github.com/pytorch/ao"},{"type":"has_code","target_id":"github:huggingface:optimum-quanto","source_url":"https://github.com/huggingface/optimum-quanto"},{"type":"has_code","target_id":"github:THUDM:SwissArmyTransformer","source_url":"https://github.com/THUDM/SwissArmyTransformer"},{"type":"has_code","target_id":"github:THUDM:CogVideo","source_url":"https://github.com/THUDM/CogVideo"},{"type":"has_code","target_id":"github:pytorch:ao","source_url":"https://github.com/pytorch/ao"},{"type":"has_code","target_id":"github:huggingface:optimum-quanto","source_url":"https://github.com/huggingface/optimum-quanto"},{"type":"has_code","target_id":"github:THUDM:CogVideo","source_url":"https://github.com/THUDM/CogVideo"},{"type":"based_on_paper","target_id":"arxiv:2408.06072","source_url":"https://arxiv.org/abs/2408.06072"}]', NULL, 'Other', 'approved', 78.2, '43c9183dde71c1056196228cc1d860a2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-fishaudio-fish-speech-1.5', 'huggingface--fishaudio--fish-speech-1.5', 'fish-speech-1.5', 'fishaudio', '--- tags: - text-to-speech license: cc-by-nc-sa-4.0 language: - zh - en - de - ja - fr - es - ko - ar - nl - ru - it - pl - pt pipeline_tag: text-to-speech inference: false extra_gated_prompt: >- You agree to not use the model to generate contents that violate DMCA or local laws. extra_gated_fields: Country: country Specific date: date_picker I agree to use this model for non-commercial use ONLY: checkbox --- **Fish Speech V1.5** is a leading text-to-speech (TTS) model trained on more than 1 ...', '["dual_ar","text-to-speech","zh","en","de","ja","fr","es","ko","ar","nl","ru","it","pl","pt","arxiv:2411.01156","license:cc-by-nc-sa-4.0","region:us"]', 'text-to-speech', 655, 1890, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/fishaudio/fish-speech-1.5","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- text-to-speech\nlicense: cc-by-nc-sa-4.0\nlanguage:\n- zh\n- en\n- de\n- ja\n- fr\n- es\n- ko\n- ar\n- nl\n- ru\n- it\n- pl\n- pt\npipeline_tag: text-to-speech\ninference: false\nextra_gated_prompt: >-\n  You agree to not use the model to generate contents that violate DMCA or local\n  laws.\nextra_gated_fields:\n  Country: country\n  Specific date: date_picker\n  I agree to use this model for non-commercial use ONLY: checkbox\n---\n\n\n# Fish Speech V1.5\n\n**Fish Speech V1.5** is a leading text-to-speech (TTS) model trained on more than 1 million hours of audio data in multiple languages.\n\nSupported languages:\n- English (en) >300k hours\n- Chinese (zh) >300k hours\n- Japanese (ja) >100k hours\n- German (de) ~20k hours\n- French (fr) ~20k hours\n- Spanish (es) ~20k hours\n- Korean (ko) ~20k hours\n- Arabic (ar) ~20k hours\n- Russian (ru) ~20k hours\n- Dutch (nl) <10k hours\n- Italian (it) <10k hours\n- Polish (pl) <10k hours\n- Portuguese (pt) <10k hours\n\nPlease refer to [Fish Speech Github](https://github.com/fishaudio/fish-speech) for more info.  \nDemo available at [Fish Audio](https://fish.audio/).\n\n## Citation\n\nIf you found this repository useful, please consider citing this work:\n\n```\n@misc{fish-speech-v1.4,\n      title={Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis}, \n      author={Shijia Liao and Yuxuan Wang and Tianyu Li and Yifan Cheng and Ruoyi Zhang and Rongzhi Zhou and Yijin Xing},\n      year={2024},\n      eprint={2411.01156},\n      archivePrefix={arXiv},\n      primaryClass={cs.SD},\n      url={https://arxiv.org/abs/2411.01156}, \n}\n```\n\n## License\n\nThis model is permissively licensed under the CC-BY-NC-SA-4.0 license.', '{"pipeline_tag":"text-to-speech","library_name":null,"framework":null,"params":null,"storage_bytes":2740334639,"files_count":7,"spaces_count":20,"gated":false,"private":false,"config":{"model_type":"dual_ar"}}', '[]', '[{"type":"has_code","target_id":"github:fishaudio:fish-speech","source_url":"https://github.com/fishaudio/fish-speech"},{"type":"based_on_paper","target_id":"arxiv:2411.01156","source_url":"https://arxiv.org/abs/2411.01156"}]', NULL, 'CC-BY-NC-SA-4.0', 'approved', 48.2, '5d7e37f17365734866abaaf684039d80', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-fal-AuraFlow', 'huggingface--fal--auraflow', 'AuraFlow', 'fal', '--- license: apache-2.0 tags: - text-to-image --- !image/png AuraFlow v0.1 is the fully open-sourced largest flow-based text-to-image generation model. This model achieves state-of-the-art results on GenEval. Read our blog post for more technical details. The model is currently in beta. We are working on improving it and the community''s feedback is important. Join fal''s Discord to give us feedback and stay in touch with the model development. Credits: A huge thank you to @cloneofsimo and @isi...', '["diffusers","safetensors","text-to-image","license:apache-2.0","diffusers:auraflowpipeline","region:us"]', 'text-to-image', 651, 537, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/fal/AuraFlow","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\ntags:\n- text-to-image\n---\n# AuraFlow\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6380ebb8471a4550ff255c62/jMkLXPFVNGdUb7P4nNTqX.png)\n\n\nAuraFlow v0.1 is the fully open-sourced largest flow-based text-to-image generation model.\n\nThis model achieves state-of-the-art results on GenEval. Read our [blog post](https://blog.fal.ai/auraflow/) for more technical details.\n\nThe model is currently in beta. We are working on improving it and the community''s feedback is important.\nJoin [fal''s Discord](https://discord.gg/fal-ai) to give us feedback and stay in touch with the model development.\n\nCredits: A huge thank you to [@cloneofsimo](https://twitter.com/cloneofsimo) and [@isidentical](https://twitter.com/isidentical) for bringing this project to life. It''s incredible what two cracked engineers can achieve in\nsuch a short period of time. We also extend our gratitude to the incredible researchers whose prior work laid the foundation for our efforts.\n\n## Usage\n\n```bash\n$ pip install transformers accelerate protobuf sentencepiece\n$ pip install git+https://github.com/huggingface/diffusers.git\n```\n\n```python\nfrom diffusers import AuraFlowPipeline\nimport torch\n\npipeline = AuraFlowPipeline.from_pretrained(\n    "fal/AuraFlow",\n    torch_dtype=torch.float16\n).to("cuda")\n\nimage = pipeline(\n    prompt="close-up portrait of a majestic iguana with vibrant blue-green scales, piercing amber eyes, and orange spiky crest. Intricate textures and details visible on scaly skin. Wrapped in dark hood, giving regal appearance. Dramatic lighting against black background. Hyper-realistic, high-resolution image showcasing the reptile''s expressive features and coloration.",\n    height=1024,\n    width=1024,\n    num_inference_steps=50, \n    generator=torch.Generator().manual_seed(666),\n    guidance_scale=3.5,\n).images[0]\n```', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":65837496085,"files_count":26,"spaces_count":55,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"AuraFlowPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:diffusers.git","source_url":"https://github.com/huggingface/diffusers.git"}]', NULL, 'Apache-2.0', 'approved', 48.1, '3b9550dd25ecdcd241116c69730d1c17', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-bigcode-starcoder2-15b', 'huggingface--bigcode--starcoder2-15b', 'starcoder2-15b', 'bigcode', '--- pipeline_tag: text-generation inference: parameters: temperature: 0.2 top_p: 0.95 widget: - text: ''def print_hello_world():'' example_title: Hello world group: Python datasets: - bigcode/the-stack-v2-train license: bigcode-openrail-m library_name: transformers tags: - code model-index: - name: starcoder2-15b results: - task: type: text-generation dataset: name: CruxEval-I type: cruxeval-i metrics: - type: pass@1 value: 48.1 - task: type: text-generation dataset: name: DS-1000 type: ds-1000...', '["transformers","safetensors","starcoder2","text-generation","code","dataset:bigcode/the-stack-v2-train","arxiv:2305.13245","arxiv:2205.14135","arxiv:2004.05150","arxiv:2207.14255","arxiv:2402.19173","license:bigcode-openrail-m","model-index","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 650, 5420, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/bigcode/starcoder2-15b","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\npipeline_tag: text-generation\ninference:\n  parameters:\n    temperature: 0.2\n    top_p: 0.95\nwidget:\n- text: ''def print_hello_world():''\n  example_title: Hello world\n  group: Python\ndatasets:\n- bigcode/the-stack-v2-train\nlicense: bigcode-openrail-m\nlibrary_name: transformers\ntags:\n- code\nmodel-index:\n- name: starcoder2-15b\n  results:\n  - task:\n      type: text-generation\n    dataset:\n      name: CruxEval-I\n      type: cruxeval-i\n    metrics:\n    - type: pass@1\n      value: 48.1\n  - task:\n      type: text-generation\n    dataset:\n      name: DS-1000\n      type: ds-1000\n    metrics:\n    - type: pass@1\n      value: 33.8\n  - task:\n      type: text-generation\n    dataset:\n      name: GSM8K (PAL)\n      type: gsm8k-pal\n    metrics:\n    - type: accuracy\n      value: 65.1\n  - task:\n      type: text-generation\n    dataset:\n      name: HumanEval+\n      type: humanevalplus\n    metrics:\n    - type: pass@1\n      value: 37.8\n  - task:\n      type: text-generation\n    dataset:\n      name: HumanEval\n      type: humaneval\n    metrics:\n    - type: pass@1\n      value: 46.3\n  - task:\n      type: text-generation\n    dataset:\n      name: RepoBench-v1.1\n      type: repobench-v1.1\n    metrics:\n    - type: edit-smiliarity\n      value: 74.08\n---\n\n# StarCoder2\n\n<center>\n    <img src="https://huggingface.co/datasets/bigcode/admin_private/resolve/main/starcoder2_banner.png" alt="SC2" width="900" height="600">\n</center>\n\n##  Table of Contents\n\n1. [Model Summary](#model-summary)\n2. [Use](#use)\n3. [Limitations](#limitations)\n4. [Training](#training)\n5. [License](#license)\n6. [Citation](#citation)\n\n## Model Summary\n\nStarCoder2-15B model is a 15B parameter model trained on 600+ programming languages from [The Stack v2](https://huggingface.co/datasets/bigcode/the-stack-v2-train), with opt-out requests excluded. The model uses [Grouped Query Attention](https://arxiv.org/abs/2305.13245), [a context window of 16,384 tokens](https://arxiv.org/abs/2205.14135) with [a sliding window attention of 4,096 tokens](https://arxiv.org/abs/2004.05150v2),  and was trained using the [Fill-in-the-Middle objective](https://arxiv.org/abs/2207.14255) on 4+ trillion tokens.  \nThe model was trained with [NVIDIA NeMoâ„¢ Framework](https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/) using the [NVIDIA Eos Supercomputer](https://blogs.nvidia.com/blog/eos/) built with [NVIDIA DGX H100](https://www.nvidia.com/en-us/data-center/dgx-h100/) systems.\n\n- **Project Website:** [bigcode-project.org](https://www.bigcode-project.org)\n- **Paper:** [Link](https://huggingface.co/papers/2402.19173)\n- **Point of Contact:** [contact@bigcode-project.org](mailto:contact@bigcode-project.org)\n- **Languages:** 600+ Programming languages\n\n## Use\n\n### Intended use\n\nThe model was trained on GitHub code as well as additional selected data sources such as Arxiv and Wikipedia. As such it is _not_ an instruction model and commands like "Write a function that computes the square root." do not work well.\n\n### Generation\nHere are some examples to get started with the model. You can find a script for fine-tuning in StarCoder2''s [GitHub repository](https://github.com/bigcode-project/starcoder2).\n\nFirst, make sure to install `transformers` from source:\n```bash\npip install git+https://github.com/huggingface/transformers.git\n```\n\n#### Running the model on CPU/GPU/multi GPU\n* _Using full precision_\n```python\n# pip install git+https://github.com/huggingface/transformers.git # TODO: merge PR to main\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = "bigcode/starcoder2-15b"\ndevice = "cuda" # for GPU usage or "cpu" for CPU usage\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map="auto")`\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n\ninputs = tokenizer.encode("def print_hello_world():", return_tensors="pt").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n\n* _Using `torch.bfloat16`_\n```python\n# pip install accelerate\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ncheckpoint = "bigcode/starcoder2-15b"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# for fp16 use `torch_dtype=torch.float16` instead\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map="auto", torch_dtype=torch.bfloat16)\n\ninputs = tokenizer.encode("def print_hello_world():", return_tensors="pt").to("cuda")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n```bash\n>>> print(f"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB")\nMemory footprint: 32251.33 MB\n```\n\n#### Quantized Versions through `bitsandbytes`\n* _Using 8-bit precision (int8)_\n\n```python\n# pip install bitsandbytes accelerate\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# to use 4bit use `load_in_4bit=True` instead\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\ncheckpoint = "bigcode/starcoder2-15b"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, quantization_config=quantization_config)\n\ninputs = tokenizer.encode("def print_hello_world():", return_tensors="pt").to("cuda")\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n```bash\n>>> print(f"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB")\n# load_in_8bit\nMemory footprint: 16900.18 MB\n# load_in_4bit\n>>> print(f"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB")\nMemory footprint: 9224.60 MB\n```\n### Attribution & Other Requirements\n\nThe pretraining dataset of the model was filtered for permissive licenses and code with no license only. Nevertheless, the model can generate source code verbatim from the dataset. The code''s license might require attribution and/or other specific requirements that must be respected. We provide a [search index](https://huggingface.co/spaces/bigcode/search-v2) that let''s you search through the pretraining data to identify where generated code came from and apply the proper attribution to your code.\n\n# Limitations\n\nThe model has been trained on source code from 600+ programming languages. The predominant language in source is English although other languages are also present. As such the model is capable to generate code snippets provided some context but the generated code is not guaranteed to work as intended. It can be inefficient, contain bugs or exploits. See [the paper](https://huggingface.co/papers/2402.19173) for an in-depth discussion of the model limitations. \n\n# Training\n\n## Model\n\n- **Architecture:** Transformer decoder with grouped-query and sliding window attention and Fill-in-the-Middle objective\n- **Pretraining steps:** 1 million\n- **Pretraining tokens:** 4+ trillion\n- **Precision:** bfloat16\n\n## Hardware\n\n- **GPUs:** 1024 x H100\n\n## Software\n\n- **Framework:** [NeMo Framework](https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/) \n- **Neural networks:** [PyTorch](https://github.com/pytorch/pytorch)\n\n# License\n\nThe model is licensed under the BigCode OpenRAIL-M v1 license agreement. You can find the full agreement [here](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement).\n\n# Citation\n\n```bash\n@misc{lozhkov2024starcoder,\n      title={StarCoder 2 and The Stack v2: The Next Generation}, \n      author={Anton Lozhkov and Raymond Li and Loubna Ben Allal and Federico Cassano and Joel Lamy-Poirier and Nouamane Tazi and Ao Tang and Dmytro Pykhtar and Jiawei Liu and Yuxiang Wei and Tianyang Liu and Max Tian and Denis Kocetkov and Arthur Zucker and Younes Belkada and Zijian Wang and Qian Liu and Dmitry Abulkhanov and Indraneil Paul and Zhuang Li and Wen-Ding Li and Megan Risdal and Jia Li and Jian Zhu and Terry Yue Zhuo and Evgenii Zheltonozhskii and Nii Osae Osae Dade and Wenhao Yu and Lucas KrauÃŸ and Naman Jain and Yixuan Su and Xuanli He and Manan Dey and Edoardo Abati and Yekun Chai and Niklas Muennighoff and Xiangru Tang and Muhtasham Oblokulov and Christopher Akiki and Marc Marone and Chenghao Mou and Mayank Mishra and Alex Gu and Binyuan Hui and Tri Dao and Armel Zebaze and Olivier Dehaene and Nicolas Patry and Canwen Xu and Julian McAuley and Han Hu and Torsten Scholak and Sebastien Paquet and Jennifer Robinson and Carolyn Jane Anderson and Nicolas Chapados and Mostofa Patwary and Nima Tajbakhsh and Yacine Jernite and Carlos MuÃ±oz Ferrandis and Lingming Zhang and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},\n      year={2024},\n      eprint={2402.19173},\n      archivePrefix={arXiv},\n      primaryClass={cs.SE}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":15957889024,"storage_bytes":63831628056,"files_count":24,"spaces_count":86,"gated":false,"private":false,"config":{"architectures":["Starcoder2ForCausalLM"],"model_type":"starcoder2","tokenizer_config":{"bos_token":"<|endoftext|>","eos_token":"<|endoftext|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:bigcode-project:starcoder2","source_url":"https://github.com/bigcode-project/starcoder2"},{"type":"has_code","target_id":"github:huggingface:transformers.git","source_url":"https://github.com/huggingface/transformers.git"},{"type":"has_code","target_id":"github:huggingface:transformers.git","source_url":"https://github.com/huggingface/transformers.git"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"based_on_paper","target_id":"arxiv:2305.13245","source_url":"https://arxiv.org/abs/2305.13245"},{"type":"based_on_paper","target_id":"arxiv:2205.14135","source_url":"https://arxiv.org/abs/2205.14135"},{"type":"based_on_paper","target_id":"arxiv:2004.05150","source_url":"https://arxiv.org/abs/2004.05150"},{"type":"based_on_paper","target_id":"arxiv:2207.14255","source_url":"https://arxiv.org/abs/2207.14255"},{"type":"based_on_paper","target_id":"arxiv:2402.19173","source_url":"https://arxiv.org/abs/2402.19173"}]', NULL, 'bigcode-openrail-m', 'approved', 63.1, '2d95f16d0973d0822ae39c30fceb538e', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-MiniMaxAI-MiniMax-Text-01', 'huggingface--minimaxai--minimax-text-01', 'MiniMax-Text-01', 'MiniMaxAI', '--- pipeline_tag: text-generation --- <div align="center"> <svg width="60%" height="auto" viewBox="0 0 144 48" fill="none" xmlns="http://www.w3.org/2000/svg"> <path d="M26.6782 7.96523C26.6782 7.02436 25.913 6.26087 24.9739 6.26087C24.0348 6.26087 23.2695 7.0261 23.2695 7.96523V36.2139C23.2695 38.4 21.4904 40.1791 19.3043 40.1791C17.1183 40.1791 15.3391 38.4 15.3391 36.2139V18.0904C15.3391 17.1496 14.5739 16.3861 13.6348 16.3861C12.6956 16.3861 11.9304 17.1513 11.9304 18.0904V25.7722C11.9304 ...', '["safetensors","minimax_text_01","text-generation","conversational","custom_code","arxiv:2501.08313","region:us"]', 'text-generation', 650, 1544, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/MiniMaxAI/MiniMax-Text-01","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\npipeline_tag: text-generation\n---\n<div align="center">\n\n<svg width="60%" height="auto" viewBox="0 0 144 48" fill="none" xmlns="http://www.w3.org/2000/svg">\n<path d="M26.6782 7.96523C26.6782 7.02436 25.913 6.26087 24.9739 6.26087C24.0348 6.26087 23.2695 7.0261 23.2695 7.96523V36.2139C23.2695 38.4 21.4904 40.1791 19.3043 40.1791C17.1183 40.1791 15.3391 38.4 15.3391 36.2139V18.0904C15.3391 17.1496 14.5739 16.3861 13.6348 16.3861C12.6956 16.3861 11.9304 17.1513 11.9304 18.0904V25.7722C11.9304 27.9583 10.1513 29.7374 7.96518 29.7374C5.7791 29.7374 4 27.9583 4 25.7722V22.9878C4 22.3635 4.50609 21.8574 5.13043 21.8574C5.75478 21.8574 6.26087 22.3635 6.26087 22.9878V25.7722C6.26087 26.713 7.02605 27.4765 7.96518 27.4765C8.90431 27.4765 9.66954 26.7113 9.66954 25.7722V18.0904C9.66954 15.9044 11.4487 14.1252 13.6348 14.1252C15.8209 14.1252 17.6 15.9044 17.6 18.0904V36.2139C17.6 37.1548 18.3652 37.9183 19.3043 37.9183C20.2435 37.9183 21.0087 37.153 21.0087 36.2139V25.1322V7.96523C21.0087 5.77914 22.7878 4 24.9739 4C27.16 4 28.9391 5.77914 28.9391 7.96523V31.3565C28.9391 31.9809 28.433 32.487 27.8087 32.487C27.1843 32.487 26.6782 31.9809 26.6782 31.3565V7.96523ZM47.6539 14.1252C45.4678 14.1252 43.6887 15.9044 43.6887 18.0904V33.2296C43.6887 34.1704 42.9235 34.9339 41.9843 34.9339C41.0452 34.9339 40.28 34.1687 40.28 33.2296V7.96523C40.28 5.77914 38.5008 4 36.3148 4C34.1287 4 32.3496 5.77914 32.3496 7.96523V40.0348C32.3496 40.9756 31.5843 41.7391 30.6452 41.7391C29.7061 41.7391 28.9409 40.9739 28.9409 40.0348V36.0643C28.9409 35.44 28.4348 34.9339 27.8104 34.9339C27.1861 34.9339 26.68 35.44 26.68 36.0643V40.0348C26.68 42.2209 28.4591 44 30.6452 44C32.8313 44 34.6104 42.2209 34.6104 40.0348V7.96523C34.6104 7.02436 35.3756 6.26087 36.3148 6.26087C37.2539 6.26087 38.0191 7.0261 38.0191 7.96523V33.2296C38.0191 35.4156 39.7982 37.1948 41.9843 37.1948C44.1704 37.1948 45.9496 35.4156 45.9496 33.2296V18.0904C45.9496 17.1496 46.7148 16.3861 47.6539 16.3861C48.593 16.3861 49.3582 17.1513 49.3582 18.0904V31.3565C49.3582 31.9809 49.8643 32.487 50.4887 32.487C51.113 32.487 51.6191 31.9809 51.6191 31.3565V18.0904C51.6191 15.9044 49.84 14.1252 47.6539 14.1252Z" fill="url(#paint0_linear_17_483)"/>\n<path d="M68.7671 16.5615H71.2541C71.3254 16.5615 71.3845 16.5859 71.435 16.6363C71.4836 16.6868 71.5097 16.7459 71.5097 16.8172V31.1824C71.5097 31.2537 71.4854 31.3128 71.435 31.3633C71.3845 31.4137 71.3254 31.4381 71.2541 31.4381H68.7671C68.6958 31.4381 68.6367 31.4137 68.5862 31.3633C68.5358 31.3146 68.5115 31.2537 68.5115 31.1824V21.812C68.5115 21.7563 68.4976 21.7268 68.4697 21.7268C68.4419 21.7268 68.4123 21.7476 68.3845 21.7911L66.1323 25.318C66.061 25.4311 65.9619 25.4885 65.8349 25.4885H64.581C64.4541 25.4885 64.3549 25.4328 64.2836 25.318L62.0315 21.7911C62.0036 21.7494 61.9741 21.7302 61.9462 21.7372C61.9184 21.7441 61.9045 21.7772 61.9045 21.8328V31.1824C61.9045 31.2537 61.8802 31.3128 61.8297 31.3633C61.7793 31.4137 61.7202 31.4381 61.6489 31.4381H59.1619C59.0906 31.4381 59.0315 31.4137 58.981 31.3633C58.9306 31.3146 58.9062 31.2537 58.9062 31.1824V16.8172C58.9062 16.7459 58.9306 16.6868 58.981 16.6363C59.0315 16.5859 59.0906 16.5615 59.1619 16.5615H61.6489C61.7758 16.5615 61.8749 16.6189 61.9462 16.732L65.1341 21.6833C65.1758 21.7685 65.2193 21.7685 65.261 21.6833L68.4697 16.732C68.541 16.6189 68.6402 16.5615 68.7671 16.5615Z" fill="currentColor"/>\n<path d="M74.1764 31.3633C74.1259 31.3146 74.1016 31.2537 74.1016 31.1824V16.8172C74.1016 16.7459 74.1259 16.6868 74.1764 16.6363C74.2268 16.5859 74.2859 16.5615 74.3572 16.5615H76.8442C76.9155 16.5615 76.9746 16.5859 77.0251 16.6363C77.0737 16.6868 77.0998 16.7459 77.0998 16.8172V31.1824C77.0998 31.2537 77.0755 31.3128 77.0251 31.3633C76.9746 31.4137 76.9155 31.4381 76.8442 31.4381H74.3572C74.2859 31.4381 74.2268 31.4137 74.1764 31.3633Z" fill="currentColor"/>\n<path d="M88.3066 16.6361C88.3553 16.5874 88.4162 16.5613 88.4875 16.5613H90.9744C91.0457 16.5613 91.1049 16.5857 91.1553 16.6361C91.204 16.6865 91.2301 16.7457 91.2301 16.817V31.1822C91.2301 31.2535 91.2057 31.3126 91.1553 31.363C91.1049 31.4135 91.0457 31.4378 90.9744 31.4378H88.5727C88.4301 31.4378 88.331 31.3822 88.2753 31.2674L82.771 22.1717C82.7431 22.13 82.7136 22.1109 82.6858 22.1178C82.6579 22.1248 82.644 22.1578 82.644 22.2135L82.6858 31.1805C82.6858 31.2518 82.6614 31.3109 82.611 31.3613C82.5606 31.4117 82.5014 31.4361 82.4301 31.4361H79.9431C79.8718 31.4361 79.8127 31.4117 79.7623 31.3613C79.7118 31.3126 79.6875 31.2518 79.6875 31.1805V16.8152C79.6875 16.7439 79.7118 16.6848 79.7623 16.6344C79.8127 16.5839 79.8718 16.5596 79.9431 16.5596H82.3449C82.4858 16.5596 82.5849 16.617 82.6423 16.73L88.124 25.7822C88.1518 25.8239 88.1797 25.8431 88.2092 25.8361C88.2371 25.8292 88.251 25.7978 88.251 25.7404L88.2301 16.8152C88.2301 16.7439 88.2545 16.6848 88.3049 16.6344L88.3066 16.6361Z" fill="currentColor"/>\n<path d="M93.8951 31.3633C93.8446 31.3146 93.8203 31.2537 93.8203 31.1824V16.8172C93.8203 16.7459 93.8446 16.6868 93.8951 16.6363C93.9455 16.5859 94.0047 16.5615 94.076 16.5615H96.5629C96.6342 16.5615 96.6934 16.5859 96.7438 16.6363C96.7925 16.6868 96.8186 16.7459 96.8186 16.8172V31.1824C96.8186 31.2537 96.7942 31.3128 96.7438 31.3633C96.6934 31.4137 96.6342 31.4381 96.5629 31.4381H94.076C94.0047 31.4381 93.9455 31.4137 93.8951 31.3633Z" fill="currentColor"/>\n<path d="M109.267 16.5615H111.754C111.825 16.5615 111.885 16.5859 111.935 16.6363C111.984 16.6868 112.01 16.7459 112.01 16.8172V31.1824C112.01 31.2537 111.985 31.3128 111.935 31.3633C111.885 31.4137 111.825 31.4381 111.754 31.4381H109.267C109.196 31.4381 109.137 31.4137 109.086 31.3633C109.036 31.3146 109.011 31.2537 109.011 31.1824V21.812C109.011 21.7563 108.998 21.7268 108.97 21.7268C108.942 21.7268 108.912 21.7476 108.885 21.7911L106.632 25.318C106.561 25.4311 106.462 25.4885 106.335 25.4885H105.081C104.954 25.4885 104.855 25.4328 104.784 25.318L102.531 21.7911C102.504 21.7494 102.474 21.7302 102.446 21.7372C102.418 21.7441 102.405 21.7772 102.405 21.8328V31.1824C102.405 31.2537 102.38 31.3128 102.33 31.3633C102.279 31.4137 102.22 31.4381 102.149 31.4381H99.6619C99.5906 31.4381 99.5315 31.4137 99.481 31.3633C99.4306 31.3146 99.4062 31.2537 99.4062 31.1824V16.8172C99.4062 16.7459 99.4306 16.6868 99.481 16.6363C99.5315 16.5859 99.5906 16.5615 99.6619 16.5615H102.149C102.276 16.5615 102.375 16.6189 102.446 16.732L105.634 21.6833C105.676 21.7685 105.719 21.7685 105.761 21.6833L108.97 16.732C109.041 16.6189 109.14 16.5615 109.267 16.5615Z" fill="currentColor"/>\n<path d="M123.782 31.2241L123.144 29.1424C123.116 29.0867 123.079 29.0572 123.038 29.0572H117.81C117.768 29.0572 117.732 29.085 117.704 29.1424L117.088 31.2241C117.046 31.3668 116.954 31.4363 116.812 31.4363H114.112C114.027 31.4363 113.963 31.412 113.921 31.3615C113.879 31.3128 113.871 31.2381 113.9 31.1389L118.49 16.7737C118.532 16.6328 118.624 16.5615 118.766 16.5615H122.102C122.243 16.5615 122.335 16.6328 122.379 16.7737L126.968 31.1389C126.982 31.1668 126.989 31.2033 126.989 31.245C126.989 31.372 126.911 31.4363 126.756 31.4363H124.057C123.916 31.4363 123.824 31.365 123.78 31.2241H123.782ZM118.554 26.7407H122.295C122.38 26.7407 122.408 26.6989 122.38 26.6137L120.467 20.3024C120.453 20.2467 120.432 20.2207 120.403 20.2276C120.375 20.2346 120.352 20.2589 120.339 20.3024L118.469 26.6137C118.455 26.6989 118.483 26.7407 118.554 26.7407Z" fill="currentColor"/>\n<path d="M128.222 31.353C128.18 31.2974 128.187 31.2261 128.243 31.1409L132.365 24.0643C132.393 24.0226 132.393 23.9791 132.365 23.9374L128.243 16.8609L128.201 16.7339C128.201 16.6209 128.28 16.5635 128.434 16.5635H131.133C131.274 16.5635 131.38 16.6209 131.452 16.7339L134.213 21.6C134.255 21.6852 134.299 21.6852 134.34 21.6L137.102 16.7339C137.173 16.6209 137.28 16.5635 137.42 16.5635H140.099C140.198 16.5635 140.269 16.5913 140.311 16.6487C140.353 16.7061 140.346 16.7756 140.29 16.8609L136.168 23.9374C136.154 23.9791 136.154 24.0226 136.168 24.0643L140.29 31.1409L140.332 31.2678C140.332 31.3809 140.253 31.4383 140.099 31.4383H137.42C137.278 31.4383 137.172 31.3826 137.102 31.2678L134.34 26.4226C134.299 26.3374 134.255 26.3374 134.213 26.4226L131.429 31.2678C131.358 31.3809 131.252 31.4383 131.111 31.4383H128.433C128.333 31.4383 128.262 31.4104 128.22 31.353H128.222Z" fill="currentColor"/>\n<defs>\n<linearGradient id="paint0_linear_17_483" x1="3.99826" y1="24" x2="51.6208" y2="24" gradientUnits="userSpaceOnUse">\n<stop stop-color="#E21680"/>\n<stop offset="1" stop-color="#FF633A"/>\n</linearGradient>\n</defs>\n</svg>\n\n</div>\n<hr>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://www.minimax.io" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://img.shields.io/badge/_Homepage-MiniMax-FF4040?style=flat-square&labelColor=2C3E50&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDkwLjE2IDQxMS43Ij48ZGVmcz48c3R5bGU+LmNscy0xe2ZpbGw6I2ZmZjt9PC9zdHlsZT48L2RlZnM+PHBhdGggY2xhc3M9ImNscy0xIiBkPSJNMjMzLjQ1LDQwLjgxYTE3LjU1LDE3LjU1LDAsMSwwLTM1LjEsMFYzMzEuNTZhNDAuODIsNDAuODIsMCwwLDEtODEuNjMsMFYxNDVhMTcuNTUsMTcuNTUsMCwxLDAtMzUuMDksMHY3OS4wNmE0MC44Miw0MC44MiwwLDAsMS04MS42MywwVjE5NS40MmExMS42MywxMS42MywwLDAsMSwyMy4yNiwwdjI4LjY2YTE3LjU1LDE3LjU1LDAsMCwwLDM1LjEsMFYxNDVBNDAuODIsNDAuODIsMCwwLDEsMTQwLDE0NVYzMzEuNTZhMTcuNTUsMTcuNTUsMCwwLDAsMzUuMSwwVjIxNy41aDBWNDAuODFhNDAuODEsNDAuODEsMCwxLDEsODEuNjIsMFYyODEuNTZhMTEuNjMsMTEuNjMsMCwxLDEtMjMuMjYsMFptMjE1LjksNjMuNEE0MC44Niw0MC44NiwwLDAsMCw0MDguNTMsMTQ1VjMwMC44NWExNy41NSwxNy41NSwwLDAsMS0zNS4wOSwwdi0yNjBhNDAuODIsNDAuODIsMCwwLDAtODEuNjMsMFYzNzAuODlhMTcuNTUsMTcuNTUsMCwwLDEtMzUuMSwwVjMzMGExMS42MywxMS42MywwLDEsMC0yMy4yNiwwdjQwLjg2YTQwLjgxLDQwLjgxLDAsMCwwLDgxLjYyLDBWNDAuODFhMTcuNTUsMTcuNTUsMCwwLDEsMzUuMSwwdjI2MGE0MC44Miw0MC44MiwwLDAsMCw4MS42MywwVjE0NWExNy41NSwxNy41NSwwLDEsMSwzNS4xLDBWMjgxLjU2YTExLjYzLDExLjYzLDAsMCwwLDIzLjI2LDBWMTQ1QTQwLjg1LDQwLjg1LDAsMCwwLDQ0OS4zNSwxMDQuMjFaIi8+PC9zdmc+&logoWidth=20" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://arxiv.org/abs/2501.08313" target="_blank" style="margin: 2px;">\n    <img alt="Paper" src="https://img.shields.io/badge/ðŸ“–_Paper-MiniMax--01-FF4040?style=flat-square&labelColor=2C3E50" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.minimax.io/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/_MiniMax_Chat-FF4040?style=flat-square&labelColor=2C3E50&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSIwIDAgNDkwLjE2IDQxMS43Ij48ZGVmcz48c3R5bGU+LmNscy0xe2ZpbGw6I2ZmZjt9PC9zdHlsZT48L2RlZnM+PHBhdGggY2xhc3M9ImNscy0xIiBkPSJNMjMzLjQ1LDQwLjgxYTE3LjU1LDE3LjU1LDAsMSwwLTM1LjEsMFYzMzEuNTZhNDAuODIsNDAuODIsMCwwLDEtODEuNjMsMFYxNDVhMTcuNTUsMTcuNTUsMCwxLDAtMzUuMDksMHY3OS4wNmE0MC44Miw0MC44MiwwLDAsMS04MS42MywwVjE5NS40MmExMS42MywxMS42MywwLDAsMSwyMy4yNiwwdjI4LjY2YTE3LjU1LDE3LjU1LDAsMCwwLDM1LjEsMFYxNDVBNDAuODIsNDAuODIsMCwwLDEsMTQwLDE0NVYzMzEuNTZhMTcuNTUsMTcuNTUsMCwwLDAsMzUuMSwwVjIxNy41aDBWNDAuODFhNDAuODEsNDAuODEsMCwxLDEsODEuNjIsMFYyODEuNTZhMTEuNjMsMTEuNjMsMCwxLDEtMjMuMjYsMFptMjE1LjksNjMuNEE0MC44Niw0MC44NiwwLDAsMCw0MDguNTMsMTQ1VjMwMC44NWExNy41NSwxNy41NSwwLDAsMS0zNS4wOSwwdi0yNjBhNDAuODIsNDAuODIsMCwwLDAtODEuNjMsMFYzNzAuODlhMTcuNTUsMTcuNTUsMCwwLDEtMzUuMSwwVjMzMGExMS42MywxMS42MywwLDEsMC0yMy4yNiwwdjQwLjg2YTQwLjgxLDQwLjgxLDAsMCwwLDgxLjYyLDBWNDAuODFhMTcuNTUsMTcuNTUsMCwwLDEsMzUuMSwwdjI2MGE0MC44Miw0MC44MiwwLDAsMCw4MS42MywwVjE0NWExNy41NSwxNy41NSwwLDEsMSwzNS4xLDBWMjgxLjU2YTExLjYzLDExLjYzLDAsMCwwLDIzLjI2LDBWMTQ1QTQwLjg1LDQwLjg1LDAsMCwwLDQ0OS4zNSwxMDQuMjFaIi8+PC9zdmc+&logoWidth=20" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://www.minimax.io/platform" style="margin: 2px;">\n    <img alt="API" src="https://img.shields.io/badge/âš¡_API-Platform-FF4040?style=flat-square&labelColor=2C3E50" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/MiniMax-AI/MiniMax-MCP" style="margin: 2px;">\n    <img alt="MCP" src="https://img.shields.io/badge/ðŸš€_MCP-MiniMax_MCP-FF4040?style=flat-square&labelColor=2C3E50" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/MiniMax-AI/MiniMax-01" target="_blank" style="margin: 2px;">\n    <img alt="GitHub" src="https://img.shields.io/badge/_GitHub-MinMax-FF4040?style=flat-square&labelColor=2C3E50" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/MiniMaxAI/MiniMax-Text-01/blob/main/LICENSE-MODEL" style="margin: 2px;">\n    <img alt="Model License" src="https://img.shields.io/badge/_Model_License-Model_Agreement-FF4040?style=flat-square&labelColor=2C3E50" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n   <a href="https://huggingface.co/MiniMaxAI/MiniMax-Text-01/blob/main/LICENSE-CODE" style="margin: 2px;">\n    <img alt="Code License" src="https://img.shields.io/badge/_Code_License-MIT-FF4040?style=flat-square&labelColor=2C3E50" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/MiniMax-AI/MiniMax-AI.github.io/blob/main/images/wechat-qrcode.jpeg" target="_blank" style="margin: 2px;">\n    WeChat\n  </a>\n</div>\n\n\n# MiniMax-Text-01\n\n## 1. Introduction\n\nMiniMax-Text-01 is a powerful language model with 456 billion total parameters, of which 45.9 billion are activated per token. To better unlock the long context capabilities of the model, MiniMax-Text-01 adopts a hybrid architecture that combines Lightning Attention, Softmax Attention and Mixture-of-Experts (MoE). Leveraging advanced parallel strategies and innovative compute-communication overlap methodsâ€”such as Linear Attention Sequence Parallelism Plus (LASP+), varlen ring attention, Expert Tensor Parallel (ETP), etc., MiniMax-Text-01''s training context length is extended to 1 million tokens, and it can handle a context of up to 4 million tokens during the inference. On various academic benchmarks, MiniMax-Text-01 also demonstrates the performance of a top-tier model.\n\n<p align="center">\n  <img width="100%" src="figures/TextBench.png">\n</p>\n\n## 2. Model Architecture\n\nThe architecture of MiniMax-Text-01 is briefly described as follows:\n- Total Parameters: 456B\n- Activated Parameters per Token: 45.9B\n- Number Layers: 80\n- Hybrid Attention: a softmax attention is positioned after every 7 lightning attention.\n  - Number of attention heads: 64\n  - Attention head dimension: 128\n- Mixture of Experts:\n  - Number of experts: 32\n  - Expert hidden dimension: 9216\n  - Top-2 routing strategy\n- Positional Encoding: Rotary Position Embedding (RoPE) applied to half of the attention head dimension with a base frequency of 10,000,000\n- Hidden Size: 6144\n- Vocab Size: 200,064\n\n## 3. Evaluation\n\n### Core Academic Benchmarks\n\n| **Tasks**                     | **GPT-4o (11-20)** | **Claude-3.5-Sonnet (10-22)** | **Gemini-1.5-Pro (002)** | **Gemini-2.0-Flash (exp)** | **Qwen2.5-72B-Inst.** | **DeepSeek-V3** | **Llama-3.1-405B-Inst.** | **MiniMax-Text-01** |\n|-------------------------------|--------------------|-------------------------------|--------------------------|----------------------------|-----------------------|-----------------|--------------------------|---------------------|\n| **General**                   |                    |                               |                          |                            |                       |                 |                          |                     |\n| MMLU<sup>*</sup>                      | 85.7               | 88.3                          | 86.8                     | 86.5                       | 86.1                  | 88.5        | **88.6**                 | 88.5                |\n| MMLU-Pro<sup>*</sup>                  | 74.4               | **78.0**                      | 75.8                     | 76.4                       | 71.1                  | 75.9            | 73.3                     | 75.7                |\n| SimpleQA                      | **39.0**           | 28.1                          | 23.4                     | 26.6                       | 10.3                  | 24.9            | 23.2                     | 23.7                |\n| C-SimpleQA                    | 64.6               | 56.8                          | 59.4                     | 63.3                       | 52.2                  | 64.8            | 54.7                     | **67.4**            |\n| IFEval _(avg)_                | 84.1               | **90.1**                      | 89.4                     | 88.4                       | 87.2                  | 87.3            | 86.4                     | 89.1                |\n| Arena-Hard                    | **92.4**           | 87.6                          | 85.3                     | 72.7                       | 81.2                  | 91.4            | 63.5                     | 89.1                |\n| **Reasoning**                 |                    |                               |                          |                            |                       |                 |                          |                     |\n| GPQA<sup>*</sup> _(diamond)_          | 46.0               | **65.0**                      | 59.1                     | 62.1                       | 49.0                  | 59.1            | 50.7                     | 54.4                |\n| DROP<sup>*</sup> _(F1)_               | 89.2               | 88.8                          | 89.2                     | 89.3                       | 85.0                  | 91.0        | **92.5**                 | 87.8                |\n| **Mathematics**               |                    |                               |                          |                            |                       |                 |                          |                     |\n| GSM8k<sup>*</sup>                     | 95.6               | **96.9**                      | 95.2                     | 95.4                       | 95.8                  | 96.7            | 96.7                     | 94.8                |\n| MATH<sup>*</sup>                      | 76.6               | 74.1                          | **84.6**                 | 83.9                       | 81.8                  | **84.6**        | 73.8                     | 77.4                |\n| **Coding**                    |                    |                               |                          |                            |                       |                 |                          |                     |\n| MBPP +                        | 76.2               | 75.1                          | 75.4                     | 75.9                       | 77.0              | **78.8**        | 73.0                     | 71.7                |\n| HumanEval                     | 90.2               | **93.7**                      | 86.6                     | 89.6                       | 86.6                  | 92.1            | 89.0                     | 86.9                |\n\n<sup>*</sup> Evaluated following a _0-shot CoT_ setting.\n\n### Long Benchmarks\n#### 4M Needle In A Haystack Test\n<p align="center">\n  <img width="90%" src="figures/niah.png">\n</p>\n\n#### Ruler\n| Model | 4k | 8k | 16k | 32k | 64k | 128k | 256k | 512k | 1M |\n|-------|----|----|-----|-----|-----|------|------|------|----|\n| **GPT-4o (11-20)** | **0.970** | 0.921 | 0.890 | 0.888 | 0.884 | - | - | - | - |\n| **Claude-3.5-Sonnet (10-22)** | 0.965 | 0.960 | 0.957 | 0.950 | **0.952** | 0.938 | - | - | - |\n| **Gemini-1.5-Pro (002)** | 0.962 | 0.960 | **0.960** | **0.958** | 0.938 | 0.917 | 0.916 | 0.861 | 0.850 |\n| **Gemini-2.0-Flash (exp)** | 0.960 | 0.960 | 0.951 | 0.957 | 0.937 | 0.860 | 0.797 | 0.709 | - |\n| **MiniMax-Text-01** | 0.963 | **0.961** | 0.953 | 0.954 | 0.943 | **0.947** | **0.945** | **0.928** | **0.910** |\n\n#### LongBench v2\n| **Model**                  | **overall** | **easy** | **hard** | **short** | **medium** | **long** |\n|----------------------------|-------------|----------|----------|------------|------------|----------|\n| Human                      | 53.7        | 100.0    | 25.1     | 47.2       | 59.1       | 53.7     |\n| **w/ CoT**                 |             |          |          |            |            |          |\n| GPT-4o (11-20)             | 51.4        | 54.2     | 49.7     | 59.6       | 48.6       | 43.5     |\n| Claude-3.5-Sonnet (10-22)  | 46.7        | 55.2     | 41.5     | 53.9       | 41.9       | 44.4     |\n| Deepseek-V3                | -           | -        | -        | -          | -          | -        |\n| Qwen2.5-72B-Inst.          | 43.5        | 47.9     | 40.8     | 48.9       | 40.9       | 39.8     |\n| **MiniMax-Text-01**        | **56.5**    | **66.1** | **50.5** | **61.7**   | **56.7**   | **47.2** |\n| **w/o CoT**                |             |          |          |            |            |          |\n| GPT-4o (11-20)             | 50.1        | 57.4     | 45.6     | 53.3       | 52.4       | 40.2     |\n| Claude-3.5-Sonnet (10-22)  | 41.0        | 46.9     | 37.3     | 46.1       | 38.6       | 37.0     |\n| Deepseek-V3                | 48.7        | -        | -        | -          | -          | -        |\n| Qwen2.5-72B-Inst.          | 42.1        | 42.7     | 41.8     | 45.6       | 38.1       | **44.4** |\n| **MiniMax-Text-01**        | **52.9**    | **60.9** | **47.9** | **58.9**   | **52.6**   | 43.5     |\n\n#### MTOB\n| **Context Type** | **no context** | **half book** | **full book** | **Î” half book** | **Î” full book** |\n|------------------|----------------|---------------|---------------|------------------|-----------------|\n| **eng â†’ kalam (ChrF)** | | | | | |\n| GPT-4o (11-20) | 9.90 | **54.30** | - | 44.40 | - |\n| Claude-3.5-Sonnet (10-22) | 20.22 | 53.62 | 55.65 | 33.39 | 35.42 |\n| Gemini-1.5-Pro (002) | 16.79 | 53.68 | **57.90** | 36.89 | 41.11 |\n| Gemini-2.0-Flash (exp) | 12.20 | 49.50 | 53.30 | 37.30 | 41.10 |\n| Qwen-Long | 16.55 | 48.48 | 45.94 | 31.92 | 29.39 |\n| **MiniMax-Text-01** | 6.0 | 51.74 | 51.60 | **45.7** | **45.6** |\n| **kalam â†’ eng (BLEURT)** | | | | | |\n| GPT-4o (11-20) | 33.20 | 58.30 | - | 25.10 | - |\n| Claude-3.5-Sonnet (10-22) | 31.42 | 59.70 | 62.30 | 28.28 | 30.88 |\n| Gemini-1.5-Pro (002) | 32.02 | **61.52** | **63.09** | **29.50** | **31.07** |\n| Gemini-2.0-Flash (exp) | 33.80 | 57.50 | 57.00 | 23.70 | 23.20 |\n| Qwen-Long | 30.13 | 53.14 | 32.15 | 23.01 | 2.02 |\n| **MiniMax-Text-01** | 33.65 | 57.10 | 58.00 | 23.45 | 24.35 |\n\n\n## 4. Quickstart\nHere we provide a simple example of loading the tokenizer and model to generate content.\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, QuantoConfig, GenerationConfig\n\n# load hf config\nhf_config = AutoConfig.from_pretrained("MiniMaxAI/MiniMax-Text-01", trust_remote_code=True)\n\n# quantization config, int8 is recommended\nquantization_config =  QuantoConfig(\n            weights="int8",\n            modules_to_not_convert=[\n                "lm_head",\n                "embed_tokens",\n            ] + [f"model.layers.{i}.coefficient" for i in range(hf_config.num_hidden_layers)]\n            + [f"model.layers.{i}.block_sparse_moe.gate" for i in range(hf_config.num_hidden_layers)]\n        )\n\n# assume 8 GPUs\nworld_size = 8\nlayers_per_device = hf_config.num_hidden_layers // world_size\n# set device map\ndevice_map = {\n    ''model.embed_tokens'': ''cuda:0'',\n    ''model.norm'': f''cuda:{world_size - 1}'',\n    ''lm_head'': f''cuda:{world_size - 1}''\n}\nfor i in range(world_size):\n    for j in range(layers_per_device):\n        device_map[f''model.layers.{i * layers_per_device + j}''] = f''cuda:{i}''\n\n# load tokenizer\ntokenizer = AutoTokenizer.from_pretrained("MiniMaxAI/MiniMax-Text-01")\nprompt = "Hello!"\nmessages = [\n    {"role": "system", "content": [{"type": "text", "text": "You are a helpful assistant created by MiniMax based on MiniMax-Text-01 model."}]},\n    {"role": "user", "content": [{"type": "text", "text": prompt}]},\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n# tokenize and move to device\nmodel_inputs = tokenizer(text, return_tensors="pt").to("cuda")\n\n# load bfloat16 model, move to device, and apply quantization\nquantized_model = AutoModelForCausalLM.from_pretrained(\n    "MiniMaxAI/MiniMax-Text-01",\n    torch_dtype="bfloat16",\n    device_map=device_map,\n    quantization_config=quantization_config,\n    trust_remote_code=True,\n    offload_buffers=True,\n)\n\n# generate response\ngeneration_config = GenerationConfig(\n    max_new_tokens=20,\n    eos_token_id=200020,\n    use_cache=True,\n)\ngenerated_ids = quantized_model.generate(**model_inputs, generation_config=generation_config)\nprint(f"generated_ids: {generated_ids}")\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n## 5. Deployment Guide\nFor production deployment, we recommend using [vLLM](https://docs.vllm.ai/en/latest/) to serve MiniMax-Text-01. vLLM provides excellent performance for serving large language models with the following features:\n\nðŸ”¥ Outstanding service throughput performance  \nâš¡ Efficient and intelligent memory management  \nðŸ“¦ Powerful batch request processing capability  \nâš™ï¸ Deeply optimized underlying performance  \n\nFor detailed deployment instructions, please refer to our [vLLM Deployment Guide](https://github.com/MiniMax-AI/MiniMax-01/blob/main/docs/vllm_deployment_guide.md).\n\n## 6. Function Calling\nMiniMax-Text-01 supports Function Calling capability, enabling the model to intelligently identify when external functions need to be called and output parameters in structured JSON format. With Function Calling, you can:\n- Let the model recognize implicit function call needs in user requests\n- Receive structured parameter outputs for seamless application integration\n- Support various complex parameter types, including nested objects and arrays\nFunction Calling supports standard OpenAI-compatible format definitions and integrates seamlessly with the Transformers library. For detailed usage instructions, please refer to our [Function Call Guide](./MiniMax-Text-01_Function_Call_Guide.md) or [Chinese Guide](./MiniMax-Text-01_Function_Call_Guide_CN.md).\n\n## 7. Citation\n\n```\n@misc{minimax2025minimax01scalingfoundationmodels,\n      title={MiniMax-01: Scaling Foundation Models with Lightning Attention}, \n      author={MiniMax and Aonian Li and Bangwei Gong and Bo Yang and Boji Shan and Chang Liu and Cheng Zhu and Chunhao Zhang and Congchao Guo and Da Chen and Dong Li and Enwei Jiao and Gengxin Li and Guojun Zhang and Haohai Sun and Houze Dong and Jiadai Zhu and Jiaqi Zhuang and Jiayuan Song and Jin Zhu and Jingtao Han and Jingyang Li and Junbin Xie and Junhao Xu and Junjie Yan and Kaishun Zhang and Kecheng Xiao and Kexi Kang and Le Han and Leyang Wang and Lianfei Yu and Liheng Feng and Lin Zheng and Linbo Chai and Long Xing and Meizhi Ju and Mingyuan Chi and Mozhi Zhang and Peikai Huang and Pengcheng Niu and Pengfei Li and Pengyu Zhao and Qi Yang and Qidi Xu and Qiexiang Wang and Qin Wang and Qiuhui Li and Ruitao Leng and Shengmin Shi and Shuqi Yu and Sichen Li and Songquan Zhu and Tao Huang and Tianrun Liang and Weigao Sun and Weixuan Sun and Weiyu Cheng and Wenkai Li and Xiangjun Song and Xiao Su and Xiaodong Han and Xinjie Zhang and Xinzhu Hou and Xu Min and Xun Zou and Xuyang Shen and Yan Gong and Yingjie Zhu and Yipeng Zhou and Yiran Zhong and Yongyi Hu and Yuanxiang Fan and Yue Yu and Yufeng Yang and Yuhao Li and Yunan Huang and Yunji Li and Yunpeng Huang and Yunzhi Xu and Yuxin Mao and Zehan Li and Zekang Li and Zewei Tao and Zewen Ying and Zhaoyang Cong and Zhen Qin and Zhenhua Fan and Zhihang Yu and Zhuo Jiang and Zijia Wu},\n      year={2025},\n      eprint={2501.08313},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.08313}, \n}\n```\n\n## 8. Chatbot & API\nFor general use and evaluation, we provide a [Chatbot](https://chat.minimax.io/) with online search capabilities and the [online API](https://www.minimax.io/platform) for developers. For general use and evaluation, we provide the [MiniMax MCP Server](https://github.com/MiniMax-AI/MiniMax-MCP) with video generation, image generation, speech synthesis, and voice cloning for developers.\n\n## 9. Contact Us\nContact us at [model@minimax.io](mailto:model@minimax.io).', '{"pipeline_tag":"text-generation","library_name":null,"framework":null,"params":456089655296,"storage_bytes":914671823987,"files_count":435,"spaces_count":14,"gated":false,"private":false,"config":{"architectures":["MiniMaxText01ForCausalLM"],"auto_map":{"AutoConfig":"configuration_minimax_text_01.MiniMaxText01Config","AutoModelForCausalLM":"modeling_minimax_text_01.MiniMaxText01ForCausalLM"},"model_type":"minimax_text_01","tokenizer_config":{"bos_token":"<beginning_of_sentence>","eos_token":"<end_of_sentence>","unk_token":"<end_of_document>","chat_template":"{% for message in messages %}{% if message[''role''] == ''system'' %}{{ ''<beginning_of_sentence>system ai_setting=assistant\\n'' + message[''content''][0][''text''] + ''<end_of_sentence>\\n''}}{% elif message[''role''] == ''user'' %}{{ ''<beginning_of_sentence>user name=user\\n'' + message[''content''][0][''text''] + ''<end_of_sentence>\\n''}}{% elif message[''role''] == ''assistant'' %}{{ ''<beginning_of_sentence>ai name=assistant\\n'' }}{% for content in message[''content''] | selectattr(''type'', ''equalto'', ''text'') %}{% generation %}{{ content[''text''] }}{% endgeneration %}{% endfor %}{{ ''<end_of_sentence>\\n'' }}{% elif message[''role''] == ''function'' %}{{ ''<beginning_of_sentence>system function_response=functions\\n'' + ''{\"name\": \"'' + message[''name''] + ''\", \"response\": '' + message[''content''][0][''text''] + ''}'' + ''<end_of_sentence>\\n''}}{% endif %}{% endfor %}{% if tools %}{% for function in tools %}{{ ''<beginning_of_sentence>system function_setting=functions\\n'' + function | tojson + ''<end_of_sentence>\\n''}}{% endfor %}{% endif %}{% if add_generation_prompt %}{{ ''<beginning_of_sentence>ai name=assistant\\n'' }}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:MiniMax-AI:MiniMax-MCP\"","source_url":"https://github.com/MiniMax-AI/MiniMax-MCP\""},{"type":"has_code","target_id":"github:MiniMax-AI:MiniMax-01\"","source_url":"https://github.com/MiniMax-AI/MiniMax-01\""},{"type":"has_code","target_id":"github:MiniMax-AI:MiniMax-AI.github.io","source_url":"https://github.com/MiniMax-AI/MiniMax-AI.github.io"},{"type":"has_code","target_id":"github:MiniMax-AI:MiniMax-01","source_url":"https://github.com/MiniMax-AI/MiniMax-01"},{"type":"has_code","target_id":"github:MiniMax-AI:MiniMax-MCP","source_url":"https://github.com/MiniMax-AI/MiniMax-MCP"},{"type":"based_on_paper","target_id":"arxiv:2501.08313","source_url":"https://arxiv.org/abs/2501.08313"}]', NULL, NULL, 'pending', 68.1, '5e08d5532400824d29bb8a50cc28c76a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tencent-HunyuanOCR', 'huggingface--tencent--hunyuanocr', 'HunyuanOCR', 'tencent', '--- license: other language: - multilingual pipeline_tag: image-text-to-text library_name: transformers base_model: - tencent/HunyuanOCR tags: - ocr - hunyuan - vision-language - image-to-text - 1B - end-to-end --- <p align="center"> <img src="https://github.com/Tencent-Hunyuan/HunyuanOCR/blob/main/assets/hyocr-head-img.png?raw=true" width="80%"/> <br> </p> <p align="center"> <a href="https://huggingface.co/spaces/tencent/HunyuanOCR"><b>ðŸŽ¯ Demo</b></a> | <a href="https://huggingface.co/tencen...', '["transformers","safetensors","hunyuan_vl","text-generation","ocr","hunyuan","vision-language","image-to-text","1b","end-to-end","image-text-to-text","conversational","multilingual","arxiv:2511.19575","base_model:tencent/hunyuanocr","base_model:finetune:tencent/hunyuanocr","license:other","endpoints_compatible","region:us"]', 'image-text-to-text', 650, 461657, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tencent/HunyuanOCR","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlanguage:\n- multilingual\npipeline_tag: image-text-to-text\nlibrary_name: transformers\nbase_model:\n- tencent/HunyuanOCR\ntags:\n- ocr\n- hunyuan\n- vision-language\n- image-to-text\n- 1B\n- end-to-end\n---\n\n<p align="center">\n <img src="https://github.com/Tencent-Hunyuan/HunyuanOCR/blob/main/assets/hyocr-head-img.png?raw=true" width="80%"/> <br>\n</p>\n\n\n<p align="center">\n<a href="https://huggingface.co/spaces/tencent/HunyuanOCR"><b>ðŸŽ¯ Demo</b></a> |\n<a href="https://huggingface.co/tencent/HunyuanOCR"><b>ðŸ“¥ Model Download</b></a> |\n<a href="https://arxiv.org/abs/2511.19575"><b>ðŸ“„ Technical Report</b></a> |\n<a href="https://github.com/Tencent-Hunyuan/HunyuanOCR"><b>ðŸŒŸ Github</b></a>\n</p>\n\n<h2>\n<p align="center">\n  <a href="https://arxiv.org/abs/2511.19575">HunyuanOCR</a>\n</p>\n</h2>\n\n\n## ðŸ“– Introduction\n**HunyuanOCR** stands as a leading end-to-end OCR expert VLM powered by Hunyuan''s native multimodal architecture. With a remarkably lightweight 1B parameter design, it has achieved multiple state-of-the-art benchmarks across the industry. The model demonstrates mastery in **complex multilingual document parsing** while excelling in practical applications including **text spotting, open-field information extraction, video subtitle extraction, and photo translation**.\n\n\n## ðŸš€ Quick Start with Transformers\n\n### Installation\n```bash\npip install git+https://github.com/huggingface/transformers@82a06db03535c49aa987719ed0746a76093b1ec4\n```\n> **Note**: We will merge it into the Transformers main branch later.\n\n### Model Inference\n\n```python\nfrom transformers import AutoProcessor\nfrom transformers import HunYuanVLForConditionalGeneration\nfrom PIL import Image\nimport torch\n\ndef clean_repeated_substrings(text):\n    """Clean repeated substrings in text"""\n    n = len(text)\n    if n<8000:\n        return text\n    for length in range(2, n // 10 + 1):\n        candidate = text[-length:] \n        count = 0\n        i = n - length\n        \n        while i >= 0 and text[i:i + length] == candidate:\n            count += 1\n            i -= length\n\n        if count >= 10:\n            return text[:n - length * (count - 1)]  \n\n    return text\n\nmodel_name_or_path = "tencent/HunyuanOCR"\nprocessor = AutoProcessor.from_pretrained(model_name_or_path, use_fast=False)\nimg_path = "path/to/your/image.jpg"\nimage_inputs = Image.open(img_path)\nmessages1 = [\n    {"role": "system", "content": ""},\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": img_path},\n            {"type": "text", "text": (\n                "æ£€æµ‹å¹¶è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼Œå°†æ–‡æœ¬åæ ‡æ ¼å¼åŒ–è¾“å‡ºã€‚"\n            )},\n        ],\n    }\n]\nmessages = [messages1]\ntexts = [\n    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n    for msg in messages\n]\ninputs = processor(\n    text=texts,\n    images=image_inputs,\n    padding=True,\n    return_tensors="pt",\n)\nmodel = HunYuanVLForConditionalGeneration.from_pretrained(\n    model_name_or_path,\n    attn_implementation="eager",\n    dtype=torch.bfloat16,\n    device_map="auto"\n)\nwith torch.no_grad():\n    device = next(model.parameters()).device\n    inputs = inputs.to(device)\n    generated_ids = model.generate(**inputs, max_new_tokens=16384, do_sample=False)\nif "input_ids" in inputs:\n    input_ids = inputs.input_ids\nelse:\n    print("inputs: # fallback", inputs)\n    input_ids = inputs.inputs\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids):] for in_ids, out_ids in zip(input_ids, generated_ids)\n]\noutput_texts = clean_repeated_substrings(processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n))\nprint(output_texts)\n```\n\n\n## ðŸš€ Quick Start with vLLM\n\nCheckout [vLLM HunyuanOCR Usage Guide](https://docs.vllm.ai/projects/recipes/en/latest/Tencent-Hunyuan/HunyuanOCR.html).\n\n### Installation\n\n```bash\nuv venv hunyuanocr\nsource hunyuanocr/bin/activate\n\nuv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\n```\n\nNote: We suggest to install [cuda-compat-12-9](https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/):\n```bash\nsudo dpkg -i cuda-compat-12-9_575.57.08-0ubuntu1_amd64.deb\necho ''export LD_LIBRARY_PATH=/usr/local/cuda-12.9/compat:$LD_LIBRARY_PATH'' >> ~/.bashrc\nsource ~/.bashrc\n# verify cuda-compat-12-9\nls /usr/local/cuda-12.9/compat\n```\n\n### Model Deploy\n```bash\nvllm serve tencent/HunyuanOCR \\n    --no-enable-prefix-caching \\n    --mm-processor-cache-gb 0 \\n    --gpu-memory-utilization 0.2\n```\n\n### Model Inference\n```python\nfrom vllm import LLM, SamplingParams\nfrom PIL import Image\nfrom transformers import AutoProcessor\n\ndef clean_repeated_substrings(text):\n    """Clean repeated substrings in text"""\n    n = len(text)\n    if n<8000:\n        return text\n    for length in range(2, n // 10 + 1):\n        candidate = text[-length:] \n        count = 0\n        i = n - length\n        \n        while i >= 0 and text[i:i + length] == candidate:\n            count += 1\n            i -= length\n\n        if count >= 10:\n            return text[:n - length * (count - 1)]  \n\n    return text\n\nmodel_path = "tencent/HunyuanOCR"\nllm = LLM(model=model_path, trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(model_path)\nsampling_params = SamplingParams(temperature=0, max_tokens=16384)\n\nimg_path = "/path/to/image.jpg"\nimg = Image.open(img_path)\nmessages = [\n    {"role": "system", "content": ""},\n    {"role": "user", "content": [\n        {"type": "image", "image": img_path},\n        {"type": "text", "text": "æ£€æµ‹å¹¶è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼Œå°†æ–‡æœ¬åæ ‡æ ¼å¼åŒ–è¾“å‡ºã€‚"}\n    ]}\n]\nprompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = {"prompt": prompt, "multi_modal_data": {"image": [img]}}\noutput = llm.generate([inputs], sampling_params)[0]\nprint(clean_repeated_substrings(output.outputs[0].text))\n```\n\n## ðŸ’¬ Application-oriented Prompts\n\n| Task | English | Chinese |\n|------|---------|---------|\n| **Spotting** | Detect and recognize text in the image, and output the text coordinates in a formatted manner. | æ£€æµ‹å¹¶è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼Œå°†æ–‡æœ¬åæ ‡æ ¼å¼åŒ–è¾“å‡ºã€‚ |\n| **Parsing** | â€¢ Identify the formula in the image and represent it using LaTeX format.<br><br>â€¢ Parse the table in the image into HTML.<br><br>â€¢ Parse the chart in the image; use Mermaid format for flowcharts and Markdown for other charts.<br><br>â€¢ Extract all information from the main body of the document image and represent it in markdown format, ignoring headers and footers. Tables should be expressed in HTML format, formulas in the document should be represented using LaTeX format, and the parsing should be organized according to the reading order. | â€¢ è¯†åˆ«å›¾ç‰‡ä¸­çš„å…¬å¼ï¼Œç”¨ LaTeX æ ¼å¼è¡¨ç¤ºã€‚<br><br>â€¢ æŠŠå›¾ä¸­çš„è¡¨æ ¼è§£æžä¸º HTMLã€‚<br><br>â€¢ è§£æžå›¾ä¸­çš„å›¾è¡¨ï¼Œå¯¹äºŽæµç¨‹å›¾ä½¿ç”¨ Mermaid æ ¼å¼è¡¨ç¤ºï¼Œå…¶ä»–å›¾è¡¨ä½¿ç”¨ Markdown æ ¼å¼è¡¨ç¤ºã€‚<br><br>â€¢ æå–æ–‡æ¡£å›¾ç‰‡ä¸­æ­£æ–‡çš„æ‰€æœ‰ä¿¡æ¯ç”¨ markdown æ ¼å¼è¡¨ç¤ºï¼Œå…¶ä¸­é¡µçœ‰ã€é¡µè„šéƒ¨åˆ†å¿½ç•¥ï¼Œè¡¨æ ¼ç”¨ html æ ¼å¼è¡¨è¾¾ï¼Œæ–‡æ¡£ä¸­å…¬å¼ç”¨ latex æ ¼å¼è¡¨ç¤ºï¼ŒæŒ‰ç…§é˜…è¯»é¡ºåºç»„ç»‡è¿›è¡Œè§£æžã€‚ |\n| **Information Extraction** | â€¢ Output the value of Key.<br><br>â€¢ Extract the content of the fields: [''key1'',''key2'', ...] from the image and return it in JSON format.<br><br>â€¢ Extract the subtitles from the image. | â€¢ è¾“å‡º Key çš„å€¼ã€‚<br><br>â€¢ æå–å›¾ç‰‡ä¸­çš„: [''key1'',''key2'', ...] çš„å­—æ®µå†…å®¹ï¼Œå¹¶æŒ‰ç…§ JSON æ ¼å¼è¿”å›žã€‚<br><br>â€¢ æå–å›¾ç‰‡ä¸­çš„å­—å¹•ã€‚ |\n| **Translation** | First extract the text, then translate the text content into English. If it is a document, ignore the header and footer. Formulas should be represented in LaTeX format, and tables should be represented in HTML format. | å…ˆæå–æ–‡å­—ï¼Œå†å°†æ–‡å­—å†…å®¹ç¿»è¯‘ä¸ºè‹±æ–‡ã€‚è‹¥æ˜¯æ–‡æ¡£ï¼Œåˆ™å…¶ä¸­é¡µçœ‰ã€é¡µè„šå¿½ç•¥ã€‚å…¬å¼ç”¨latexæ ¼å¼è¡¨ç¤ºï¼Œè¡¨æ ¼ç”¨htmlæ ¼å¼è¡¨ç¤ºã€‚ |\n\n## ðŸ¤ Join Our Community\n\n<div align="center">\n\n| Wechat Discussion Group | Discord Group |\n| :---: | :---: |\n| <img src="https://github.com/Tencent-Hunyuan/HunyuanOCR/blob/main/assets/qrcode_for_hunyuanocr_wechat.jpg?raw=true" width="150"> | [Join HunyuanOCR Discord](https://discord.gg/XeD3p2MRDk) |\n\n</div>\n\n## ðŸ“š Citation\n```\n@misc{hunyuanvisionteam2025hunyuanocrtechnicalreport,\n      title={HunyuanOCR Technical Report}, \n      author={Hunyuan Vision Team and Pengyuan Lyu and Xingyu Wan and Gengluo Li and Shangpin Peng and Weinong Wang and Liang Wu and Huawen Shen and Yu Zhou and Canhui Tang and Qi Yang and Qiming Peng and Bin Luo and Hower Yang and Xinsong Zhang and Jinnian Zhang and Houwen Peng and Hongming Yang and Senhao Xie and Longsha Zhou and Ge Pei and Binghong Wu and Kan Wu and Jieneng Yang and Bochao Wang and Kai Liu and Jianchen Zhu and Jie Jiang and Linus and Han Hu and Chengquan Zhang},\n      year={2025},\n      journal={arXiv preprint arXiv:2511.19575},\n      url={https://arxiv.org/abs/2511.19575}, \n}\n```\n\n## ðŸ™ Acknowledgements\nWe would like to thank [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR), [MinerU](https://github.com/opendatalab/MinerU), [MonkeyOCR](https://github.com/Yuliang-Liu/MonkeyOCR), [DeepSeek-OCR](https://github.com/deepseek-ai/DeepSeek-OCR), [dots.ocr](https://github.com/rednote-hilab/dots.ocr) for their valuable models and ideas.\nWe also appreciate the benchmarks: [OminiDocBench](https://github.com/opendatalab/OmniDocBench), [OCRBench](https://github.com/Yuliang-Liu/MultimodalOCR/tree/main/OCRBench), [DoTA](https://github.com/liangyupu/DIMTDA).', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":996208112,"storage_bytes":1992495848,"files_count":15,"spaces_count":9,"gated":false,"private":false,"config":{"architectures":["HunYuanVLForConditionalGeneration"],"model_type":"hunyuan_vl","tokenizer_config":{"bos_token":"<ï½œhy_beginâ–ofâ–sentenceï½œ>","eos_token":"<ï½œhy_Assistantï½œ>","pad_token":"<ï½œhy_â–padâ–ï½œ>","chat_template":"{% if messages[0][''role''] == ''system'' %}{% set loop_messages = messages[1:] %}{% if messages[0][''content''] is string %}{% set system_message = messages[0][''content''] %}{% else %}{% set system_message = messages[0][''content''][''text''] %}{% endif %}<ï½œhy_beginâ–ofâ–sentenceï½œ>{{ system_message }}<ï½œhy_placeâ–holderâ–noâ–3ï½œ>{% else %}{% set loop_messages = messages %}<ï½œhy_beginâ–ofâ–sentenceï½œ>{% endif %}{% for message in loop_messages %}{% if message[''role''] == ''user'' %}{% if message[''content''] is string %}{{ message[''content''] }}{% else %}{% for content in message[''content''] %}{% if content[''type''] == ''image'' or ''image'' in content or ''image_url'' in content %}<ï½œhy_placeâ–holderâ–noâ–100ï½œ><ï½œhy_placeâ–holderâ–noâ–102ï½œ><ï½œhy_placeâ–holderâ–noâ–101ï½œ>{% elif ''text'' in content %}{{ content[''text''] }}{% endif %}{% endfor %}{% endif %}<ï½œhy_Userï½œ>{% elif message[''role''] == ''assistant'' %}{{ message[''content''] }}<ï½œhy_Assistantï½œ>{% endif %}{% endfor %}"}}}', '[]', '[{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanOCR","source_url":"https://github.com/Tencent-Hunyuan/HunyuanOCR"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanOCR\"><b>ðŸŒŸ","source_url":"https://github.com/Tencent-Hunyuan/HunyuanOCR\"><b>ðŸŒŸ"},{"type":"has_code","target_id":"github:huggingface:transformers@82a06db03535c49aa987719ed0746a76093b1ec4","source_url":"https://github.com/huggingface/transformers@82a06db03535c49aa987719ed0746a76093b1ec4"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanOCR","source_url":"https://github.com/Tencent-Hunyuan/HunyuanOCR"},{"type":"has_code","target_id":"github:PaddlePaddle:PaddleOCR","source_url":"https://github.com/PaddlePaddle/PaddleOCR"},{"type":"has_code","target_id":"github:opendatalab:MinerU","source_url":"https://github.com/opendatalab/MinerU"},{"type":"has_code","target_id":"github:Yuliang-Liu:MonkeyOCR","source_url":"https://github.com/Yuliang-Liu/MonkeyOCR"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-OCR","source_url":"https://github.com/deepseek-ai/DeepSeek-OCR"},{"type":"has_code","target_id":"github:rednote-hilab:dots.ocr","source_url":"https://github.com/rednote-hilab/dots.ocr"},{"type":"has_code","target_id":"github:opendatalab:OmniDocBench","source_url":"https://github.com/opendatalab/OmniDocBench"},{"type":"has_code","target_id":"github:Yuliang-Liu:MultimodalOCR","source_url":"https://github.com/Yuliang-Liu/MultimodalOCR"},{"type":"has_code","target_id":"github:liangyupu:DIMTDA","source_url":"https://github.com/liangyupu/DIMTDA"},{"type":"based_on_paper","target_id":"arxiv:2511.19575","source_url":"https://arxiv.org/abs/2511.19575"}]', NULL, 'Other', 'approved', 63.1, 'b4ce886cab7db837d9df12d4181595e3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-TheBloke-Mixtral-8x7B-Instruct-v0.1-GGUF', 'huggingface--thebloke--mixtral-8x7b-instruct-v0.1-gguf', 'Mixtral-8x7B-Instruct-v0.1-GGUF', 'TheBloke', '--- base_model: mistralai/Mixtral-8x7B-Instruct-v0.1 inference: false language: - fr - it - de - es - en license: apache-2.0 model_creator: Mistral AI_ model_name: Mixtral 8X7B Instruct v0.1 model_type: mixtral prompt_template: ''[INST] {prompt} [/INST] '' quantized_by: TheBloke widget: - output: text: ''Arr, shiver me timbers! Ye have a llama on yer lawn, ye say? Well, that be a new one for me! Here''''s what I''''d suggest, arr: 1. Firstly, ensure yer safety. Llamas may look gentle, but they can b...', '["transformers","gguf","mixtral","fr","it","de","es","en","base_model:mistralai/mixtral-8x7b-instruct-v0.1","license:apache-2.0","region:us","conversational"]', 'other', 649, 32523, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '---\nbase_model: mistralai/Mixtral-8x7B-Instruct-v0.1\ninference: false\nlanguage:\n- fr\n- it\n- de\n- es\n- en\nlicense: apache-2.0\nmodel_creator: Mistral AI_\nmodel_name: Mixtral 8X7B Instruct v0.1\nmodel_type: mixtral\nprompt_template: ''[INST] {prompt} [/INST]\n\n  ''\nquantized_by: TheBloke\nwidget:\n- output:\n    text: ''Arr, shiver me timbers! Ye have a llama on yer lawn, ye say? Well, that\n      be a new one for me! Here''''s what I''''d suggest, arr:\n\n\n      1. Firstly, ensure yer safety. Llamas may look gentle, but they can be protective\n      if they feel threatened.\n\n      2. Try to make the area less appealing to the llama. Remove any food sources\n      or water that might be attracting it.\n\n      3. Contact local animal control or a wildlife rescue organization. They be the\n      experts and can provide humane ways to remove the llama from yer property.\n\n      4. If ye have any experience with animals, you could try to gently herd the\n      llama towards a nearby field or open space. But be careful, arr!\n\n\n      Remember, arr, it be important to treat the llama with respect and care. It\n      be a creature just trying to survive, like the rest of us.''\n  text: ''[INST] You are a pirate chatbot who always responds with Arr and pirate speak!\n\n    There''''s a llama on my lawn, how can I get rid of him? [/INST]''\n---\n<!-- markdownlint-disable MD041 -->\n\n<!-- header start -->\n<!-- 200823 -->\n<div style="width: auto; margin-left: auto; margin-right: auto">\n<img src="https://i.imgur.com/EBdldam.jpg" alt="TheBlokeAI" style="width: 100%; min-width: 400px; display: block; margin: auto;">\n</div>\n<div style="display: flex; justify-content: space-between; width: 100%;">\n    <div style="display: flex; flex-direction: column; align-items: flex-start;">\n        <p style="margin-top: 0.5em; margin-bottom: 0em;"><a href="https://discord.gg/theblokeai">Chat & support: TheBloke''s Discord server</a></p>\n    </div>\n    <div style="display: flex; flex-direction: column; align-items: flex-end;">\n        <p style="margin-top: 0.5em; margin-bottom: 0em;"><a href="https://www.patreon.com/TheBlokeAI">Want to contribute? TheBloke''s Patreon page</a></p>\n    </div>\n</div>\n<div style="text-align:center; margin-top: 0em; margin-bottom: 0em"><p style="margin-top: 0.25em; margin-bottom: 0em;">TheBloke''s LLM work is generously supported by a grant from <a href="https://a16z.com">andreessen horowitz (a16z)</a></p></div>\n<hr style="margin-top: 1.0em; margin-bottom: 1.0em;">\n<!-- header end -->\n\n# Mixtral 8X7B Instruct v0.1 - GGUF\n- Model creator: [Mistral AI_](https://huggingface.co/mistralai)\n- Original model: [Mixtral 8X7B Instruct v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)\n\n<!-- description start -->\n## Description\n\nThis repo contains GGUF format model files for [Mistral AI_''s Mixtral 8X7B Instruct v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1).\n\n<!-- description end -->\n<!-- README_GGUF.md-about-gguf start -->\n### About GGUF\n\nGGUF is a new format introduced by the llama.cpp team on August 21st 2023. It is a replacement for GGML, which is no longer supported by llama.cpp.\n\n### Mixtral GGUF\n\nSupport for Mixtral was merged into Llama.cpp on December 13th.\n\nThese Mixtral GGUFs are known to work in:\n\n* llama.cpp as of December 13th\n* KoboldCpp 1.52 as later\n* LM Studio 0.2.9 and later\n* llama-cpp-python 0.2.23 and later\n\nOther clients/libraries, not listed above, may not yet work.\n\n<!-- README_GGUF.md-about-gguf end -->\n<!-- repositories-available start -->\n## Repositories available\n\n* [AWQ model(s) for GPU inference.](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ)\n* [GPTQ models for GPU inference, with multiple quantisation parameter options.](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ)\n* [2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF)\n* [Mistral AI_''s original unquantised fp16 model in pytorch format, for GPU inference and for further conversions](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)\n<!-- repositories-available end -->\n\n<!-- prompt-template start -->\n## Prompt template: Mistral\n\n```\n[INST] {prompt} [/INST]\n\n```\n\n<!-- prompt-template end -->\n\n\n<!-- compatibility_gguf start -->\n## Compatibility\n\nThese Mixtral GGUFs are compatible with llama.cpp from December 13th onwards. Other clients/libraries may not work yet.\n\n## Explanation of quantisation methods\n\n<details>\n  <summary>Click to see details</summary>\n\nThe new methods available are:\n\n* GGML_TYPE_Q2_K - "type-1" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\n* GGML_TYPE_Q3_K - "type-0" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\n* GGML_TYPE_Q4_K - "type-1" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\n* GGML_TYPE_Q5_K - "type-1" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\n* GGML_TYPE_Q6_K - "type-0" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\n\nRefer to the Provided Files table below to see what files use which methods, and how.\n</details>\n<!-- compatibility_gguf end -->\n\n<!-- README_GGUF.md-provided-files start -->\n## Provided files\n\n| Name | Quant method | Bits | Size | Max RAM required | Use case |\n| ---- | ---- | ---- | ---- | ---- | ----- |\n| [mixtral-8x7b-instruct-v0.1.Q2_K.gguf](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/blob/main/mixtral-8x7b-instruct-v0.1.Q2_K.gguf) | Q2_K | 2 | 15.64 GB| 18.14 GB | smallest, significant quality loss - not recommended for most purposes |\n| [mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/blob/main/mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf) | Q3_K_M | 3 | 20.36 GB| 22.86 GB | very small, high quality loss |\n| [mixtral-8x7b-instruct-v0.1.Q4_0.gguf](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/blob/main/mixtral-8x7b-instruct-v0.1.Q4_0.gguf) | Q4_0 | 4 | 26.44 GB| 28.94 GB | legacy; small, very high quality loss - prefer using Q3_K_M |\n| [mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/blob/main/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf) | Q4_K_M | 4 | 26.44 GB| 28.94 GB | medium, balanced quality - recommended |\n| [mixtral-8x7b-instruct-v0.1.Q5_0.gguf](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/blob/main/mixtral-8x7b-instruct-v0.1.Q5_0.gguf) | Q5_0 | 5 | 32.23 GB| 34.73 GB | legacy; medium, balanced quality - prefer using Q4_K_M |\n| [mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/blob/main/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf) | Q5_K_M | 5 | 32.23 GB| 34.73 GB | large, very low quality loss - recommended |\n| [mixtral-8x7b-instruct-v0.1.Q6_K.gguf](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/blob/main/mixtral-8x7b-instruct-v0.1.Q6_K.gguf) | Q6_K | 6 | 38.38 GB| 40.88 GB | very large, extremely low quality loss |\n| [mixtral-8x7b-instruct-v0.1.Q8_0.gguf](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/blob/main/mixtral-8x7b-instruct-v0.1.Q8_0.gguf) | Q8_0 | 8 | 49.62 GB| 52.12 GB | very large, extremely low quality loss - not recommended |\n\n**Note**: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.\n\n\n\n<!-- README_GGUF.md-provided-files end -->\n\n<!-- README_GGUF.md-how-to-download start -->\n## How to download GGUF files\n\n**Note for manual downloaders:** You almost never want to clone the entire repo! Multiple different quantisation formats are provided, and most users only want to pick and download a single file.\n\nThe following clients/libraries will automatically download models for you, providing a list of available models to choose from:\n\n* LM Studio\n* LoLLMS Web UI\n* Faraday.dev\n\n### In `text-generation-webui`\n\nUnder Download Model, you can enter the model repo: TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF and below it, a specific filename to download, such as: mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf.\n\nThen click Download.\n\n### On the command line, including multiple files at once\n\nI recommend using the `huggingface-hub` Python library:\n\n```shell\npip3 install huggingface-hub\n```\n\nThen you can download any individual model file to the current directory, at high speed, with a command like this:\n\n```shell\nhuggingface-cli download TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n```\n\n<details>\n  <summary>More advanced huggingface-cli download usage (click to read)</summary>\n\nYou can also download multiple files at once with a pattern:\n\n```shell\nhuggingface-cli download TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF --local-dir . --local-dir-use-symlinks False --include=''*Q4_K*gguf''\n```\n\nFor more documentation on downloading with `huggingface-cli`, please see: [HF -> Hub Python Library -> Download files -> Download from the CLI](https://huggingface.co/docs/huggingface_hub/guides/download#download-from-the-cli).\n\nTo accelerate downloads on fast connections (1Gbit/s or higher), install `hf_transfer`:\n\n```shell\npip3 install hf_transfer\n```\n\nAnd set environment variable `HF_HUB_ENABLE_HF_TRANSFER` to `1`:\n\n```shell\nHF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n```\n\nWindows Command Line users: You can set the environment variable by running `set HF_HUB_ENABLE_HF_TRANSFER=1` before the download command.\n</details>\n<!-- README_GGUF.md-how-to-download end -->\n\n<!-- README_GGUF.md-how-to-run start -->\n## Example `llama.cpp` command\n\nMake sure you are using `llama.cpp` from commit [d0cee0d](https://github.com/ggerganov/llama.cpp/commit/d0cee0d36d5be95a0d9088b674dbb27354107221) or later.\n\n```shell\n./main -ngl 35 -m mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p "[INST] {prompt} [/INST]"\n```\n\nChange `-ngl 32` to the number of layers to offload to GPU. Remove it if you don''t have GPU acceleration.\n\nChange `-c 2048` to the desired sequence length. For extended sequence models - eg 8K, 16K, 32K - the necessary RoPE scaling parameters are read from the GGUF file and set by llama.cpp automatically. Note that longer sequence lengths require much more resources, so you may need to reduce this value.\n\nIf you want to have a chat-style conversation, replace the `-p <PROMPT>` argument with `-i -ins`\n\nFor other parameters and how to use them, please refer to [the llama.cpp documentation](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md)\n\n## How to run in `text-generation-webui`\n\nNote that text-generation-webui may not yet be compatible with Mixtral GGUFs. Please check compatibility first.\n\nFurther instructions can be found in the text-generation-webui documentation, here: [text-generation-webui/docs/04 â€ Model Tab.md](https://github.com/oobabooga/text-generation-webui/blob/main/docs/04%20%E2%80%90%20Model%20Tab.md#llamacpp).\n\n## How to run from Python code\n\nYou can use GGUF models from Python using the [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) version 0.2.23 and later.\n\n### How to load this model in Python code, using llama-cpp-python\n\nFor full documentation, please see: [llama-cpp-python docs](https://abetlen.github.io/llama-cpp-python/).\n\n#### First install the package\n\nRun one of the following commands, according to your system:\n\n```shell\n# Base ctransformers with no GPU acceleration\npip install llama-cpp-python\n# With NVidia CUDA acceleration\nCMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python\n# Or with OpenBLAS acceleration\nCMAKE_ARGS="-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python\n# Or with CLBLast acceleration\nCMAKE_ARGS="-DLLAMA_CLBLAST=on" pip install llama-cpp-python\n# Or with AMD ROCm GPU acceleration (Linux only)\nCMAKE_ARGS="-DLLAMA_HIPBLAS=on" pip install llama-cpp-python\n# Or with Metal GPU acceleration for macOS systems only\nCMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python\n\n# In windows, to set the variables CMAKE_ARGS in PowerShell, follow this format; eg for NVidia CUDA:\n$env:CMAKE_ARGS = "-DLLAMA_OPENBLAS=on"\npip install llama-cpp-python\n```\n\n#### Simple llama-cpp-python example code\n\n```python\nfrom llama_cpp import Llama\n\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\nllm = Llama(\n  model_path="./mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf",  # Download the model file first\n  n_ctx=2048,  # The max sequence length to use - note that longer sequence lengths require much more resources\n  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n  n_gpu_layers=35         # The number of layers to offload to GPU, if you have GPU acceleration available\n)\n\n# Simple inference example\noutput = llm(\n  "[INST] {prompt} [/INST]", # Prompt\n  max_tokens=512,  # Generate up to 512 tokens\n  stop=["</s>"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n  echo=True        # Whether to echo the prompt\n)\n\n# Chat Completion API\n\nllm = Llama(model_path="./mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf", chat_format="llama-2")  # Set chat_format according to the model you are using\nllm.create_chat_completion(\n    messages = [\n        {"role": "system", "content": "You are a story writing assistant."},\n        {\n            "role": "user",\n            "content": "Write a story about llamas."\n        }\n    ]\n)\n```\n\n## How to use with LangChain\n\nHere are guides on using llama-cpp-python and ctransformers with LangChain:\n\n* [LangChain + llama-cpp-python](https://python.langchain.com/docs/integrations/llms/llamacpp)\n\n<!-- README_GGUF.md-how-to-run end -->\n\n<!-- footer start -->\n<!-- 200823 -->\n## Discord\n\nFor further support, and discussions on these models and AI in general, join us at:\n\n[TheBloke AI''s Discord server](https://discord.gg/theblokeai)\n\n## Thanks, and how to contribute\n\nThanks to the [chirper.ai](https://chirper.ai) team!\n\nThanks to Clay from [gpus.llm-utils.org](llm-utils)!\n\nI''ve had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\n\nIf you''re able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\n\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\n\n* Patreon: https://patreon.com/TheBlokeAI\n* Ko-Fi: https://ko-fi.com/TheBlokeAI\n\n**Special thanks to**: Aemon Algiz.\n\n**Patreon special mentions**: Michael Levine, é˜¿æ˜Ž, Trailburnt, Nikolai Manek, John Detwiler, Randy H, Will Dee, Sebastain Graf, NimbleBox.ai, Eugene Pentland, Emad Mostaque, Ai Maven, Jim Angel, Jeff Scroggin, Michael Davis, Manuel Alberto Morcote, Stephen Murray, Robert, Justin Joy, Luke @flexchar, Brandon Frisco, Elijah Stavena, S_X, Dan Guido, Undi ., Komninos Chatzipapas, Shadi, theTransient, Lone Striker, Raven Klaugh, jjj, Cap''n Zoog, Michel-Marie MAUDET (LINAGORA), Matthew Berman, David, Fen Risland, Omer Bin Jawed, Luke Pendergrass, Kalila, OG, Erik BjÃ¤reholt, Rooh Singh, Joseph William Delisle, Dan Lewis, TL, John Villwock, AzureBlack, Brad, Pedro Madruga, Caitlyn Gatomon, K, jinyuan sun, Mano Prime, Alex, Jeffrey Morgan, Alicia Loh, Illia Dulskyi, Chadd, transmissions 11, fincy, Rainer Wilmers, ReadyPlayerEmma, knownsqashed, Mandus, biorpg, Deo Leter, Brandon Phillips, SuperWojo, Sean Connelly, Iucharbius, Jack West, Harry Royden McLaughlin, Nicholas, terasurfer, Vitor Caleffi, Duane Dunston, Johann-Peter Hartmann, David Ziegler, Olakabola, Ken Nordquist, Trenton Dambrowitz, Tom X Nguyen, Vadim, Ajan Kanaga, Leonard Tan, Clay Pascal, Alexandros Triantafyllidis, JM33133, Xule, vamX, ya boyyy, subjectnull, Talal Aujan, Alps Aficionado, wassieverse, Ari Malik, James Bentley, Woland, Spencer Kim, Michael Dempsey, Fred von Graf, Elle, zynix, William Richards, Stanislav Ovsiannikov, Edmond Seymore, Jonathan Leane, Martin Kemka, usrbinkat, Enrico Ros\n\n\nThank you to all my generous patrons and donaters!\n\nAnd thank you again to a16z for their generous grant.\n\n<!-- footer end -->\n\n<!-- original-model-card start -->\n# Original model card: Mistral AI_''s Mixtral 8X7B Instruct v0.1\n\n# Model Card for Mixtral-8x7B\nThe Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks we tested.\n\nFor full details of this model please read our [release blog post](https://mistral.ai/news/mixtral-of-experts/).\n\n## Warning\nThis repo contains weights that are compatible with [vLLM](https://github.com/vllm-project/vllm) serving of the model as well as Hugging Face [transformers](https://github.com/huggingface/transformers) library. It is based on the original Mixtral [torrent release](magnet:?xt=urn:btih:5546272da9065eddeb6fcd7ffddeef5b75be79a7&dn=mixtral-8x7b-32kseqlen&tr=udp%3A%2F%http://2Fopentracker.i2p.rocks%3A6969%2Fannounce&tr=http%3A%2F%http://2Ftracker.openbittorrent.com%3A80%2Fannounce), but the file format and parameter names are different. Please note that model cannot (yet) be instantiated with HF.\n\n## Instruction format\n\nThis format must be strictly respected, otherwise the model will generate sub-optimal outputs.\n\nThe template used to build a prompt for the Instruct model is defined as follows:\n```\n<s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]\n```\nNote that `<s>` and `</s>` are special tokens for beginning of string (BOS) and end of string (EOS) while [INST] and [/INST] are regular strings.\n\nAs reference, here is the pseudo-code used to tokenize instructions during fine-tuning:\n```python\ndef tokenize(text):\n    return tok.encode(text, add_special_tokens=False)\n\n[BOS_ID] +\ntokenize("[INST]") + tokenize(USER_MESSAGE_1) + tokenize("[/INST]") +\ntokenize(BOT_MESSAGE_1) + [EOS_ID] +\nâ€¦\ntokenize("[INST]") + tokenize(USER_MESSAGE_N) + tokenize("[/INST]") +\ntokenize(BOT_MESSAGE_N) + [EOS_ID]\n```\n\nIn the pseudo-code above, note that the `tokenize` method should not add a BOS or EOS token automatically, but should add a prefix space.\n\n## Run the model\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = "mistralai/Mixtral-8x7B-Instruct-v0.1"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\ntext = "Hello my name is"\ninputs = tokenizer(text, return_tensors="pt")\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\nBy default, transformers will load the model in full precision. Therefore you might be interested to further reduce down the memory requirements to run the model through the optimizations we offer in HF ecosystem:\n\n### In half-precision\n\nNote `float16` precision only works on GPU devices\n\n<details>\n<summary> Click to expand </summary>\n\n```diff\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = "mistralai/Mixtral-8x7B-Instruct-v0.1"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(0)\n\ntext = "Hello my name is"\n+ inputs = tokenizer(text, return_tensors="pt").to(0)\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n</details>\n\n### Lower precision using (8-bit & 4-bit) using `bitsandbytes`\n\n<details>\n<summary> Click to expand </summary>\n\n```diff\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = "mistralai/Mixtral-8x7B-Instruct-v0.1"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)\n\ntext = "Hello my name is"\n+ inputs = tokenizer(text, return_tensors="pt").to(0)\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n</details>\n\n### Load the model with Flash Attention 2\n\n<details>\n<summary> Click to expand </summary>\n\n```diff\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = "mistralai/Mixtral-8x7B-Instruct-v0.1"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=True)\n\ntext = "Hello my name is"\n+ inputs = tokenizer(text, return_tensors="pt").to(0)\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n</details>\n\n## Limitations\n\nThe Mixtral-8x7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.\nIt does not have any moderation mechanisms. We''re looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\n\n# The Mistral AI Team\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed.\n\n<!-- original-model-card end -->\n', '{"pipeline_tag":null,"library_name":"transformers","framework":"transformers","params":null,"storage_bytes":482704079616,"files_count":11,"spaces_count":29,"gated":false,"private":false,"config":{"model_type":"mixtral"}}', '[]', '[{"type":"has_code","target_id":"github:ggerganov:llama.cpp","source_url":"https://github.com/ggerganov/llama.cpp"},{"type":"has_code","target_id":"github:ggerganov:llama.cpp","source_url":"https://github.com/ggerganov/llama.cpp"},{"type":"has_code","target_id":"github:oobabooga:text-generation-webui","source_url":"https://github.com/oobabooga/text-generation-webui"},{"type":"has_code","target_id":"github:abetlen:llama-cpp-python","source_url":"https://github.com/abetlen/llama-cpp-python"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"}]', NULL, 'Apache-2.0', 'approved', 78.1, 'ace5667a2f7df91fd73f348b1f9cff6f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-allenai-OLMo-7B', 'huggingface--allenai--olmo-7b', 'OLMo-7B', 'allenai', '--- license: apache-2.0 datasets: - allenai/dolma language: - en --- <img src="https://allenai.org/olmo/olmo-7b-animation.gif" alt="OLMo Logo" width="800" style="margin-left:''auto'' margin-right:''auto'' display:''block''"/> <!-- Provide a quick summary of what the model is/does. --> **For transformers versions v4.40.0 or newer, we suggest using OLMo 7B HF instead.** OLMo is a series of **O**pen **L**anguage **Mo**dels designed to enable the science of language models. The OLMo models are trained ...', '["transformers","pytorch","safetensors","hf_olmo","text-generation","custom_code","en","dataset:allenai/dolma","arxiv:2402.00838","arxiv:2302.13971","license:apache-2.0","region:us"]', 'text-generation', 649, 2549, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/allenai/OLMo-7B","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ndatasets:\n- allenai/dolma\nlanguage:\n- en\n---\n\n[![mof-class1-qualified](https://mot.isitopen.ai/modules/mof/assets/badge_class1_qualified.png)](https://mot.isitopen.ai/model/1035)\n\n\n<img src="https://allenai.org/olmo/olmo-7b-animation.gif" alt="OLMo Logo" width="800" style="margin-left:''auto'' margin-right:''auto'' display:''block''"/>\n\n\n# Model Card for OLMo 7B\n\n<!-- Provide a quick summary of what the model is/does. -->\n\n**For transformers versions v4.40.0 or newer, we suggest using [OLMo 7B HF](https://huggingface.co/allenai/OLMo-7B-hf) instead.**\n\nOLMo is a series of **O**pen **L**anguage **Mo**dels designed to enable the science of language models.\nThe OLMo models are trained on the [Dolma](https://huggingface.co/datasets/allenai/dolma) dataset.\nWe release all code, checkpoints, logs (coming soon), and details involved in training these models.\n\n*A new version of this model with a 24 point improvement on MMLU is available [here](https://huggingface.co/allenai/OLMo-1.7-7B)*.\n\n## Model Details\n\nThe core models released in this batch are the following: \n| Size | Training Tokens | Layers | Hidden Size | Attention Heads | Context Length |\n|------|--------|---------|-------------|-----------------|----------------|\n| [OLMo 1B](https://huggingface.co/allenai/OLMo-1B)   | 3 Trillion |16     | 2048        | 16              | 2048  |\n| [OLMo 7B](https://huggingface.co/allenai/OLMo-7B) | 2.5 Trillion   | 32     | 4096        | 32              |  2048  |\n| [OLMo 7B Twin 2T](https://huggingface.co/allenai/OLMo-7B-Twin-2T) | 2 Trillion   | 32     | 4096        | 32              |  2048  |\n\nWe are releasing many checkpoints for these models, for every 1000 traing steps.\nThe naming convention is `step1000-tokens4B`.\nIn particular, we focus on four revisions of the 7B models:\n\n| Name | HF Repo | Model Revision |  Tokens | Note |\n|------------|---------|----------------|-------------------|------|\n|OLMo 7B| [allenai/OLMo-7B](https://huggingface.co/allenai/OLMo-7B)|`main`| 2.5T|The base OLMo 7B model|\n|OLMo 7B (not annealed)|[allenai/OLMo-7B](https://huggingface.co/allenai/OLMo-7B)|step556000-tokens2460B|2.5T| learning rate not annealed to 0|\n|OLMo 7B-2T|[allenai/OLMo-7B](https://huggingface.co/allenai/OLMo-7B)| step452000-tokens2000B |2T| OLMo checkpoint at 2T tokens|\n|OLMo-7B-Twin-2T|[allenai/OLMo-7B-Twin-2T](https://huggingface.co/allenai/OLMo-7B-Twin-2T)|`main`|2T| Twin version on different hardware|\n\nTo load a specific model revision with HuggingFace, simply add the argument `revision`:\n```bash\nfrom hf_olmo import OLMoForCausalLM  # pip install ai2-olmo\n\nolmo = OLMoForCausalLM.from_pretrained("allenai/OLMo-7B", revision="step1000-tokens4B")\n```\n\nAll revisions/branches are listed in the file `revisions.txt`. \nOr, you can access all the revisions for the models via the following code snippet:\n```python\nfrom huggingface_hub import list_repo_refs\nout = list_repo_refs("allenai/OLMo-7B")\nbranches = [b.name for b in out.branches]\n```\nA few revisions were lost due to an error, but the vast majority are present.\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\n- **Developed by:** Allen Institute for AI (AI2)\n- **Supported by:** Databricks, Kempner Institute for the Study of Natural and Artificial Intelligence at Harvard University, AMD, CSC (Lumi Supercomputer), UW\n- **Model type:** a Transformer style autoregressive language model.\n- **Language(s) (NLP):** English\n- **License:** The code and model are released under Apache 2.0.\n- **Contact:** Technical inquiries: `olmo at allenai dot org`. Press: `press at allenai dot org`\n- **Date cutoff:** Feb./March 2023 based on Dolma dataset version.\n\n\n### Model Sources\n\n<!-- Provide the basic links for the model. -->\n\n- **Project Page:** https://allenai.org/olmo\n- **Repositories:** \n    - Core repo (training, inference, fine-tuning etc.): https://github.com/allenai/OLMo\n    - Evaluation code: https://github.com/allenai/OLMo-Eval\n    - Further fine-tuning code: https://github.com/allenai/open-instruct\n- **Paper:** [Link](https://arxiv.org/abs/2402.00838)\n- **Technical blog post:** https://blog.allenai.org/olmo-open-language-model-87ccfc95f580\n- **W&B Logs:** https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5\n<!-- - **Press release:** TODO -->\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Inference\nQuickly get inference running with the following required installation:\n```bash\npip install ai2-olmo\n```\nNow, proceed as usual with HuggingFace:\n```python\nfrom hf_olmo import OLMoForCausalLM, OLMoTokenizerFast\n\nolmo = OLMoForCausalLM.from_pretrained("allenai/OLMo-7B")\ntokenizer = OLMoTokenizerFast.from_pretrained("allenai/OLMo-7B")\nmessage = ["Language modeling is"]\ninputs = tokenizer(message, return_tensors=''pt'', return_token_type_ids=False)\n# optional verifying cuda\n# inputs = {k: v.to(''cuda'') for k,v in inputs.items()}\n# olmo = olmo.to(''cuda'')\nresponse = olmo.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\nprint(tokenizer.batch_decode(response, skip_special_tokens=True)[0])\n>> ''Language modeling is the first step to build natural language generation...''\n```\n\nYou can make this slightly faster by quantizing the model, e.g. `AutoModelForCausalLM.from_pretrained("allenai/OLMo-7B", torch_dtype=torch.float16, load_in_8bit=True)` (requires `bitsandbytes`).\nThe quantized model is more sensitive to typing / cuda, so it is recommended to pass the inputs as `inputs.input_ids.to(''cuda'')` to avoid potential issues.\n\nNote, you may see the following error if `ai2-olmo` is not installed correctly, which is caused by internal Python check naming. We''ll update the code soon to make this error clearer.\n```bash\n    raise ImportError(\nImportError: This modeling file requires the following packages that were not found in your environment: hf_olmo. Run `pip install hf_olmo`\n```\n\n### Fine-tuning\nModel fine-tuning can be done from the final checkpoint (the `main` revision of this model) or many intermediate checkpoints. Two recipes for tuning are available.\n1. Fine-tune with the OLMo repository:\n```bash\ntorchrun --nproc_per_node=8 scripts/train.py {path_to_train_config} \\n    --data.paths=[{path_to_data}/input_ids.npy] \\n    --data.label_mask_paths=[{path_to_data}/label_mask.npy] \\n    --load_path={path_to_checkpoint} \\n    --reset_trainer_state\n```\nFor more documentation, see the [GitHub readme](https://github.com/allenai/OLMo?tab=readme-ov-file#fine-tuning).\n\n2. Further fine-tuning support is being developing in AI2''s Open Instruct repository. Details are [here](https://github.com/allenai/open-instruct).\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\nCore model results for the 7B model are found below.\n\n|                                   | [Llama 7B](https://arxiv.org/abs/2302.13971) | [Llama 2 7B](https://huggingface.co/meta-llama/Llama-2-7b) | [Falcon 7B](https://huggingface.co/tiiuae/falcon-7b) | [MPT 7B](https://huggingface.co/mosaicml/mpt-7b) | **OLMo 7B** (ours) |\n| --------------------------------- | -------- | ---------- | --------- | ------ | ------- |\n| arc_challenge       | 44.5             | 39.8             | 47.5         | 46.5         | 48.5            |\n| arc_easy            | 57.0             | 57.7             | 70.4         | 70.5         | 65.4            |\n| boolq               | 73.1             | 73.5             | 74.6         | 74.2         | 73.4            |\n| copa                | 85.0             | 87.0             | 86.0         | 85.0         | 90              |\n| hellaswag           | 74.5             | 74.5             | 75.9         | 77.6         | 76.4            |\n| openbookqa          | 49.8             | 48.4             | 53.0         | 48.6         | 50.2            |\n| piqa                | 76.3             | 76.4             | 78.5         | 77.3         | 78.4            |\n| sciq                | 89.5             | 90.8             | 93.9         | 93.7         | 93.8            |\n| winogrande          | 68.2             | 67.3             | 68.9         | 69.9         | 67.9            |\n| **Core tasks average**  | 68.7             | 68.4             | 72.1         | 71.5         | 71.6            |\n| truthfulQA (MC2)    | 33.9             | 38.5             | 34.0         | 33           | 36.0            |\n| MMLU (5 shot MC)    | 31.5             | 45.0             | 24.0         | 30.8         | 28.3            |\n| GSM8k (mixed eval.) | 10.0 (8shot CoT) | 12.0 (8shot CoT) | 4.0 (5 shot) | 4.5 (5 shot) | 8.5 (8shot CoT) |\n| **Full average**        | 57.8             | 59.3             | 59.2         | 59.3         | 59.8            |\n\nAnd for the 1B model:\n\n| task       | random | [StableLM 2 1.6b](https://huggingface.co/stabilityai/stablelm-2-1_6b)\* | [Pythia 1B](https://huggingface.co/EleutherAI/pythia-1b) | [TinyLlama 1.1B](https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T) | **OLMo 1B** (ours) |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------ | ----------------- | --------- | -------------------------------------- | ------- |\n| arc_challenge | 25     | 43.81             | 33.11     | 34.78                                  | 34.45   |\n| arc_easy      | 25     | 63.68             | 50.18     | 53.16                                  | 58.07   |\n| boolq         | 50     | 76.6              | 61.8      | 64.6                                   | 60.7    |\n| copa          | 50     | 84                | 72        | 78                                     | 79      |\n| hellaswag     | 25     | 68.2              | 44.7      | 58.7                                   | 62.5    |\n| openbookqa    | 25     | 45.8              | 37.8      | 43.6                                   | 46.4    |\n| piqa          | 50     | 74                | 69.1      | 71.1                                   | 73.7    |\n| sciq          | 25     | 94.7              | 86        | 90.5                                   | 88.1    |\n| winogrande    | 50     | 64.9              | 53.3      | 58.9                                   | 58.9    |\n| Average       | 36.11  | 68.41             | 56.44     | 61.48                                  | 62.42   |\n\n\*Unlike OLMo, Pythia, and TinyLlama, StabilityAI has not disclosed yet the data StableLM was trained on, making comparisons with other efforts challenging.\n\n## Model Details\n\n### Data\nFor training data details, please see the [Dolma](https://huggingface.co/datasets/allenai/dolma) documentation.\n\n### Architecture\n\nOLMo 7B architecture with peer models for comparison.\n\n|                        | **OLMo 7B**   | [Llama 2 7B](https://huggingface.co/meta-llama/Llama-2-7b) | [OpenLM 7B](https://laion.ai/blog/open-lm/) | [Falcon 7B](https://huggingface.co/tiiuae/falcon-7b) | PaLM 8B |\n|------------------------|-------------------|---------------------|--------------------|--------------------|------------------|\n| d_model     | 4096              | 4096                | 4096               | 4544               | 4096             |\n| num heads              | 32                | 32                  | 32                 | 71                 | 16               |\n| num layers             | 32                | 32                  | 32                 | 32                 | 32               |\n| MLP ratio              | ~8/3         | ~8/3           | ~8/3          | 4                  | 4                |\n| LayerNorm type         | non-parametric LN | RMSNorm             | parametric LN      | parametric LN      | parametric LN    |\n| pos embeddings         | RoPE              | RoPE                | RoPE               | RoPE               | RoPE             |\n| attention variant      | full              | GQA                 | full               | MQA                | MQA              |\n| biases                 | none              | none                | in LN only         | in LN only         | none             |\n| block type             | sequential        | sequential          | sequential         | parallel           | parallel         |\n| activation             | SwiGLU            | SwiGLU              | SwiGLU             | GeLU               | SwiGLU           |\n| sequence length        | 2048              | 4096                | 2048               | 2048               | 2048             |\n| batch size (instances) | 2160              | 1024                | 2048               | 2304               | 512              |\n| batch size (tokens)    | ~4M          | ~4M            | ~4M           | ~4M           | ~1M         |\n| weight tying           | no                | no                  | no                 | no                 | yes              |\n\n\n### Hyperparameters \n\nAdamW optimizer parameters are shown below.\n\n| Size | Peak LR    | Betas           | Epsilon     | Weight Decay |\n|------|------------|-----------------|-------------|--------------|\n| 1B   | 4.0E-4   | (0.9, 0.95)   | 1.0E-5    | 0.1          |\n| 7B   | 3.0E-4   | (0.9, 0.99)   | 1.0E-5    | 0.1          |\n\nOptimizer settings comparison with peer models.\n\n|                       | **OLMo 7B**  | [Llama 2 7B](https://huggingface.co/meta-llama/Llama-2-7b) | [OpenLM 7B](https://laion.ai/blog/open-lm/) | [Falcon 7B](https://huggingface.co/tiiuae/falcon-7b) |\n|-----------------------|------------------|---------------------|--------------------|--------------------|\n| warmup steps          | 5000             | 2000                | 2000               | 1000               |\n| peak LR               | 3.0E-04 | 3.0E-04    | 3.0E-04   | 6.0E-04  |\n| minimum LR            | 3.0E-05 | 3.0E-05    | 3.0E-05   | 1.2E-05   |\n| weight decay          | 0.1              | 0.1                 | 0.1                | 0.1                |\n| beta1                 | 0.9              | 0.9                 | 0.9                | 0.99               |\n| beta2                 | 0.95             | 0.95                | 0.95               | 0.999              |\n| epsilon               | 1.0E-05 | 1.0E-05    | 1.0E-05   | 1.0E-05   |\n| LR schedule           | linear           | cosine              | cosine             | cosine             |\n| gradient clipping     | global 1.0       | global 1.0          | global 1.0         | global 1.0         |\n| gradient reduce dtype | FP32             | FP32                | FP32               | BF16               |\n| optimizer state dtype | FP32             | most likely FP32    | FP32               | FP32               |\n\n\n\n## Environmental Impact\n\nOLMo 7B variants were either trained on MI250X GPUs at the LUMI supercomputer, or A100-40GB GPUs provided by MosaicML.\nA summary of the environmental impact. Further details are available in the paper.\n\n|           | GPU Type   | Power Consumption From GPUs | Carbon Intensity (kg COâ‚‚e/KWh) | Carbon Emissions (tCOâ‚‚eq) |\n|-----------|------------|-----------------------------|--------------------------------|---------------------------|\n| OLMo 7B Twin  | MI250X ([LUMI supercomputer](https://www.lumi-supercomputer.eu))   |  135 MWh                     | 0*                             | 0*                        |\n| OLMo 7B   | A100-40GB ([MosaicML](https://www.mosaicml.com)) |  104 MWh                     | 0.656                          | 75.05                     |\n\n## Bias, Risks, and Limitations\n\nLike any base language model or fine-tuned model without safety filtering, it is relatively easy for a user to prompt these models to generate harmful and generally sensitive content.\nSuch content can also be produced unintentionally, especially in the case of bias, so we recommend users consider the risks of applications of this technology.\n\nOtherwise, many facts from OLMo or any LLM will often not be true, so they should be checked.\n\n\n## Citation\n\n**BibTeX:**\n\n```\n@article{Groeneveld2023OLMo,\n  title={OLMo: Accelerating the Science of Language Models},\n  author={Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and Arora, Shane and Atkinson, David and Authur, Russell and Chandu, Khyathi and Cohan, Arman and Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel, Jack and Khot, Tushar and Merrill, William and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew E. and Pyatkin, Valentina and Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh and Smith, Will and Subramani, Nishant and Wortsman, Mitchell and Dasigi, Pradeep and Lambert, Nathan and Richardson, Kyle and Dodge, Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A. and Hajishirzi, Hannaneh},\n  journal={Preprint},\n  year={2024}\n}\n```\n\n**APA:**\n\nGroeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney, R., Tafjord, O., Jha, A., Ivison, H., Magnusson, I., Wang, Y., Arora, S., Atkinson, D., Authur, R., Chandu, K., Cohan, A., Dumas, J., Elazar, Y., Gu, Y., Hessel, J., Khot, T., Merrill, W., Morrison, J., Muennighoff, N., Naik, A., Nam, C., Peters, M., Pyatkin, V., Ravichander, A., Schwenk, D., Shah, S., Smith, W., Subramani, N., Wortsman, M., Dasigi, P., Lambert, N., Richardson, K., Dodge, J., Lo, K., Soldaini, L., Smith, N., & Hajishirzi, H. (2024). OLMo: Accelerating the Science of Language Models. Preprint.\n\n## Model Card Contact\n\n\nFor errors in this model card, contact Nathan or Akshita, `{nathanl, akshitab} at allenai dot org`.', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":6888095744,"storage_bytes":30087235370820,"files_count":13,"spaces_count":10,"gated":false,"private":false,"config":{"architectures":["OLMoForCausalLM"],"model_type":"hf_olmo","auto_map":{"AutoConfig":"configuration_olmo.OLMoConfig","AutoModelForCausalLM":"modeling_olmo.OLMoForCausalLM"},"tokenizer_config":{"eos_token":"<|endoftext|>","pad_token":"<|padding|>"}}}', '[]', '[{"type":"has_code","target_id":"github:allenai:OLMo","source_url":"https://github.com/allenai/OLMo"},{"type":"has_code","target_id":"github:allenai:OLMo-Eval","source_url":"https://github.com/allenai/OLMo-Eval"},{"type":"has_code","target_id":"github:allenai:open-instruct","source_url":"https://github.com/allenai/open-instruct"},{"type":"has_code","target_id":"github:allenai:OLMo","source_url":"https://github.com/allenai/OLMo?tab=readme-ov-file#fine-tuning"},{"type":"has_code","target_id":"github:allenai:open-instruct","source_url":"https://github.com/allenai/open-instruct"},{"type":"based_on_paper","target_id":"arxiv:2402.00838","source_url":"https://arxiv.org/abs/2402.00838"},{"type":"based_on_paper","target_id":"arxiv:2302.13971","source_url":"https://arxiv.org/abs/2302.13971"}]', NULL, 'Apache-2.0', 'approved', 78.1, '38fba4150e3d3c1126af70f7b0e3dc64', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-stable-diffusion-3.5-large-turbo', 'huggingface--stabilityai--stable-diffusion-3.5-large-turbo', 'stable-diffusion-3.5-large-turbo', 'stabilityai', '', '["diffusers","safetensors","text-to-image","stable-diffusion","en","arxiv:2403.03206","license:other","diffusers:stablediffusion3pipeline","region:us"]', 'text-to-image', 649, 12107, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/stable-diffusion-3.5-large-turbo","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":59027362128,"files_count":45,"spaces_count":100,"gated":"auto","private":false,"config":{"diffusers":{"_class_name":"StableDiffusion3Pipeline"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2403.03206","source_url":"https://arxiv.org/abs/2403.03206"}]', NULL, 'Other', 'approved', 58.1, 'fdda4f85b1f45c8ec9f08055078b26f1', NULL, 'https://huggingface.co/stabilityai/stable-diffusion-3.5-large-turbo/resolve/main/sd3.5_large_turbo_demo.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-stabilityai-stable-diffusion-3.5-large-turbo from https://huggingface.co/stabilityai/stable-diffusion-3.5-large-turbo/resolve/main/sd3.5_large_turbo_demo.png
HTTP error: 401 Unauthorized

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-ali-vilab-text-to-video-ms-1.7b', 'huggingface--ali-vilab--text-to-video-ms-1.7b', 'text-to-video-ms-1.7b', 'ali-vilab', '--- license: cc-by-nc-4.0 tags: - text-to-video duplicated_from: diffusers/text-to-video-ms-1.7b --- This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported. **We Are Hiring!** (Based in Beijing / Hangzhou, China.) If you''re looking for an exciting challenge and the opportunity to work with cutting-edge technologies in AIGC and large-scale pretraining, ...', '["diffusers","safetensors","text-to-video","arxiv:2308.06571","license:cc-by-nc-4.0","diffusers:texttovideosdpipeline","region:us"]', 'text-to-video', 648, 11838, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/ali-vilab/text-to-video-ms-1.7b","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: cc-by-nc-4.0\ntags:\n- text-to-video\nduplicated_from: diffusers/text-to-video-ms-1.7b\n---\n\n# Text-to-video-synthesis Model in Open Domain\n\nThis model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n\n**We Are Hiring!** (Based in Beijing / Hangzhou, China.)\n\nIf you''re looking for an exciting challenge and the opportunity to work with cutting-edge technologies in AIGC and large-scale pretraining, then we are the place for you. We are looking for talented, motivated and creative individuals to join our team. If you are interested, please send your CV to us.\n\nEMAIL: yingya.zyy@alibaba-inc.com\n\n## Model description\n\nThe text-to-video generation diffusion model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input. The diffusion model adopts a UNet3D structure, and implements video generation through the iterative denoising process from the pure Gaussian noise video.\n\nThis model is meant for research purposes. Please look at the [model limitations and biases and misuse](#model-limitations-and-biases), [malicious use and excessive use](#misuse-malicious-use-and-excessive-use) sections.\n\n## Model Details\n\n- **Developed by:** [ModelScope](https://modelscope.cn/)\n- **Model type:** Diffusion-based text-to-video generation model\n- **Language(s):** English\n- **License:**[ CC-BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/4.0/)\n- **Resources for more information:** [ModelScope GitHub Repository](https://github.com/modelscope/modelscope), [Summary](https://modelscope.cn/models/damo/text-to-video-synthesis/summary).\n- **Cite as:**\n\n## Use cases\n\nThis model has a wide range of applications and can reason and generate videos based on arbitrary English text descriptions. \n\n## Usage \n\nLet''s first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```python\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained("damo-vilab/text-to-video-ms-1.7b", torch_dtype=torch.float16, variant="fp16")\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = "Spiderman is surfing"\nvideo_frames = pipe(prompt, num_inference_steps=25).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        <td><center>\n        An astronaut riding a horse.\n        <br>\n        <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/astr.gif"\n            alt="An astronaut riding a horse."\n            style="width: 300px;" />\n        </center></td>\n        <td ><center>\n        Darth vader surfing in waves.\n        <br>\n        <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/vader.gif"\n            alt="Darth vader surfing in waves."\n            style="width: 300px;" />\n        </center></td>\n    </tr>\n</table>\n\n## Long Video Generation\n\nYou can optimize for memory usage by enabling attention and VAE slicing and using Torch 2.0.\nThis should allow you to generate videos up to 25 seconds on less than 16GB of GPU VRAM.\n\n```bash\n$ pip install git+https://github.com/huggingface/diffusers transformers accelerate\n```\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\n# load pipeline\npipe = DiffusionPipeline.from_pretrained("damo-vilab/text-to-video-ms-1.7b", torch_dtype=torch.float16, variant="fp16")\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n\n# optimize for GPU memory\npipe.enable_model_cpu_offload()\npipe.enable_vae_slicing()\n\n# generate\nprompt = "Spiderman is surfing. Darth Vader is also surfing and following Spiderman"\nvideo_frames = pipe(prompt, num_inference_steps=25, num_frames=200).frames\n\n# convent to video\nvideo_path = export_to_video(video_frames)\n```\n\n\n## View results\n\nThe above code will display the save path of the output video, and the current encoding format can be played with [VLC player](https://www.videolan.org/vlc/).\n\nThe output mp4 file can be viewed by [VLC media player](https://www.videolan.org/vlc/). Some other media players may not view it normally.\n\n## Model limitations and biases\n\n* The model is trained based on public data sets such as Webvid, and the generated results may have deviations related to the distribution of training data.\n* This model cannot achieve perfect film and television quality generation.\n* The model cannot generate clear text.\n* The model is mainly trained with English corpus and does not support other languages â€‹â€‹at the moment**.\n* The performance of this model needs to be improved on complex compositional generation tasks.\n\n## Misuse, Malicious Use and Excessive Use\n\n* The model was not trained to realistically represent people or events, so using it to generate such content is beyond the model''s capabilities.\n* It is prohibited to generate content that is demeaning or harmful to people or their environment, culture, religion, etc.\n* Prohibited for pornographic, violent and bloody content generation.\n* Prohibited for error and false information generation.\n\n## Training data\n\nThe training data includes [LAION5B](https://huggingface.co/datasets/laion/laion2B-en), [ImageNet](https://www.image-net.org/), [Webvid](https://m-bain.github.io/webvid-dataset/) and other public datasets. Image and video filtering is performed after pre-training such as aesthetic score, watermark score, and deduplication.\n\n_(Part of this model card has been taken from [here](https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis))_\n\n## Citation\n\n```bibtex\n    @article{wang2023modelscope,\n      title={Modelscope text-to-video technical report},\n      author={Wang, Jiuniu and Yuan, Hangjie and Chen, Dayou and Zhang, Yingya and Wang, Xiang and Zhang, Shiwei},\n      journal={arXiv preprint arXiv:2308.06571},\n      year={2023}\n    }\n    @InProceedings{VideoFusion,\n        author    = {Luo, Zhengxiong and Chen, Dayou and Zhang, Yingya and Huang, Yan and Wang, Liang and Shen, Yujun and Zhao, Deli and Zhou, Jingren and Tan, Tieniu},\n        title     = {VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation},\n        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n        month     = {June},\n        year      = {2023}\n    }\n```\n', '{"pipeline_tag":"text-to-video","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":33317340308,"files_count":23,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"TextToVideoSDPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:modelscope:modelscope","source_url":"https://github.com/modelscope/modelscope"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"based_on_paper","target_id":"arxiv:2308.06571","source_url":"https://arxiv.org/abs/2308.06571"}]', NULL, 'CC-BY-NC-4.0', 'approved', 63.1, 'cc7afe87e5d9d509269fde89bb45b6f8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tencent-HunyuanImage-2.1', 'huggingface--tencent--hunyuanimage-2.1', 'HunyuanImage-2.1', 'tencent', '--- library_name: HunyuanImage-2.1 license: other license_name: tencent-hunyuan-community license_link: https://github.com/Tencent-Hunyuan/HunyuanImage-2.1/blob/master/LICENSE language: - en - zh tags: - text-to-image pipeline_tag: text-to-image extra_gated_eu_disallowed: true --- ä¸­æ–‡é˜…è¯» <p align="center"> <img src="./assets/logo.png" height=100> </p> <div align="center"> </div> <div align="center"> <a href=https://github.com/Tencent-Hunyuan/HunyuanImage-2.1 target="_blank"><img src=https://img...', '["hunyuanimage-2.1","safetensors","text-to-image","en","zh","arxiv:2509.04545","license:other","region:us"]', 'text-to-image', 648, 358, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tencent/HunyuanImage-2.1","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: HunyuanImage-2.1\nlicense: other\nlicense_name: tencent-hunyuan-community\nlicense_link: https://github.com/Tencent-Hunyuan/HunyuanImage-2.1/blob/master/LICENSE\nlanguage:\n  - en\n  - zh\ntags:\n  - text-to-image\npipeline_tag: text-to-image\nextra_gated_eu_disallowed: true\n---\n\n[ä¸­æ–‡é˜…è¯»](./README_CN.md)\n\n<p align="center">\n  <img src="./assets/logo.png"  height=100>\n</p>\n\n<div align="center">\n\n# HunyuanImage-2.1: An Efficient Diffusion Model for High-Resolution (2K) Text-to-Image Generationâ€‹\n\n</div>\n\n<div align="center">\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanImage-2.1 target="_blank"><img src=https://img.shields.io/badge/Code-black.svg?logo=github height=22px></a>\n  <a href="https://huggingface.co/spaces/tencent/HunyuanImage-2.1" target="_blank">\n    <img src="https://img.shields.io/badge/Demo%20Page-blue" height="22px"></a>\n  <a href=https://huggingface.co/tencent/HunyuanImage-2.1 target="_blank"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href="#" target="_blank"><img src="https://img.shields.io/badge/Report-Coming%20Soon-blue" height="22px"></a><br/>\n  <a href="https://www.arxiv.org/abs/2509.04545" target="https://arxiv.org/abs/2509.04545"><img src="https://img.shields.io/badge/PromptEnhancer-Report-yellow" height="22px"></a>\n  <a href= https://hunyuan-promptenhancer.github.io/ target="_blank"><img src=https://img.shields.io/badge/PromptEnhancer-bb8a2e.svg?logo=github height=22px></a><br/>\n  <a href=https://x.com/TencentHunyuan target="_blank"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n</div>\n\n<p align="center">\n    ðŸ‘‹ Join our <a href="https://github.com/Tencent-Hunyuan/HunyuanImage-2.1/blob/main/assets/WECHAT.md" target="_blank">WeChat</a> \n</p>\n\n-----\n\nThis repo contains PyTorch model definitions, pretrained weights and inference/sampling code for our HunyuanImage-2.1. You can find more visualizations on our [project page](https://hunyuan.tencent.com/image/en?tabIndex=0).\n\n\n## ðŸ”¥ðŸ”¥ðŸ”¥ Latest Updates\n\n- September 12, 2025: ðŸš€ Released FP8 quantized models! Making it possible to generate 2K images with only 24GB GPU memory!\n- September 8, 2025: ðŸš€ Released inference code and model weights for HunyuanImage-2.1.\n\n\n## ðŸŽ¥ Demo\n\n<div align="center">\n  <img src="./assets/show_cases.png" width=100% alt="HunyuanImage 2.1 Demo">\n</div>\n\n## Contents\n- [HunyuanImage-2.1: An Efficient Diffusion Model for High-Resolution (2K) Text-to-Image Generationâ€‹](#hunyuanimage-21-an-efficient-diffusion-model-for-high-resolution-2k-text-to-image-generation)\n  - [ðŸ”¥ðŸ”¥ðŸ”¥ Latest Updates](#-latest-updates)\n  - [ðŸŽ¥ Demo](#-demo)\n  - [Contents](#contents)\n  - [Abstract](#abstract)\n  - [HunyuanImage-2.1 Overall Pipeline](#hunyuanimage-21-overall-pipeline)\n    - [Training Data and Caption](#training-data-and-caption)\n    - [Text-to-Image Model Architecture](#text-to-image-model-architecture)\n    - [Reinforcement Learning from Human Feedback](#reinforcement-learning-from-human-feedback)\n    - [Rewriting Model](#rewriting-model)\n    - [Model distillation](#model-distillation)\n  - [ðŸŽ‰ HunyuanImage-2.1 Key Features](#-hunyuanimage-21-key-features)\n  - [Prompt Enhanced Demo](#prompt-enhanced-demo)\n  - [ðŸ“ˆ Comparisons](#-comparisons)\n    - [SSAE Evaluation](#ssae-evaluation)\n    - [GSB Evaluation](#gsb-evaluation)\n  - [ðŸ“œ System Requirements](#-system-requirements)\n  - [ðŸ› ï¸ Dependencies and Installation](#ï¸-dependencies-and-installation)\n  - [ðŸ§± Download Pretrained Models](#-download-pretrained-models)\n  - [ðŸ”‘ Usage](#-usage)\n  - [ðŸ”— BibTeX](#-bibtex)\n  - [Acknowledgements](#acknowledgements)\n  - [Github Star History](#github-star-history)\n\n---\n<!-- - [ðŸ§© Community Contributions](#-community-contributions) -->\n## Abstract\nWe present HunyuanImage-2.1, a highly efficient text-to-image model that is capable of generating 2K (2048 Ã— 2048) resolution images. Leveraging an extensive dataset and structured captions involving multiple expert models, we significantly enhance text-image alignment capabilities. The model employs a highly expressive VAE with a (32 Ã— 32) spatial compression ratio, substantially reducing computational costs.\n\nOur architecture consists of two stages:\n1. â€‹Base text-to-image Model:â€‹â€‹ The first stage is a text-to-image model that utilizes two text encoders: a multimodal large language model (MLLM) to improve image-text alignment, and a multi-language, character-aware encoder to enhance text rendering across various languages. This stage features a single- and dual-stream diffusion transformer with 17 billion parameters. To optimize aesthetics and structural coherence, we apply reinforcement learning from human feedback (RLHF).\n2. Refiner Model: The second stage introduces a refiner model that further enhances image quality and clarity, while minimizing artifacts. \n\nAdditionally, we developed the PromptEnhancer module to further boost model performance, and employed meanflow distillation for efficient inference. HunyuanImage-2.1 demonstrates robust semantic alignment and cross-scenario generalization, leading to improved consistency between text and image, enhanced control of scene details, character poses, and expressions, and the ability to generate multiple objects with distinct descriptions.\n\n\n \n\n## HunyuanImage-2.1 Overall Pipeline\n\n### Training Data and Caption\n\nStructured captions provide hierarchical semantic information at short, medium, long, and extra-long levels, significantly enhancing the modelâ€™s responsiveness to complex semantics. Innovatively, an OCR agent and IP RAG are introduced to address the shortcomings of general VLM captioners in dense text and world knowledge descriptions, while a bidirectional verification strategy ensures caption accuracy.\n\n\n### Text-to-Image Model Architecture\n\n<p align="center">\n  <img src="./assets/framework_overall.png" width=100% alt="HunyuanImage 2.1 Architecture">\n</p>\n\n\n\nCore Components:\n* High-Compression VAE with REPA Training Acceleration:\n  * A VAE with a 32Ã— compression rate drastically reduces the number of input tokens for the DiT model. By aligning its feature space with DINOv2 features, we facilitate the training of high-compression VAEs. As a result, our model generates 2K images with the same token length (and thus similar inference time) as other models require for 1K images, achieving superior inference efficiency.\n  * Multi-bucket, multi-resolution REPA loss aligns DiT features with a high-dimensional semantic feature space, accelerating model convergence.\n* Dual Text Encoder:\n  * A vision-language multimodal encoder is employed to better understand scene descriptions, character actions, and detailed requirements.\n  * A multilingual ByT5 text encoder is introduced to specialize in text generation and multilingual expression.\n* Network: A single- and dual-stream diffusion transformer with 17 billion parameters.\n\n### Reinforcement Learning from Human Feedback\nTwo-Stage Post-Training with Reinforcement Learning: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are applied sequentially in two post-training stages. We introduce a Reward Distribution Alignment algorithm, which innovatively incorporates high-quality images as selected samples to ensure stable and improved reinforcement learning outcomes.\n\n### Rewriting Model\n<p align="center">\n  <img src="./assets/framework_prompt_rewrite.png" width=90% alt="HunyuanImage 2.1 Architecture">\n</p>\n\n* The first systematic industrial-level rewriting model. SFT training structurally rewrites user text instructions to enrich visual expression, while GRPO training employs a fine-grained semantic AlignEvaluator reward model to substantially improve the semantics of images generated from rewritten text. The AlignEvaluator covers 6 major categories and 24 fine-grained assessment points. PromptEnhancer supports both Chinese and English rewriting and demonstrates general applicability in enhancing semantics for both open-source and proprietary text-to-image models.\n\n### Model distillation\nWe propose a novel distillation method based on meanflow that addresses the key challenges of instability and inefficiency inherent in standard meanflow training. This approach enables high-quality image generation with only a few sampling steps. To our knowledge, this is the first successful application of meanflow to an industrial-scale model.\n\n\n\n\n\n## ðŸŽ‰ HunyuanImage-2.1 Key Features\n\n- **High-Quality Generation**: Efficiently produces ultra-high-definition (2K) images with cinematic composition.\n- **Multilingual Support**: Provides native support for both Chinese and English prompts.\n- **Advanced Architecture**: Built on a multi-modal, single- and dual-stream combined DiT (Diffusion Transformer) backbone.\n- **Glyph-Aware Processing**: Utilizes ByT5''s text rendering capabilities for improved text generation accuracy.\n- **Flexible Aspect Ratios**: Supports a variety of image aspect ratios (1:1, 16:9, 9:16, 4:3, 3:4, 3:2, 2:3).\n- **Prompt Enhancement**: Automatically rewrites prompts to improve descriptive accuracy and visual quality.\n\n\n## Prompt Enhanced Demo\nTo improve the quality and detail of generated images, we use a prompt rewriting model. This model automatically enhances user-provided text prompts by adding detailed and descriptive information.\n<p align="center">\n  <img src="./assets/reprompt.png" width=100% alt="Human Evaluation with Other Models">\n</p>\n\n\n## ðŸ“ˆ Comparisons\n\n### SSAE Evaluation\nSSAE (Structured Semantic Alignment Evaluation) is an intelligent evaluation metric for image-text alignment based on advanced multimodal large language models (MLLMs). We extracted 3500 key points across 12 categories, then used multimodal large language models to automatically evaluate and score by comparing the generated images with these key points based on the visual content of the images. Mean Image Accuracy represents the image-wise average score across all key points, while Global Accuracy directly calculates the average score across all key points.\n<p align="center">\n<table>\n<thead>\n<tr>\n    <th rowspan="2">Model</th>  <th rowspan="2">Open Source</th> <th rowspan="2">Mean Image Accuracy</th> <th rowspan="2">Global Accuracy</th> <th colspan="4" style="text-align: center;">Primary Subject</th> <th colspan="3" style="text-align: center;">Secondary Subject</th> <th colspan="2" style="text-align: center;">Scene</th> <th colspan="3" style="text-align: center;">Other</th>\n</tr>\n<tr>\n    <th>Noun</th> <th>Key Attributes</th> <th>Other Attributes</th> <th>Action</th> <th>Noun</th> <th>Attributes</th> <th>Action</th> <th>Noun</th> <th>Attributes</th> <th>Shot</th> <th>Style</th> <th>Composition</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n    <td>FLUX-dev</td> <td>âœ…</td> <td>0.7122</td> <td>0.6995</td> <td>0.7965</td> <td>0.7824</td> <td>0.5993</td> <td>0.5777</td> <td>0.7950</td> <td>0.6826</td> <td>0.6923</td> <td>0.8453</td> <td>0.8094</td> <td>0.6452</td> <td>0.7096</td> <td>0.6190</td>\n</tr>\n<tr>\n    <td>Seedream-3.0</td> <td>âŒ</td> <td>0.8827</td> <td>0.8792</td> <td>0.9490</td> <td>0.9311</td> <td>0.8242</td> <td>0.8177</td> <td>0.9747</td> <td>0.9103</td> <td>0.8400</td> <td>0.9489</td> <td>0.8848</td> <td>0.7582</td> <td>0.8726</td> <td>0.7619</td>\n</tr>\n<tr>\n    <td>Qwen-Image</td> <td>âœ…</td> <td>0.8854</td> <td>0.8828</td> <td>0.9502</td> <td>0.9231</td> <td>0.8351</td> <td>0.8161</td> <td>0.9938</td> <td>0.9043</td> <td>0.8846</td> <td>0.9613</td> <td>0.8978</td> <td>0.7634</td> <td>0.8548</td> <td>0.8095</td>\n</tr>\n<tr>\n    <td>GPT-Image</td>  <td>âŒ</td> <td> 0.8952</td> <td>0.8929</td> <td>0.9448</td> <td>0.9289</td> <td>0.8655</td> <td>0.8445</td> <td>0.9494</td> <td>0.9283</td> <td>0.8800</td> <td>0.9432</td> <td>0.9017</td> <td>0.7253</td> <td>0.8582</td> <td>0.7143</td>\n</tr>\n<tr>\n    <td><strong>HunyuanImage 2.1</strong></td> <td>âœ…</td> <td><strong>0.8888</strong></td> <td><strong>0.8832</strong></td> <td>0.9339</td> <td>0.9341</td> <td>0.8363</td> <td>0.8342</td> <td>0.9627</td> <td>0.8870</td> <td>0.9615</td> <td>0.9448</td> <td>0.9254</td> <td>0.7527</td> <td>0.8689</td> <td>0.7619</td>\n</tr>\n</tbody>\n</table>\n</p>\n\nFrom the SSAE evaluation results, our model has currently achieved the optimal performance among open-source models in terms of semantic alignment, and is very close to the performance of closed-source commercial models (GPT-Image).\n\n### GSB Evaluation\n\n<p align="center">\n  <img src="./assets/gsb.png" width=70% alt="Human Evaluation with Other Models">\n</p>\n\nWe adopted the GSB evaluation method commonly used to assess the relative performance between two models from an overall image perception perspective. In total, we utilized 1000 text prompts, generating an equal number of image samples for all compared models in a single run. For a fair comparison, we conducted inference only once for each prompt, avoiding any cherry-picking of results. When comparing with the baseline methods, we maintained the default settings for all selected models. The evaluation was performed by more than 100 professional evaluators.\nFrom the results, HunyuanImage 2.1 achieved a relative win rate of -1.36% against Seedream3.0 (closed-source) and 2.89% outperforming Qwen-Image (open-source). The GSB evaluation results demonstrate that HunyuanImage 2.1, as an open-source model, has reached a level of image generation quality comparable to closed-source commercial models (Seedream3.0), while showing certain advantages in comparison with similar open-source models (Qwen-Image). This fully validates the technical advancement and practical value of HunyuanImage 2.1 in text-to-image generation tasks.\n\n## ðŸ“œ System Requirements\n\n\n**Hardware and OS Requirements:**\n- NVIDIA GPU with CUDA support.\n\n  **Minimum requrement for now:** 24 GB GPU memory for 2048x2048 image generation.\n  \n  > **Note:** The memory requirements above are measured with model CPU offloading and FP8 quantization enabled. If your GPU has sufficient memory, you may disable offloading for improved inference speed.\n- Supported operating system: Linux.\n\n\n## ðŸ› ï¸ Dependencies and Installation\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/Tencent-Hunyuan/HunyuanImage-2.1.git\ncd HunyuanImage-2.1\n```\n\n2. Install dependencies:\n```bash\npip install -r requirements.txt\npip install flash-attn==2.7.3 --no-build-isolation\n```\n\n## ðŸ§± Download Pretrained Models\n\nThe details of download pretrained models are shown [here](checkpoints-download.md).\n\n## ðŸ”‘ Usage\nHunyuanImage-2.1 only supports 2K image generation (e.g. 2048x2048 for 1:1 images, 2560x1536 for 16:9 images, etc.).\nGenerating images with 1K resolution will result in artifacts.\nAdditionally, we recommend using the full generation pipeline for better quality (i.e. enabling prompt enhancement and refinment).\n\n```python\nimport os\nos.environ[''PYTORCH_CUDA_ALLOC_CONF''] = ''expandable_segments:True''\nimport torch\nfrom hyimage.diffusion.pipelines.hunyuanimage_pipeline import HunyuanImagePipeline\n\n# Supported model_name: hunyuanimage-v2.1, hunyuanimage-v2.1-distilled\nmodel_name = "hunyuanimage-v2.1"\npipe = HunyuanImagePipeline.from_pretrained(model_name=model_name, use_fp8=True)\npipe = pipe.to("cuda")\n\nprompt = "A cute, cartoon-style anthropomorphic penguin plush toy with fluffy fur, standing in a painting studio, wearing a red knitted scarf and a red beret with the word â€œTencentâ€ on it, holding a paintbrush with a focused expression as it paints an oil painting of the Mona Lisa, rendered in a photorealistic photographic style."\nimage = pipe(\n    prompt=prompt,\n    # Examples of supported resolutions and aspect ratios for HunyuanImage-2.1:\n    # 16:9  -> width=2560, height=1536\n    # 4:3   -> width=2304, height=1792\n    # 1:1   -> width=2048, height=2048\n    # 3:4   -> width=1792, height=2304\n    # 9:16  -> width=1536, height=2560\n    # Please use one of the above width/height pairs for best results.\n    width=2048,\n    height=2048,\n    use_reprompt=False,  # Enable prompt enhancement (which may result in higher GPU memory usage)\n    use_refiner=True,   # Enable refiner model\n    # For the distilled model, use 8 steps for faster inference.\n    # For the non-distilled model, use 50 steps for better quality.\n    num_inference_steps=8 if "distilled" in model_name else 50, \n    guidance_scale=3.25 if "distilled" in model_name else 3.5,\n    shift=4 if "distilled" in model_name else 5,\n    seed=649151,\n)\n\nimage.save(f"generated_image.png")\n```\n\n\n## ðŸ”— BibTeX\n\nIf you find this project useful for your research and applications, please cite as:\n\n```BibTeX\n@misc{HunyuanImage-2.1,\n  title={HunyuanImage 2.1: An Efficient Diffusion Model for High-Resolution (2K) Text-to-Image Generation},\n  author={Tencent Hunyuan Team},\n  year={2025},\n  howpublished={\url{https://github.com/Tencent-Hunyuan/HunyuanImage-2.1}},\n}\n```\n\n## Acknowledgements\n\nWe would like to thank the following open-source projects and communities for their contributions to open research and exploration: [Qwen](https://huggingface.co/Qwen), [FLUX](https://github.com/black-forest-labs/flux), [diffusers](https://github.com/huggingface/diffusers) and [HuggingFace](https://huggingface.co).\n\n## Github Star History\n<a href="https://star-history.com/#Tencent-Hunyuan/HunyuanImage-2.1&Date">\n <picture>\n   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=Tencent-Hunyuan/HunyuanImage-2.1&type=Date1&theme=dark" />\n   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=Tencent-Hunyuan/HunyuanImage-2.1&type=Date1" />\n   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=Tencent-Hunyuan/HunyuanImage-2.1&type=Date1" />\n </picture>\n</a>\n\n', '{"pipeline_tag":"text-to-image","library_name":"HunyuanImage-2.1","framework":"HunyuanImage-2.1","params":null,"storage_bytes":183759744966,"files_count":41,"spaces_count":18,"gated":false,"private":false,"config":{}}', '[]', '[{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanImage-2.1","source_url":"https://github.com/Tencent-Hunyuan/HunyuanImage-2.1"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanImage-2.1","source_url":"https://github.com/Tencent-Hunyuan/HunyuanImage-2.1"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanImage-2.1","source_url":"https://github.com/Tencent-Hunyuan/HunyuanImage-2.1"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanImage-2.1.git","source_url":"https://github.com/Tencent-Hunyuan/HunyuanImage-2.1.git"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanImage-2.1}},","source_url":"https://github.com/Tencent-Hunyuan/HunyuanImage-2.1}},"},{"type":"has_code","target_id":"github:black-forest-labs:flux","source_url":"https://github.com/black-forest-labs/flux"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"based_on_paper","target_id":"arxiv:2509.04545","source_url":"https://arxiv.org/abs/2509.04545"}]', NULL, 'Other', 'approved', 98.1, '123ac971116eaff4e6ec8acc18b9d353', NULL, 'https://huggingface.co/tencent/HunyuanImage-2.1/resolve/main/assets/comparison.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-tencent-HunyuanImage-2.1 from https://huggingface.co/tencent/HunyuanImage-2.1/resolve/main/assets/comparison.png
Image converted to WebP: data/images/huggingface-tencent-HunyuanImage-2.1.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-cerebras-Cerebras-GPT-13B', 'huggingface--cerebras--cerebras-gpt-13b', 'Cerebras-GPT-13B', 'cerebras', '--- language: - en inference: false tags: - pytorch - causal-lm license: apache-2.0 datasets: - the_pile pipeline_tag: text-generation --- Check out our Blog Post and arXiv paper! The Cerebras-GPT family is released to facilitate research into LLM scaling laws using open architectures and data sets and demonstrate the simplicity of and scalability of training LLMs on the Cerebras software and hardware stack. All Cerebras-GPT models are available on Hugging Face. The family includes 111M, 256M...', '["transformers","pytorch","gpt2","feature-extraction","causal-lm","text-generation","en","dataset:the_pile","arxiv:2304.03208","arxiv:2203.15556","arxiv:2101.00027","license:apache-2.0","text-generation-inference","region:us"]', 'text-generation', 646, 1109, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/cerebras/Cerebras-GPT-13B","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage:\n- en\ninference: false\ntags:\n- pytorch\n- causal-lm\nlicense: apache-2.0\ndatasets:\n- the_pile\npipeline_tag: text-generation\n---\n\n# Cerebras-GPT 13B\nCheck out our [Blog Post](https://www.cerebras.net/cerebras-gpt) and [arXiv paper](https://arxiv.org/abs/2304.03208)!\n\n## Model Description\n\nThe Cerebras-GPT family is released to facilitate research into LLM scaling laws using open architectures and data sets and demonstrate the simplicity of and scalability of training LLMs on the Cerebras software and hardware stack. All Cerebras-GPT models are available on Hugging Face.\n\nThe family includes 111M, 256M, 590M, 1.3B, 2.7B, 6.7B, and 13B models.\n\nAll models in the Cerebras-GPT family have been trained in accordance with [Chinchilla scaling laws](https://arxiv.org/abs/2203.15556) (20 tokens per model parameter) which is compute-optimal.\n\nThese models were trained on the [Andromeda](https://www.cerebras.net/andromeda/) AI supercomputer comprised of 16 CS-2 wafer scale systems. Cerebras'' [weight streaming technology](https://www.cerebras.net/blog/linear-scaling-made-possible-with-weight-streaming) simplifies the training of LLMs by disaggregating compute from model storage. This allowed for efficient scaling of training across nodes using simple data parallelism.\n\nCerebras systems for pre-training and fine tuning are available in the cloud via the [Cerebras Model Studio](https://www.cerebras.net/product-cloud/). Cerebras CS-2 compatible checkpoints are available in [Cerebras Model Zoo](https://github.com/Cerebras/modelzoo).\n\n## Model Details\n* Developed by: [Cerebras Systems](https://www.cerebras.net/)\n* License: Apache 2.0\n* Model type: Transformer-based Language Model\n* Architecture: GPT-3 style architecture\n* Data set: The Pile\n* Tokenizer: Byte Pair Encoding\n* Vocabulary Size: 50257\n* Sequence Length: 2048\n* Optimizer: AdamW, (Î²1, Î²2) = (0.9, 0.95), adam_eps = 1eâˆ’8 (1eâˆ’9 for larger models)\n* Positional Encoding: Learned\n* Language: English\n* Learn more: Dense Scaling Laws Paper for training procedure, config files, and details on how to use.\n\n**Contact**: To ask questions about Cerebras-GPT models, join the [Cerebras Discord](https://discord.gg/q6bZcMWJVu).\n\nThis is the standard parameterization version of Cerebras-GPT with **13B** parameters\n\nRelated models: [Cerebras-GPT Models](https://huggingface.co/models?sort=downloads&search=cerebras-gpt)\n\n<br><br>\n\n| Model         | Parameters | Layers | d_model | Heads | d_head | d_ffn  | LR       | BS (seq) | BS (tokens)     |\n|---------------|------------|--------|---------|-------|--------|--------|----------|----------|----------------|\n| Cerebras-GPT  | 111M       | 10     | 768     | 12    | 64     | 3072   | 6.0E-04 | 120      | 246K           |\n| Cerebras-GPT  | 256M       | 14     | 1088    | 17    | 64     | 4352   | 6.0E-04 | 264      | 541K           |\n| Cerebras-GPT  | 590M       | 18     | 1536    | 12    | 128    | 6144   | 2.0E-04 | 264      | 541K           |\n| Cerebras-GPT  | 1.3B       | 24     | 2048    | 16    | 128    | 8192   | 2.0E-04 | 528      | 1.08M          |\n| Cerebras-GPT  | 2.7B       | 32     | 2560    | 32    | 80     | 10240  | 2.0E-04 | 528      | 1.08M          |\n| Cerebras-GPT  | 6.7B       | 32     | 4096    | 32    | 128    | 16384  | 1.2E-04 | 1040     | 2.13M          |\n| Cerebras-GPT  | 13B        | 40     | 5120    | 40    | 128    | 20480  | 1.2E-04 | 720 &rarr; 1080 | 1.47M &rarr; 2.21M    |\n\n<br><br>\n\n## Quickstart \n\nThis model can be easily loaded using the AutoModelForCausalLM functionality:\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained("cerebras/Cerebras-GPT-13B")\nmodel = AutoModelForCausalLM.from_pretrained("cerebras/Cerebras-GPT-13B")\n\ntext = "Generative AI is "\n```\n\nAnd can be used with Hugging Face Pipelines\n\n```python\nfrom transformers import pipeline\n\npipe = pipeline("text-generation", model=model, tokenizer=tokenizer)\ngenerated_text = pipe(text, max_length=50, do_sample=False, no_repeat_ngram_size=2)[0]\nprint(generated_text[''generated_text''])\n```\n\nor with `model.generate()`\n\n```python\ninputs = tokenizer(text, return_tensors="pt")\noutputs = model.generate(**inputs, num_beams=5, \n                        max_new_tokens=50, early_stopping=True,\n                        no_repeat_ngram_size=2)\ntext_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\nprint(text_output[0])\n```\n<br><br>\n\n## Training data\n\nCerebras-GPT is trained using [the Pile](https://pile.eleuther.ai) dataset from [EleutherAI](https://www.eleuther.ai). See the [Pile paper](https://arxiv.org/abs/2101.00027) for a more detailed breakdown of data sources and methodology. The Pile was cleaned using the ftfy library to normalize the text, then filtered using scripts provided by Eleuther.\n\nWe tokenized the data using byte-pair encoding using the GPT-2 vocabulary. Our tokenized version of the Pile has 371B tokens. We include more details about the training dataset preprocessing in Appendix A.1 of our paper.\n\nRecent works find significant duplicate data present in the Pile. Eleutherâ€™s Pythia applies a deduplication process to reduce replicated data, decreasing the Pile dataset size. Pythia was trained on both the standard dataset and deduplicated dataset to characterize the impact. Our models are trained on the standard Pile without deduplication, which may present an opportunity for further improvement with the deduplicated data set.\n\n<br><br>\n\n## Training procedure\n\nWe use the GPT-3 style model architecture. All of our layers use full attention as opposed to the GPT-3 style sparse banded attention. The model shapes were selected to either follow aspect ratio 80 or are the same shape as GPT-3 models. Learning rate warmed up for 375M tokens (1500 steps for 111M and 256M models) and 10x cosine decayed. No dropout was used and weight decay was set to 0.1. All models are trained with MSL of 2048.\n\nAll models were trained to Chinchilla point: 20 tokens per model parameter. Number of steps was chosen based on optimal batch size (varied by model) and fixed sequence length (2048). See Training Table, below, for details. \n\n<br>\n\nModel Params | Sequence Length | Batch Size | Number of Steps | Tokens | Tokens per Parameter | Flops\n------------ | -------------- | ---------- | --------------- | ------ | -------------------- | -----\n111M         | 2048           | 120        | 9037            | 2.22E+09 | 20                  | 2.6E+18\n256M         | 2048           | 264        | 9468            | 5.12E+09 | 20                  | 1.3E+19\n590M         | 2048           | 264        | 21836           | 1.18E+10 | 20                  | 6.1E+19\n1.3B         | 2048           | 528        | 24334           | 2.63E+10 | 20                  | 2.8E+20\n2.7B         | 2048           | 528        | 49041           | 5.30E+10 | 20                  | 1.1E+21\n6.7B         | 2048           | 1040       | 62522           | 1.33E+11 | 20                  | 6.3E+21\n13B          | 2048           | 720        | 174335          | 2.57E+11 | 20                  | 2.3E+22\n\n<br><br>\n\n## Evaluations\n\nWe trained models from smallest to largest and fit a power law as we went along. The power law was helpful for extrapolating the validation loss of the next largest model we trained and provided confidence about whether the training run was going well.\n\nWe performed upstream (pre-training) evaluations of text prediction cross-entropy using the Pile validation and test splits. We performed downstream evaluations of text generation accuracy on standardized tasks using the [Eleuther lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness). Results are compared against many publicly available large language models in Section 3 of the paper.\n\n\n#### 0-shot Evaluation\n| Model   | Params | Training FLOPs | PILE test xent | Hella-Swag | PIQA  | Wino-Grande | Lambada | ARC-e | ARC-c | OpenBookQA | Downstream Average |\n| ------- | ----- | -------------- | -------------- | ---------- | ----- | ----------- | ------- | ----- | ----- | ---------- | ------------------ |\n| Cerebras-GPT | 111M  | 2.6E+18        | 2.566          | 0.268      | 0.594 | 0.488       | 0.194   | 0.380 | 0.166 | 0.118      | 0.315              |\n| Cerebras-GPT | 256M  | 1.3E+19        | 2.299          | 0.274      | 0.613 | 0.511       | 0.293   | 0.410 | 0.170 | 0.158      | 0.347              |\n| Cerebras-GPT | 590M  | 6.1E+19        | 2.184          | 0.291      | 0.627 | 0.498       | 0.366   | 0.464 | 0.190 | 0.158      | 0.370              |\n| Cerebras-GPT | 1.3B  | 2.8E+20        | 1.996          | 0.325      | 0.664 | 0.521       | 0.462   | 0.508 | 0.224 | 0.166      | 0.410              |\n| Cerebras-GPT | 2.7B  | 1.1E+21        | 1.834          | 0.386      | 0.701 | 0.559       | 0.567   | 0.571 | 0.246 | 0.206      | 0.462              |\n| Cerebras-GPT | 6.7B  | 6.3E+21        | 1.704          | 0.447      | 0.739 | 0.602       | 0.636   | 0.643 | 0.282 | 0.238      | 0.512              |\n| Cerebras-GPT | 13B   | 2.3E+22        | 1.575          | 0.513      | 0.766 | 0.646       | 0.696   | 0.714 | 0.367 | 0.286      | 0.570              |\n\n#### 5-shot Evaluation\n| Model    | Params | Hella-Swag | PIQA  | Wino-Grande | Lambada | ARC-e | ARC-c | OpenBookQA |\n| -------- | ----- | ----------| ----- | ----------- | -------| ----- | ----- | ---------- |\n| Cerebras-GPT | 111M  | 0.267     | 0.588 | 0.475       | 0.158  | 0.356 | 0.166 | 0.136      |\n| Cerebras-GPT | 256M  | 0.278     | 0.606 | 0.522       | 0.225  | 0.422 | 0.183 | 0.164      |\n| Cerebras-GPT | 590M  | 0.291     | 0.634 | 0.479       | 0.281  | 0.475 | 0.206 | 0.152      |\n| Cerebras-GPT | 1.3B  | 0.326     | 0.668 | 0.536       | 0.395  | 0.529 | 0.241 | 0.174      |\n| Cerebras-GPT | 2.7B  | 0.382     | 0.697 | 0.543       | 0.487  | 0.590 | 0.267 | 0.224      |\n| Cerebras-GPT | 6.7B  | 0.444     | 0.736 | 0.590       | 0.591  | 0.667 | 0.314 | 0.270      |\n| Cerebras-GPT | 13B   | 0.514     | 0.768 | 0.674       | 0.655  | 0.743 | 0.398 | 0.318      |\n\n\n<br><br>\n\n## Uses and Limitations\n\n### Intended Use\nThe primary intended use is to further research into large language models. These models can be used as a foundation model for NLP, applications, ethics, and alignment research. Our primary intended users are researchers who are working to improve LLMs and practitioners seeking reference implementations, training setups, hyperparameters, or pre-trained models. We release these models with a fully permissive Apache license for the community to use freely.\n\nYou may fine-tune and adapt Cerebras-GPT models for deployment via either Cerebras [Model Studio](https://www.cerebras.net/product-cloud/) or third-party libraries. Further safety-related testing and mitigations should be applied beore using the Cerebras-GPT model family in production downstream applications. \n\nDue to financial and compute budgets, Cerebras-GPT models were only trained and evaluated following the approaches described in the paper.\n\n### Out of Scope Use\nCerebras-GPT models are trained on the Pile, with English language only, and are not suitable for machine translation tasks.\n\nCerebras-GPT models have not been tuned for human-facing dialog applications like chatbots and will not respond to prompts in a similar way to models that have received instruction tuning or reinforcement learning from human feedback (RLHF) like Flan-T5 or ChatGPT. Cerebras-GPT models can be tuned using those methods.\n\n### Risk, Bias, Ethical Considerations\n* **Data**: The Pile dataset has been thoroughly analyzed from various ethical standpoints such as toxicity analysis, gender bias, pejorative content, racially sensitive content etc. Please refer to Pile dataset references.\n* **Human life**: The outputs from this model may or may not align with human values. The risk needs to be thoroughly investigated before deploying this model in a production environment where it can directly impact human life.\n* **Risks and harms**: There can be distributional bias in the Pile dataset that can manifest in various forms in the downstream model deployment. There are other risks associated with large language models such as amplifying stereotypes, memorizing training data, or revealing private or secure information.\n* **Mitigations**: Only mitigations in standard Pile dataset pre-processing were employed when pre-training Cerebras-GPT.\n\n<br><br>\n\n## Acknowledgements\n\nWe are thankful to all Cerebras engineers, past and present, that made this work possible.', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":103162871514,"files_count":8,"spaces_count":34,"gated":false,"private":false,"config":{"architectures":["GPT2Model"],"model_type":"gpt2"}}', '[]', '[{"type":"has_code","target_id":"github:Cerebras:modelzoo","source_url":"https://github.com/Cerebras/modelzoo"},{"type":"has_code","target_id":"github:EleutherAI:lm-evaluation-harness","source_url":"https://github.com/EleutherAI/lm-evaluation-harness"},{"type":"based_on_paper","target_id":"arxiv:2304.03208","source_url":"https://arxiv.org/abs/2304.03208"},{"type":"based_on_paper","target_id":"arxiv:2203.15556","source_url":"https://arxiv.org/abs/2203.15556"},{"type":"based_on_paper","target_id":"arxiv:2101.00027","source_url":"https://arxiv.org/abs/2101.00027"}]', NULL, 'Apache-2.0', 'approved', 78.1, '006aac4ac5ff9f10f7ed47ef67410d0a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-riffusion-riffusion-model-v1', 'huggingface--riffusion--riffusion-model-v1', 'riffusion-model-v1', 'riffusion', '--- license: creativeml-openrail-m tags: - stable-diffusion - stable-diffusion-diffusers - text-to-audio inference: true extra_gated_prompt: |- This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage. The CreativeML OpenRAIL License specifies: 1. You can''t use the model to deliberately produce nor share illegal or harmful outputs or content 2. Riffusion claims no rights on the outputs you generate, you are free to use them and a...', '["diffusers","stable-diffusion","stable-diffusion-diffusers","text-to-audio","arxiv:2112.10752","arxiv:2103.00020","arxiv:2205.11487","arxiv:2210.08402","license:creativeml-openrail-m","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-audio', 645, 2048, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/riffusion/riffusion-model-v1","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-audio\ninference: true\nextra_gated_prompt: |-\n  This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\n  The CreativeML OpenRAIL License specifies: \n\n  1. You can''t use the model to deliberately produce nor share illegal or harmful outputs or content \n  2. Riffusion claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n  3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n  Please read the full license carefully here: https://huggingface.co/spaces/CompVis/stable-diffusion-license\n      \nextra_gated_heading: Please read the LICENSE to access this model\n---\n\n# Riffusion\n\nRiffusion is an app for real-time music generation with stable diffusion.\n\nRead about it at https://www.riffusion.com/about and try it at https://www.riffusion.com/.\n\n* Code: https://github.com/riffusion/riffusion\n* Web app: https://github.com/hmartiro/riffusion-app\n* Model checkpoint: https://huggingface.co/riffusion/riffusion-model-v1\n* Discord: https://discord.gg/yu6SRwvX4v\n\nThis repository contains the model files, including:\n\n * a diffusers formated library\n * a compiled checkpoint file\n * a traced unet for improved inference speed\n * a seed image library for use with riffusion-app\n\n## Riffusion v1 Model\n\nRiffusion is a latent text-to-image diffusion model capable of generating spectrogram images given any text input. These spectrograms can be converted into audio clips.\n\nThe model was created by [Seth Forsgren](https://sethforsgren.com/) and [Hayk Martiros](https://haykmartiros.com/) as a hobby project.\n\nYou can use the Riffusion model directly, or try the [Riffusion web app](https://www.riffusion.com/).\n\nThe Riffusion model was created by fine-tuning the **Stable-Diffusion-v1-5** checkpoint. Read about Stable Diffusion here [ðŸ¤—''s Stable Diffusion blog](https://huggingface.co/blog/stable_diffusion).\n\n### Model Details\n- **Developed by:** Seth Forsgren, Hayk Martiros\n- **Model type:** Diffusion-based text-to-image generation model\n- **Language(s):** English\n- **License:** [The CreativeML OpenRAIL M license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) is an [Open RAIL M license](https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses), adapted from the work that [BigScience](https://bigscience.huggingface.co/) and [the RAIL Initiative](https://www.licenses.ai/) are jointly carrying in the area of responsible AI licensing. See also [the article about the BLOOM Open RAIL license](https://bigscience.huggingface.co/blog/the-bigscience-rail-license) on which our license is based.\n- **Model Description:** This is a model that can be used to generate and modify images based on text prompts. It is a [Latent Diffusion Model](https://arxiv.org/abs/2112.10752) that uses a fixed, pretrained text encoder ([CLIP ViT-L/14](https://arxiv.org/abs/2103.00020)) as suggested in the [Imagen paper](https://arxiv.org/abs/2205.11487).\n\n### Direct Use \nThe model is intended for research purposes only. Possible research areas and\ntasks include\n\n- Generation of artworks, audio, and use in creative processes.\n- Applications in educational or creative tools.\n- Research on generative models.\n\n### Datasets\nThe original Stable Diffusion v1.5 was trained on the [LAION-5B](https://arxiv.org/abs/2210.08402) dataset using the [CLIP text encoder](https://openai.com/blog/clip/), which provided an amazing starting point with an in-depth understanding of language, including musical concepts. The team at LAION also compiled a fantastic audio dataset from many general, speech, and music sources that we recommend at [LAION-AI/audio-dataset](https://github.com/LAION-AI/audio-dataset/blob/main/data_collection/README.md).\n\n### Fine Tuning\n\nCheck out the [diffusers training examples](https://huggingface.co/docs/diffusers/training/overview) from Hugging Face. Fine tuning requires a dataset of spectrogram images of short audio clips, with associated text describing them. Note that the CLIP encoder is able to understand and connect many words even if they never appear in the dataset. It is also possible to use a [dreambooth](https://huggingface.co/blog/dreambooth) method to get custom styles.\n\n## Citation\n\nIf you build on this work, please cite it as follows:\n\n```\n@article{Forsgren_Martiros_2022,\n  author = {Forsgren, Seth* and Martiros, Hayk*},\n  title = {{Riffusion - Stable diffusion for real-time music generation}},\n  url = {https://riffusion.com/about},\n  year = {2022}\n}\n```\n', '{"pipeline_tag":"text-to-audio","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":49112183819,"files_count":61,"spaces_count":40,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:riffusion:riffusion","source_url":"https://github.com/riffusion/riffusion"},{"type":"has_code","target_id":"github:hmartiro:riffusion-app","source_url":"https://github.com/hmartiro/riffusion-app"},{"type":"has_code","target_id":"github:LAION-AI:audio-dataset","source_url":"https://github.com/LAION-AI/audio-dataset"},{"type":"based_on_paper","target_id":"arxiv:2112.10752","source_url":"https://arxiv.org/abs/2112.10752"},{"type":"based_on_paper","target_id":"arxiv:2103.00020","source_url":"https://arxiv.org/abs/2103.00020"},{"type":"based_on_paper","target_id":"arxiv:2205.11487","source_url":"https://arxiv.org/abs/2205.11487"},{"type":"based_on_paper","target_id":"arxiv:2210.08402","source_url":"https://arxiv.org/abs/2210.08402"}]', NULL, 'creativeml-openrail-m', 'approved', 63.1, 'e598fc4b56e5a8053aaf5fe795327169', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-canopylabs-orpheus-3b-0.1-ft', 'huggingface--canopylabs--orpheus-3b-0.1-ft', 'orpheus-3b-0.1-ft', 'canopylabs', '', '["transformers","safetensors","llama","text-generation","text-to-speech","en","base_model:canopylabs/orpheus-3b-0.1-pretrained","license:apache-2.0","text-generation-inference","endpoints_compatible","region:us"]', 'text-to-speech', 644, 20038, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/canopylabs/orpheus-3b-0.1-ft","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-to-speech","library_name":"transformers","framework":"transformers","params":3782986752,"storage_bytes":56694154778,"files_count":21,"spaces_count":19,"gated":"auto","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","chat_template":"{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- if strftime_now is defined %}\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n    {%- else %}\n        {%- set date_string = \"26 Jul 2024\" %}\n    {%- endif %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][''role''] == ''system'' %}\n    {%- set system_message = messages[0][''content'']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][''content'']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there''s no first user message!\") }}\n{%- endif %}\n    {{- ''<|start_header_id|>user<|end_header_id|>\\n\\n'' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == ''ipython'' or message.role == ''tool'' or ''tool_calls'' in message) %}\n        {{- ''<|start_header_id|>'' + message[''role''] + ''<|end_header_id|>\\n\\n''+ message[''content''] | trim + ''<|eot_id|>'' }}\n    {%- elif ''tool_calls'' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' -}}\n        {{- ''{\"name\": \"'' + tool_call.name + ''\", '' }}\n        {{- ''\"parameters\": '' }}\n        {{- tool_call.arguments | tojson }}\n        {{- \"}\" }}\n        {{- \"<|eot_id|>\" }}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' }}\n{%- endif %}\n","eos_token":"<|eot_id|>"}}}', '[]', '[]', NULL, 'Apache-2.0', 'approved', 38.1, '227b5b2a5b947520a28359c29cb51297', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-bosonai-higgs-audio-v2-generation-3B-base', 'huggingface--bosonai--higgs-audio-v2-generation-3b-base', 'higgs-audio-v2-generation-3B-base', 'bosonai', '--- license: other language: - en - zh - de - ko pipeline_tag: text-to-speech --- <div align="center" style="display: flex; justify-content: center; margin-top: 10px; flex-wrap: wrap; gap: 8px;"> <a href="https://boson.ai/blog/higgs-audio-v2"><img src=''https://img.shields.io/badge/ðŸš€-Launch Blogpost-228B22'' style="margin-right: 5px;"></a> <a href="https://github.com/boson-ai/higgs-audio"><img src="https://img.shields.io/badge/ðŸ’»-Github%20Repo-9C276A" style="margin-right: 5px;"></a> <a href="h...', '["safetensors","higgs_audio","text-to-speech","en","zh","de","ko","arxiv:2505.23009","license:other","region:us"]', 'text-to-speech', 644, 158428, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlanguage:\n- en\n- zh\n- de\n- ko\npipeline_tag: text-to-speech\n---\n\n# Higgs Audio V2: Redefining Expressiveness in Audio Generation\n\n<div align="center" style="display: flex; justify-content: center; margin-top: 10px; flex-wrap: wrap; gap: 8px;">\n  <a href="https://boson.ai/blog/higgs-audio-v2"><img src=''https://img.shields.io/badge/ðŸš€-Launch Blogpost-228B22'' style="margin-right: 5px;"></a>\n  <a href="https://github.com/boson-ai/higgs-audio"><img src="https://img.shields.io/badge/ðŸ’»-Github%20Repo-9C276A" style="margin-right: 5px;"></a>\n  <a href="https://huggingface.co/spaces/smola/higgs_audio_v2"><img src="https://img.shields.io/badge/ðŸŽ®-HF%20Space%20Playground-8A2BE2" style="margin-right: 5px;"></a>\n  <a href="https://huggingface.co/bosonai/higgs-audio-v2-tokenizer"><img src="https://img.shields.io/badge/ðŸŽ§-Audio%20Tokenizer-6A5ACD.svg" style="margin-right: 5px;"></a>\n</div>\n\nCheck our open-source repository https://github.com/boson-ai/higgs-audio for more details!\n\nWe are open-sourcing Higgs Audio v2, a powerful audio foundation model pretrained on over 10 million hours of audio data and a diverse set of text data.\nDespite having no post-training or fine-tuning, Higgs Audio v2 excels in expressive audio generation, thanks to its deep language and acoustic understanding.\n\nOn [EmergentTTS-Eval](https://github.com/boson-ai/emergenttts-eval-public), the model achieves win rates of **75.7%** and **55.7%** over "gpt-4o-mini-tts" on the "Emotions" and "Questions" categories, respectively. It also obtains state-of-the-art performance on traditional TTS benchmarks like Seed-TTS Eval and Emotional Speech Dataset (ESD). Moreover, the model demonstrates capabilities rarely seen in previous systems, including automatic prosody adaptation during narration, zero-shot generation of natural multi-speaker dialogues in multiple languages, melodic humming with the cloned voice, and simultaneous generation of speech and background music.\n\n\n<p>\n    <img src="./emergent-tts-emotions-win-rate.png" width=900>\n</p>\n\nHere''s the demo video that shows some of its emergent capabilities (remember to unmute):\n\n<div align="left">\n    <video width="95%" controls>\n        <source src="https://cdn-uploads.huggingface.co/production/uploads/64fa072a52e82dd432460767/bjbWGg1IKoMtWXnl0Od8G.mp4" type="video/mp4">\n        Your browser does not support the video tag.\n    </video>\n</div>\n\nHere''s another demo video that show-cases the model''s multilingual capability and how it enabled live translation (remember to unmute):\n\n<div align="left">\n    <video width="95%" controls>\n        <source src="https://cdn-uploads.huggingface.co/production/uploads/64fa072a52e82dd432460767/9cN-ky02GzmUgogsIh1Wg.mp4" type="video/mp4">\n        Your browser does not support the video tag.\n    </video>\n</div>\n\n## Technical Details\n\n<p>\n    <img src="./higgs_audio_v2_architecture_combined.png" width=900>\n</p>\n\nHiggs Audio v2 adopts the "generation variant" depicted in the architecture figure above. Its strong performance is driven by three key technical innovations:\n\n- We developed an automated annotation pipeline that leverages multiple ASR models, sound event classification models, and our in-house audio understanding model. Using this pipeline, we cleaned and annotated 10 million hours audio data, which we refer to as AudioVerse. The in-house understanding model is finetuned on top of Higgs Audio v1 Understanding, which adopts the "understanding variant" shown in the architecture figure.\n- We trained a unified audio tokenizer from scratch that captures both semantic and acoustic features.\n- We proposed the DualFFN architecture, which enhances the LLMâ€™s ability to model acoustics tokens with minimal computational overhead.\n\n\n### Audio Tokenizer\n\n<p>\n    <img src="./higgs_audio_tokenizer_architecture.png" width=900>\n</p>\n\nWe introduce a new discretized audio tokenizer that runs at just 25 frames per second while keepingâ€”or even improvingâ€”audio quality compared to tokenizers with twice the bitrate.\nOur model is the first to train on 24 kHz data covering speech, music, and sound events in one unified system.\nIt also uses a simple non-diffusion encoder/decoder for fast, batch inference. It achieves state-of-the-art performance in semantic and acoustic evaluations.\nCheck https://huggingface.co/bosonai/higgs-audio-v2-tokenizer for more information about the tokenizer.\n\n### Model Architecture -- Dual FFN\n\nHiggs Audio v2 is built on top of [Llama-3.2-3B](https://huggingface.co/meta-llama/Llama-3.2-3B). To enhance the modelâ€™s ability to process audio tokens,\nwe incorporate the "DualFFN" architecture as an audio adapter.\nDualFFN acts as an audio-specific expert, boosting the LLM''s performance with minimal computational overhead.\nOur implementation preserves 91% of the original LLMâ€™s training speed with the inclusion of DualFFN, which has 2.2B parameters.\nThus, the total number of parameter for Higgs Audio v2 is 3.6B (LLM) + 2.2B (Audio Dual FFN), and it has the same training / inference FLOPs as Llama-3.2-3B.\nAblation study shows that the model equipped with DualFFN consistently outperforms its counterpart in terms of word error rate (WER) and speaker similarity.\nSee [our architecture blog](https://github.com/boson-ai/higgs-audio/blob/main/tech_blogs/ARCHITECTURE_BLOG.md) for more information.\n\n\n## Evaluation\n\nHere''s the performance of Higgs Audio v2 on four benchmarks,  [Seed-TTS Eval](https://github.com/BytedanceSpeech/seed-tts-eval), [Emotional Speech Dataset (ESD)](https://paperswithcode.com/dataset/esd), [EmergentTTS-Eval](https://arxiv.org/abs/2505.23009), and Multi-speaker Eval:\n\n#### Seed-TTS Eval & ESD\n\nWe prompt Higgs Audio v2 with the reference text, reference audio, and target text for zero-shot TTS. We use the standard evaluation metrics from Seed-TTS Eval and ESD.\n\n|                              | SeedTTS-Eval| | ESD   |                 |\n|------------------------------|--------|--------|---------|-------------------|\n|                              | WER â†“ | SIM â†‘ | WER â†“ | SIM (emo2vec) â†‘ |\n| Cosyvoice2                   | 2.28   | 65.49  | 2.71    | 80.48             |\n| Qwen2.5-omniâ€                 | 2.33   | 64.10  | -       | -                 |\n| ElevenLabs Multilingual V2   | **1.43**   | 50.00  | 1.66    | 65.87             |\n| Higgs Audio v1                | 2.18   | 66.27  | **1.49**    | 82.84             |\n| Higgs Audio v2 (base)         | 2.44   | **67.70**  | 1.78    | **86.13**         |\n\n\n#### EmergentTTS-Eval ("Emotions" and "Questions")\n\nFollowing the [EmergentTTS-Eval Paper](https://arxiv.org/abs/2505.23009), we report the win-rate over "gpt-4o-mini-tts" with the "alloy" voice. Results of Higgs Audio v2 is obtained with the voice of "belinda". The judge model is Gemini 2.5 Pro.\n\n| Model                              | Emotions (%) â†‘ | Questions (%) â†‘ |\n|------------------------------------|--------------|----------------|\n| Higgs Audio v2 (base)               | **75.71%**   | **55.71%**         |\n| [gpt-4o-audio-previewâ€ ](https://platform.openai.com/docs/models/gpt-4o-audio-preview)       | 61.64%       | 47.85%         |\n| [Hume.AI](https://www.hume.ai/research)                            | 61.60%       | 43.21%         |\n| **BASELINE:** [gpt-4o-mini-tts](https://platform.openai.com/docs/models/gpt-4o-mini-tts)  | 50.00%       | 50.00%         |\n| [Qwen 2.5 Omniâ€ ](https://github.com/QwenLM/Qwen2.5-Omni)      | 41.60%       | 51.78%         |\n| [minimax/speech-02-hd](https://replicate.com/minimax/speech-02-hd)               | 40.86%        | 47.32%         |\n| [ElevenLabs Multilingual v2](https://elevenlabs.io/blog/eleven-multilingual-v2)         | 30.35%       | 39.46%         |\n| [DeepGram Aura-2](https://deepgram.com/learn/introducing-aura-2-enterprise-text-to-speech)                    | 29.28%       | 48.21%         |\n| [Sesame csm-1B](https://github.com/SesameAILabs/csm)                      | 15.96%       | 31.78%         |\n\n<sup><sub>''â€ '' means using the strong-prompting method described in the paper.</sub></sup>\n\n\n#### Multi-speaker Eval\n\nWe also designed a multi-speaker evaluation benchmark to evaluate the capability of Higgs Audio v2 for multi-speaker dialog generation. The benchmark contains three subsets\n\n- `two-speaker-conversation`: 1000 synthetic dialogues involving two speakers. We fix two reference audio clips to evaluate the model''s ability in double voice cloning for utterances ranging from 4 to 10 dialogues between two randomly chosen persona.\n- `small talk (no ref)`: 250 synthetic dialogues curated in the same way as above, but are characterized by short utterances and a limited number of turns (4â€“6), we do not fix reference audios in this case and this set is designed to evaluate the model''s ability to automatically assign appropriate voices to speakers.\n- `small talk (ref)`: 250 synthetic dialogues similar to above, but contains even shorter utterances as this set is meant to include reference clips in it''s context, similar to `two-speaker-conversation`.\n\n\nWe report the word-error-rate (WER) and the geometric mean between intra-speaker similarity and inter-speaker dis-similarity on these three subsets. Other than Higgs Audio v2, we also evaluated [MoonCast](https://github.com/jzq2000/MoonCast) and [nari-labs/Dia-1.6B-0626](https://huggingface.co/nari-labs/Dia-1.6B-0626), two of the most popular open-source models capable of multi-speaker dialog generation.\nResults are summarized in the following table. We are not able to run [nari-labs/Dia-1.6B-0626](https://huggingface.co/nari-labs/Dia-1.6B-0626) on our "two-speaker-conversation" subset due to its strict limitation on the length of the utterances and output audio.\n\n|                                                | two-speaker-conversation |                |small talk |                | small talk (no ref) |                |\n| ---------------------------------------------- | -------------- | ------------------ | ---------- | -------------- | ------------------- | -------------- |\n|                                                | WER â†“                      | Mean Sim & Dis-sim â†‘ | WER â†“       |  Mean Sim & Dis-sim â†‘ | WER â†“               | Mean Sim & Dis-sim â†‘ |\n| [MoonCast](https://github.com/jzq2000/MoonCast) | 38.77                    | 46.02         | **8.33**       | 63.68          | 24.65               | 53.94 |\n| [nari-labs/Dia-1.6B-0626](https://huggingface.co/nari-labs/Dia-1.6B-0626)         | \-                       | \-             | 17.62      | 63.15          | 19.46               | **61.14**          |\n| Higgs Audio v2 (base)     | **18.88**                    | **51.95**          | 11.89      | **67.92**              | **14.65**               | 55.28              |\n\n\n## Get Started\n\nYou need to first install the [higgs-audio](https://github.com/boson-ai/higgs-audio):\n\n```bash\ngit clone https://github.com/boson-ai/higgs-audio.git\n\ncd higgs-audio\npython3 -m venv higgs_audio_env\nsource higgs_audio_env/bin/activate\npip install -r requirements.txt\npip install -e .\n```\n\nAfterwards, try to run the following python code snippet to convert text to speech.\n\n```python\nfrom boson_multimodal.serve.serve_engine import HiggsAudioServeEngine, HiggsAudioResponse\nfrom boson_multimodal.data_types import ChatMLSample, Message, AudioContent\n\nimport torch\nimport torchaudio\nimport time\nimport click\n\nMODEL_PATH = "bosonai/higgs-audio-v2-generation-3B-base"\nAUDIO_TOKENIZER_PATH = "bosonai/higgs-audio-v2-tokenizer"\n\nsystem_prompt = (\n    "Generate audio following instruction.\n\n<|scene_desc_start|>\nAudio is recorded from a quiet room.\n<|scene_desc_end|>"\n)\n\nmessages = [\n    Message(\n        role="system",\n        content=system_prompt,\n    ),\n    Message(\n        role="user",\n        content="The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years.",\n    ),\n]\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\n\nserve_engine = HiggsAudioServeEngine(MODEL_PATH, AUDIO_TOKENIZER_PATH, device=device)\n\noutput: HiggsAudioResponse = serve_engine.generate(\n    chat_ml_sample=ChatMLSample(messages=messages),\n    max_new_tokens=1024,\n    temperature=0.3,\n    top_p=0.95,\n    top_k=50,\n    stop_strings=["<|end_of_text|>", "<|eot_id|>"],\n)\ntorchaudio.save(f"output.wav", torch.from_numpy(output.audio)[None, :], output.sampling_rate)\n```\n\nYou can also check https://github.com/boson-ai/higgs-audio/tree/main/examples for more example scripts.\n## License\n\nSee [LICENSE](./LICENSE)', '{"pipeline_tag":"text-to-speech","library_name":null,"framework":null,"params":5771283456,"storage_bytes":23118624910,"files_count":17,"spaces_count":10,"gated":false,"private":false,"config":{"architectures":["HiggsAudioModel"],"model_type":"higgs_audio","tokenizer_config":{"bos_token":"<|begin_of_text|>","eos_token":"<|end_of_text|>"}}}', '[]', '[{"type":"has_code","target_id":"github:boson-ai:higgs-audio\"><img","source_url":"https://github.com/boson-ai/higgs-audio\"><img"},{"type":"has_code","target_id":"github:boson-ai:higgs-audio","source_url":"https://github.com/boson-ai/higgs-audio"},{"type":"has_code","target_id":"github:boson-ai:emergenttts-eval-public","source_url":"https://github.com/boson-ai/emergenttts-eval-public"},{"type":"has_code","target_id":"github:boson-ai:higgs-audio","source_url":"https://github.com/boson-ai/higgs-audio"},{"type":"has_code","target_id":"github:BytedanceSpeech:seed-tts-eval","source_url":"https://github.com/BytedanceSpeech/seed-tts-eval"},{"type":"has_code","target_id":"github:QwenLM:Qwen2.5-Omni","source_url":"https://github.com/QwenLM/Qwen2.5-Omni"},{"type":"has_code","target_id":"github:SesameAILabs:csm","source_url":"https://github.com/SesameAILabs/csm"},{"type":"has_code","target_id":"github:jzq2000:MoonCast","source_url":"https://github.com/jzq2000/MoonCast"},{"type":"has_code","target_id":"github:jzq2000:MoonCast","source_url":"https://github.com/jzq2000/MoonCast"},{"type":"has_code","target_id":"github:boson-ai:higgs-audio","source_url":"https://github.com/boson-ai/higgs-audio"},{"type":"has_code","target_id":"github:boson-ai:higgs-audio.git","source_url":"https://github.com/boson-ai/higgs-audio.git"},{"type":"has_code","target_id":"github:boson-ai:higgs-audio","source_url":"https://github.com/boson-ai/higgs-audio"},{"type":"based_on_paper","target_id":"arxiv:2505.23009","source_url":"https://arxiv.org/abs/2505.23009"}]', NULL, 'Other', 'approved', 98.1, 'eb12f58196446aa5ee672b327ab06aaf', NULL, 'https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base/resolve/main/higgs_audio_tokenizer_architecture.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-bosonai-higgs-audio-v2-generation-3B-base from https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base/resolve/main/higgs_audio_tokenizer_architecture.png
Image converted to WebP: data/images/huggingface-bosonai-higgs-audio-v2-generation-3B-base.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-upstage-SOLAR-10.7B-Instruct-v1.0', 'huggingface--upstage--solar-10.7b-instruct-v1.0', 'SOLAR-10.7B-Instruct-v1.0', 'upstage', '--- datasets: - c-s-ale/alpaca-gpt4-data - Open-Orca/OpenOrca - Intel/orca_dpo_pairs - allenai/ultrafeedback_binarized_cleaned language: - en license: cc-by-nc-4.0 base_model: - upstage/SOLAR-10.7B-v1.0 --- <p align="left"> <a href="https://console.upstage.ai/"> <img src="https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0/resolve/main/solar-api-banner.png" width="100%"/> </a> <p> **(This model is upstage/SOLAR-10.7B-v1.0 fine-tuned version for single-turn conversation.)** We introduce S...', '["transformers","safetensors","llama","text-generation","conversational","en","dataset:c-s-ale/alpaca-gpt4-data","dataset:open-orca/openorca","dataset:intel/orca_dpo_pairs","dataset:allenai/ultrafeedback_binarized_cleaned","arxiv:2312.15166","arxiv:2309.12284","arxiv:2403.19270","base_model:upstage/solar-10.7b-v1.0","base_model:finetune:upstage/solar-10.7b-v1.0","license:cc-by-nc-4.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 641, 31737, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\ndatasets:\n- c-s-ale/alpaca-gpt4-data\n- Open-Orca/OpenOrca\n- Intel/orca_dpo_pairs\n- allenai/ultrafeedback_binarized_cleaned\nlanguage:\n- en\nlicense: cc-by-nc-4.0\nbase_model:\n  - upstage/SOLAR-10.7B-v1.0\n---\n\n<p align="left">\n  <a href="https://console.upstage.ai/">\n    <img src="https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0/resolve/main/solar-api-banner.png" width="100%"/>\n  </a>\n<p>\n\n# **Meet 10.7B Solar: Elevating Performance with Upstage Depth UP Scaling!**\n\n**(This model is [upstage/SOLAR-10.7B-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-v1.0) fine-tuned version for single-turn conversation.)**\n\n\n# **Introduction**\nWe introduce SOLAR-10.7B, an advanced large language model (LLM) with 10.7 billion parameters, demonstrating superior performance in various natural language processing (NLP) tasks. It''s compact, yet remarkably powerful, and demonstrates unparalleled state-of-the-art performance in models with parameters under 30B.\n\nWe present a methodology for scaling LLMs called depth up-scaling (DUS) , which encompasses architectural modifications and continued pretraining. In other words, we integrated Mistral 7B weights into the upscaled layers, and finally, continued pre-training for the entire model.\n\n\nSOLAR-10.7B has remarkable performance. It outperforms models with up to 30B parameters, even surpassing the recent Mixtral 8X7B model. For detailed information, please refer to the experimental table.\nSolar 10.7B is an ideal choice for fine-tuning. SOLAR-10.7B offers robustness and adaptability for your fine-tuning needs. Our simple instruction fine-tuning using the SOLAR-10.7B pre-trained model yields significant performance improvements.\n\nFor full details of this model please read our [paper](https://arxiv.org/abs/2312.15166).\n\n\n# **Instruction Fine-Tuning Strategy**\n\nWe utilize state-of-the-art instruction fine-tuning methods including supervised fine-tuning (SFT) and direct preference optimization (DPO) [1].\n\nWe used a mixture of the following datasets\n- c-s-ale/alpaca-gpt4-data (SFT)\n- Open-Orca/OpenOrca (SFT)\n- in-house generated data utilizing Metamath [2] (SFT, DPO)\n- Intel/orca_dpo_pairs (DPO)\n- allenai/ultrafeedback_binarized_cleaned (DPO)\n\nwhere we were careful of data contamination by not using GSM8K samples when generating data and filtering tasks when applicable via the following list.\n```python\nfiltering_task_list = [\n    ''task228_arc_answer_generation_easy'',\n    ''ai2_arc/ARC-Challenge:1.0.0'',\n    ''ai2_arc/ARC-Easy:1.0.0'',\n    ''task229_arc_answer_generation_hard'',\n    ''hellaswag:1.1.0'', \n    ''task1389_hellaswag_completion'',\n    ''cot_gsm8k'',\n    ''cot_gsm8k_ii'',\n    ''drop:2.0.0'',\n    ''winogrande:1.1.0''\n]\n```\n\nUsing the datasets mentioned above, we applied SFT and iterative DPO training, a proprietary alignment strategy, to maximize the performance of our resulting model.\n\n[1] Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C.D. and Finn, C., 2023. Direct preference optimization: Your language model is secretly a reward model. NeurIPS.\n\n[2] Yu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J.T., Li, Z., Weller, A. and Liu, W., 2023. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284.\n\n# **Data Contamination Test Results**\n\nRecently, there have been contamination issues in some models on the LLM leaderboard. \nWe note that we made every effort to exclude any benchmark-related datasets from training.\nWe also ensured the integrity of our model by conducting a data contamination test [3] that is also used by the HuggingFace team [4, 5].\n\nOur results, with `result < 0.1, %:` being well below 0.9, indicate that our model is free from contamination.\n\n*The data contamination test results of HellaSwag and Winograde will be added once [3] supports them.*\n\n| Model                        | ARC   | MMLU | TruthfulQA | GSM8K |\n|------------------------------|-------|-------|-------|-------|\n| **SOLAR-10.7B-Instruct-v1.0**| result < 0.1, %: 0.06 |result < 0.1, %: 0.15 | result < 0.1, %: 0.28 | result < 0.1, %: 0.70 |\n\n[3] https://github.com/swj0419/detect-pretrain-code-contamination\n\n[4] https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard/discussions/474#657f2245365456e362412a06\n\n[5] https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard/discussions/265#657b6debf81f6b44b8966230\n\n# **Evaluation Results**\n\n| Model                                  | H6    | Model Size |\n|----------------------------------------|-------|------------|\n| **SOLAR-10.7B-Instruct-v1.0**              | **74.20** | **~ 11B**      |\n| mistralai/Mixtral-8x7B-Instruct-v0.1   | 72.62 | ~ 46.7B    |\n| 01-ai/Yi-34B-200K                      | 70.81 | ~ 34B      |\n| 01-ai/Yi-34B                           | 69.42 | ~ 34B      |\n| mistralai/Mixtral-8x7B-v0.1            | 68.42 | ~ 46.7B    |\n| meta-llama/Llama-2-70b-hf              | 67.87 | ~ 70B      |\n| tiiuae/falcon-180B                     | 67.85 | ~ 180B     |\n| **SOLAR-10.7B-v1.0**                   | **66.04** | **~11B**   |\n| mistralai/Mistral-7B-Instruct-v0.2     | 65.71 | ~ 7B       |\n| Qwen/Qwen-14B                          | 65.86 | ~ 14B      |\n| 01-ai/Yi-34B-Chat                      | 65.32 | ~34B       |\n| meta-llama/Llama-2-70b-chat-hf         | 62.4  | ~ 70B      |\n| mistralai/Mistral-7B-v0.1              | 60.97 | ~ 7B       |\n| mistralai/Mistral-7B-Instruct-v0.1     | 54.96 | ~ 7B       |\n\n# **Usage Instructions**\n\nThis model has been fine-tuned primarily for single-turn conversation, making it less suitable for multi-turn conversations such as chat.\n\n### **Version**\n\nMake sure you have the correct version of the transformers library installed:\n\n```sh\npip install transformers==4.35.2\n```\n\n### **Loading the Model**\n\nUse the following Python code to load the model:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained("Upstage/SOLAR-10.7B-Instruct-v1.0")\nmodel = AutoModelForCausalLM.from_pretrained(\n    "Upstage/SOLAR-10.7B-Instruct-v1.0",\n    device_map="auto",\n    torch_dtype=torch.float16,\n)\n```\n\n### **Conducting Single-Turn Conversation**\n\n```python\nconversation = [ {''role'': ''user'', ''content'': ''Hello?''} ] \n\nprompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors="pt").to(model.device) \noutputs = model.generate(**inputs, use_cache=True, max_length=4096)\noutput_text = tokenizer.decode(outputs[0]) \nprint(output_text)\n```\n\nBelow is an example of the output.\n```\n<s> ### User:\nHello?\n\n### Assistant:\nHello, how can I assist you today? Please feel free to ask any questions or request help with a specific task.</s>\n```\n\n### **License**\n- [upstage/SOLAR-10.7B-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-v1.0): apache-2.0\n- [upstage/SOLAR-10.7B-Instruct-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0): cc-by-nc-4.0\n  - Since some non-commercial datasets such as Alpaca are used for fine-tuning, we release this model as cc-by-nc-4.0.\n\n### **How to Cite**\n\nPlease cite the following papers using the below format when using this model.\n\n```bibtex\n@misc{kim2023solar,\n      title={SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling}, \n      author={Dahyun Kim and Chanjun Park and Sanghoon Kim and Wonsung Lee and Wonho Song and Yunsu Kim and Hyeonwoo Kim and Yungi Kim and Hyeonju Lee and Jihoo Kim and Changbae Ahn and Seonghoon Yang and Sukyung Lee and Hyunbyung Park and Gyoungjin Gim and Mikyoung Cha and Hwalsuk Lee and Sunghun Kim},\n      year={2023},\n      eprint={2312.15166},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n```bibtext\n@misc{kim2024sdpo,\n      title={sDPO: Don''t Use Your Data All at Once}, \n      author={Dahyun Kim and Yungi Kim and Wonho Song and Hyeonwoo Kim and Yunsu Kim and Sanghoon Kim and Chanjun Park},\n      year={2024},\n      eprint={2403.19270},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n### **The Upstage AI Team** ###\nUpstage is creating the best LLM and DocAI. Please find more information at https://upstage.ai \n\n### **Contact Us** ###\nAny questions and suggestions, please use the discussion tab. If you want to contact us directly, drop an email to [contact@upstage.ai](mailto:contact@upstage.ai)', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":10731524096,"storage_bytes":21463591819,"files_count":15,"spaces_count":81,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"chat_template":"{% for message in messages %}{% if message[''role''] == ''system'' %}{% if message[''content'']%}{{''### System:\n'' + message[''content'']+''\n\n''}}{% endif %}{% elif message[''role''] == ''user'' %}{{''### User:\n'' + message[''content'']+''\n\n''}}{% elif message[''role''] == ''assistant'' %}{{''### Assistant:\n''  + message[''content'']}}{% endif %}{% if loop.last and add_generation_prompt %}{{ ''### Assistant:\n'' }}{% endif %}{% endfor %}","bos_token":"<s>","eos_token":"</s>","pad_token":"</s>","unk_token":"<unk>","use_default_system_prompt":true}}}', '[]', '[{"type":"has_code","target_id":"github:swj0419:detect-pretrain-code-contamination","source_url":"https://github.com/swj0419/detect-pretrain-code-contamination"},{"type":"based_on_paper","target_id":"arxiv:2312.15166","source_url":"https://arxiv.org/abs/2312.15166"},{"type":"based_on_paper","target_id":"arxiv:2309.12284","source_url":"https://arxiv.org/abs/2309.12284"},{"type":"based_on_paper","target_id":"arxiv:2403.19270","source_url":"https://arxiv.org/abs/2403.19270"}]', NULL, 'CC-BY-NC-4.0', 'approved', 63.1, 'ea691f744e342e06dc89ec1b07b07104', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-ACE-Step-ACE-Step-v1-3.5B', 'huggingface--ace-step--ace-step-v1-3.5b', 'ACE-Step-v1-3.5B', 'ACE-Step', '--- license: apache-2.0 tags: - music - text2music - acestep pipeline_tag: text-to-audio language: - en - zh - de - fr - es - it - pt - pl - tr - ru - cs - nl - ar - ja - hu - ko - hi --- !ACE-Step Framework ACE-Step is a novel open-source foundation model for music generation that overcomes key limitations of existing approaches through a holistic architectural design. It integrates diffusion-based generation with Sana''s Deep Compression AutoEncoder (DCAE) and a lightweight linear transforme...', '["diffusers","safetensors","music","text2music","acestep","text-to-audio","en","zh","de","fr","es","it","pt","pl","tr","ru","cs","nl","ar","ja","hu","ko","hi","license:apache-2.0","region:us"]', 'text-to-audio', 641, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\ntags:\n- music\n- text2music\n- acestep\npipeline_tag: text-to-audio\nlanguage:\n- en\n- zh\n- de\n- fr\n- es\n- it\n- pt\n- pl\n- tr\n- ru\n- cs\n- nl\n- ar\n- ja\n- hu\n- ko\n- hi\n---\n\n# ACE-Step: A Step Towards Music Generation Foundation Model\n\n![ACE-Step Framework](https://github.com/ACE-Step/ACE-Step/raw/main/assets/ACE-Step_framework.png)\n\n## Model Description\n\nACE-Step is a novel open-source foundation model for music generation that overcomes key limitations of existing approaches through a holistic architectural design. It integrates diffusion-based generation with Sana''s Deep Compression AutoEncoder (DCAE) and a lightweight linear transformer, achieving state-of-the-art performance in generation speed, musical coherence, and controllability.\n\n**Key Features:**\n- 15Ã— faster than LLM-based baselines (20s for 4-minute music on A100)\n- Superior musical coherence across melody, harmony, and rhythm\n- full-song generation, duration control and accepts natural language descriptions\n\n## Uses\n\n### Direct Use\nACE-Step can be used for:\n- Generating original music from text descriptions\n- Music remixing and style transfer\n- edit song lyrics\n\n### Downstream Use\nThe model serves as a foundation for:\n- Voice cloning applications\n- Specialized music generation (rap, jazz, etc.)\n- Music production tools\n- Creative AI assistants\n\n### Out-of-Scope Use\nThe model should not be used for:\n- Generating copyrighted content without permission\n- Creating harmful or offensive content\n- Misrepresenting AI-generated music as human-created\n\n## How to Get Started\n\nsee: https://github.com/ace-step/ACE-Step\n\n## Hardware Performance\n\n| Device        | 27 Steps | 60 Steps |\n|---------------|----------|----------|\n| NVIDIA A100   | 27.27x   | 12.27x   |\n| RTX 4090      | 34.48x   | 15.63x   |\n| RTX 3090      | 12.76x   | 6.48x    |\n| M2 Max        | 2.27x    | 1.03x    |\n\n*RTF (Real-Time Factor) shown - higher values indicate faster generation*\n\n\n## Limitations\n\n- Performance varies by language (top 10 languages perform best)\n- Longer generations (>5 minutes) may lose structural coherence\n- Rare instruments may not render perfectly\n- Output Inconsistency: Highly sensitive to random seeds and input duration, leading to varied "gacha-style" results.\n- Style-specific Weaknesses: Underperforms on certain genres (e.g. Chinese rap/zh_rap) Limited style adherence and musicality ceiling\n- Continuity Artifacts: Unnatural transitions in repainting/extend operations\n- Vocal Quality: Coarse vocal synthesis lacking nuance\n- Control Granularity: Needs finer-grained musical parameter control\n\n## Ethical Considerations\n\nUsers should:\n- Verify originality of generated works\n- Disclose AI involvement\n- Respect cultural elements and copyrights\n- Avoid harmful content generation\n\n\n## Model Details\n\n**Developed by:** ACE Studio and StepFun  \n**Model type:** Diffusion-based music generation with transformer conditioning  \n**License:** Apache 2.0  \n**Resources:**  \n- [Project Page](https://ace-step.github.io/)\n- [Demo Space](https://huggingface.co/spaces/ACE-Step/ACE-Step)\n- [GitHub Repository](https://github.com/ACE-Step/ACE-Step)\n\n\n## Citation\n\n```bibtex\n@misc{gong2025acestep,\n  title={ACE-Step: A Step Towards Music Generation Foundation Model},\n  author={Junmin Gong, Wenxiao Zhao, Sen Wang, Shengyuan Xu, Jing Guo}, \n  howpublished={\url{https://github.com/ace-step/ACE-Step}},\n  year={2025},\n  note={GitHub repository}\n}\n```\n\n## Acknowledgements\nThis project is co-led by ACE Studio and StepFun.', '{"pipeline_tag":"text-to-audio","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":8276582969,"files_count":14,"spaces_count":11,"gated":false,"private":false,"config":{}}', '[]', '[{"type":"has_code","target_id":"github:ACE-Step:ACE-Step","source_url":"https://github.com/ACE-Step/ACE-Step"},{"type":"has_code","target_id":"github:ace-step:ACE-Step","source_url":"https://github.com/ace-step/ACE-Step"},{"type":"has_code","target_id":"github:ACE-Step:ACE-Step","source_url":"https://github.com/ACE-Step/ACE-Step"},{"type":"has_code","target_id":"github:ace-step:ACE-Step}},","source_url":"https://github.com/ace-step/ACE-Step}},"}]', NULL, 'Apache-2.0', 'approved', 63.1, '0276aff71dc8a81a836deb7d9f5d1f2f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-Math-V2', 'huggingface--deepseek-ai--deepseek-math-v2', 'DeepSeek-Math-V2', 'deepseek-ai', '--- license: apache-2.0 library_name: transformers base_model: - deepseek-ai/DeepSeek-Math-V2 --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/"><img alt="Homepage" src="https://...', '["transformers","safetensors","deepseek_v32","text-generation","conversational","base_model:deepseek-ai/deepseek-math-v2","base_model:quantized:deepseek-ai/deepseek-math-v2","license:apache-2.0","endpoints_compatible","fp8","region:us"]', 'text-generation', 641, 9504, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-Math-V2","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlibrary_name: transformers\nbase_model:\n  - deepseek-ai/DeepSeek-Math-V2\n---\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/"><img alt="Homepage"\n    src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true"/></a>\n  <a href="https://chat.deepseek.com/"><img alt="Chat"\n    src="https://img.shields.io/badge/ðŸ¤–%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white"/></a>\n  <a href="https://huggingface.co/deepseek-ai"><img alt="Hugging Face"\n    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white"/></a>\n  <br>\n  <a href="https://discord.gg/Tc7c45Zzu5"><img alt="Discord"\n    src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da"/></a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true"><img alt="Wechat"\n    src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white"/></a>\n  <a href="https://twitter.com/deepseek_ai"><img alt="Twitter Follow"\n    src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white"/></a>\n  <br>\n  <a href="LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-Apache 2.0-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <br>\n</div>\n\n# DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning\n\n## 1. Introduction\n\nLarge language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced.\nBy scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year.\nHowever, this approach faces fundamental limitations.\nPursuing higher final answer accuracy doesn''t address a key issue: correct answers don''t guarantee correct reasoning.\nMoreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable.\nTo push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning.\nSelf-verification is particularly important for scaling test-time compute, especially for open problems without known solutions.\nTowards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving.\nWe then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them.\nTo maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier.\nOur resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.\nWhile much work remains, these results suggest that self-verifiable mathematical reasoning is a feasible research direction that may help develop more capable mathematical AI systems.\n\n## 2. Evaluation Results\n\nBelow are evaluation results on [IMO-ProofBench](https://github.com/google-deepmind/superhuman/tree/main/imobench) (developed by the DeepMind team behind DeepThink IMO-Gold) and recent mathematics competitions including IMO 2025, CMO 2024, and Putnam 2024.\n\n**IMO-ProofBench**\n\n<p align="center">\n  <img width="100%" src="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math-V2/refs/heads/main/figures/IMO-ProofBench.png">\n</p>\n\n\n---\n\n**Mathematics Competitions**\n\n<p align="center">\n  <img width=41%" src="https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math-V2/refs/heads/main/figures/Competitions.png">\n</p>\n\n## 4. Quick Start\n\nDeepSeekMath-V2 is built on top of DeepSeek-V3.2-Exp-Base.\nFor inference support, please refer to [the DeepSeek-V3.2-Exp github repository](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp).\n\n## 6. License\nThis repository and the model weights are licensed under [the Apache License, Version 2.0 (Apache 2.0)](LICENSE).\n\n## 7. Citation\n\n```\n@misc{deepseek-math-v2,\n  author = {Zhihong Shao, Yuxiang Luo, Chengda Lu, Z.Z. Ren, Jiewen Hu, Tian Ye, Zhibin Gou, Shirong Ma, Xiaokang Zhang},\n  title = {DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning},\n  year = {2025},\n}\n```\n\n## 8. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](mailto:service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":685396921376,"storage_bytes":689483049129,"files_count":179,"spaces_count":0,"gated":false,"private":false,"config":{"architectures":["DeepseekV32ForCausalLM"],"model_type":"deepseek_v32","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ï½œbeginâ–ofâ–sentenceï½œ>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ï½œendâ–ofâ–sentenceï½œ>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ï½œendâ–ofâ–sentenceï½œ>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% if not thinking is defined %}{% set thinking = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, system_prompt='''', is_first_sp=true, is_last_user=false, is_only_sys=false, is_prefix=false) %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message[''content''] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + ''\n\n'' + message[''content''] %}{%- endif %}{% set ns.is_only_sys = true %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{''<ï½œUserï½œ>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''tool_calls''] is defined and message[''tool_calls''] is not none %}{%- if ns.is_last_user or ns.is_only_sys %}{{''<ï½œAssistantï½œ></think>''}}{%- endif %}{%- set ns.is_last_user = false -%}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls''] %}{%- if not ns.is_first %}{%- if message[''content''] is none %}{{''<ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>''+ tool[''function''][''name''] + ''<ï½œtoolâ–sepï½œ>'' + tool[''function''][''arguments''] + ''<ï½œtoolâ–callâ–endï½œ>''}}{%- else %}{{message[''content''] + ''<ï½œtoolâ–callsâ–beginï½œ><ï½œtoolâ–callâ–beginï½œ>'' + tool[''function''][''name''] + ''<ï½œtoolâ–sepï½œ>'' + tool[''function''][''arguments''] + ''<ï½œtoolâ–callâ–endï½œ>''}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{''<ï½œtoolâ–callâ–beginï½œ>''+ tool[''function''][''name''] + ''<ï½œtoolâ–sepï½œ>'' + tool[''function''][''arguments''] + ''<ï½œtoolâ–callâ–endï½œ>''}}{%- endif %}{%- endfor %}{{''<ï½œtoolâ–callsâ–endï½œ><ï½œendâ–ofâ–sentenceï½œ>''}}{%- endif %}{%- if message[''role''] == ''assistant'' and (message[''tool_calls''] is not defined or message[''tool_calls''] is none) %}{%- if ns.is_last_user %}{{''<ï½œAssistantï½œ>''}}{%- if message[''prefix''] is defined and message[''prefix''] and thinking %}{{''<think>''}}{%- else %}{{''</think>''}}{%- endif %}{%- endif %}{%- if message[''prefix''] is defined and message[''prefix''] %}{%- set ns.is_prefix = true -%}{%- endif %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{message[''content''] + ''<ï½œendâ–ofâ–sentenceï½œ>''}}{%- set ns.is_tool = false -%}{%- else %}{%- set content = message[''content''] -%}{%- if ''</think>'' in content %}{%- set content = content.split(''</think>'', 1)[1] -%}{%- endif %}{{content + ''<ï½œendâ–ofâ–sentenceï½œ>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{{''<ï½œtoolâ–outputâ–beginï½œ>'' + message[''content''] + ''<ï½œtoolâ–outputâ–endï½œ>''}}{%- endif %}{%- if message[''role''] != ''system'' %}{% set ns.is_only_sys = false %}{%- endif %}{%- endfor -%}{% if add_generation_prompt and not ns.is_tool%}{% if ns.is_last_user or ns.is_only_sys or not ns.is_prefix %}{{''<ï½œAssistantï½œ>''}}{%- if not thinking %}{{''</think>''}}{%- else %}{{''<think>''}}{%- endif %}{% endif %}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:google-deepmind:superhuman","source_url":"https://github.com/google-deepmind/superhuman"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3.2-Exp","source_url":"https://github.com/deepseek-ai/DeepSeek-V3.2-Exp"}]', NULL, 'Apache-2.0', 'approved', 63.1, 'ade85522613f3ce9c51af9bb615095fa', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-microsoft-Phi-4-mini-instruct', 'huggingface--microsoft--phi-4-mini-instruct', 'Phi-4-mini-instruct', 'microsoft', '--- language: - multilingual - ar - zh - cs - da - nl - en - fi - fr - de - he - hu - it - ja - ko - ''no'' - pl - pt - ru - es - sv - th - tr - uk library_name: transformers license: mit license_link: https://huggingface.co/microsoft/Phi-4-mini-instruct/resolve/main/LICENSE pipeline_tag: text-generation tags: - nlp - code widget: - messages: - role: user content: Can you provide ways to eat combinations of bananas and dragonfruits? --- ðŸŽ‰**Phi-4**: [mini-reasoning | reasoning] | [multimodal-in...', '["transformers","safetensors","phi3","text-generation","nlp","code","conversational","custom_code","multilingual","ar","zh","cs","da","nl","en","fi","fr","de","he","hu","it","ja","ko","no","pl","pt","ru","es","sv","th","tr","uk","arxiv:2503.01743","license:mit","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 640, 330155, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/microsoft/Phi-4-mini-instruct","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- multilingual\n- ar\n- zh\n- cs\n- da\n- nl\n- en\n- fi\n- fr\n- de\n- he\n- hu\n- it\n- ja\n- ko\n- ''no''\n- pl\n- pt\n- ru\n- es\n- sv\n- th\n- tr\n- uk\nlibrary_name: transformers\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Phi-4-mini-instruct/resolve/main/LICENSE\npipeline_tag: text-generation\ntags:\n- nlp\n- code\nwidget:\n- messages:\n  - role: user\n    content: Can you provide ways to eat combinations of bananas and dragonfruits?\n---\nðŸŽ‰**Phi-4**: [[mini-reasoning](https://huggingface.co/microsoft/Phi-4-mini-reasoning) | [reasoning](https://huggingface.co/microsoft/Phi-4-reasoning)] | [[multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct) | [onnx](https://huggingface.co/microsoft/Phi-4-multimodal-instruct-onnx)]; \n[[mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct) | [onnx](https://huggingface.co/microsoft/Phi-4-mini-instruct-onnx)]\n\n\n## Model Summary\n \nPhi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures.\n \nðŸ“° [Phi-4-mini Microsoft Blog](https://aka.ms/phi4-feb2025) <br>\nðŸ“– [Phi-4-mini Technical Report](https://aka.ms/phi-4-multimodal/techreport) <br>\nðŸ‘©â€ðŸ³ [Phi Cookbook](https://github.com/microsoft/PhiCookBook) <br>\nðŸ¡ [Phi Portal](https://azure.microsoft.com/en-us/products/phi) <br>\nðŸ–¥ï¸ Try It [Azure](https://aka.ms/phi-4-mini/azure), [Huggingface](https://huggingface.co/spaces/microsoft/phi-4-mini) <br>\n\nðŸš€ [Model paper](https://huggingface.co/papers/2503.01743)\n \n\n## Intended Uses\n \n### Primary Use Cases\n \nThe model is intended for broad multilingual commercial and research use. The model provides uses for general purpose AI systems and applications which require:\n \n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially math and logic).\n \nThe model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features.\n \n### Use Case Considerations\n \nThe model is not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models, as well as performance difference across languages, as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios.\nDevelopers should be aware of and adhere to applicable laws or regulations (including but not limited to privacy, trade compliance laws, etc.) that are relevant to their use case.\n \n***Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.***\n \n## Release Notes\n \nThis release of Phi-4-mini-instruct is based on valuable user feedback from the Phi-3 series. The Phi-4-mini model employed new architecture for efficiency, larger vocabulary for multilingual support, and better post-training techniques were used for instruction following, function calling, as well as additional data leading to substantial gains on key capabilities. It is anticipated that most use cases will benefit from this release, but users are encouraged to test in their particular AI applications. The enthusiastic support for the Phi-4 series is greatly appreciated. Feedback on Phi-4-mini-instruct is welcomed and crucial to the modelâ€™s evolution and improvement.\n \n### Model Quality\n \nTo understand the capabilities, the 3.8B parameters Phi-4-mini-instruct  model was compared with a set of models over a variety of benchmarks using an internal benchmark platform (See Appendix A for benchmark methodology). A high-level overview of the model quality is as follows:\n \n| Benchmark                        | Similar size |                   |                   |                   |                 |2x size                |                   |                   |                   |                   |                   |\n|----------------------------------|-------------|-------------------|-------------------|-------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|\n|                                  | Phi-4 mini-Ins | Phi-3.5-mini-Ins | Llama-3.2-3B-Ins | Mistral-3B | Qwen2.5-3B-Ins | Qwen2.5-7B-Ins | Mistral-8B-2410 | Llama-3.1-8B-Ins | Llama-3.1-Tulu-3-8B | Gemma2-9B-Ins | GPT-4o-mini-2024-07-18 |\n| **Popular aggregated benchmark** |             |                   |                   |                   |                 |                 |                 |                 |                 |                 |                 |\n| Arena Hard                       | 32.8        | 34.4              | 17.0              | 26.9              | 32.0            | 55.5            | 37.3            | 25.7            | 42.7            | 43.7            | 53.7            |\n| BigBench Hard (0-shot, CoT)      | 70.4        | 63.1              | 55.4              | 51.2              | 56.2            | 72.4            | 53.3            | 63.4            | 55.5            | 65.7            | 80.4            |\n| MMLU (5-shot)                    | 67.3        | 65.5              | 61.8              | 60.8              | 65.0            | 72.6            | 63.0            | 68.1            | 65.0            | 71.3            | 77.2            |\n| MMLU-Pro (0-shot, CoT)           | 52.8        | 47.4              | 39.2              | 35.3              | 44.7            | 56.2            | 36.6            | 44.0            | 40.9            | 50.1            | 62.8            |\n| **Reasoning**                    |             |                   |                   |                   |                 |                 |                 |                 |                 |                 |                 |\n| ARC Challenge (10-shot)          | 83.7        | 84.6              | 76.1              | 80.3              | 82.6            | 90.1            | 82.7            | 83.1            | 79.4            | 89.8            | 93.5            |\n| BoolQ (2-shot)                   | 81.2        | 77.7              | 71.4              | 79.4              | 65.4            | 80.0            | 80.5            | 82.8            | 79.3            | 85.7            | 88.7            |\n| GPQA (0-shot, CoT)               | 25.2        | 26.6              | 24.3              | 24.4              | 23.4            | 30.6            | 26.3            | 26.3            | 29.9            | 39.1            | 41.1            |\n| HellaSwag (5-shot)               | 69.1        | 72.2              | 77.2              | 74.6              | 74.6            | 80.0            | 73.5            | 72.8            | 80.9            | 87.1            | 88.7            |\n| OpenBookQA (10-shot)             | 79.2        | 81.2              | 72.6              | 79.8              | 79.3            | 82.6            | 80.2            | 84.8            | 79.8            | 90.0            | 90.0            |\n| PIQA (5-shot)                    | 77.6        | 78.2              | 68.2              | 73.2              | 72.6            | 76.2            | 81.2            | 83.2            | 78.3            | 83.7            | 88.7            |\n| Social IQA (5-shot)              | 72.5        | 75.1              | 68.3              | 73.9              | 75.3            | 75.3            | 77.6            | 71.8            | 73.4            | 74.7            | 82.9            |\n| TruthfulQA (MC2) (10-shot)       | 66.4        | 65.2              | 59.2              | 62.9              | 64.3            | 69.4            | 63.0            | 69.2            | 64.1            | 76.6            | 78.2            |\n| Winogrande (5-shot)              | 67.0        | 72.2              | 53.2              | 59.8              | 63.3            | 71.1            | 63.1            | 64.7            | 65.4            | 74.0            | 76.9            |\n| **Multilingual**                 |             |                   |                   |                   |                 |                 |                 |                 |                 |                 |                 |\n| Multilingual MMLU (5-shot)       | 49.3        | 51.8              | 48.1              | 46.4              | 55.9            | 64.4            | 53.7            | 56.2            | 54.5            | 63.8            | 72.9            |\n| MGSM (0-shot, CoT)               | 63.9        | 49.6              | 44.6              | 44.6              | 53.5            | 64.5            | 56.7            | 56.7            | 58.6            | 75.1            | 81.7            |\n| **Math**                         |             |                   |                   |                   |                 |                 |                 |                 |                 |                 |                 |\n| GSM8K (8-shot, CoT)              | 88.6        | 76.9              | 75.6              | 80.1              | 80.6            | 88.7            | 81.9            | 82.4            | 84.3            | 84.9            | 91.3            |\n| MATH (0-shot, CoT)               | 64.0        | 49.8              | 46.7              | 41.8              | 61.7            | 60.4            | 41.6            | 47.6            | 46.1            | 51.3            | 70.2            |\n| **Overall**                      | **63.5**    | **60.5**          | **56.2**          | **56.9**          | **60.1**        | **67.9**        | **60.2**        | **62.3**        | **60.9**        | **65.0**        | **75.5**        |\n \nOverall, the model with only 3.8B-param achieves a similar level of multilingual language understanding and reasoning ability as much larger models. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much factual knowledge, therefore, users may experience factual incorrectness. However, it may be possible to resolve such weakness by augmenting Phi-4 with a search engine, particularly when using the model under RAG settings.\n \n## Usage\n \n### Tokenizer\n \nPhi-4-mini-instruct supports a vocabulary size of up to `200064` tokens. The [tokenizer files](https://huggingface.co/microsoft/Phi-4-mini-instruct/blob/main/added_tokens.json) already provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model''s vocabulary size.\n \n### Input Formats\n \nGiven the nature of the training data, the Phi-4-mini-instruct\nmodel is best suited for prompts using specific formats.\nBelow are the two primary formats:\n \n#### Chat format\n \nThis format is used for general conversation and instructions:\n \n```yaml\n<|system|>Insert System Message<|end|><|user|>Insert User Message<|end|><|assistant|>\n```\n \n#### Tool-enabled function-calling format\n \nThis format is used when the user wants the model to provide function calls based on the given tools. The user should provide the available tools in the system prompt, wrapped by <|tool|> and <|/tool|> tokens. The tools should be specified in JSON format, using a JSON dump structure. Example:\n \n`\n<|system|>You are a helpful assistant with some tools.<|tool|>[{"name": "get_weather_updates", "description": "Fetches weather updates for a given city using the RapidAPI Weather API.", "parameters": {"city": {"description": "The name of the city for which to retrieve weather information.", "type": "str", "default": "London"}}}]<|/tool|><|end|><|user|>What is the weather like in Paris today?<|end|><|assistant|>\n`\n\n### Inference with vLLM\n\n#### Requirements\n\nList of required packages:\n\n```\nflash_attn==2.7.4.post1\ntorch==2.5.1\nvllm>=0.7.3\n```\n\n#### Example\n\nTo perform inference using vLLM, you can use the following code snippet:\n\n```python\nfrom vllm import LLM, SamplingParams\n\nllm = LLM(model="microsoft/Phi-4-mini-instruct", trust_remote_code=True)\n\nmessages = [\n    {"role": "system", "content": "You are a helpful AI assistant."},\n    {"role": "user", "content": "Can you provide ways to eat combinations of bananas and dragonfruits?"},\n    {"role": "assistant", "content": "Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey."},\n    {"role": "user", "content": "What about solving an 2x + 3 = 7 equation?"},\n]\n\nsampling_params = SamplingParams(\n  max_tokens=500,\n  temperature=0.0,\n)\n\noutput = llm.chat(messages=messages, sampling_params=sampling_params)\nprint(output[0].outputs[0].text)\n```\n\n### Inference with Transformers\n\n#### Requirements\n\n \nPhi-4 family has been integrated in the `4.49.0` version of `transformers`. The current `transformers` version can be verified with: `pip list | grep transformers`.\nPython 3.8 and 3.10 will work best. \nList of required packages:\n\n```\nflash_attn==2.7.4.post1\ntorch==2.5.1\ntransformers==4.49.0\naccelerate==1.3.0\n```\n \nPhi-4-mini-instruct is also available in [Azure AI Studio]()\n\n#### Example\n \nAfter obtaining the Phi-4-mini-instruct model checkpoints, users can use this sample code for inference.\n \n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n \ntorch.random.manual_seed(0)\n\nmodel_path = "microsoft/Phi-4-mini-instruct"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    device_map="auto",\n    torch_dtype="auto",\n    trust_remote_code=True,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n \nmessages = [\n    {"role": "system", "content": "You are a helpful AI assistant."},\n    {"role": "user", "content": "Can you provide ways to eat combinations of bananas and dragonfruits?"},\n    {"role": "assistant", "content": "Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey."},\n    {"role": "user", "content": "What about solving an 2x + 3 = 7 equation?"},\n]\n \npipe = pipeline(\n    "text-generation",\n    model=model,\n    tokenizer=tokenizer,\n)\n \ngeneration_args = {\n    "max_new_tokens": 500,\n    "return_full_text": False,\n    "temperature": 0.0,\n    "do_sample": False,\n}\n \noutput = pipe(messages, **generation_args)\nprint(output[0][''generated_text''])\n```\n \n## Responsible AI Considerations\n \nLike other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:\n \n+ Quality of Service: The Phi models are trained primarily on English text and some additional multilingual text. Languages other than English will experience worse performance as well as performance disparities across non-English. English language varieties with less representation in the training data might experience worse performance than standard American English.  \n+ Multilingual performance and safety gaps: We believe it is important to make language models more widely available across different languages, but the Phi 4 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards.\n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.\n+ Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the case.\n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.  \n+ Limited Scope for Code: The majority of Phi 4 training data is based in Python and uses common packages such as "typing, math, random, collections, datetime, itertools". If the model generates Python scripts that utilize other packages or scripts in other languages, it is  strongly recommended that users manually verify all API uses.\n+ Long Conversation: Phi 4 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift.\n \nDevelopers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. Phi 4 family of models are general purpose models. As developers plan to deploy these models for specific use cases, they are encouraged to fine-tune the models for their use case and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include:  \n \n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).  \n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n \n \n## Training\n \n### Model\n \n+ **Architecture:** Phi-4-mini-instruct has 3.8B parameters and is a dense decoder-only Transformer model. When compared with Phi-3.5-mini, the major changes with Phi-4-mini-instruct are 200K vocabulary, grouped-query attention, and shared input and output embedding.<br>\n+ **Inputs:** Text. It is best suited for prompts using the chat format.<br>\n+ **Context length:** 128K tokens<br>\n+ **GPUs:** 512 A100-80G<br>\n+ **Training time:** 21 days<br>\n+ **Training data:** 5T tokens<br>\n+ **Outputs:** Generated text in response to the input<br>\n+ **Dates:** Trained between November and December 2024<br>\n+ **Status:** This is a static model trained on offline datasets with the cutoff date of June 2024 for publicly available data.<br>\n+ **Supported languages:** Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian<br>\n+ **Release date:** February 2025<br>\n \n### Training Datasets\n \nPhi-4-miniâ€™s training data includes a wide variety of sources, totaling 5 trillion tokens, and is a combination of\n1) publicly available documents filtered for quality, selected high-quality educational data, and code\n2) newly created synthetic, â€œtextbook-likeâ€ data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (e.g., science, daily activities, theory of mind, etc.)\n3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. Focus was placed on the quality of data that could potentially improve the reasoning ability for the model, and the publicly available documents were filtered to contain a preferred level of knowledge. As an example, the result of a game in premier league on a particular day might be good training data for frontier models, but such information was removed to leave more model capacity for reasoning for the modelâ€™s small size. More details about data can be found in the Phi-4-mini-instruct technical report.\n \nThe decontamination process involved normalizing and tokenizing the dataset, then generating and comparing n-grams between the target dataset and benchmark datasets. Samples with matching n-grams above a threshold were flagged as contaminated and removed from the dataset. A detailed contamination report was generated, summarizing the matched text, matching ratio, and filtered results for further analysis.\n \n### Fine-tuning\n \nA basic example of multi-GPUs supervised fine-tuning (SFT) with TRL and Accelerate modules is provided [here](https://huggingface.co/microsoft/Phi-4-mini-instruct/resolve/main/sample_finetune.py).\n \n## Safety Evaluation and Red-Teaming\n \nVarious evaluation techniques including red teaming, adversarial conversation simulations, and multilingual safety evaluation benchmark datasets were leveraged to evaluate Phi-4 modelsâ€™ propensity to produce undesirable outputs across multiple languages and risk categories. Several approaches were used to compensate for the limitations of one approach alone. Findings across the various evaluation methods indicate that safety post-training that was done as detailed in the Phi 3 Safety Post-Training paper had a positive impact across multiple languages and risk categories as observed by refusal rates (refusal to output undesirable outputs) and robustness to jailbreak techniques. Details on prior red team evaluations across Phi models can be found in the Phi 3 Safety Post-Training paper. For this release, the red team tested the model in English, Chinese, Japanese, Spanish, Portuguese, Arabic, Thai, and Russian for the following potential harms: Hate Speech and Bias, Violent Crimes, Specialized Advice, and Election Information. Their findings indicate that the model is resistant to jailbreak techniques across languages, but that language-specific attack prompts leveraging cultural context can cause the model to output harmful content. Another insight was that with function calling scenarios, the model could sometimes hallucinate function names or URLâ€™s.  The model may also be more susceptible to longer multi-turn jailbreak techniques across both English and non-English languages. These findings highlight the need for industry-wide investment in the development of high-quality safety evaluation datasets across multiple languages, including low resource languages, and risk areas that account for cultural nuances where those languages are spoken.\n \n## Software\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [Transformers](https://github.com/huggingface/transformers)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n \n## Hardware\nNote that by default, the Phi-4-mini-instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\n* NVIDIA A100\n* NVIDIA A6000\n* NVIDIA H100\n \nIf you want to run the model on:\n* NVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained() with attn_implementation="eager"\n \n## License\nThe model is licensed under the [MIT license](./LICENSE).\n \n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must followâ€¯[Microsoftâ€™s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-partyâ€™s policies.\n \n \n## Appendix A: Benchmark Methodology\n \nWe include a brief word on methodology here - and in particular, how we think about optimizing prompts.\nIn an ideal world, we would never change any prompts in our benchmarks to ensure it is always an apples-to-apples comparison when comparing different models. Indeed, this is our default approach, and is the case in the vast majority of models we have run to date.\nThere are, however, some exceptions to this. In some cases, we see a model that performs worse than expected on a given eval due to a failure to respect the output format. For example:\n \n+ A model may refuse to answer questions (for no apparent reason), or in coding tasks models may prefix their response with â€œSure, I can help with that. â€¦â€ which may break the parser. In such cases, we have opted to try different system messages (e.g. â€œYou must always respond to a questionâ€ or â€œGet to the point!â€).\n+ With some models, we observed that few shots actually hurt model performance. In this case we did allow running the benchmarks with 0-shots for all cases.\n+ We have tools to convert between chat and completions APIs. When converting a chat prompt to a completion prompt, some models have different keywords e.g. Human vs User. In these cases, we do allow for model-specific mappings for chat to completion prompts.\n \nHowever, we do not:\n \n+ Pick different few-shot examples. Few shots will always be the same when comparing different models.\n+ Change prompt format: e.g. if it is an A/B/C/D multiple choice, we do not tweak this to 1/2/3/4 multiple choice.\n \n### Benchmark datasets\n \nThe model was evaluated across a breadth of public and internal benchmarks to understand the modelâ€™s capabilities under multiple tasks and conditions. While most evaluations use English, the leading multilingual benchmark was incorporated that covers performance in select languages.  More specifically,\n \n+ Reasoning:\n  + Winogrande: commonsense reasoning around pronoun resolution\n  + PIQA: physical commonsense reasoning around everyday situations\n  + ARC-challenge: grade-school multiple choice science questions\n  + GPQA: very hard questions written and validated by experts in biology, physics, and chemistry\n  + MedQA: medical questions answering\n  + Social IQA: social commonsense intelligence\n  + BoolQ: natural questions from context\n  + TruthfulQA: grounded reasoning\n+ Language understanding:\n  + HellaSwag: commonsense natural language inference around everyday events\n  + ANLI: adversarial natural language inference\n+ Function calling:\n  + Berkeley function calling function and tool call\n  + Internal function calling benchmarks\n+ World knowledge:\n  + TriviaQA: trivia question on general topics\n+ Math:\n  + GSM8K: grade-school math word problems\n  + GSM8K Hard: grade-school math word problems with large values and some absurdity.\n  + MATH: challenging competition math problems\n+ Code:\n  + HumanEval HumanEval+, MBPP, MBPP+: python coding tasks\n  + LiveCodeBenh, LiveBench: contamination-free code tasks\n  + BigCode Bench: challenging programming tasks\n  + Spider: SQL query tasks\n  + Internal coding benchmarks\n+ Instructions following:\n  + IFEval: verifiable instructions\n  + Internal instructions following benchmarks\n+ Multilingual:\n  + MGSM: multilingual grade-school math\n  + Multilingual MMLU and MMLU-pro\n  + MEGA: multilingual NLP tasks\n+ Popular aggregated datasets: MMLU, MMLU-pro, BigBench-Hard, AGI Eval\n+ Multi-turn conversations:\n  + Data generated by in-house adversarial conversation simulation tool\n+ Single-turn trustworthiness evaluation:\n  + DecodingTrust: a collection of trustworthiness benchmarks in eight different perspectives\n  + XSTest: exaggerated safety evaluation\n  + Toxigen: adversarial and hate speech detection\n+ Red Team:\n  + Responses to prompts provided by AI Red Team at Microsoft', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":3836021760,"storage_bytes":7687590311,"files_count":20,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Phi3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_phi3.Phi3Config","AutoModelForCausalLM":"modeling_phi3.Phi3ForCausalLM","AutoTokenizer":"Xenova/gpt-4o"},"model_type":"phi3","tokenizer_config":{"bos_token":"<|endoftext|>","chat_template":"{% for message in messages %}{% if message[''role''] == ''system'' and ''tools'' in message and message[''tools''] is not none %}{{ ''<|'' + message[''role''] + ''|>'' + message[''content''] + ''<|tool|>'' + message[''tools''] + ''<|/tool|>'' + ''<|end|>'' }}{% else %}{{ ''<|'' + message[''role''] + ''|>'' + message[''content''] + ''<|end|>'' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ ''<|assistant|>'' }}{% else %}{{ eos_token }}{% endif %}","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:microsoft:PhiCookBook","source_url":"https://github.com/microsoft/PhiCookBook"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:HazyResearch:flash-attention","source_url":"https://github.com/HazyResearch/flash-attention"},{"type":"based_on_paper","target_id":"arxiv:2503.01743","source_url":"https://arxiv.org/abs/2503.01743"}]', NULL, 'MIT', 'approved', 78.1, '460d07e6e8025c1ac09ebb9b3cba8617', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Bingsu-adetailer', 'huggingface--bingsu--adetailer', 'adetailer', 'Bingsu', '--- license: apache-2.0 library_name: ultralytics datasets: - wider_face - skytnt/anime-segmentation tags: - pytorch --- - Anime Face CreateML - xml2txt - AN - wider face - AnHDet - hand-detection-fuao9 - coco2017 (only person) - AniSeg - skytnt/anime-segmentation - deepfashion2 | id | label | | --- | --------------------- | | 0 | short_sleeved_shirt | | 1 | long_sleeved_shirt | | 2 | short_sleeved_outwear | | 3 | long_sleeved_outwear | | 4 | vest | | 5 | sling | | 6 | shorts | | 7 | trousers...', '["ultralytics","pytorch","dataset:wider_face","dataset:skytnt/anime-segmentation","doi:10.57967/hf/3633","license:apache-2.0","region:us"]', 'other', 637, 13514267, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Bingsu/adetailer","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\nlibrary_name: ultralytics\ndatasets:\n- wider_face\n- skytnt/anime-segmentation\ntags:\n- pytorch\n---\n\n# YOLOv8 Detection Model\n\n## Datasets\n\n### Face\n\n- [Anime Face CreateML](https://universe.roboflow.com/my-workspace-mph8o/anime-face-createml)\n- [xml2txt](https://universe.roboflow.com/0oooooo0/xml2txt-njqx1)\n- [AN](https://universe.roboflow.com/sed-b8vkf/an-lfg5i)\n- [wider face](http://shuoyang1213.me/WIDERFACE/index.html)\n\n### Hand\n\n- [AnHDet](https://universe.roboflow.com/1-yshhi/anhdet)\n- [hand-detection-fuao9](https://universe.roboflow.com/catwithawand/hand-detection-fuao9)\n\n### Person\n\n- [coco2017](https://cocodataset.org/#home) (only person)\n- [AniSeg](https://github.com/jerryli27/AniSeg)\n- [skytnt/anime-segmentation](https://huggingface.co/datasets/skytnt/anime-segmentation)\n\n### deepfashion2\n\n- [deepfashion2](https://github.com/switchablenorms/DeepFashion2)\n\n| id  | label                 |\n| --- | --------------------- |\n| 0   | short_sleeved_shirt   |\n| 1   | long_sleeved_shirt    |\n| 2   | short_sleeved_outwear |\n| 3   | long_sleeved_outwear  |\n| 4   | vest                  |\n| 5   | sling                 |\n| 6   | shorts                |\n| 7   | trousers              |\n| 8   | skirt                 |\n| 9   | short_sleeved_dress   |\n| 10  | long_sleeved_dress    |\n| 11  | vest_dress            |\n| 12  | sling_dress           |\n\n## Info\n\n| Model                       | Target                | mAP 50                        | mAP 50-95                     |\n| --------------------------- | --------------------- | ----------------------------- | ----------------------------- |\n| face_yolov8n.pt             | 2D / realistic face   | 0.660                         | 0.366                         |\n| face_yolov8n_v2.pt          | 2D / realistic face   | 0.669                         | 0.372                         |\n| face_yolov8s.pt             | 2D / realistic face   | 0.713                         | 0.404                         |\n| face_yolov8m.pt             | 2D / realistic face   | 0.737                         | 0.424                         |\n| face_yolov9c.pt             | 2D / realistic face   | 0.748                         | 0.433                         |\n| hand_yolov8n.pt             | 2D / realistic hand   | 0.767                         | 0.505                         |\n| hand_yolov8s.pt             | 2D / realistic hand   | 0.794                         | 0.527                         |\n| hand_yolov9c.pt             | 2D / realistic hand   | 0.810                         | 0.550                         |\n| person_yolov8n-seg.pt       | 2D / realistic person | 0.782 (bbox)<br/>0.761 (mask) | 0.555 (bbox)<br/>0.460 (mask) |\n| person_yolov8s-seg.pt       | 2D / realistic person | 0.824 (bbox)<br/>0.809 (mask) | 0.605 (bbox)<br/>0.508 (mask) |\n| person_yolov8m-seg.pt       | 2D / realistic person | 0.849 (bbox)<br/>0.831 (mask) | 0.636 (bbox)<br/>0.533 (mask) |\n| deepfashion2_yolov8s-seg.pt | realistic clothes     | 0.849 (bbox)<br/>0.840 (mask) | 0.763 (bbox)<br/>0.675 (mask) |\n\n## Usage\n\n```python\nfrom huggingface_hub import hf_hub_download\nfrom ultralytics import YOLO\n\npath = hf_hub_download("Bingsu/adetailer", "face_yolov8n.pt")\nmodel = YOLO(path)\n```\n\n```python\nimport cv2\nfrom PIL import Image\n\nimg = "https://farm5.staticflickr.com/4139/4887614566_6b57ec4422_z.jpg"\noutput = model(img)\npred = output[0].plot()\npred = cv2.cvtColor(pred, cv2.COLOR_BGR2RGB)\npred = Image.fromarray(pred)\npred\n```\n\n![image](https://i.imgur.com/9ny1wmD.png)\n\n\n## Unsafe files\n\n![image](https://i.imgur.com/9Btuy8j.png)\n\nSince `getattr` is classified as a dangerous pickle function, any segmentation model that uses it is classified as unsafe.\n\nAll models were created and saved using the official [ultralytics](https://github.com/ultralytics/ultralytics) library, so it''s okay to use files downloaded from a trusted source.\n\nSee also: https://huggingface.co/docs/hub/security-pickle\n', '{"pipeline_tag":null,"library_name":"ultralytics","framework":"ultralytics","params":null,"storage_bytes":1130245365,"files_count":14,"spaces_count":11,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:jerryli27:AniSeg","source_url":"https://github.com/jerryli27/AniSeg"},{"type":"has_code","target_id":"github:switchablenorms:DeepFashion2","source_url":"https://github.com/switchablenorms/DeepFashion2"},{"type":"has_code","target_id":"github:ultralytics:ultralytics","source_url":"https://github.com/ultralytics/ultralytics"}]', NULL, 'Apache-2.0', 'approved', 63, 'f63cf661f013cbf60711c50d8c768335', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-rhymes-ai-Aria', 'huggingface--rhymes-ai--aria', 'Aria', 'rhymes-ai', '--- language: - en library_name: transformers license: apache-2.0 pipeline_tag: image-text-to-text tags: - multimodal - aria base_model: - rhymes-ai/Aria-Base-64K --- <!-- <p align="center"> <br>Aria</br> </p> --> [Dec 1, 2024] *We have released the base models (with native multimodal pre-training) for Aria (Aria-Base-8K and Aria-Base-64K) for research purposes and continue training.* <!-- - Aria is the **first open multimodal native MoE** model, capable of seamlessly handling various input m...', '["transformers","safetensors","aria","any-to-any","multimodal","image-text-to-text","conversational","en","arxiv:2410.05993","base_model:rhymes-ai/aria-base-64k","base_model:finetune:rhymes-ai/aria-base-64k","license:apache-2.0","endpoints_compatible","region:us"]', 'image-text-to-text', 637, 41201, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/rhymes-ai/Aria","fetched_at":"2025-12-08T10:39:52.039Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\nlibrary_name: transformers\nlicense: apache-2.0\npipeline_tag: image-text-to-text\ntags:\n- multimodal\n- aria\nbase_model:\n- rhymes-ai/Aria-Base-64K\n---\n<!-- <p align="center">\n  <br>Aria</br>\n</p>  -->\n\n\n# Aria Model Card\n\n[Dec 1, 2024] *We have released the base models (with native multimodal pre-training) for Aria ([Aria-Base-8K](https://huggingface.co/rhymes-ai/Aria-Base-8K) and [Aria-Base-64K](https://huggingface.co/rhymes-ai/Aria-Base-64K)) for research purposes and continue training.*\n<!-- \n- Aria is the **first open multimodal native MoE** model, capable of seamlessly handling various input modalities within a MoE architecture.\n- Aria performs **on par with GPT-4o mini and Gemini 1.5 Flash** across a range of multimodal tasks while maintaining strong performance on **text**-only tasks.\n- Compared to similar or even larger models, Aria boasts **faster speeds** and **lower costs**. This high efficiency stems from its ability to activate only 3.9B parameters during inference â€“ the **fewest** among models with comparable performance.\n -->\n## Key features\n\n- **SoTA Multimodal Native Performance**: Aria achieves strong performance on a wide range of multimodal, language, and coding tasks. It is superior in video and document understanding.\n- **Lightweight and Fast**: Aria is a mixture-of-expert model with 3.9B activated parameters per token. It efficently encodes visual input of variable sizes and aspect ratios.  \n- **Long Multimodal Context Window**: Aria supports multimodal input of up to 64K tokens. It can caption a 256-frame video in 10 seconds.\n\n<p align="center">\nðŸ”— <a href="https://rhymes.ai/" target="_blank"> Try Aria!</a> Â· ðŸ“– <a href="https://www.rhymes.ai/blog-details/aria-first-open-multimodal-native-moe-model" target="_blank">Blog</a> Â· ðŸ“Œ <a href="https://arxiv.org/pdf/2410.05993" target="_blank">Paper</a> \n Â· â­ <a href="https://github.com/rhymes-ai/Aria" target="_blank">GitHub</a> Â· ðŸŸ£ <a href="https://discord.com/invite/u8HxU23myj" target="_blank"> Discord </a>\n</p> \n\n\n<!-- # Model Info\n\n| Model  | Download  | Parameter | Context Length |\n| :---- | :------- | :------------ | :------ |\n| Aria | < HF link - TBD> | â€¢ Activation: 3.9B (3.5B MoE + 0.4B Visual Encoder) <br> â€¢ Total: 25.3B | 64K           | -->\n\n## Benchmark\n| Category                            | Benchmark         |  Aria  | Pixtral 12B | Llama3.2 11B | GPT-4o mini | Gemini-1.5 Flash |\n|:-------------------------------------|:-------------------|:--------:|:-------------:|:--------------:|:-------------:|:------------------:|\n| **Knowledge (Multimodal)**          | MMMU              |  54.9  |    52.5     |    50.7      |    59.4     |      56.1        |\n| **Math (Multimodal)**               | MathVista         |  66.1  |    58.0     |    51.5      |      -      |      58.4        |\n| **Document**                        | DocQA             |  92.6  |    90.7     |    84.4      |      -      |      89.9        |\n| **Chart**                           | ChartQA           |  86.4  |    81.8     |    83.4      |      -      |      85.4        |\n| **Scene Text**                      | TextVQA           |  81.1  |      -      |      -       |      -      |      78.7        |\n| **General Visual QA**               | MMBench-1.1       |  80.3  |      -      |      -       |    76.0     |        -         |\n| **Video Understanding**             | LongVideoBench    |  65.3  |    47.4     |    45.7      |    58.8     |      62.4        |\n| **Knowledge (Language)**            | MMLU (5-shot)     |  73.3  |    69.2     |    69.4      |      -      |      78.9        |\n| **Math (Language)**                 | MATH              |  50.8  |    48.1     |    51.9      |    70.2     |        -         |\n| **Reasoning (Language)**            | ARC Challenge     |  91.0  |      -      |    83.4      |    96.4     |        -         |\n| **Coding**                          | HumanEval         |  73.2  |    72.0     |    72.6      |    87.2     |      74.3        |\n\n\n## Quick Start\n### Installation\n```\npip install "transformers>=4.48.0" accelerate sentencepiece torchvision requests torch Pillow\npip install flash-attn --no-build-isolation\n\n# For better inference performance, you can install grouped-gemm, which may take 3-5 minutes to install\npip install grouped_gemm==0.1.6\n```\n\n### Inference\n\nAria has 25.3B total parameters, it can be loaded in one A100 (80GB) GPU with bfloat16 precision.\n\nHere is a code snippet to show you how to use Aria.\n\n```python\nimport requests\nimport torch\nfrom PIL import Image\n\nfrom transformers import AriaProcessor, AriaForConditionalGeneration\n\n\nmodel_id_or_path = "rhymes-ai/Aria"\nmodel = AriaForConditionalGeneration.from_pretrained(\n    model_id_or_path, device_map="auto", torch_dtype=torch.bfloat16\n)\n\nprocessor = AriaProcessor.from_pretrained(model_id_or_path)\n\nimage = Image.open(requests.get("http://images.cocodataset.org/val2017/000000039769.jpg", stream=True).raw)\n\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image"},\n            {"text": "what is the image?", "type": "text"},\n        ],\n    }\n]\n\ntext = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=text, images=image, return_tensors="pt")\ninputs[''pixel_values''] = inputs[''pixel_values''].to(torch.bfloat16)\ninputs.to(model.device)\n\noutput = model.generate(\n    **inputs,\n    max_new_tokens=15,\n    stop_strings=["<|im_end|>"],\n    tokenizer=processor.tokenizer,\n    do_sample=True,\n    temperature=0.9,\n)\noutput_ids = output[0][inputs["input_ids"].shape[1]:]\nresponse = processor.decode(output_ids, skip_special_tokens=True)\nprint(response)\n```\n\n-----------\nFrom transformers>=v4.48, you can also pass image url or local path to the conversation history, and let the chat template handle the rest.\nChat template will load the image for you and return inputs in `torch.Tensor` which you can pass directly to `model.generate()`.\n\nHere is how to rewrite the above example\n\n```python\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "url": "http://images.cocodataset.org/val2017/000000039769.jpg"}\n            {"type": "text", "text": "what is the image?"},\n        ],\n    },\n]\n\ninputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors"pt")\nipnuts = inputs.to(model.device, torch.bfloat16)\n\noutput = model.generate(\n    **inputs,\n    max_new_tokens=15,\n    stop_strings=["<|im_end|>"],\n    tokenizer=processor.tokenizer,\n    do_sample=True,\n    temperature=0.9,\n)\noutput_ids = output[0][inputs["input_ids"].shape[1]:]\nresponse = processor.decode(output_ids, skip_special_tokens=True)\nprint(response)\n```\n\n### Advanced Inference and Fine-tuning\nWe provide a [codebase](https://github.com/rhymes-ai/Aria) for more advanced usage of Aria,\nincluding vllm inference, cookbooks, and fine-tuning on custom datasets.\n\n\n\n## Citation\nIf you find our work helpful, please consider citing.\n```\n@article{aria,\n  title={Aria: An Open Multimodal Native Mixture-of-Experts Model}, \n  author={Dongxu Li and Yudong Liu and Haoning Wu and Yue Wang and Zhiqi Shen and Bowen Qu and Xinyao Niu and Guoyin Wang and Bei Chen and Junnan Li},\n  year={2024},\n  journal={arXiv preprint arXiv:2410.05993},\n}\n```', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":25307311216,"storage_bytes":106186808426,"files_count":25,"spaces_count":7,"gated":false,"private":false,"config":{"architectures":["AriaForConditionalGeneration"],"model_type":"aria","processor_config":{"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}<|im_start|>{{ message[''role''] }}\n{% if message[''content''] is string %}{{ message[''content''] }}{% elif message[''content''] is iterable %}{% for item in message[''content''] %}{% if item[''type''] == ''text'' %}{{ item[''text''] }}{% elif item[''type''] == ''image'' %}<fim_prefix><|img|><fim_suffix>{% endif %}{% endfor %}{% endif %}<|im_end|>\n{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"},"tokenizer_config":{"bos_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}<|im_start|>{{ message[''role''] }}\n{% if message[''content''] is string %}{{ message[''content''] }}{% elif message[''content''] is iterable %}{% for item in message[''content''] %}{% if item[''type''] == ''text'' %}{{ item[''text''] }}{% elif item[''type''] == ''image'' %}<fim_prefix><|img|><fim_suffix>{% endif %}{% endfor %}{% endif %}<|im_end|>\n{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}","eos_token":null,"pad_token":"<|endoftext|>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:rhymes-ai:Aria\"","source_url":"https://github.com/rhymes-ai/Aria\""},{"type":"has_code","target_id":"github:rhymes-ai:Aria","source_url":"https://github.com/rhymes-ai/Aria"},{"type":"based_on_paper","target_id":"arxiv:2410.05993","source_url":"https://arxiv.org/abs/2410.05993"}]', NULL, 'Apache-2.0', 'approved', 63, '32b2482c63c4f1589cb5f271dfae2308', NULL, NULL, CURRENT_TIMESTAMP);
