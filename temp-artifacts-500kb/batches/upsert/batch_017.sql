/* LOGS:
Downloading image for github-aymericdamien-TensorFlow-Examples from https://github.com/aymericdamien.png
Image converted to WebP: data/images/github-aymericdamien-TensorFlow-Examples.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-apache-airflow', 'github--apache--airflow', 'airflow', 'apache', '<!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in w...', '["airflow","apache","apache-airflow","automation","dag","data-engineering","data-integration","data-orchestrator","data-pipelines","data-science","elt","etl","machine-learning","mlops","orchestration","python","scheduler","workflow","workflow-engine","workflow-orchestration","python"]', 'other', 43457, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/apache/airflow","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n "License"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n-->\n\n<!-- START Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n# Apache Airflow\n\n| Category   | Badges                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n|------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| License    | [![License](https://img.shields.io/:license-Apache%202-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0.txt)                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| PyPI       | [![PyPI version](https://badge.fury.io/py/apache-airflow.svg)](https://badge.fury.io/py/apache-airflow) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/apache-airflow.svg)](https://pypi.org/project/apache-airflow/) [![PyPI - Downloads](https://img.shields.io/pypi/dm/apache-airflow)](https://pypi.org/project/apache-airflow/)                                                                                                                                                                               |\n| Containers | [![Docker Pulls](https://img.shields.io/docker/pulls/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow) [![Docker Stars](https://img.shields.io/docker/stars/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow) [![Artifact HUB](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/apache-airflow)](https://artifacthub.io/packages/search?repo=apache-airflow)                                                                                                                      |\n| Community  | [![Contributors](https://img.shields.io/github/contributors/apache/airflow)](https://github.com/apache/airflow/graphs/contributors) [![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](https://s.apache.org/airflow-slack) ![Commit Activity](https://img.shields.io/github/commit-activity/m/apache/airflow) [![LFX Health Score](https://insights.linuxfoundation.org/api/badge/health-score?project=apache-airflow)](https://insights.linuxfoundation.org/project/apache-airflow)  |\n| Dev tools  | [![prek](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/j178/prek/master/docs/assets/badge-v0.json)](https://github.com/j178/prek)                                                                                                                                                                                                                                                                                                                                                                                 |\n\n\n| Version | Build Status                                                                                                                                                    |\n|---------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Main    | [![GitHub Build main](https://github.com/apache/airflow/actions/workflows/ci-amd-arm.yml/badge.svg)](https://github.com/apache/airflow/actions)                 |\n| 3.x     | [![GitHub Build 3.1](https://github.com/apache/airflow/actions/workflows/ci-amd-arm.yml/badge.svg?branch=v3-1-test)](https://github.com/apache/airflow/actions) |\n| 2.x     | [![GitHub Build 2.11](https://github.com/apache/airflow/actions/workflows/ci.yml/badge.svg?branch=v2-11-test)](https://github.com/apache/airflow/actions)       |\n\n\n\n<picture width="500">\n  <img\n    src="https://github.com/apache/airflow/blob/19ebcac2395ef9a6b6ded3a2faa29dc960c1e635/docs/apache-airflow/img/logos/wordmark_1.png?raw=true"\n    alt="Apache Airflow logo"\n  />\n</picture>\n\n[Apache Airflow](https://airflow.apache.org/docs/apache-airflow/stable/) (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.\n\nWhen workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.\n\nUse Airflow to author workflows (Dags) that orchestrate tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\n\n<!-- END Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON''T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n**Table of contents**\n\n- [Project Focus](#project-focus)\n- [Principles](#principles)\n- [Requirements](#requirements)\n- [Getting started](#getting-started)\n- [Installing from PyPI](#installing-from-pypi)\n- [Installation](#installation)\n- [Official source code](#official-source-code)\n- [Convenience packages](#convenience-packages)\n- [User Interface](#user-interface)\n- [Semantic versioning](#semantic-versioning)\n- [Version Life Cycle](#version-life-cycle)\n- [Support for Python and Kubernetes versions](#support-for-python-and-kubernetes-versions)\n- [Base OS support for reference Airflow images](#base-os-support-for-reference-airflow-images)\n- [Approach to dependencies of Airflow](#approach-to-dependencies-of-airflow)\n- [Contributing](#contributing)\n- [Voting Policy](#voting-policy)\n- [Who uses Apache Airflow?](#who-uses-apache-airflow)\n- [Who maintains Apache Airflow?](#who-maintains-apache-airflow)\n- [What goes into the next release?](#what-goes-into-the-next-release)\n- [Can I use the Apache Airflow logo in my presentation?](#can-i-use-the-apache-airflow-logo-in-my-presentation)\n- [Links](#links)\n- [Sponsors](#sponsors)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n## Project Focus\n\nAirflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include [Luigi](https://github.com/spotify/luigi), [Oozie](https://oozie.apache.org/) and [Azkaban](https://azkaban.github.io/).\n\nAirflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow''s [XCom feature](https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html)). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work.\n\nAirflow is not a streaming solution, but it is often used to process real-time data, pulling data off streams in batches.\n\n## Principles\n\n- **Dynamic**: Pipelines are defined in code, enabling dynamic dag generation and parameterization.\n- **Extensible**: The Airflow framework includes a wide range of built-in operators and can be extended to fit your needs.\n- **Flexible**: Airflow leverages the [**Jinja**](https://jinja.palletsprojects.com) templating engine, allowing rich customizations.\n\n<!-- START Requirements, please keep comment here to allow auto update of PyPI readme.md -->\n## Requirements\n\nApache Airflow is tested with:\n\n|            | Main version (dev)           | Stable version (3.1.3) |\n|------------|------------------------------|------------------------|\n| Python     | 3.10, 3.11, 3.12, 3.13       | 3.10, 3.11, 3.12, 3.13 |\n| Platform   | AMD64/ARM64(\*)              | AMD64/ARM64(\*)        |\n| Kubernetes | 1.30, 1.31, 1.32, 1.33, 1.34 | 1.30, 1.31, 1.32, 1.33 |\n| PostgreSQL | 14, 15, 16, 17, 18           | 13, 14, 15, 16, 17     |\n| MySQL      | 8.0, 8.4, Innovation         | 8.0, 8.4, Innovation   |\n| SQLite     | 3.15.0+                      | 3.15.0+                |\n\n\* Experimental\n\n**Note**: MariaDB is not tested/recommended.\n\n**Note**: SQLite is used in Airflow tests. Do not use it in production. We recommend\nusing the latest stable version of SQLite for local development.\n\n**Note**: Airflow currently can be run on POSIX-compliant Operating Systems. For development, it is regularly\ntested on fairly modern Linux Distros and recent versions of macOS.\nOn Windows you can run it via WSL2 (Windows Subsystem for Linux 2) or via Linux Containers.\nThe work to add Windows support is tracked via [#10388](https://github.com/apache/airflow/issues/10388), but\nit is not a high priority. You should only use Linux-based distros as "Production" execution environment\nas this is the only environment that is supported. The only distro that is used in our CI tests and that\nis used in the [Community managed DockerHub image](https://hub.docker.com/p/apache/airflow) is\n`Debian Bookworm`.\n\n<!-- END Requirements, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START Getting started, please keep comment here to allow auto update of PyPI readme.md -->\n## Getting started\n\nVisit the official Airflow website documentation (latest **stable** release) for help with\n[installing Airflow](https://airflow.apache.org/docs/apache-airflow/stable/installation/),\n[getting started](https://airflow.apache.org/docs/apache-airflow/stable/start.html), or walking\nthrough a more complete [tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/).\n\n> Note: If you''re looking for documentation for the main branch (latest development branch): you can find it on [s.apache.org/airflow-docs](https://s.apache.org/airflow-docs/).\n\nFor more information on Airflow Improvement Proposals (AIPs), visit\nthe [Airflow Wiki](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals).\n\nDocumentation for dependent projects like provider distributions, Docker image, Helm Chart, you''ll find it in [the documentation index](https://airflow.apache.org/docs/).\n\n<!-- END Getting started, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md -->\n\n## Installing from PyPI\n\nWe publish Apache Airflow as `apache-airflow` package in PyPI. Installing it however might be sometimes tricky\nbecause Airflow is a bit of both a library and application. Libraries usually keep their dependencies open, and\napplications usually pin them, but we should do neither and both simultaneously. We decided to keep\nour dependencies as open as possible (in `pyproject.toml`) so users can install different versions of libraries\nif needed. This means that `pip install apache-airflow` will not work from time to time or will\nproduce unusable Airflow installation.\n\nTo have repeatable installation, however, we keep a set of "known-to-be-working" constraint\nfiles in the orphan `constraints-main` and `constraints-2-0` branches. We keep those "known-to-be-working"\nconstraints files separately per major/minor Python version.\nYou can use them as constraint files when installing Airflow from PyPI. Note that you have to specify\ncorrect Airflow tag/version/branch and Python versions in the URL.\n\n1. Installing just Airflow:\n\n> Note: Only `pip` installation is currently officially supported.\n\nWhile it is possible to install Airflow with tools like [Poetry](https://python-poetry.org) or\n[pip-tools](https://pypi.org/project/pip-tools), they do not share the same workflow as\n`pip` - especially when it comes to constraint vs. requirements management.\nInstalling via `Poetry` or `pip-tools` is not currently supported.\n\nIf you wish to install Airflow using those tools, you should use the constraint files and convert\nthem to the appropriate format and workflow that your tool requires.\n\n\n```bash\npip install ''apache-airflow==3.1.3'' \\n --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-3.1.3/constraints-3.10.txt"\n```\n\n2. Installing with extras (i.e., postgres, google)\n\n```bash\npip install ''apache-airflow[postgres,google]==3.1.3'' \\n --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-3.1.3/constraints-3.10.txt"\n```\n\nFor information on installing provider distributions, check\n[providers](http://airflow.apache.org/docs/apache-airflow-providers/index.html).\n\n<!-- END Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md -->\n\n## Installation\n\nFor comprehensive instructions on setting up your local development environment and installing Apache Airflow, please refer to the [INSTALLING.md](INSTALLING.md) file.\n\n<!-- START Official source code, please keep comment here to allow auto update of PyPI readme.md -->\n## Official source code\n\nApache Airflow is an [Apache Software Foundation](https://www.apache.org) (ASF) project,\nand our official source code releases:\n\n- Follow the [ASF Release Policy](https://www.apache.org/legal/release-policy.html)\n- Can be downloaded from [the ASF Distribution Directory](https://downloads.apache.org/airflow)\n- Are cryptographically signed by the release manager\n- Are officially voted on by the PMC members during the\n  [Release Approval Process](https://www.apache.org/legal/release-policy.html#release-approval)\n\nFollowing the ASF rules, the source packages released must be sufficient for a user to build and test the\nrelease provided they have access to the appropriate platform and tools.\n\n<!-- END Official source code, please keep comment here to allow auto update of PyPI readme.md -->\n## Convenience packages\n\nThere are other ways of installing and using Airflow. Those are "convenience" methods - they are\nnot "official releases" as stated by the `ASF Release Policy`, but they can be used by the users\nwho do not want to build the software themselves.\n\nThose are - in the order of most common ways people install Airflow:\n\n- [PyPI releases](https://pypi.org/project/apache-airflow/) to install Airflow using standard `pip` tool\n- [Docker Images](https://hub.docker.com/r/apache/airflow) to install airflow via\n  `docker` tool, use them in Kubernetes, Helm Charts, `docker-compose`, `docker swarm`, etc. You can\n  read more about using, customizing, and extending the images in the\n  [Latest docs](https://airflow.apache.org/docs/docker-stack/index.html), and\n  learn details on the internals in the [images](https://airflow.apache.org/docs/docker-stack/index.html) document.\n- [Tags in GitHub](https://github.com/apache/airflow/tags) to retrieve the git project sources that\n  were used to generate official source packages via git\n\nAll those artifacts are not official releases, but they are prepared using officially released sources.\nSome of those artifacts are "development" or "pre-release" ones, and they are clearly marked as such\nfollowing the ASF Policy.\n\n## User Interface\n\n- **DAGs**: Overview of all DAGs in your environment.\n\n  ![DAGs](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/dags.png)\n\n- **Assets**: Overview of Assets with dependencies.\n\n  ![Asset Dependencies](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/assets_graph.png)\n\n- **Grid**: Grid representation of a DAG that spans across time.\n\n  ![Grid](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/grid.png)\n\n- **Graph**: Visualization of a DAG''s dependencies and their current status for a specific run.\n\n  ![Graph](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/graph.png)\n\n- **Home**: Summary statistics of your Airflow environment.\n\n  ![Home](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/home.png)\n\n- **Backfill**: Backfilling a DAG for a specific date range.\n\n  ![Backfill](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/backfill.png)\n\n- **Code**: Quick way to view source code of a DAG.\n\n  ![Code](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/code.png)\n\n## Semantic versioning\n\nAs of Airflow 2.0.0, we support a strict [SemVer](https://semver.org/) approach for all packages released.\n\nThere are few specific rules that we agreed to that define details of versioning of the different\npackages:\n\n* **Airflow**: SemVer rules apply to core airflow only (excludes any changes to providers).\n  Changing limits for versions of Airflow dependencies is not a breaking change on its own.\n* **Airflow Providers**: SemVer rules apply to changes in the particular provider''s code only.\n  SemVer MAJOR and MINOR versions for the packages are independent of the Airflow version.\n  For example, `google 4.1.0` and `amazon 3.1.1` providers can happily be installed\n  with `Airflow 2.1.2`. If there are limits of cross-dependencies between providers and Airflow packages,\n  they are present in providers as `install_requires` limitations. We aim to keep backwards\n  compatibility of providers with all previously released Airflow 2 versions but\n  there will sometimes be breaking changes that might make some, or all\n  providers, have minimum Airflow version specified.\n* **Airflow Helm Chart**: SemVer rules apply to changes in the chart only. SemVer MAJOR and MINOR\n  versions for the chart are independent of the Airflow version. We aim to keep backwards\n  compatibility of the Helm Chart with all released Airflow 2 versions, but some new features might\n  only work starting from specific Airflow releases. We might however limit the Helm\n  Chart to depend on minimal Airflow version.\n* **Airflow API clients**: Their versioning is independent from Airflow versions. They follow their own\n  SemVer rules for breaking changes and new features - which for example allows to change the way we generate\n  the clients.\n\n## Version Life Cycle\n\nApache Airflow version life cycle:\n\n<!-- This table is automatically updated by prek scripts/ci/prek/supported_versions.py -->\n<!-- Beginning of auto-generated table -->\n\n| Version   | Current Patch/Minor   | State     | First Release   | Limited Maintenance   | EOL/Terminated   |\n|-----------|-----------------------|-----------|-----------------|-----------------------|------------------|\n| 3         | 3.1.3                 | Supported | Apr 22, 2025    | TBD                   | TBD              |\n| 2         | 2.11.0                | Supported | Dec 17, 2020    | Oct 22, 2025          | Apr 22, 2026     |\n| 1.10      | 1.10.15               | EOL       | Aug 27, 2018    | Dec 17, 2020          | June 17, 2021    |\n| 1.9       | 1.9.0                 | EOL       | Jan 03, 2018    | Aug 27, 2018          | Aug 27, 2018     |\n| 1.8       | 1.8.2                 | EOL       | Mar 19, 2017    | Jan 03, 2018          | Jan 03, 2018     |\n| 1.7       | 1.7.1.2               | EOL       | Mar 28, 2016    | Mar 19, 2017          | Mar 19, 2017     |\n\n<!-- End of auto-generated table -->\n\nLimited support versions will be supported with security and critical bug fix only.\nEOL versions will not get any fixes nor support.\nWe always recommend that all users run the latest available minor release for whatever major version is in use.\nWe **highly** recommend upgrading to the latest Airflow major release at the earliest convenient time and before the EOL date.\n\n## Support for Python and Kubernetes versions\n\nAs of Airflow 2.0, we agreed to certain rules we follow for Python and Kubernetes support.\nThey are based on the official release schedule of Python and Kubernetes, nicely summarized in the\n[Python Developer''s Guide](https://devguide.python.org/#status-of-python-branches) and\n[Kubernetes version skew policy](https://kubernetes.io/docs/setup/release/version-skew-policy/).\n\n1. We drop support for Python and Kubernetes versions when they reach EOL. Except for Kubernetes, a\n   version stays supported by Airflow if two major cloud providers still provide support for it. We drop\n   support for those EOL versions in main right after EOL date, and it is effectively removed when we release\n   the first new MINOR (Or MAJOR if there is no new MINOR version) of Airflow. For example, for Python 3.10 it\n   means that we will drop support in main right after 27.06.2023, and the first MAJOR or MINOR version of\n   Airflow released after will not have it.\n\n2. We support a new version of Python/Kubernetes in main after they are officially released, as soon as we\n   make them work in our CI pipeline (which might not be immediate due to dependencies catching up with\n   new versions of Python mostly) we release new images/support in Airflow based on the working CI setup.\n\n3. This policy is best-effort which means there may be situations where we might terminate support earlier\n   if circumstances require it.\n\n## Base OS support for reference Airflow images\n\nThe Airflow Community provides conveniently packaged container images that are published whenever\nwe publish an Apache Airflow release. Those images contain:\n\n* Base OS with necessary packages to install Airflow (stable Debian OS)\n* Base Python installation in versions supported at the time of release for the MINOR version of\n  Airflow released (so there could be different versions for 2.3 and 2.2 line for example)\n* Libraries required to connect to supported Databases (again the set of databases supported depends\n  on the MINOR version of Airflow)\n* Predefined set of popular providers (for details see the [Dockerfile](https://raw.githubusercontent.com/apache/airflow/main/Dockerfile)).\n* Possibility of building your own, custom image where the user can choose their own set of providers\n  and libraries (see [Building the image](https://airflow.apache.org/docs/docker-stack/build.html))\n* In the future Airflow might also support a "slim" version without providers nor database clients installed\n\nThe version of the base OS image is the stable version of Debian. Airflow supports using all currently active\nstable versions - as soon as all Airflow dependencies support building, and we set up the CI pipeline for\nbuilding and testing the OS version. Approximately 6 months before the end-of-regular support of a\nprevious stable version of the OS, Airflow switches the images released to use the latest supported\nversion of the OS.\n\nFor example switch from ``Debian Bullseye`` to ``Debian Bookworm`` has been implemented\nbefore 2.8.0 release in October 2023 and ``Debian Bookworm`` will be the only option supported as of\nAirflow 2.10.0.\n\nUsers will continue to be able to build their images using stable Debian releases until the end of regular\nsupport and building and verifying of the images happens in our CI but no unit tests were executed using\nthis image in the `main` branch.\n\n## Approach to dependencies of Airflow\n\nAirflow has a lot of dependencies - direct and transitive, also Airflow is both - library and application,\ntherefore our policies to dependencies has to include both - stability of installation of application,\nbut also ability to install newer version of dependencies for those users who develop DAGs. We developed\nthe approach where `constraints` are used to make sure airflow can be installed in a repeatable way, while\nwe do not limit our users to upgrade most of the dependencies. As a result we decided not to upper-bound\nversion of Airflow dependencies by default, unless we have good reasons to believe upper-bounding them is\nneeded because of importance of the dependency as well as risk it involves to upgrade specific dependency.\nWe also upper-bound the dependencies that we know cause problems.\n\nThe constraint mechanism of ours takes care about finding and upgrading all the non-upper bound dependencies\nautomatically (providing that all the tests pass). Our `main` build failures will indicate in case there\nare versions of dependencies that break our tests - indicating that we should either upper-bind them or\nthat we should fix our code/tests to account for the upstream changes from those dependencies.\n\nWhenever we upper-bound such a dependency, we should always comment why we are doing it - i.e. we should have\na good reason why dependency is upper-bound. And we should also mention what is the condition to remove the\nbinding.\n\n### Approach for dependencies for Airflow Core\n\nThose dependencies are maintained in ``pyproject.toml``.\n\nThere are few dependencies that we decided are important enough to upper-bound them by default, as they are\nknown to follow predictable versioning scheme, and we know that new versions of those are very likely to\nbring breaking changes. We commit to regularly review and attempt to upgrade to the newer versions of\nthe dependencies as they are released, but this is manual process.\n\nThe important dependencies are:\n\n* `SQLAlchemy`: upper-bound to specific MINOR version (SQLAlchemy is known to remove deprecations and\n   introduce breaking changes especially that support for different Databases varies and changes at\n   various speed)\n* `Alembic`: it is important to handle our migrations in predictable and performant way. It is developed\n   together with SQLAlchemy. Our experience with Alembic is that it very stable in MINOR version\n* `Flask`: We are using Flask as the back-bone of our web UI and API. We know major version of Flask\n   are very likely to introduce breaking changes across those so limiting it to MAJOR version makes sense\n* `werkzeug`: the library is known to cause problems in new versions. It is tightly coupled with Flask\n   libraries, and we should update them together\n* `celery`: Celery is a crucial component of Airflow as it used for CeleryExecutor (and similar). Celery\n   [follows SemVer](https://docs.celeryq.dev/en/stable/contributing.html?highlight=semver#versions), so\n   we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library,\n   we should make sure Celery Provider minimum Airflow version is updated.\n* `kubernetes`: Kubernetes is a crucial component of Airflow as it is used for the KubernetesExecutor\n   (and similar). Kubernetes Python library [follows SemVer](https://github.com/kubernetes-client/python#compatibility),\n   so we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library,\n   we should make sure Kubernetes Provider minimum Airflow version is updated.\n\n### Approach for dependencies in Airflow Providers and extras\n\nThe main part of the Airflow is the Airflow Core, but the power of Airflow also comes from a number of\nproviders that extend the core functionality and are released separately, even if we keep them (for now)\nin the same monorepo for convenience. You can read more about the providers in the\n[Providers documentation](https://airflow.apache.org/docs/apache-airflow-providers/index.html). We also\nhave set of policies implemented for maintaining and releasing community-managed providers as well\nas the approach for community vs. 3rd party providers in the [providers](https://github.com/apache/airflow/blob/main/PROVIDERS.rst) document.\n\nThose `extras` and `providers` dependencies are maintained in `provider.yaml` of each provider.\n\nBy default, we should not upper-bound dependencies for providers, however each provider''s maintainer\nmight decide to add additional limits (and justify them with comment).\n\n<!-- START Contributing, please keep comment here to allow auto update of PyPI readme.md -->\n\n## Contributing\n\nWant to help build Apache Airflow? Check out our [contributors'' guide](https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) for a comprehensive overview of how to contribute, including setup instructions, coding standards, and pull request guidelines.\n\nIf you can''t wait to contribute, and want to get started asap, check out the [contribution quickstart](https://github.com/apache/airflow/blob/main/contributing-docs/03a_contributors_quick_start_beginners.rst) here!\n\nOfficial Docker (container) images for Apache Airflow are described in [images](https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md).\n\n<!-- END Contributing, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START Who uses Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n\n## Voting Policy\n\n* Commits need a +1 vote from a committer who is not the author\n* When we do AIP voting, both PMC member''s and committer''s `+1s` are considered a binding vote.\n\n## Who uses Apache Airflow?\n\nWe know about around 500 organizations that are using Apache Airflow (but there are likely many more)\n[in the wild](https://github.com/apache/airflow/blob/main/INTHEWILD.md).\n\nIf you use Airflow - feel free to make a PR to add your organisation to the list.\n\n<!-- END Who uses Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START Who maintains Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n\n## Who maintains Apache Airflow?\n\nAirflow is the work of the [community](https://github.com/apache/airflow/graphs/contributors),\nbut the [core committers/maintainers](https://people.apache.org/committers-by-project.html#airflow)\nare responsible for reviewing and merging PRs as well as steering conversations around new feature requests.\nIf you would like to become a maintainer, please review the Apache Airflow\n[committer requirements](https://github.com/apache/airflow/blob/main/COMMITTERS.rst#guidelines-to-become-an-airflow-committer).\n\n<!-- END Who maintains Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n\n## What goes into the next release?\n\nOften you will see an issue that is assigned to specific milestone with Airflow version, or a PR that gets merged\nto the main branch and you might wonder which release the merged PR(s) will be released in or which release the fixed\nissues will be in. The answer to this is as usual - it depends on various scenarios. The answer is different for PRs and Issues.\n\nTo add a bit of context, we are following the [Semver](https://semver.org/) versioning scheme as described in\n[Airflow release process](https://airflow.apache.org/docs/apache-airflow/stable/release-process.html). More\ndetails are explained in detail in this README under the [Semantic versioning](#semantic-versioning) chapter, but\nin short, we have `MAJOR.MINOR.PATCH` versions of Airflow.\n\n* `MAJOR` version is incremented in case of breaking changes\n* `MINOR` version is incremented when there are new features added\n* `PATCH` version is incremented when there are only bug-fixes and doc-only changes\n\nGenerally we release `MINOR` versions of Airflow from a branch that is named after the MINOR version. For example\n`2.7.*` releases are released from `v2-7-stable` branch, `2.8.*` releases are released from `v2-8-stable`\nbranch, etc.\n\n1. Most of the time in our release cycle, when the branch for next `MINOR` branch is not yet created, all\nPRs merged to `main` (unless they get reverted), will find their way to the next `MINOR` release. For example\nif the last release is `2.7.3` and `v2-8-stable` branch is not created yet, the next `MINOR` release\nis `2.8.0` and all PRs merged to main will be released in `2.8.0`. However, some PRs (bug-fixes and\ndoc-only changes) when merged, can be cherry-picked to current `MINOR` branch and released in the\nnext `PATCHLEVEL` release. For example, if `2.8.1` is already released and we are working on `2.9.0dev`,  then\nmarking a PR with `2.8.2` milestone means that it will be cherry-picked to `v2-8-test` branch and\nreleased in `2.8.2rc1`, and eventually in `2.8.2`.\n\n2. When we prepare for the next `MINOR` release, we cut new `v2-*-test` and `v2-*-stable` branch\nand prepare `alpha`, `beta` releases for the next `MINOR` version, the PRs merged to main will still be\nreleased in the next `MINOR` release until `rc` version is cut. This is happening because the `v2-*-test`\nand `v2-*-stable` branches are rebased on top of main when next `beta` and `rc` releases are prepared.\nFor example, when we cut `2.10.0beta1` version, anything merged to main before `2.10.0rc1` is released,\nwill find its way to 2.10.0rc1.\n\n3. Then, once we prepare the first RC candidate for the MINOR release, we stop moving the `v2-*-test` and\n`v2-*-stable` branches and the PRs merged to main will be released in the next `MINOR` release.\nHowever, some PRs (bug-fixes and doc-only changes) when merged, can be cherry-picked to current `MINOR`\nbranch and released in the next `PATCHLEVEL` release - for example when the last released version from `v2-10-stable`\nbranch is `2.10.0rc1`, some of the PRs from main can be marked as `2.10.0` milestone by committers,\nthe release manager will try to cherry-pick them into the release branch.\nIf successful, they will be released in `2.10.0rc2` and subsequently in `2.10.0`. This also applies to\nsubsequent `PATCHLEVEL` versions. When for example `2.10.1` is already released, marking a PR with\n`2.10.2` milestone will mean that it will be cherry-picked to `v2-10-stable` branch and released in `2.10.2rc1`\nand eventually in `2.10.2`.\n\nThe final decision about cherry-picking is made by the release manager.\n\nMarking issues with a milestone is a bit different. Maintainers do not mark issues with a milestone usually,\nnormally they are only marked in PRs. If PR linked to the issue (and "fixing it") gets merged and released\nin a specific version following the process described above, the issue will be automatically closed, no\nmilestone will be set for the issue, you need to check the PR that fixed the issue to see which version\nit was released in.\n\nHowever, sometimes maintainers mark issues with specific milestone, which means that the\nissue is important to become a candidate to take a look when the release is being prepared. Since this is an\nOpen-Source project, where basically all contributors volunteer their time, there is no guarantee that specific\nissue will be fixed in specific version. We do not want to hold the release because some issue is not fixed,\nso in such case release manager will reassign such unfixed issues to the next milestone in case they are not\nfixed in time for the current release. Therefore, the milestone for issue is more of an intent that it should be\nlooked at, than promise it will be fixed in the version.\n\nMore context and **FAQ** about the patchlevel release can be found in the\n[What goes into the next release](dev/WHAT_GOES_INTO_THE_NEXT_RELEASE.md) document in the `dev` folder of the\nrepository.\n\n## Can I use the Apache Airflow logo in my presentation?\n\nYes! Be sure to abide by the Apache Foundation [trademark policies](https://www.apache.org/foundation/marks/#books) and the Apache Airflow [Brandbook](https://cwiki.apache.org/confluence/display/AIRFLOW/Brandbook). The most up-to-date logos are found in [this repo](https://github.com/apache/airflow/tree/main/airflow-core/docs/img/logos/) and on the Apache Software Foundation [website](https://www.apache.org/logos/about.html).\n\n## Links\n\n- [Documentation](https://airflow.apache.org/docs/apache-airflow/stable/)\n- [Chat](https://s.apache.org/airflow-slack)\n- [Community Information](https://airflow.apache.org/community/)\n\n## Sponsors\n\nThe CI infrastructure for Apache Airflow has been sponsored by:\n\n<!-- Ordered by most recently "funded" -->\n\n<a href="https://astronomer.io"><img src="https://assets2.astronomer.io/logos/logoForLIGHTbackground.png" alt="astronomer.io" width="250px"></a>\n<a href="https://aws.amazon.com/opensource/"><img src="https://github.com/apache/airflow/blob/main/providers/amazon/docs/integration-logos/AWS-Cloud-alt_light-bg@4x.png?raw=true" alt="AWS OpenSource" width="130px"></a>\n', '{"language":"Python","stars":43457,"forks":16066,"watchers":43457,"open_issues":1694,"topics":["airflow","apache","apache-airflow","automation","dag","data-engineering","data-integration","data-orchestrator","data-pipelines","data-science","elt","etl","machine-learning","mlops","orchestration","python","scheduler","workflow","workflow-engine","workflow-orchestration"],"default_branch":"main","size_kb":506778,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:j178:prek","source_url":"https://github.com/j178/prek"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:spotify:luigi","source_url":"https://github.com/spotify/luigi"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:kubernetes-client:python","source_url":"https://github.com/kubernetes-client/python#compatibility"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"}]', NULL, 'Apache-2.0', 'approved', 80, '518a4234b96ceef2a8a8ae4fe2838852', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-apache-airflow from https://github.com/apache.png
Image converted to WebP: data/images/github-apache-airflow.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-streamlit-streamlit', 'github--streamlit--streamlit', 'streamlit', 'streamlit', '<br> <img src="https://user-images.githubusercontent.com/7164864/217935870-c0bc60a3-6fc0-4047-b011-7b4c59488c91.png" alt="Streamlit logo" style="margin-top:50px"></img> **A faster way to build and share data apps.** Streamlit lets you transform Python scripts into interactive web apps in minutes, instead of weeks. Build dashboards, generate reports, or create chat apps. Once you‚Äôve created an app, you can use our Community Cloud platform to deploy, manage, and share your app. - **Simple and P...', '["data-analysis","data-science","data-visualization","deep-learning","developer-tools","machine-learning","python","streamlit","python"]', 'other', 42570, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/streamlit/streamlit","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<br>\n\n<img src="https://user-images.githubusercontent.com/7164864/217935870-c0bc60a3-6fc0-4047-b011-7b4c59488c91.png" alt="Streamlit logo" style="margin-top:50px"></img>\n\n# Welcome to Streamlit üëã\n\n**A faster way to build and share data apps.**\n\n## What is Streamlit?\n\nStreamlit lets you transform Python scripts into interactive web apps in minutes, instead of weeks. Build dashboards, generate reports, or create chat apps. Once you‚Äôve created an app, you can use our [Community Cloud platform](https://streamlit.io/cloud) to deploy, manage, and share your app.\n\n### Why choose Streamlit?\n\n- **Simple and Pythonic:** Write beautiful, easy-to-read code.\n- **Fast, interactive prototyping:** Let others interact with your data and provide feedback quickly.\n- **Live editing:** See your app update instantly as you edit your script.\n- **Open-source and free:** Join a vibrant community and contribute to Streamlit''s future.\n\n## Installation\n\nOpen a terminal and run:\n\n```bash\n$ pip install streamlit\n$ streamlit hello\n```\n\nIf this opens our sweet _Streamlit Hello_ app in your browser, you''re all set! If not, head over to [our docs](https://docs.streamlit.io/get-started) for specific installs.\n\nThe app features a bunch of examples of what you can do with Streamlit. Jump to the [quickstart](#quickstart) section to understand how that all works.\n\n<img src="https://user-images.githubusercontent.com/7164864/217936487-1017784e-68ec-4e0d-a7f6-6b97525ddf88.gif" alt="Streamlit Hello" width=500 href="none"></img>\n\n## Quickstart\n\n### A little example\n\nCreate a new file named `streamlit_app.py` in your project directory with the following code:\n```python\nimport streamlit as st\nx = st.slider("Select a value")\nst.write(x, "squared is", x * x)\n```\n\nNow run it to open the app!\n```\n$ streamlit run streamlit_app.py\n```\n\n<img src="https://user-images.githubusercontent.com/7164864/215172915-cf087c56-e7ae-449a-83a4-b5fa0328d954.gif" width=300 alt="Little example"></img>\n\n### Give me more!\n\nStreamlit comes in with [a ton of additional powerful elements](https://docs.streamlit.io/develop/api-reference) to spice up your data apps and delight your viewers. Some examples:\n\n<table border="0">\n  <tr>\n    <td>\n      <a target="_blank" href="https://docs.streamlit.io/develop/api-reference/widgets">\n        <img src="https://user-images.githubusercontent.com/7164864/217936099-12c16f8c-7fe4-44b1-889a-1ac9ee6a1b44.png" style="max-height:150px; width:auto; display:block;">\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.streamlit.io/develop/api-reference/data/st.dataframe">\n        <img src="https://user-images.githubusercontent.com/7164864/215110064-5eb4e294-8f30-4933-9563-0275230e52b5.gif" style="max-height:150px; width:auto; display:block;">\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.streamlit.io/develop/api-reference/charts">\n        <img src="https://user-images.githubusercontent.com/7164864/215174472-bca8a0d7-cf4b-4268-9c3b-8c03dad50bcd.gif" style="max-height:150px; width:auto; display:block;">\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.streamlit.io/develop/api-reference/layout">\n        <img src="https://user-images.githubusercontent.com/7164864/217936149-a35c35be-0d96-4c63-8c6a-1c4b52aa8f60.png" style="max-height:150px; width:auto; display:block;">\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.streamlit.io/develop/concepts/multipage-apps">\n        <img src="https://user-images.githubusercontent.com/7164864/215173883-eae0de69-7c1d-4d78-97d0-3bc1ab865e5b.gif" style="max-height:150px; width:auto; display:block;">\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://streamlit.io/gallery">\n        <img src="https://user-images.githubusercontent.com/7164864/215109229-6ae9111f-e5c1-4f0b-b3a2-87a79268ccc9.gif" style="max-height:150px; width:auto; display:block;">\n      </a>\n    </td>\n  </tr>\n  <tr>\n    <td>Input widgets</td>\n    <td>Dataframes</td>\n    <td>Charts</td>\n    <td>Layout</td>\n    <td>Multi-page apps</td>\n    <td>Fun</td>\n  </tr>\n</table>\n\n\nOur vibrant creators community also extends Streamlit capabilities using ¬†üß© [Streamlit Components](https://streamlit.io/components).\n\n## Get inspired\n\nThere''s so much you can build with Streamlit:\n- ü§ñ¬†¬†[LLMs & chatbot apps](https://streamlit.io/gallery?category=llms)\n- üß¨¬†¬†[Science & technology apps](https://streamlit.io/gallery?category=science-technology)\n- üí¨¬†¬†[NLP & language apps](https://streamlit.io/gallery?category=nlp-language)\n- üè¶¬†¬†[Finance & business apps](https://streamlit.io/gallery?category=finance-business)\n- üó∫¬†¬†[Geography & society apps](https://streamlit.io/gallery?category=geography-society)\n- and more!\n\n**Check out [our gallery!](https://streamlit.io/gallery)** üéà\n\n## Community Cloud\n\nDeploy, manage and share your apps for free using our [Community Cloud](https://streamlit.io/cloud)! Sign-up [here](https://share.streamlit.io/signup). <br><br>\n<img src="https://user-images.githubusercontent.com/7164864/214965336-64500db3-0d79-4a20-8052-2dda883902d2.gif" width="400"></img>\n\n## Resources\n\n- Explore our [docs](https://docs.streamlit.io) to learn how Streamlit works.\n- Ask questions and get help in our [community forum](https://discuss.streamlit.io).\n- Read our [blog](https://blog.streamlit.io) for tips from developers and creators.\n- Extend Streamlit''s capabilities by installing or creating your own [Streamlit Components](https://streamlit.io/components).\n- Help others find and play with your app by using the Streamlit GitHub badge in your repository:\n```markdown\n[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](URL_TO_YOUR_APP)\n```\n[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io/streamlit/roadmap)\n\n## Contribute\n\nüéâ Thanks for your interest in helping improve Streamlit! üéâ\n\nBefore contributing, please read our guidelines here: https://github.com/streamlit/streamlit/wiki/Contributing\n\n## License\n\nStreamlit is completely free and open-source and licensed under the [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) license.\n', '{"language":"Python","stars":42570,"forks":3943,"watchers":42570,"open_issues":1307,"topics":["data-analysis","data-science","data-visualization","deep-learning","developer-tools","machine-learning","python","streamlit"],"default_branch":"develop","size_kb":790952,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:streamlit:streamlit","source_url":"https://github.com/streamlit/streamlit"}]', NULL, 'Apache-2.0', 'approved', 65, 'a0699b965473363ec49d99eceeed15bc', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-streamlit-streamlit from https://github.com/streamlit.png
Image converted to WebP: data/images/github-streamlit-streamlit.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-deepspeedai-DeepSpeed', 'github--deepspeedai--deepspeed', 'DeepSpeed', 'deepspeedai', '<div align="center"> <img src="docs/assets/images/DeepSpeed_light.svg#gh-light-mode-only" width="400px"> <img src="docs/assets/images/DeepSpeed_dark_transparent.svg#gh-dark-mode-only" width="400px"> </div> * [2025/10] We hosted the Ray x DeepSpeed Meetup at Anyscale. We shared our most recent work on SuperOffload, ZenFlow, Muon Optimizer Support, Arctic Long Sequence Training and DeepCompile. Please find the meetup slides here. * [2025/10] SuperOffload: Unleashing the Power of Large-Scale LLM...', '["billion-parameters","compression","data-parallelism","deep-learning","gpu","inference","machine-learning","mixture-of-experts","model-parallelism","pipeline-parallelism","pytorch","trillion-parameters","zero","python"]', 'other', 40937, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/deepspeedai/DeepSpeed","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '[![License Apache 2.0](https://badgen.net/badge/license/apache2.0/blue)](https://github.com/deepspeedai/DeepSpeed/blob/master/LICENSE)\n[![PyPI version](https://badge.fury.io/py/deepspeed.svg)](https://pypi.org/project/deepspeed/)\n[![Downloads](https://static.pepy.tech/badge/deepspeed)](https://pepy.tech/project/deepspeed)\n[![Build](https://badgen.net/badge/build/check-status/blue)](#build-pipeline-status)\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/9530/badge)](https://www.bestpractices.dev/projects/9530)\n[![Twitter](https://img.shields.io/twitter/follow/DeepSpeedAI)](https://twitter.com/intent/follow?screen_name=DeepSpeedAI)\n[![Japanese Twitter](https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9ETwitter-%40DeepSpeedAI_JP-blue)](https://twitter.com/DeepSpeedAI_JP)\n[![Chinese Zhihu](https://img.shields.io/badge/%E7%9F%A5%E4%B9%8E-%E5%BE%AE%E8%BD%AFDeepSpeed-blue)](https://www.zhihu.com/people/deepspeed)\n[![Slack](https://img.shields.io/badge/Slack-4A154B?style=for-the-badge&logo=slack&logoColor=white)](https://join.slack.com/t/deepspeedworkspace/shared_invite/zt-3a8pjd8dd-PCj2hMvR4Y2syPwVnjEoww)\n\n\n<div align="center">\n <img src="docs/assets/images/DeepSpeed_light.svg#gh-light-mode-only" width="400px">\n <img src="docs/assets/images/DeepSpeed_dark_transparent.svg#gh-dark-mode-only" width="400px">\n</div>\n\n## Latest News\n\n* [2025/10] We hosted the [Ray x DeepSpeed Meetup](https://luma.com/3wctqteh) at Anyscale. We shared our most recent work on SuperOffload, ZenFlow, Muon Optimizer Support, Arctic Long Sequence Training and DeepCompile. Please find the meetup slides [here](https://docs.google.com/presentation/d/1eM3mY6oW9GYkRy1Xz0iOnbbEr5T1t0JJXOM5BKtR-Ks/edit?slide=id.g38615d6b4c2_0_87#slide=id.g38615d6b4c2_0_87).\n\n* [2025/10] [SuperOffload: Unleashing the Power of Large-Scale LLM Training on Superchips](https://pytorch.org/blog/superoffload-unleashing-the-power-of-large-scale-llm-training-on-superchips/)\n\n* [2025/10] [Study of ZenFlow and ZeRO offload performance with DeepSpeed CPU core binding](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/zenflow-corebinding/README.md)\n\n* [2025/08] [ZenFlow: Stall-Free Offloading Engine for LLM Training](https://pytorch.org/blog/zenflow-stall-free-offloading-engine-for-llm-training/)\n\n* [2025/06] [Arctic Long Sequence Training (ALST) with DeepSpeed: Scalable And Efficient Training For Multi-Million Token Sequences](https://www.snowflake.com/en/engineering-blog/arctic-long-sequence-training-multi-million-token-ai/)\n\n* [2025/06] [DeepNVMe: Affordable I/O scaling for Deep Learning Applications](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepnvme/06-2025/README.md)\n\n\n<!-- NOTE: we must use html for news items otherwise links will be broken in the ''more news'' section -->\n<details>\n<!-- NOTE: Maintain only three items in ''more news'' section -->\n <summary>More news</summary>\n <ul>\n\n   <li>[2025/04] <a href="https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepcompile/README.md">DeepCompile: Unlocking Compiler Optimization for Distributed Training</a></li>\n\n   <li>[2025/03] <a href="https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/huggingface-tp/README.md">DeepSpeed AutoTP: Automatic Tensor Parallel Training of Hugging Face models</a></li>\n\n<li>[2024/12] <a href="https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/ulysses-offload/README.md">Ulysses-Offload: Democratizing Long Context LLM Training</a></li>\n\n </ul>\n</details>\n\n---\n\n# Extreme Speed and Scale for DL Training\n\n***[DeepSpeed](https://www.deepspeed.ai/) enabled the world''s most powerful language models (at the time of this writing) such as [MT-530B](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) and [BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed)***. DeepSpeed offers a confluence of [system innovations](https://www.deepspeed.ai/training/), that has made large scale DL training effective, and efficient, greatly improved ease of use, and redefined the DL training landscape in terms of scale that is possible. These innovations include ZeRO, ZeRO-Infinity, 3D-Parallelism, Ulysses Sequence Parallelism, DeepSpeed-MoE, etc.\n\n---\n\n# DeepSpeed Adoption\n\nDeepSpeed was an important part of Microsoft‚Äôs\n[AI at Scale](https://www.microsoft.com/en-us/research/project/ai-at-scale/)\ninitiative to enable next-generation AI capabilities at scale, where you can find more\ninformation [here](https://innovation.microsoft.com/en-us/exploring-ai-at-scale).\n\nDeepSpeed has been used to train many different large-scale models, below is a list of several examples that we are aware of (if you''d like to include your model please submit a PR):\n\n  * [Megatron-Turing NLG (530B)](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)\n  * [Jurassic-1 (178B)](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf)\n  * [BLOOM (176B)](https://huggingface.co/blog/bloom-megatron-deepspeed)\n  * [GLM (130B)](https://github.com/THUDM/GLM-130B)\n  * [xTrimoPGLM (100B)](https://www.biorxiv.org/content/10.1101/2023.07.05.547496v2)\n  * [YaLM (100B)](https://github.com/yandex/YaLM-100B)\n  * [GPT-NeoX (20B)](https://github.com/EleutherAI/gpt-neox)\n  * [AlexaTM (20B)](https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning)\n  * [Turing NLG (17B)](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)\n  * [METRO-LM (5.4B)](https://arxiv.org/pdf/2204.06644.pdf)\n\nDeepSpeed has been integrated with several different popular open-source DL frameworks such as:\n\n|                                                                                                | Documentation                                |\n| ---------------------------------------------------------------------------------------------- | -------------------------------------------- |\n<img src="docs/assets/images/transformers-light.png#gh-light-mode-only" width="250px"><img src="docs/assets/images/transformers-dark.png#gh-dark-mode-only" width="250px"> | [Transformers with DeepSpeed](https://huggingface.co/docs/transformers/deepspeed) |\n| <img src="docs/assets/images/accelerate-light.png#gh-light-mode-only" width="250px"><img src="docs/assets/images/accelerate-dark.png#gh-dark-mode-only" width="250px"> | [Accelerate with DeepSpeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed) |\n| <img src="docs/assets/images/lightning-light.svg#gh-light-mode-only" width="200px"><img src="docs/assets/images/lightning-dark.svg#gh-dark-mode-only" width="200px"> | [Lightning with DeepSpeed](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html#deepspeed) |\n| <img src="docs/assets/images/mosaicml.svg" width="200px"> | [MosaicML with DeepSpeed](https://docs.mosaicml.com/projects/composer/en/latest/trainer/using_the_trainer.html?highlight=deepspeed#deepspeed-integration) |\n| <img src="docs/assets/images/determined.svg" width="225px"> | [Determined with DeepSpeed](https://docs.determined.ai/latest/training/apis-howto/deepspeed/overview.html) |\n| <img src="https://user-images.githubusercontent.com/58739961/187154444-fce76639-ac8d-429b-9354-c6fac64b7ef8.jpg" width=150> | [MMEngine with DeepSpeed](https://mmengine.readthedocs.io/en/latest/common_usage/large_model_training.html#deepspeed) |\n\n---\n\n# Build Pipeline Status\n\n| Description | Status |\n| ----------- | ------ |\n| NVIDIA | [![nv-torch-latest-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml)  [![nv-inference](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-inference.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-inference.yml) [![nv-nightly](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-nightly.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-nightly.yml) |\n| AMD | [![amd-mi200](https://github.com/deepspeedai/DeepSpeed/actions/workflows/amd-mi200.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/amd-mi200.yml) |\n| CPU | [![torch-latest-cpu](https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-torch-latest.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-torch-latest.yml) |\n| Intel Gaudi | [![hpu-gaudi2](https://github.com/deepspeedai/DeepSpeed/actions/workflows/hpu-gaudi2.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/hpu-gaudi2.yml) |\n| Intel XPU | [![xpu-max1100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/xpu-max1100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/xpu-max1100.yml) |\n| PyTorch Nightly | [![nv-torch-nightly-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml) |\n| Integrations | [![nv-transformers-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-transformers-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-transformers-v100.yml) [![nv-lightning-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-lightning-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-lightning-v100.yml) [![nv-accelerate-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-accelerate-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-accelerate-v100.yml) [![nv-mii](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-mii.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-mii.yml) [![nv-ds-chat](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-ds-chat.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-ds-chat.yml) [![nv-sd](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-sd.yml/badge.svg)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-sd.yml) |\n| Misc | [![Formatting](https://github.com/deepspeedai/DeepSpeed/actions/workflows/formatting.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/formatting.yml) [![pages-build-deployment](https://github.com/deepspeedai/DeepSpeed/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/pages/pages-build-deployment) [![Documentation Status](https://readthedocs.org/projects/deepspeed/badge/?version=latest)](https://deepspeed.readthedocs.io/en/latest/?badge=latest)[![python](https://github.com/deepspeedai/DeepSpeed/actions/workflows/python.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/python.yml) |\n| Huawei Ascend NPU | [![Huawei Ascend NPU](https://github.com/Ascend/Ascend-CI/actions/workflows/deepspeed.yaml/badge.svg?branch=main)](https://github.com/Ascend/Ascend-CI/actions/workflows/deepspeed.yaml) |\n\n# Installation\n\nThe quickest way to get started with DeepSpeed is via pip, this will install\nthe latest release of DeepSpeed which is not tied to specific PyTorch or CUDA\nversions. DeepSpeed includes several C++/CUDA extensions that we commonly refer\nto as our ''ops''.  By default, all of these extensions/ops will be built\njust-in-time (JIT) using [torch''s JIT C++ extension loader that relies on\nninja](https://pytorch.org/docs/stable/cpp_extension.html) to build and\ndynamically link them at runtime.\n\n## Requirements\n* [PyTorch](https://pytorch.org/) must be installed _before_ installing DeepSpeed.\n* For full feature support we recommend a version of PyTorch that is >= 1.9 and ideally the latest PyTorch stable release.\n* A CUDA or ROCm compiler such as [nvcc](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#introduction) or [hipcc](https://github.com/ROCm-Developer-Tools/HIPCC) used to compile C++/CUDA/HIP extensions.\n* Specific GPUs we develop and test against are listed below, this doesn''t mean your GPU will not work if it doesn''t fall into this category it''s just DeepSpeed is most well tested on the following:\n  * NVIDIA: Pascal, Volta, Ampere, and Hopper architectures\n  * AMD: MI100 and MI200\n\n## Contributed HW support\n* DeepSpeed now support various HW accelerators.\n\n| Contributor | Hardware                            | Accelerator Name | Contributor validated | Upstream validated |\n|-------------|-------------------------------------|------------------| --------------------- |--------------------|\n| Huawei      | Huawei Ascend NPU                   | npu              | Yes | No                 |\n| Intel       | Intel(R) Gaudi(R) 2 AI accelerator  | hpu              | Yes | Yes                |\n| Intel       | Intel(R) Xeon(R) Processors         | cpu              | Yes | Yes                |\n| Intel       | Intel(R) Data Center GPU Max series | xpu              | Yes | Yes                |\n| Tecorigin   | Scalable Data Analytics Accelerator | sdaa             | Yes | No                 |\n\n## PyPI\nWe regularly push releases to [PyPI](https://pypi.org/project/deepspeed/) and encourage users to install from there in most cases.\n\n```bash\npip install deepspeed\n```\n\nAfter installation, you can validate your install and see which extensions/ops\nyour machine is compatible with via the DeepSpeed environment report.\n\n```bash\nds_report\n```\n\nIf you would like to pre-install any of the DeepSpeed extensions/ops (instead\nof JIT compiling) or install pre-compiled ops via PyPI please see our [advanced\ninstallation instructions](https://www.deepspeed.ai/tutorials/advanced-install/).\n\n## Windows\nMany DeepSpeed features are supported on Windows for both training and inference. You can read more about this in the original blog post [here](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/windows/08-2024/README.md). Among features that are currently not supported are async io (AIO) and GDS (which does not support Windows).\n1. Install PyTorch, such as pytorch 2.3+cu121.\n2. Install Visual C++ build tools, such as VS2022 C++ x64/x86 build tools.\n3. Launch Cmd console with Administrator permissions for creating required symlink folders and ensure MSVC tools are added to your PATH or launch the Developer Command Prompt for Visual Studio 2022 with administrator permissions.\n4. Run `build_win.bat` to build wheel in `dist` folder.\n\n\n# Further Reading\n\nAll DeepSpeed documentation, tutorials, and blogs can be found on our website: [deepspeed.ai](https://www.deepspeed.ai/)\n\n\n|                                                                                                | Description                                  |\n| ---------------------------------------------------------------------------------------------- | -------------------------------------------- |\n| [Getting Started](https://www.deepspeed.ai/getting-started/)                                   |  First steps with DeepSpeed                  |\n| [DeepSpeed JSON Configuration](https://www.deepspeed.ai/docs/config-json/)                     |  Configuring DeepSpeed                       |\n| [API Documentation](https://deepspeed.readthedocs.io/en/latest/)                               |  Generated DeepSpeed API documentation       |\n| [Tutorials](https://www.deepspeed.ai/tutorials/)                                               |  Tutorials                                   |\n| [Blogs](https://www.deepspeed.ai/posts/)                                                       |  Blogs                                   |\n\n\n# CI funding\n\nThis being an open source project we rely on others to provide us resources for CI hardware. At this moment Modal is kindly supporting our GPU CI runs by funding the hardware for us. Modal is an AI infrastructure platform for inference, fine-tuning, batch jobs and more. Get started with $30/mo in free credits today at https://modal.com. We have been getting an amazing support from Modal''s team and will surely recommend them to your business.\n\n# Contributing\nDeepSpeed welcomes your contributions! Please see our\n[contributing](CONTRIBUTING.md) guide for more details on formatting, testing,\netc.<br/>\nThanks so much to all of our amazing contributors!\n\n<a href="https://github.com/deepspeedai/DeepSpeed/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=microsoft/DeepSpeed&r="  width="800px"/>\n</a>\n\n## Contributor License Agreement\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to, and\nactually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply\nfollow the instructions provided by the bot. You will only need to do this once across\nall repos using our CLA.\n\n## Code of Conduct\nThis project has adopted the [Microsoft Open Source Code of\nConduct](https://opensource.microsoft.com/codeofconduct/). For more information see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact\n[opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Publications\n1. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He. (2019) ZeRO: memory optimizations toward training trillion parameter models. [arXiv:1910.02054](https://arxiv.org/abs/1910.02054) and [In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC ''20)](https://dl.acm.org/doi/10.5555/3433701.3433727).\n2. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. (2020) DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. [In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD ''20, Tutorial)](https://dl.acm.org/doi/10.1145/3394486.3406703).\n3. Minjia Zhang, Yuxiong He. (2020) Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping. [arXiv:2010.13369](https://arxiv.org/abs/2010.13369) and [NeurIPS 2020](https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html).\n4. Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He. (2021) ZeRO-Offload: Democratizing Billion-Scale Model Training. [arXiv:2101.06840](https://arxiv.org/abs/2101.06840) and [USENIX ATC 2021](https://www.usenix.org/conference/atc21/presentation/ren-jie). [[paper]](https://arxiv.org/abs/2101.06840) [[slides]](https://www.usenix.org/system/files/atc21_slides_ren-jie.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)\n5. Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam''s Convergence Speed. [arXiv:2102.02888](https://arxiv.org/abs/2102.02888) and [ICML 2021](http://proceedings.mlr.press/v139/tang21a.html).\n6. Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He. (2021) ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. [arXiv:2104.07857](https://arxiv.org/abs/2104.07857) and [SC 2021](https://dl.acm.org/doi/abs/10.1145/3458817.3476205). [[paper]](https://arxiv.org/abs/2104.07857) [[slides]](docs/assets/files/SC21-ZeRO-Infinity.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/)\n7. Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, Yuxiong He. (2021) 1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB''s Convergence Speed. [arXiv:2104.06069](https://arxiv.org/abs/2104.06069) and [HiPC 2022](https://hipc.org/advance-program/).\n8. Conglong Li, Minjia Zhang, Yuxiong He. (2021) The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models. [arXiv:2108.06084](https://arxiv.org/abs/2108.06084) and [NeurIPS 2022](https://openreview.net/forum?id=JpZ5du_Kdh).\n9. Yucheng Lu, Conglong Li, Minjia Zhang, Christopher De Sa, Yuxiong He. (2022) Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam. [arXiv:2202.06009](https://arxiv.org/abs/2202.06009).\n10. Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He. (2022) DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale [arXiv:2201.05596](https://arxiv.org/abs/2201.05596) and [ICML 2022](https://proceedings.mlr.press/v162/rajbhandari22a.html). [[pdf]](https://arxiv.org/abs/2201.05596) [[slides]](docs/assets/files/ICML-5mins.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/)\n11. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro. (2022) Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model [arXiv:2201.11990](https://arxiv.org/abs/2201.11990).\n12. Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, Yuxiong He. (2022) Extreme Compression for Pre-trained Transformers Made Simple and Efficient. [arXiv:2206.01859](https://arxiv.org/abs/2206.01859) and [NeurIPS 2022](https://openreview.net/forum?id=xNeAhc2CNAl).\n13. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He. (2022) ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. [arXiv:2206.01861](https://arxiv.org/abs/2206.01861) and [NeurIPS 2022](https://openreview.net/forum?id=f-fVCElZ-G1) [[slides]](docs/assets/files/zeroquant_series.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-compression-a-composable-library-for-extreme-compression-and-zero-cost-quantization/)\n14. Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, Yuxiong He. (2022) DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. [arXiv:2207.00032](https://arxiv.org/abs/2207.00032) and [SC 2022](https://dl.acm.org/doi/abs/10.5555/3571885.3571946). [[paper]](https://arxiv.org/abs/2207.00032) [[slides]](docs/assets/files/sc22-ds-inference.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/)\n15. Zhewei Yao, Xiaoxia Wu, Conglong Li, Connor Holmes, Minjia Zhang, Cheng Li, Yuxiong He. (2022) Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers. [arXiv:2211.11586](https://arxiv.org/abs/2211.11586).\n16. Conglong Li, Zhewei Yao, Xiaoxia Wu, Minjia Zhang, Yuxiong He. (2022) DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. [arXiv:2212.03597](https://arxiv.org/abs/2212.03597) [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/)\n17. Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, Yuxiong He. (2023) Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. [arXiv:2301.12017](https://arxiv.org/abs/2301.12017) and [ICML2023](https://icml.cc/Conferences/2023).\n18. Syed Zawad, Cheng Li, Zhewei Yao, Elton Zheng, Yuxiong He, Feng Yan. (2023) DySR: Adaptive Super-Resolution via Algorithm and System Co-design. [ICLR:2023](https://openreview.net/forum?id=Pgtn4l6eKjv).\n19. Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, Yuxiong He. (2023) Scaling Vision-Language Models with Sparse Mixture of Experts. [arXiv:2303.07226](https://arxiv.org/abs/2303.07226) and [Finding at EMNLP2023](https://2023.emnlp.org/).\n20. Quentin Anthony, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He, Aamir Shafi, Mustafa Abduljabbar, Hari Subramoni, Dhabaleswar Panda. (2023) MCR-DL: Mix-and-Match Communication Runtime for Deep Learning [arXiv:2303.08374](https://arxiv.org/abs/2303.08374) and will appear at IPDPS 2023.\n21. Siddharth Singh, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He, Abhinav Bhatele. (2023) A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training [arXiv:2303.06318](https://arxiv.org/abs/2303.06318) and [ICS 2023](https://dl.acm.org/doi/10.1145/3577193.3593704).\n22. Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Xiaoxia Wu, Connor Holmes, Zhewei Yao, Samyam Rajbhandari, Olatunji Ruwase, Feng Yan, Lei Yang, Yuxiong He. (2023) ZeRO++: Extremely Efficient Collective Communication for Giant Model Training [arXiv:2306.10209](https://arxiv.org/abs/2306.10209) and [ML for Sys Workshop at NeurIPS2023](http://mlforsystems.org/) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/)\n23. Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, Yuxiong He. (2023) ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation [arXiv:2303.08302](https://arxiv.org/abs/2303.08302) and [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/) [[slides]](docs/assets/files/zeroquant_series.pdf)\n24. Pareesa Ameneh Golnari, Zhewei Yao, Yuxiong He. (2023) Selective Guidance: Are All the Denoising Steps of Guided Diffusion Important? [arXiv:2305.09847](https://arxiv.org/abs/2305.09847)\n25. Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, Yuxiong He. (2023) DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales [arXiv:2308.01320](https://arxiv.org/abs/2308.01320).\n26. Xiaoxia Wu, Zhewei Yao, Yuxiong He. (2023) ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats [arXiv:2307.09782](https://arxiv.org/abs/2307.09782) and [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/) [[slides]](docs/assets/files/zeroquant_series.pdf)\n27. Zhewei Yao, Xiaoxia Wu, Conglong Li, Minjia Zhang, Heyang Qin, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He. (2023) DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention [arXiv:2309.14327](https://arxiv.org/pdf/2309.14327.pdf)\n28. Shuaiwen Leon Song, Bonnie Kruft, Minjia Zhang, Conglong Li, Shiyang Chen, Chengming Zhang, Masahiro Tanaka, Xiaoxia Wu, Jeff Rasley, Ammar Ahmad Awan, Connor Holmes, Martin Cai, Adam Ghanem, Zhongzhu Zhou, Yuxiong He, et al. (2023) DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies [arXiv:2310.04610](https://arxiv.org/abs/2310.04610) [[blog]](https://www.microsoft.com/en-us/research/blog/announcing-the-deepspeed4science-initiative-enabling-large-scale-scientific-discovery-through-sophisticated-ai-system-technologies/)\n29. Zhewei Yao, Reza Yazdani Aminabadi, Stephen Youn, Xiaoxia Wu, Elton Zheng, Yuxiong He. (2023) ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers [arXiv:2310.17723](https://arxiv.org/abs/2310.17723)\n\n30. Xiaoxia Wu, Haojun Xia, Stephen Youn, Zhen Zheng, Shiyang Chen, Arash Bakhtiari, Michael Wyatt, Reza Yazdani Aminabadi, Yuxiong He, Olatunji Ruwase, Leon Song, Zhewei Yao (2023) ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks [arXiv:2312.08583](https://arxiv.org/abs/2312.08583)\n\n31. Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon Song. (2024) FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design  [arXiv:2401.14112](https://arxiv.org/abs/2401.14112)\n32. Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Reza Yazdani Aminadabi, Shuaiwen Leon Song, Samyam Rajbhandari, Yuxiong He. (2024) [System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models](https://dl.acm.org/doi/10.1145/3662158.3662806)\n33. Xinyu Lian, Sam Ade Jacobs, Lev Kurilenko, Masahiro Tanaka, Stas Bekman, Olatunji Ruwase, Minjia Zhang. (2024) Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training [arXiv:2406.18820](https://arxiv.org/abs/2406.18820)\n34. Stas Bekman, Samyam Rajbhandari, Michael Wyatt, Jeff Rasley, Tunji Ruwase, Zhewei Yao, Aurick Qiao, Yuxiong He. (2025) Arctic Long Sequence Training: Scalable And Efficient Training For Multi-Million Token Sequences [arXiv:2506.13996](https://arxiv.org/abs/2506.13996)\n35. Tingfeng Lan, Yusen Wu, Bin Ma, Zhaoyuan Su, Rui Yang, Tekin Bicer, Masahiro Tanaka, Olatunji Ruwase, Dong Li, Yue Cheng. (2025) ZenFlow: Enabling Stall-Free Offloading Training via Asynchronous Updates [arXiv:2505.12242](https://arxiv.org/abs/2505.12242)\n36. Xinyu Lian, Masahiro Tanaka, Olatunji Ruwase, Minjia Zhang. (2026) SuperOffload: Unleashing the Power of Large-Scale LLM Training on Superchips [arxiv](https://arxiv.org/abs/2509.21271), [ASPLOS 2026](https://www.asplos-conference.org/asplos2026)\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)\n4. [Large Model Training and Inference with DeepSpeed // Samyam Rajbhandari // LLMs in Prod Conference](https://www.youtube.com/watch?v=cntxC3g22oU) [[slides]](docs/assets/files/presentation-mlops.pdf)\n5. Community Tutorials\n    * [DeepSpeed: All the tricks to scale to gigantic models (Mark Saroufim)](https://www.youtube.com/watch?v=pDGI668pNg0)\n    * [Turing-NLG, DeepSpeed and the ZeRO optimizer (Yannic Kilcher)](https://www.youtube.com/watch?v=tC01FRB0M7w)\n    * [Ultimate Guide To Scaling ML Models (The AI Epiphany)](https://www.youtube.com/watch?v=hc0u4avAkuM)\n', '{"language":"Python","stars":40937,"forks":4652,"watchers":40937,"open_issues":1247,"topics":["billion-parameters","compression","data-parallelism","deep-learning","gpu","inference","machine-learning","mixture-of-experts","model-parallelism","pipeline-parallelism","pytorch","trillion-parameters","zero"],"default_branch":"master","size_kb":243722,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:THUDM:GLM-130B","source_url":"https://github.com/THUDM/GLM-130B"},{"type":"has_code","target_id":"github:yandex:YaLM-100B","source_url":"https://github.com/yandex/YaLM-100B"},{"type":"has_code","target_id":"github:EleutherAI:gpt-neox","source_url":"https://github.com/EleutherAI/gpt-neox"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:Ascend:Ascend-CI","source_url":"https://github.com/Ascend/Ascend-CI"},{"type":"has_code","target_id":"github:Ascend:Ascend-CI","source_url":"https://github.com/Ascend/Ascend-CI"},{"type":"has_code","target_id":"github:ROCm-Developer-Tools:HIPCC","source_url":"https://github.com/ROCm-Developer-Tools/HIPCC"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"}]', NULL, 'Apache-2.0', 'approved', 80, '5f4f07e99e5164d833b03b8c97512c2c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-deepspeedai-DeepSpeed from https://github.com/deepspeedai.png
Image converted to WebP: data/images/github-deepspeedai-DeepSpeed.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-gradio-app-gradio', 'github--gradio-app--gradio', 'gradio', 'gradio-app', '<!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE OR TEMPLATES AND THEN RUN SCRIPT. --> <div align="center"> <a href="https://gradio.app"> <img src="readme_files/gradio.svg" alt="gradio" width=350> </a> </div> <div align="center"> <span> <a href="https://www.producthunt.com/posts/gradio-5-0?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-gradio&#0045;5&#0045;0" target="_blank"><img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=501906&th...', '["data-analysis","data-science","data-visualization","deep-learning","deploy","gradio","gradio-interface","interface","machine-learning","models","python","python-notebook","ui","ui-components","python"]', 'other', 40851, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/gradio-app/gradio","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR `guides/01_getting-started/01_quickstart.md` TEMPLATES AND THEN RUN `render_readme.py` SCRIPT. -->\n\n<div align="center">\n<a href="https://gradio.app">\n<img src="readme_files/gradio.svg" alt="gradio" width=350>\n</a>\n</div>\n\n<div align="center">\n<span>\n<a href="https://www.producthunt.com/posts/gradio-5-0?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-gradio&#0045;5&#0045;0" target="_blank"><img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=501906&theme=light" alt="Gradio&#0032;5&#0046;0 - the&#0032;easiest&#0032;way&#0032;to&#0032;build&#0032;AI&#0032;web&#0032;apps | Product Hunt" style="width: 150px; height: 54px;" width="150" height="54" /></a>\n<a href="https://trendshift.io/repositories/2145" target="_blank"><img src="https://trendshift.io/api/badge/repositories/2145" alt="gradio-app%2Fgradio | Trendshift" style="width: 150px; height: 55px;" width="150" height="55"/></a>\n</span>\n\n[![gradio-backend](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml)\n[![gradio-ui](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml) \n[![PyPI](https://img.shields.io/pypi/v/gradio)](https://pypi.org/project/gradio/)\n[![PyPI downloads](https://img.shields.io/pypi/dm/gradio)](https://pypi.org/project/gradio/)\n![Python version](https://img.shields.io/badge/python-3.10+-important)\n[![Twitter follow](https://img.shields.io/twitter/follow/gradio?style=social&label=follow)](https://twitter.com/gradio)\n\n[Website](https://gradio.app)\n| [Documentation](https://gradio.app/docs/)\n| [Guides](https://gradio.app/guides/)\n| [Getting Started](https://gradio.app/getting_started/)\n| [Examples](demo/)\n\n</div>\n\n<div align="center">\n\nEnglish | [‰∏≠Êñá](readme_files/zh-cn#readme)\n\n</div>\n\n# Gradio: Build Machine Learning Web Apps ‚Äî in Python\n\n\n\nGradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio''s built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*\n\n<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/gif-version.gif" style="padding-bottom: 10px">\n\nIt just takes a few lines of Python to create your own demo, so let''s get started üí´\n\n\n### Installation\n\n**Prerequisite**: Gradio requires [Python 3.10 or higher](https://www.python.org/downloads/).\n\n\nWe recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:\n\n```bash\npip install --upgrade gradio\n```\n\n\n> [!TIP]\n > It is best to install Gradio in a virtual environment. Detailed installation instructions for all common operating systems <a href="https://www.gradio.app/main/guides/installing-gradio-in-a-virtual-environment">are provided here</a>. \n\n### Building Your First Demo\n\nYou can run Gradio in your favorite code editor, Jupyter notebook, Google Colab, or anywhere else you write Python. Let''s write your first Gradio app:\n\n\n```python\nimport gradio as gr\n\ndef greet(name, intensity):\n    return "Hello, " + name + "!" * int(intensity)\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=["text", "slider"],\n    outputs=["text"],\n)\n\ndemo.launch()\n```\n\n\n\n> [!TIP]\n > We shorten the imported name from <code>gradio</code> to <code>gr</code>. This is a widely adopted convention for better readability of code. \n\nNow, run your code. If you''ve written the Python code in a file named `app.py`, then you would run `python app.py` from the terminal.\n\nThe demo below will open in a browser on [http://localhost:7860](http://localhost:7860) if running from a file. If you are running within a notebook, the demo will appear embedded within the notebook.\n\n![`hello_world_4` demo](demo/hello_world_4/screenshot.gif)\n\nType your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.\n\n> [!TIP]\n > When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. You can also enable <strong>vibe mode</strong> by using the <code>--vibe</code> flag, e.g. <code>gradio --vibe app.py</code>, which provides an in-browser chat that can be used to write or edit your Gradio app using natural language. Learn more in the <a href="https://www.gradio.app/guides/developing-faster-with-reload-mode">Hot Reloading Guide</a>.\n\n\n**Understanding the `Interface` Class**\n\nYou''ll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. \n\nThe `Interface` class has three core arguments:\n\n- `fn`: the function to wrap a user interface (UI) around\n- `inputs`: the Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n- `outputs`: the Gradio component(s) to use for the output. The number of components should match the number of return values from your function.\n\nThe `fn` argument is very flexible -- you can pass *any* Python function that you want to wrap with a UI. In the example above, we saw a relatively simple function, but the function could be anything from a music generator to a tax calculator to the prediction function of a pretrained machine learning model.\n\nThe `inputs` and `outputs` arguments take one or more Gradio components. As we''ll see, Gradio includes more than [30 built-in components](https://www.gradio.app/docs/gradio/introduction) (such as the `gr.Textbox()`, `gr.Image()`, and `gr.HTML()` components) that are designed for machine learning applications. \n\n> [!TIP]\n > For the `inputs` and `outputs` arguments, you can pass in the name of these components as a string (`"textbox"`) or an instance of the class (`gr.Textbox()`).\n\nIf your function accepts more than one argument, as is the case above, pass a list of input components to `inputs`, with each input component corresponding to one of the arguments of the function, in order. The same holds true if your function returns more than one value: simply pass in a list of components to `outputs`. This flexibility makes the `Interface` class a very powerful way to create demos.\n\nWe''ll dive deeper into the `gr.Interface` on our series on [building Interfaces](https://www.gradio.app/main/guides/the-interface-class).\n\n### Sharing Your Demo\n\nWhat good is a beautiful demo if you can''t share it? Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set `share=True` in `launch()`, and a publicly accessible URL will be created for your demo. Let''s revisit our example demo,  but change the last line as follows:\n\n```python\nimport gradio as gr\n\ndef greet(name):\n    return "Hello " + name + "!"\n\ndemo = gr.Interface(fn=greet, inputs="textbox", outputs="textbox")\n    \ndemo.launch(share=True)  # Share your demo with just 1 extra parameter üöÄ\n```\n\nWhen you run this code, a public URL will be generated for your demo in a matter of seconds, something like:\n\nüëâ &nbsp; `https://a23dsf231adb.gradio.live`\n\nNow, anyone around the world can try your Gradio demo from their browser, while the machine learning model and all computation continues to run locally on your computer.\n\nTo learn more about sharing your demo, read our dedicated guide on [sharing your Gradio application](https://www.gradio.app/guides/sharing-your-app).\n\n\n### An Overview of Gradio\n\nSo far, we''ve been discussing the `Interface` class, which is a high-level class that lets you build demos quickly with Gradio. But what else does Gradio include?\n\n#### Custom Demos with `gr.Blocks`\n\nGradio offers a low-level approach for designing web apps with more customizable layouts and data flows with the `gr.Blocks` class. Blocks supports things like controlling where components appear on the page, handling multiple data flows and more complex interactions (e.g. outputs can serve as inputs to other functions), and updating properties/visibility of components based on user interaction ‚Äî still all in Python. \n\nYou can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n#### Chatbots with `gr.ChatInterface`\n\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you''re interested in creating a chatbot, you can jump straight to [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast).\n\n#### The Gradio Python & JavaScript Ecosystem\n\nThat''s the gist of the core `gradio` Python library, but Gradio is actually so much more! It''s an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\n* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio_client`): query any Gradio app programmatically in Python.\n* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript.\n* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications ‚Äî for free!\n\n### What''s Next?\n\nKeep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [let''s dive deeper into the Interface class](https://www.gradio.app/guides/the-interface-class).\n\nOr, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/).\n\n\n### Gradio Sketch\n\nYou can also build Gradio applications without writing any code. Simply type `gradio sketch` into your terminal to open up an editor that lets you define and modify Gradio components, adjust their layouts, add events, all through a web editor. Or [use this hosted version of Gradio Sketch, running on Hugging Face Spaces](https://huggingface.co/spaces/aliabid94/Sketch).\n\n## Questions?\n\nIf you''d like to report a bug or have a feature request, please create an [issue on GitHub](https://github.com/gradio-app/gradio/issues/new/choose). For general questions about usage, we are available on [our Discord server](https://discord.com/invite/feTf9x3ZSB) and happy to help.\n\nIf you like Gradio, please leave us a ‚≠ê on GitHub!\n\n## Open Source Stack\n\nGradio is built on top of many wonderful open-source libraries!\n\n[<img src="readme_files/huggingface_mini.svg" alt="huggingface" height=40>](https://huggingface.co)\n[<img src="readme_files/python.svg" alt="python" height=40>](https://www.python.org)\n[<img src="readme_files/fastapi.svg" alt="fastapi" height=40>](https://fastapi.tiangolo.com)\n[<img src="readme_files/encode.svg" alt="encode" height=40>](https://www.encode.io)\n[<img src="readme_files/svelte.svg" alt="svelte" height=40>](https://svelte.dev)\n[<img src="readme_files/vite.svg" alt="vite" height=40>](https://vitejs.dev)\n[<img src="readme_files/pnpm.svg" alt="pnpm" height=40>](https://pnpm.io)\n[<img src="readme_files/tailwind.svg" alt="tailwind" height=40>](https://tailwindcss.com)\n[<img src="readme_files/storybook.svg" alt="storybook" height=40>](https://storybook.js.org/)\n[<img src="readme_files/chromatic.svg" alt="chromatic" height=40>](https://www.chromatic.com/)\n\n## License\n\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository.\n\n## Citation\n\nAlso check out the paper _[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019_, and please cite it if you use Gradio in your work.\n\n```\n@article{abid2019gradio,\n  title = {Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild},\n  author = {Abid, Abubakar and Abdalla, Ali and Abid, Ali and Khan, Dawood and Alfozan, Abdulrahman and Zou, James},\n  journal = {arXiv preprint arXiv:1906.02569},\n  year = {2019},\n}\n```\n', '{"language":"Python","stars":40851,"forks":3177,"watchers":40851,"open_issues":434,"topics":["data-analysis","data-science","data-visualization","deep-learning","deploy","gradio","gradio-interface","interface","machine-learning","models","python","python-notebook","ui","ui-components"],"default_branch":"main","size_kb":312578,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"},{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"},{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"},{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"}]', NULL, 'Apache-2.0', 'approved', 80, 'e002e1b5a98a4ec77bf4ee340f62aa9c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-gradio-app-gradio from https://github.com/gradio-app.png
Image converted to WebP: data/images/github-gradio-app-gradio.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-ray-project-ray', 'github--ray-project--ray', 'ray', 'ray-project', '.. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/ray_header_logo.png .. image:: https://readthedocs.org/projects/ray/badge/?version=master :target: http://docs.ray.io/en/master/?badge=master .. image:: https://img.shields.io/badge/Ray-Join%20Slack-blue :target: https://www.ray.io/join-slack .. image:: https://img.shields.io/badge/Discuss-Ask%20Questions-blue :target: https://discuss.ray.io/ .. image:: https://img.shields.io/twitter/follow/raydistributed.svg?style=soc...', '["data-science","deep-learning","deployment","distributed","hyperparameter-optimization","hyperparameter-search","large-language-models","llm","llm-inference","llm-serving","machine-learning","optimization","parallel","python","pytorch","ray","reinforcement-learning","rllib","serving","tensorflow","python"]', 'other', 40208, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/ray-project/ray","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '.. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/ray_header_logo.png\n\n.. image:: https://readthedocs.org/projects/ray/badge/?version=master\n    :target: http://docs.ray.io/en/master/?badge=master\n\n.. image:: https://img.shields.io/badge/Ray-Join%20Slack-blue\n    :target: https://www.ray.io/join-slack\n\n.. image:: https://img.shields.io/badge/Discuss-Ask%20Questions-blue\n    :target: https://discuss.ray.io/\n\n.. image:: https://img.shields.io/twitter/follow/raydistributed.svg?style=social&logo=twitter\n    :target: https://x.com/raydistributed\n\n.. image:: https://img.shields.io/badge/Get_started_for_free-3C8AE9?logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8%2F9hAAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAEKADAAQAAAABAAAAEAAAAAA0VXHyAAABKElEQVQ4Ea2TvWoCQRRGnWCVWChIIlikC9hpJdikSbGgaONbpAoY8gKBdAGfwkfwKQypLQ1sEGyMYhN1Pd%2B6A8PqwBZeOHt%2FvsvMnd3ZXBRFPQjBZ9K6OY8ZxF%2B0IYw9PW3qz8aY6lk92bZ%2BVqSI3oC9T7%2FyCVnrF1ngj93us%2B540sf5BrCDfw9b6jJ5lx%2FyjtGKBBXc3cnqx0INN4ImbI%2Bl%2BPnI8zWfFEr4chLLrWHCp9OO9j19Kbc91HX0zzzBO8EbLK2Iv4ZvNO3is3h6jb%2BCwO0iL8AaWqB7ILPTxq3kDypqvBuYuwswqo6wgYJbT8XxBPZ8KS1TepkFdC79TAHHce%2F7LbVioi3wEfTpmeKtPRGEeoldSP%2FOeoEftpP4BRbgXrYZefsAI%2BP9JU7ImyEAAAAASUVORK5CYII%3D\n   :target: https://www.anyscale.com/ray-on-anyscale?utm_source=github&utm_medium=ray_readme&utm_campaign=get_started_badge\n\nRay is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a set of AI libraries for simplifying ML compute:\n\n.. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/what-is-ray-padded.svg\n\n..\n  https://docs.google.com/drawings/d/1Pl8aCYOsZCo61cmp57c7Sja6HhIygGCvSZLi_AuBuqo/edit\n\nLearn more about `Ray AI Libraries`_:\n\n- `Data`_: Scalable Datasets for ML\n- `Train`_: Distributed Training\n- `Tune`_: Scalable Hyperparameter Tuning\n- `RLlib`_: Scalable Reinforcement Learning\n- `Serve`_: Scalable and Programmable Serving\n\nOr more about `Ray Core`_ and its key abstractions:\n\n- `Tasks`_: Stateless functions executed in the cluster.\n- `Actors`_: Stateful worker processes created in the cluster.\n- `Objects`_: Immutable values accessible across the cluster.\n\nLearn more about Monitoring and Debugging:\n\n- Monitor Ray apps and clusters with the `Ray Dashboard <https://docs.ray.io/en/latest/ray-core/ray-dashboard.html>`__.\n- Debug Ray apps with the `Ray Distributed Debugger <https://docs.ray.io/en/latest/ray-observability/ray-distributed-debugger.html>`__.\n\nRay runs on any machine, cluster, cloud provider, and Kubernetes, and features a growing\n`ecosystem of community integrations`_.\n\nInstall Ray with: ``pip install ray``. For nightly wheels, see the\n`Installation page <https://docs.ray.io/en/latest/ray-overview/installation.html>`__.\n\n.. _`Serve`: https://docs.ray.io/en/latest/serve/index.html\n.. _`Data`: https://docs.ray.io/en/latest/data/dataset.html\n.. _`Workflow`: https://docs.ray.io/en/latest/workflows/\n.. _`Train`: https://docs.ray.io/en/latest/train/train.html\n.. _`Tune`: https://docs.ray.io/en/latest/tune/index.html\n.. _`RLlib`: https://docs.ray.io/en/latest/rllib/index.html\n.. _`ecosystem of community integrations`: https://docs.ray.io/en/latest/ray-overview/ray-libraries.html\n\n\nWhy Ray?\n--------\n\nToday''s ML workloads are increasingly compute-intensive. As convenient as they are, single-node development environments such as your laptop cannot scale to meet these demands.\n\nRay is a unified way to scale Python and AI applications from a laptop to a cluster.\n\nWith Ray, you can seamlessly scale the same code from a laptop to a cluster. Ray is designed to be general-purpose, meaning that it can performantly run any kind of workload. If your application is written in Python, you can scale it with Ray, no other infrastructure required.\n\nMore Information\n----------------\n\n- `Documentation`_\n- `Ray Architecture whitepaper`_\n- `Exoshuffle: large-scale data shuffle in Ray`_\n- `Ownership: a distributed futures system for fine-grained tasks`_\n- `RLlib paper`_\n- `Tune paper`_\n\n*Older documents:*\n\n- `Ray paper`_\n- `Ray HotOS paper`_\n- `Ray Architecture v1 whitepaper`_\n\n.. _`Ray AI Libraries`: https://docs.ray.io/en/latest/ray-air/getting-started.html\n.. _`Ray Core`: https://docs.ray.io/en/latest/ray-core/walkthrough.html\n.. _`Tasks`: https://docs.ray.io/en/latest/ray-core/tasks.html\n.. _`Actors`: https://docs.ray.io/en/latest/ray-core/actors.html\n.. _`Objects`: https://docs.ray.io/en/latest/ray-core/objects.html\n.. _`Documentation`: http://docs.ray.io/en/latest/index.html\n.. _`Ray Architecture v1 whitepaper`: https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview\n.. _`Ray Architecture whitepaper`: https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview\n.. _`Exoshuffle: large-scale data shuffle in Ray`: https://arxiv.org/abs/2203.05072\n.. _`Ownership: a distributed futures system for fine-grained tasks`: https://www.usenix.org/system/files/nsdi21-wang.pdf\n.. _`Ray paper`: https://arxiv.org/abs/1712.05889\n.. _`Ray HotOS paper`: https://arxiv.org/abs/1703.03924\n.. _`RLlib paper`: https://arxiv.org/abs/1712.09381\n.. _`Tune paper`: https://arxiv.org/abs/1807.05118\n\nGetting Involved\n----------------\n\n.. list-table::\n   :widths: 25 50 25 25\n   :header-rows: 1\n\n   * - Platform\n     - Purpose\n     - Estimated Response Time\n     - Support Level\n   * - `Discourse Forum`_\n     - For discussions about development and questions about usage.\n     - < 1 day\n     - Community\n   * - `GitHub Issues`_\n     - For reporting bugs and filing feature requests.\n     - < 2 days\n     - Ray OSS Team\n   * - `Slack`_\n     - For collaborating with other Ray users.\n     - < 2 days\n     - Community\n   * - `StackOverflow`_\n     - For asking questions about how to use Ray.\n     - 3-5 days\n     - Community\n   * - `Meetup Group`_\n     - For learning about Ray projects and best practices.\n     - Monthly\n     - Ray DevRel\n   * - `Twitter`_\n     - For staying up-to-date on new features.\n     - Daily\n     - Ray DevRel\n\n.. _`Discourse Forum`: https://discuss.ray.io/\n.. _`GitHub Issues`: https://github.com/ray-project/ray/issues\n.. _`StackOverflow`: https://stackoverflow.com/questions/tagged/ray\n.. _`Meetup Group`: https://www.meetup.com/Bay-Area-Ray-Meetup/\n.. _`Twitter`: https://x.com/raydistributed\n.. _`Slack`: https://www.ray.io/join-slack?utm_source=github&utm_medium=ray_readme&utm_campaign=getting_involved\n', '{"language":"Python","stars":40208,"forks":6980,"watchers":40208,"open_issues":3218,"topics":["data-science","deep-learning","deployment","distributed","hyperparameter-optimization","hyperparameter-search","large-language-models","llm","llm-inference","llm-serving","machine-learning","optimization","parallel","python","pytorch","ray","reinforcement-learning","rllib","serving","tensorflow"],"default_branch":"master","size_kb":643105,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:ray-project:ray","source_url":"https://github.com/ray-project/ray"},{"type":"has_code","target_id":"github:ray-project:ray","source_url":"https://github.com/ray-project/ray"},{"type":"has_code","target_id":"github:ray-project:ray","source_url":"https://github.com/ray-project/ray"}]', NULL, 'Apache-2.0', 'approved', 65, '4e810944c5ced749b319403d13f5b309', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-ray-project-ray from https://github.com/ray-project.png
Image converted to WebP: data/images/github-ray-project-ray.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-photoprism-photoprism', 'github--photoprism--photoprism', 'photoprism', 'photoprism', 'PhotoPrism: Browse Your Life in Pictures ======================================== PhotoPrism¬Æ is an AI-Powered Photos App for the Decentralized Web. It makes use of the latest technologies to tag and find pictures automatically without getting in your way. You can run it at home, on a private server, or in the cloud. To get a first impression, you are welcome to play with our public demo. Please be careful not to upload any private, unlawful or offensive pictures. **Our mission is to provide ...', '["ai","golang","google-photos","machine-learning","photography","private-cloud","self-hosted","tensorflow","go"]', 'other', 38891, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/photoprism/photoprism","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', 'PhotoPrism: Browse Your Life in Pictures\n========================================\n\n[![License: AGPL](https://img.shields.io/badge/license-AGPL%203.0-454377.svg)](https://docs.photoprism.app/license/agpl/)\n[![Documentation](https://img.shields.io/badge/read-the%20docs-4d6a91.svg)](https://docs.photoprism.app/)\n[![Community Chat](https://img.shields.io/badge/chat-on%20gitter-4d6a91.svg)](https://link.photoprism.app/chat)\n[![GitHub Discussions](https://img.shields.io/badge/ask-%20on%20github-4d6a91.svg)](https://link.photoprism.app/discussions)\n[![Bluesky Social](https://dl.photoprism.app/img/badges/badge-bluesky.svg)](https://bsky.app/profile/photoprism.app)\n[![Mastodon](https://dl.photoprism.app/img/badges/badge-floss-social.svg)](https://floss.social/@photoprism)\n\nPhotoPrism¬Æ is an AI-Powered Photos App for the [Decentralized Web](https://en.wikipedia.org/wiki/Decentralized_web).\nIt makes use of the latest technologies to tag and find pictures automatically without getting in your way.\nYou can run it at home, on a private server, or in the cloud.\n\n![](https://dl.photoprism.app/img/ui/2025/desktop-search.jpg)\n\nTo get a first impression, you are welcome to play with our [public demo](https://try.photoprism.app/). Please be careful not to upload any private, unlawful or offensive pictures.\n\n## Feature Overview ##\n\n**Our mission is to provide the most user- and privacy-friendly solution to keep your pictures organized and accessible.** That''s why PhotoPrism was built from the ground up to run wherever you need it, without compromising freedom, privacy, or functionality:\n\n<img align="right" height="270" src="https://dl.photoprism.app/img/ui/2025/iphone-crocus-540px.png">\n\n* Browse [all your pictures](https://docs.photoprism.app/user-guide/organize/browse/) without worrying about [RAW images](https://www.photoprism.app/kb/file-formats) or [video formats](https://docs.photoprism.app/user-guide/organize/video/)\n* Whether you''re using a phone, tablet, or desktop computer, our [intuitive PWA](https://try.photoprism.app/) provides a native app-like experience and can be [easily installed](https://docs.photoprism.app/user-guide/pwa/) on your home screen\n* Quickly find specific photos and videos with [powerful search filters](https://docs.photoprism.app/user-guide/search/filters/) that can be combined and are available for [many different properties](https://docs.photoprism.app/user-guide/search/filters/#filter-reference), including [labels](https://try.photoprism.app/library/labels), [location](https://try.photoprism.app/library/places?q=s2:47a85a63f764), [resolution](https://try.photoprism.app/library/browse?view=cards&q=mp:4), [color](https://try.photoprism.app/library/browse?view=cards&q=color:red), [chroma](https://try.photoprism.app/library/browse?view=cards&q=mono%3Atrue), and [quality](https://try.photoprism.app/library/review)\n* [Automatically labels your pictures](https://try.photoprism.app/library/labels) based on content and location, and recognizes the faces of [your family and friends](https://try.photoprism.app/library/people/new)\n* [Live Photos](https://try.photoprism.app/library/live) start playing when you [hover over them](https://try.photoprism.app/library/browse?view=cards&q=type%3Alive) and when viewing a slideshow\n* Six high-resolution [World Maps](https://try.photoprism.app/library/places) and our [privacy-preserving geocoding service](https://docs.photoprism.app/getting-started/#maps-places) help bring back memories of your favorite trips and let you explore the world\n* Metadata can be extracted and merged from Exif, XMP, and other sources like Google Photos\n* [Use compatible apps](https://docs.photoprism.app/user-guide/native-apps/) like [PhotoSync](https://link.photoprism.app/photosync) to back up iOS and Android phones in the background\n* WebDAV clients such as [Microsoft''s Windows Explorer](https://docs.photoprism.app/user-guide/sync/webdav/#__tabbed_1_2) and [Apple''s Finder](https://docs.photoprism.app/user-guide/sync/webdav/#connect-to-a-webdav-server) can [connect directly to PhotoPrism](https://docs.photoprism.app/user-guide/sync/webdav/), allowing you to open, edit, and delete files from your computer as if they were local\n\nBeing completely [**self-funded and independent**](https://link.photoprism.app/membership), we can promise you that we will [never sell your data](https://www.photoprism.app/privacy) and that we will [always be transparent](https://www.photoprism.app/terms) about our software and services. Your data will never be shared with Google, Amazon, Microsoft or Apple unless you intentionally upload files to one of their services. üîí\n\n## Getting Started ##\n\nStep-by-step [installation instructions](https://docs.photoprism.app/getting-started/) for our self-hosted [community edition](https://link.photoprism.app/personal-editions) can be found on [docs.photoprism.app](https://docs.photoprism.app/getting-started/) - all you need is a Web browser and [Docker](https://docs.docker.com/get-docker/) to run the server. It is available for Mac, Linux, and Windows.\n\nThe [stable releases](https://docs.photoprism.app/release-notes/) and [development preview](https://docs.photoprism.app/getting-started/updates/#development-preview) are available as a [multi-arch image](https://link.photoprism.app/docker-hub) for 64-bit AMD, Intel, and ARM processors.\nThat means, [Raspberry Pi](https://docs.photoprism.app/getting-started/raspberry-pi/) and Apple Silicon users enjoy the exact same functionality and can follow the same [installation steps](https://docs.photoprism.app/getting-started/docker-compose/).\n\nSee our [Getting Started FAQ](https://docs.photoprism.app/getting-started/faq/#how-can-i-install-photoprism-without-docker) for alternative installation methods, for example using the [*tar.gz* packages](https://dl.photoprism.app/pkg/linux/README.html) we provide.\n\n## Support Our Mission üíé ##\n\n**PhotoPrism is 100% self-funded and independent.** Your [continued support](https://link.photoprism.app/membership) helps us [provide more features to the public](https://www.photoprism.app/oss/faq#what-functionality-is-generally-available), release [regular updates](https://docs.photoprism.app/release-notes/), and remain independent!\n\nOur members [enjoy additional features](https://www.photoprism.app/kb/personal), including access to [interactive world maps](https://try.photoprism.app/library/places), and can join our private chat room to [connect with our team](https://www.photoprism.app/about/team). We currently have the following membership options:\n\n- You can [sign up directly on our website](https://link.photoprism.app/membership) and pay with credit card or SEPA through Stripe, so you don''t need to [link an external account](https://www.photoprism.app/kb/activation) and can easily upgrade or downgrade at any time\n- Alternatively, [Patreon](https://link.photoprism.app/patreon) also supports PayPal, additional currencies, and lets you choose between monthly and annual billing for all tiers\n\nIf you currently support us through [GitHub Sponsors](https://link.photoprism.app/sponsor), you can also [register on our website](https://my.photoprism.app/register) and use the *Activate GitHub Sponsors Membership* button to link your account. For details on this and how to [link your Patreon account](https://www.patreon.com/pledges), see our [Activation Guide](https://www.photoprism.app/kb/activation).\n\nYou are [welcome to contact us](https://www.photoprism.app/contact) for change requests, membership questions, and business partnerships.\n\n[View Membership FAQ ‚Ä∫](https://www.photoprism.app/kb/membership)‚ÄÉ[Sign Up ‚Ä∫](https://link.photoprism.app/membership)\n\n### Why Your Support Matters ###\n\n- Your continued support helps us provide regular updates and remain independent, so we can fulfill our mission and protect your privacy\n- Sustained funding is key to quickly releasing new features requested by you and other community members\n- Being self-funded and independent, we can personally promise you that we will never sell your data and that we will always be transparent about our software and services\n\nPlease also leave [a star](https://github.com/photoprism/photoprism/stargazers) on GitHub if you like this project. It provides additional motivation to keep going.\n\n**A big thank you to all current and past sponsors, whose generous support has been and continues to be essential to the success of the project!**\n\n[View Sponsors ‚Ä∫](SPONSORS.md)‚ÄÉ[View Credits ‚Ä∫](https://docs.photoprism.app/credits/)\n\n## Getting Support ##\n\nVisit [docs.photoprism.app/user-guide](https://docs.photoprism.app/user-guide/) to learn how to [sync](https://docs.photoprism.app/user-guide/sync/webdav/), [organize](https://docs.photoprism.app/user-guide/library/), and [share](https://docs.photoprism.app/user-guide/share/) your pictures. If you need help installing our software at home, you are welcome to post your question in [GitHub Discussions](https://link.photoprism.app/discussions) or ask in our [Community Chat](https://link.photoprism.app/chat).\nCommon problems can be quickly diagnosed and solved using our [Troubleshooting Checklists](https://docs.photoprism.app/getting-started/troubleshooting/). Eligible [members](https://link.photoprism.app/membership) are also welcome to email us for technical support and advice.\n\n## Upcoming Features and Enhancements ##\n\n<a href="https://github.com/orgs/photoprism/projects/5"><img align="right" height="240" src="https://dl.photoprism.app/img/ui/2025/upcoming-features-240px.png"></a>\n\nOur [Project Roadmap](https://link.photoprism.app/roadmap) shows what tasks are in progress and what features will be implemented next. You are invited to give ideas you like a thumbs-up, so we know what''s most popular.\n\nBe aware that we have a zero-bug policy and do our best to help users when they need support or have other questions. This comes at a price though, as we can''t give exact release dates for new features. Our team receives many more requests than can be implemented, so we want to emphasize that we are in no way obligated to implement the features, enhancements, or other changes you request. We do, however, appreciate your feedback and carefully consider all requests.\n\n**Because sustained funding is key to quickly releasing new features, we encourage you to support our mission by [signing up for a personal membership](https://link.photoprism.app/membership) or [purchasing a commercial license](https://www.photoprism.app/teams#compare).**\n\n[Become a Member ‚Ä∫](https://link.photoprism.app/membership)\n\n## GitHub Issues ‚ö†Ô∏è ##\n\nWe kindly ask you not to report bugs via GitHub Issues **unless you are certain to have found a fully reproducible and previously unreported issue** that must be fixed directly in the app. Thank you for your careful consideration!\n\n- When browsing issues, please note that **our team and all issue subscribers receive an email notification** from GitHub whenever a new comment is added, so these should only be used for sharing important information and not for [discussions, questions](https://github.com/photoprism/photoprism/discussions), or [expressing personal opinions](https://www.photoprism.app/code-of-conduct)\n- In order for us to investigate [new bug reports](https://www.photoprism.app/kb/reporting-bugs), they must include **a complete list of steps to reproduce the problem**, the software versions used and information about the environment in which the problem occurred, such as [browser type, browser version, browser plug-ins](https://docs.photoprism.app/getting-started/troubleshooting/browsers/), operating system, [storage type](https://docs.photoprism.app/getting-started/troubleshooting/performance/#storage), [processor type](https://docs.photoprism.app/getting-started/troubleshooting/performance/#server-cpu), and [memory size](https://docs.photoprism.app/getting-started/troubleshooting/performance/#memory)\n- [Contact us](https://www.photoprism.app/contact) or [a community member](https://link.photoprism.app/discussions) if you need help, it could be a local configuration problem, or a misunderstanding in how the software works\n- This gives us the opportunity to [improve our documentation](https://docs.photoprism.app/getting-started/troubleshooting/) and provide best-in-class support instead of dealing with unclear/duplicate bug reports or triggering a flood of notifications by replying to comments\n\n## Connect with the Community ##\n\n<a href="https://link.photoprism.app/chat"><img align="right" width="144" height="144" src="https://dl.photoprism.app/img/brands/element-logo.svg"></a>\n\nFollow us on [Mastodon](https://floss.social/@photoprism), [Bluesky](https://bsky.app/profile/photoprism.app), or join the [Community Chat](https://link.photoprism.app/chat) to get regular updates, connect with other users, and discuss your ideas. Our [Code of Conduct](https://www.photoprism.app/code-of-conduct) explains the "dos and don‚Äôts" when interacting with other community members.\n\nAs a [contributor](CONTRIBUTING.md), you are also welcome to [contact us directly](https://www.photoprism.app/contact) if you have something on your mind that you don''t want to discuss publicly. Please note, however, that due to the high volume of emails we receive, our team may be unable to get back to you immediately. We do our best to respond within five business days or less.\n\n## Every Contribution Makes a Difference ##\n\nWe welcome [contributions](CONTRIBUTING.md) of any kind, including blog posts, tutorials, translations, testing, writing documentation, and pull requests. Our [Developer Guide](https://docs.photoprism.app/developer-guide/) contains all the information necessary for you to get started.\n\n----\n\n*PhotoPrism¬Æ is a [registered trademark](https://www.photoprism.app/trademark). By using the software and services we provide, you agree to our [Terms of Service](https://www.photoprism.app/terms), [Privacy Policy](https://www.photoprism.app/privacy), and [Code of Conduct](https://www.photoprism.app/code-of-conduct). Docs are [available](https://link.photoprism.app/github-docs) under the [CC BY-NC-SA 4.0 License](https://creativecommons.org/licenses/by-nc-sa/4.0/); [additional terms](https://github.com/photoprism/photoprism/blob/develop/assets/README.md) may apply.*\n', '{"language":"Go","stars":38891,"forks":2173,"watchers":38891,"open_issues":421,"topics":["ai","golang","google-photos","machine-learning","photography","private-cloud","self-hosted","tensorflow"],"default_branch":"develop","size_kb":321324,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:photoprism:photoprism","source_url":"https://github.com/photoprism/photoprism"},{"type":"has_code","target_id":"github:orgs:photoprism","source_url":"https://github.com/orgs/photoprism"},{"type":"has_code","target_id":"github:photoprism:photoprism","source_url":"https://github.com/photoprism/photoprism"},{"type":"has_code","target_id":"github:photoprism:photoprism","source_url":"https://github.com/photoprism/photoprism"}]', NULL, 'NOASSERTION', 'approved', 80, 'fd216a0633506fe9f5dbb1ec54bdbda2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-photoprism-photoprism from https://github.com/photoprism.png
Image converted to WebP: data/images/github-photoprism-photoprism.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-LAION-AI-Open-Assistant', 'github--laion-ai--open-assistant', 'Open-Assistant', 'LAION-AI', '<h1 align="center"> <span>Open-Assistant</span> <img width="auto" height="50px" src="https://github.com/LAION-AI/Open-Assistant/blob/main/assets/logo_crop.png"/> </h1> <blockquote> <p>:memo: <strong>NOTE</strong>: OpenAssistant is completed, and the project is now finished. Thank you to everyone who contributed! Check out our <a href="https://projects.laion.ai/Open-Assistant/blog/2023/10/25/open-assistant-is-completed">blog post</a> for more information. The final published oasst2 dataset can...', '["ai","assistant","chatgpt","discord-bot","language-model","machine-learning","nextjs","python","rlhf","python"]', 'other', 37497, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/LAION-AI/Open-Assistant","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<h1 align="center">\n    <span>Open-Assistant</span>\n  <img width="auto" height="50px" src="https://github.com/LAION-AI/Open-Assistant/blob/main/assets/logo_crop.png"/>\n</h1>\n\n<blockquote>\n<p>:memo: <strong>NOTE</strong>: OpenAssistant is completed, and the project is now finished. Thank you to everyone who contributed! Check out our <a href="https://projects.laion.ai/Open-Assistant/blog/2023/10/25/open-assistant-is-completed">blog post</a> for more information. The final published oasst2 dataset can be found on HuggingFace at <a href="https://huggingface.co/datasets/OpenAssistant/oasst2">OpenAssistant/oasst2</a></p>\n</blockquote>\n\n<div align="center">\n\n<a href="https://github.com/LAION-AI/Open-Assistant/stargazers">![GitHub Repo stars](https://img.shields.io/github/stars/LAION-AI/Open-Assistant?style=social)</a>\n<a href="https://laion-ai.github.io/Open-Assistant/">![Docs](https://img.shields.io/badge/docs-laion--ai.github.io%2FOpen--Assistant%2F-green)</a>\n<a href="https://github.com/LAION-AI/Open-Assistant/actions/workflows/build-frontend.yaml">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/build-frontend.yaml?label=build-frontend)</a>\n<a href="https://github.com/LAION-AI/Open-Assistant/actions/workflows/build-postgres.yaml">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/build-postgres.yaml?label=build-postgres)</a>\n<a href="https://github.com/LAION-AI/Open-Assistant/actions/workflows/pre-commit.yaml">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/pre-commit.yaml?label=pre-commit)</a>\n<a href="https://github.com/LAION-AI/Open-Assistant/actions/workflows/test-api-contract.yaml">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/test-api-contract.yaml?label=tests-api)</a>\n<a href="https://github.com/LAION-AI/Open-Assistant/actions/workflows/test-e2e.yaml">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/test-e2e.yaml?label=tests-web)</a>\n<a href="https://github.com/LAION-AI/Open-Assistant/actions/workflows/deploy-docs-site.yaml">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/deploy-docs-site.yaml?label=deploy-docs)</a>\n<a href="https://github.com/LAION-AI/Open-Assistant/actions/workflows/production-deploy.yaml">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/production-deploy.yaml?label=deploy-production)</a>\n<a href="https://github.com/LAION-AI/Open-Assistant/actions/workflows/release.yaml">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/release.yaml?label=deploy-release)</a>\n<a href="https://github.com/LAION-AI/Open-Assistant/releases">![GitHub release (latest by date)](https://img.shields.io/github/v/release/LAION-AI/Open-Assistant)</a>\n<a href="https://github-com.translate.goog/LAION-AI/Open-Assistant/blob/main/README.md?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en&_x_tr_pto=wapp">![Translate](https://img.shields.io/badge/Translate-blue)</a>\n\n</div>\n\n# Table of Contents\n\n- [What is Open Assistant?](#what-is-open-assistant)\n- [Useful Links](#useful-links)\n- [How To Try It Out](#how-to-try-it-out)\n- [The Vision](#the-vision)\n- [The Plan](#the-plan)\n- [How You Can Help](#how-you-can-help)\n\n---\n\n## What is Open Assistant?\n\n<p align="center">\nOpen Assistant is a project meant to give everyone access to a great chat based\nlarge language model.\n</p>\n\nWe believe that by doing this we will create a revolution in innovation in\nlanguage. In the same way that stable-diffusion helped the world make art and\nimages in new ways we hope Open Assistant can help improve the world by\nimproving language itself.\n\n# Useful Links\n\n- [Data Collection](https://open-assistant.io)\n\n- [Chat](https://open-assistant.io/chat)\n\n- [Project Documentation](https://projects.laion.ai/Open-Assistant/)\n\n## How To Try It Out\n\n### Chatting with the AI\n\nThe chat frontend is now live [here](https://open-assistant.io/chat). Log in and\nstart chatting! Please try to react with a thumbs up or down for the assistant''s\nresponses when chatting.\n\n### Contributing to Data Collection\n\nThe data collection frontend is now live [here](https://open-assistant.io/). Log\nin and start taking on tasks! We want to collect a high volume of quality data.\nBy submitting, ranking, and labelling model prompts and responses you will be\ndirectly helping to improve the capabilities of Open Assistant.\n\n### Running the Development Setup Locally (without chat)\n\n**You do not need to run the project locally unless you are contributing to the\ndevelopment process. The website link above will take you to the public website\nwhere you can use the data collection app and the chat.**\n\nIf you would like to run the data collection app locally for development, you\ncan set up an entire stack needed to run **Open-Assistant**, including the\nwebsite, backend, and associated dependent services, with Docker.\n\nTo start the demo, run this in the root directory of the repository (check\n[this FAQ](https://projects.laion.ai/Open-Assistant/docs/faq#docker-compose-instead-of-docker-compose)\nif you have problems):\n\n```sh\ndocker compose --profile ci up --build --attach-dependencies\n```\n\n> **Note:** when running on MacOS with an M1 chip you have to use:\n> `DB_PLATFORM=linux/x86_64 docker compose ...`\n\nThen, navigate to `http://localhost:3000` (It may take some time to boot up) and\ninteract with the website.\n\n> **Note:** If an issue occurs with the build, please head to the\n> [FAQ](https://projects.laion.ai/Open-Assistant/docs/faq) and check out the\n> entries about Docker.\n\n> **Note:** When logging in via email, navigate to `http://localhost:1080` to\n> get the magic email login link.\n\n> **Note:** If you would like to run this in a standardized development\n> environment (a\n> ["devcontainer"](https://code.visualstudio.com/docs/devcontainers/containers))\n> using\n> [vscode locally](https://code.visualstudio.com/docs/devcontainers/create-dev-container#_create-a-devcontainerjson-file)\n> or in a web browser using\n> [GitHub Codespaces](https://github.com/features/codespaces), you can use the\n> provided [`.devcontainer`](.devcontainer/) folder.\n\n### Running the Development Setup Locally for Chat\n\n**You do not need to run the project locally unless you are contributing to the\ndevelopment process. The website link above will take you to the public website\nwhere you can use the data collection app and the chat.**\n\n**Also note that the local setup is only for development and is not meant to be\nused as a local chatbot, unless you know what you are doing.**\n\nIf you _do_ know what you are doing, then see the `inference` folder for getting\nthe inference system up and running, or have a look at `--profile inference` in\naddition to `--profile ci` in the above command.\n\n## The Vision\n\nWe are not going to stop at replicating ChatGPT. We want to build the assistant\nof the future, able to not only write email and cover letters, but do meaningful\nwork, use APIs, dynamically research information, and much more, with the\nability to be personalized and extended by anyone. And we want to do this in a\nway that is open and accessible, which means we must not only build a great\nassistant, but also make it small and efficient enough to run on consumer\nhardware.\n\n## The Plan\n\n##### We want to get to an initial MVP as fast as possible, by following the 3-steps outlined in the [InstructGPT paper](https://arxiv.org/abs/2203.02155)\n\n1. Collect high-quality human generated Instruction-Fulfillment samples\n   (prompt + response), goal >50k. We design a crowdsourced process to collect\n   and reviewed prompts. We do not want to train on\n   flooding/toxic/spam/junk/personal information data. We will have a\n   leaderboard to motivate the community that shows progress and the most active\n   users. Swag will be given to the top-contributors.\n2. For each of the collected prompts we will sample multiple completions.\n   Completions of one prompt will then be shown randomly to users to rank them\n   from best to worst. Again this should happen crowd-sourced, e.g. we need to\n   deal with unreliable potentially malicious users. At least multiple votes by\n   independent users have to be collected to measure the overall agreement. The\n   gathered ranking-data will be used to train a reward model.\n3. Now follows the RLHF training phase based on the prompts and the reward\n   model.\n\nWe can then take the resulting model and continue with completion sampling step\n2 for a next iteration.\n\n### Slide Decks\n\n[Vision & Roadmap](https://docs.google.com/presentation/d/1n7IrAOVOqwdYgiYrXc8Sj0He8krn5MVZO_iLkCjTtu0/edit?usp=sharing)\n\n[Important Data Structures](https://docs.google.com/presentation/d/1iaX_nxasVWlvPiSNs0cllR9L_1neZq0RJxd6MFEalUY/edit?usp=sharing)\n\n## How You Can Help\n\nAll open source projects begin with people like you. Open source is the belief\nthat if we collaborate we can together gift our knowledge and technology to the\nworld for the benefit of humanity.\n\nCheck out our [contributing guide](CONTRIBUTING.md) to get started.\n', '{"language":"Python","stars":37497,"forks":3300,"watchers":37497,"open_issues":291,"topics":["ai","assistant","chatgpt","discord-bot","language-model","machine-learning","nextjs","python","rlhf"],"default_branch":"main","size_kb":35477,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:features:codespaces","source_url":"https://github.com/features/codespaces"}]', NULL, 'Apache-2.0', 'approved', 65, '088d2fba3d79519bdbd83d25cd9b1a78', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-LAION-AI-Open-Assistant from https://github.com/LAION-AI.png
Image converted to WebP: data/images/github-LAION-AI-Open-Assistant.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-google-research-google-research', 'github--google-research--google-research', 'google-research', 'google-research', 'This repository contains code released by Google Research. All datasets in this repository are released under the CC BY 4.0 International license, which can be found here: https://creativecommons.org/licenses/by/4.0/legalcode. All source files in this repository are released under the Apache 2.0 license, the text of which can be found in the LICENSE file. --- Because the repo is large, we recommend you download only the subdirectory of interest: * Use GitHub editor to open the project. To ope...', '["ai","machine-learning","research","jupyter notebook"]', 'other', 36858, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/google-research/google-research","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Google Research\n\nThis repository contains code released by\n[Google Research](https://research.google).\n\nAll datasets in this repository are released under the CC BY 4.0 International\nlicense, which can be found here:\nhttps://creativecommons.org/licenses/by/4.0/legalcode.  All source files in this\nrepository are released under the Apache 2.0 license, the text of which can be\nfound in the LICENSE file.\n\n---\n\nBecause the repo is large, we recommend you download only the subdirectory of\ninterest:\n\n* Use GitHub editor to open the project. To open the editor change the url from\ngithub.com to github.dev in the address bar.\n* In the left navigation panel, right-click on the folder of interest and select\ndownload.\n\nIf you''d like to submit a pull request, you''ll need to clone the repository;\nwe recommend making a shallow clone (without history).\n\n```\ngit clone git@github.com:google-research/google-research.git --depth=1\n```\n\n---\n\n*Disclaimer: This is not an official Google product.*\n\nUpdated in 2023.', '{"language":"Jupyter Notebook","stars":36858,"forks":8264,"watchers":36858,"open_issues":1705,"topics":["ai","machine-learning","research"],"default_branch":"master","size_kb":1149754,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[]', NULL, 'Apache-2.0', 'approved', 50, '6ccd1e88bdd18645fe4251374ef7b23c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-google-research-google-research from https://github.com/google-research.png
Image converted to WebP: data/images/github-google-research-google-research.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-roboflow-supervision', 'github--roboflow--supervision', 'supervision', 'roboflow', '<div align="center"> <p> <a align="center" href="" target="https://supervision.roboflow.com"> <img width="100%" src="https://media.roboflow.com/open-source/supervision/rf-supervision-banner.png?updatedAt=1678995927529" > </a> </p> <br> notebooks | inference | autodistill | maestro <br> <div align="center"> <a href="https://trendshift.io/repositories/124" target="_blank"><img src="https://trendshift.io/api/badge/repositories/124" alt="roboflow%2Fsupervision | Trendshift" style="width: 250px; h...', '["classification","coco","computer-vision","deep-learning","hacktoberfest","image-processing","instance-segmentation","low-code","machine-learning","metrics","object-detection","oriented-bounding-box","pascal-voc","python","pytorch","tensorflow","tracking","video-processing","yolo","python"]', 'other', 36118, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/roboflow/supervision","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n  <p>\n    <a align="center" href="" target="https://supervision.roboflow.com">\n      <img\n        width="100%"\n        src="https://media.roboflow.com/open-source/supervision/rf-supervision-banner.png?updatedAt=1678995927529"\n      >\n    </a>\n  </p>\n\n<br>\n\n[notebooks](https://github.com/roboflow/notebooks) | [inference](https://github.com/roboflow/inference) | [autodistill](https://github.com/autodistill/autodistill) | [maestro](https://github.com/roboflow/multimodal-maestro)\n\n<br>\n\n[![version](https://badge.fury.io/py/supervision.svg)](https://badge.fury.io/py/supervision)\n[![downloads](https://img.shields.io/pypi/dm/supervision)](https://pypistats.org/packages/supervision)\n[![snyk](https://snyk.io/advisor/python/supervision/badge.svg)](https://snyk.io/advisor/python/supervision)\n[![license](https://img.shields.io/pypi/l/supervision)](https://github.com/roboflow/supervision/blob/main/LICENSE.md)\n[![python-version](https://img.shields.io/pypi/pyversions/supervision)](https://badge.fury.io/py/supervision)\n[![colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow/supervision/blob/main/demo.ipynb)\n[![gradio](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/Roboflow/Annotators)\n[![discord](https://img.shields.io/discord/1159501506232451173?logo=discord&label=discord&labelColor=fff&color=5865f2&link=https%3A%2F%2Fdiscord.gg%2FGbfgXGJ8Bk)](https://discord.gg/GbfgXGJ8Bk)\n[![built-with-material-for-mkdocs](https://img.shields.io/badge/Material_for_MkDocs-526CFE?logo=MaterialForMkDocs&logoColor=white)](https://squidfunk.github.io/mkdocs-material/)\n\n  <div align="center">\n    <a href="https://trendshift.io/repositories/124"  target="_blank"><img src="https://trendshift.io/api/badge/repositories/124" alt="roboflow%2Fsupervision | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n  </div>\n\n</div>\n\n## üëã hello\n\n**We write your reusable computer vision tools.** Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us! ü§ù\n\n## üíª install\n\nPip install the supervision package in a\n[**Python>=3.9**](https://www.python.org/) environment.\n\n```bash\npip install supervision\n```\n\nRead more about conda, mamba, and installing from source in our [guide](https://roboflow.github.io/supervision/).\n\n## üî• quickstart\n\n### models\n\nSupervision was designed to be model agnostic. Just plug in any classification, detection, or segmentation model. For your convenience, we have created [connectors](https://supervision.roboflow.com/latest/detection/core/#detections) for the most popular libraries like Ultralytics, Transformers, or MMDetection.\n\n```python\nimport cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nimage = cv2.imread(...)\nmodel = YOLO("yolov8s.pt")\nresult = model(image)[0]\ndetections = sv.Detections.from_ultralytics(result)\n\nlen(detections)\n# 5\n```\n\n<details>\n<summary>üëâ more model connectors</summary>\n\n- inference\n\n  Running with [Inference](https://github.com/roboflow/inference) requires a [Roboflow API KEY](https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key).\n\n  ```python\n  import cv2\n  import supervision as sv\n  from inference import get_model\n\n  image = cv2.imread(...)\n  model = get_model(model_id="yolov8s-640", api_key=<ROBOFLOW API KEY>)\n  result = model.infer(image)[0]\n  detections = sv.Detections.from_inference(result)\n\n  len(detections)\n  # 5\n  ```\n\n</details>\n\n### annotators\n\nSupervision offers a wide range of highly customizable [annotators](https://supervision.roboflow.com/latest/detection/annotators/), allowing you to compose the perfect visualization for your use case.\n\n```python\nimport cv2\nimport supervision as sv\n\nimage = cv2.imread(...)\ndetections = sv.Detections(...)\n\nbox_annotator = sv.BoxAnnotator()\nannotated_frame = box_annotator.annotate(\n  scene=image.copy(),\n  detections=detections)\n```\n\nhttps://github.com/roboflow/supervision/assets/26109316/691e219c-0565-4403-9218-ab5644f39bce\n\n### datasets\n\nSupervision provides a set of [utils](https://supervision.roboflow.com/latest/datasets/core/) that allow you to load, split, merge, and save datasets in one of the supported formats.\n\n```python\nimport supervision as sv\nfrom roboflow import Roboflow\n\nproject = Roboflow().workspace(<WORKSPACE_ID>).project(<PROJECT_ID>)\ndataset = project.version(<PROJECT_VERSION>).download("coco")\n\nds = sv.DetectionDataset.from_coco(\n    images_directory_path=f"{dataset.location}/train",\n    annotations_path=f"{dataset.location}/train/_annotations.coco.json",\n)\n\npath, image, annotation = ds[0]\n    # loads image on demand\n\nfor path, image, annotation in ds:\n    # loads image on demand\n```\n\n<details close>\n<summary>üëâ more dataset utils</summary>\n\n- load\n\n  ```python\n  dataset = sv.DetectionDataset.from_yolo(\n      images_directory_path=...,\n      annotations_directory_path=...,\n      data_yaml_path=...\n  )\n\n  dataset = sv.DetectionDataset.from_pascal_voc(\n      images_directory_path=...,\n      annotations_directory_path=...\n  )\n\n  dataset = sv.DetectionDataset.from_coco(\n      images_directory_path=...,\n      annotations_path=...\n  )\n  ```\n\n- split\n\n  ```python\n  train_dataset, test_dataset = dataset.split(split_ratio=0.7)\n  test_dataset, valid_dataset = test_dataset.split(split_ratio=0.5)\n\n  len(train_dataset), len(test_dataset), len(valid_dataset)\n  # (700, 150, 150)\n  ```\n\n- merge\n\n  ```python\n  ds_1 = sv.DetectionDataset(...)\n  len(ds_1)\n  # 100\n  ds_1.classes\n  # [''dog'', ''person'']\n\n  ds_2 = sv.DetectionDataset(...)\n  len(ds_2)\n  # 200\n  ds_2.classes\n  # [''cat'']\n\n  ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])\n  len(ds_merged)\n  # 300\n  ds_merged.classes\n  # [''cat'', ''dog'', ''person'']\n  ```\n\n- save\n\n  ```python\n  dataset.as_yolo(\n      images_directory_path=...,\n      annotations_directory_path=...,\n      data_yaml_path=...\n  )\n\n  dataset.as_pascal_voc(\n      images_directory_path=...,\n      annotations_directory_path=...\n  )\n\n  dataset.as_coco(\n      images_directory_path=...,\n      annotations_path=...\n  )\n  ```\n\n- convert\n\n  ```python\n  sv.DetectionDataset.from_yolo(\n      images_directory_path=...,\n      annotations_directory_path=...,\n      data_yaml_path=...\n  ).as_pascal_voc(\n      images_directory_path=...,\n      annotations_directory_path=...\n  )\n  ```\n\n</details>\n\n## üé¨ tutorials\n\nWant to learn how to use Supervision? Explore our [how-to guides](https://supervision.roboflow.com/develop/how_to/detect_and_annotate/), [end-to-end examples](https://github.com/roboflow/supervision/tree/develop/examples), [cheatsheet](https://roboflow.github.io/cheatsheet-supervision/), and [cookbooks](https://supervision.roboflow.com/develop/cookbooks/)!\n\n<br/>\n\n<p align="left">\n<a href="https://youtu.be/hAWpsIuem10" title="Dwell Time Analysis with Computer Vision | Real-Time Stream Processing"><img src="https://github.com/SkalskiP/SkalskiP/assets/26109316/a742823d-c158-407d-b30f-063a5d11b4e1" alt="Dwell Time Analysis with Computer Vision | Real-Time Stream Processing" width="300px" align="left" /></a>\n<a href="https://youtu.be/hAWpsIuem10" title="Dwell Time Analysis with Computer Vision | Real-Time Stream Processing"><strong>Dwell Time Analysis with Computer Vision | Real-Time Stream Processing</strong></a>\n<div><strong>Created: 5 Apr 2024</strong></div>\n<br/>Learn how to use computer vision to analyze wait times and optimize processes. This tutorial covers object detection, tracking, and calculating time spent in designated zones. Use these techniques to improve customer experience in retail, traffic management, or other scenarios.</p>\n\n<br/>\n\n<p align="left">\n<a href="https://youtu.be/uWP6UjDeZvY" title="Speed Estimation & Vehicle Tracking | Computer Vision | Open Source"><img src="https://github.com/SkalskiP/SkalskiP/assets/26109316/61a444c8-b135-48ce-b979-2a5ab47c5a91" alt="Speed Estimation & Vehicle Tracking | Computer Vision | Open Source" width="300px" align="left" /></a>\n<a href="https://youtu.be/uWP6UjDeZvY" title="Speed Estimation & Vehicle Tracking | Computer Vision | Open Source"><strong>Speed Estimation & Vehicle Tracking | Computer Vision | Open Source</strong></a>\n<div><strong>Created: 11 Jan 2024</strong></div>\n<br/>Learn how to track and estimate the speed of vehicles using YOLO, ByteTrack, and Roboflow Inference. This comprehensive tutorial covers object detection, multi-object tracking, filtering detections, perspective transformation, speed estimation, visualization improvements, and more.</p>\n\n## üíú built with supervision\n\nDid you build something cool using supervision? [Let us know!](https://github.com/roboflow/supervision/discussions/categories/built-with-supervision)\n\nhttps://user-images.githubusercontent.com/26109316/207858600-ee862b22-0353-440b-ad85-caa0c4777904.mp4\n\nhttps://github.com/roboflow/supervision/assets/26109316/c9436828-9fbf-4c25-ae8c-60e9c81b3900\n\nhttps://github.com/roboflow/supervision/assets/26109316/3ac6982f-4943-4108-9b7f-51787ef1a69f\n\n## üìö documentation\n\nVisit our [documentation](https://roboflow.github.io/supervision) page to learn how supervision can help you build computer vision applications faster and more reliably.\n\n## üèÜ contribution\n\nWe love your input! Please see our [contributing guide](https://github.com/roboflow/supervision/blob/main/CONTRIBUTING.md) to get started. Thank you üôè to all our contributors!\n\n<p align="center">\n    <a href="https://github.com/roboflow/supervision/graphs/contributors">\n      <img src="https://contrib.rocks/image?repo=roboflow/supervision" />\n    </a>\n</p>\n\n<br>\n\n<div align="center">\n\n<div align="center">\n      <a href="https://youtube.com/roboflow">\n          <img\n            src="https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949634652"\n            width="3%"\n          />\n      </a>\n      <img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"/>\n      <a href="https://roboflow.com">\n          <img\n            src="https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949746649"\n            width="3%"\n          />\n      </a>\n      <img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"/>\n      <a href="https://www.linkedin.com/company/roboflow-ai/">\n          <img\n            src="https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633691"\n            width="3%"\n          />\n      </a>\n      <img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"/>\n      <a href="https://docs.roboflow.com">\n          <img\n            src="https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949634511"\n            width="3%"\n          />\n      </a>\n      <img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"/>\n      <a href="https://discuss.roboflow.com">\n          <img\n            src="https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633584"\n            width="3%"\n          />\n      <img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"/>\n      <a href="https://blog.roboflow.com">\n          <img\n            src="https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633605"\n            width="3%"\n          />\n      </a>\n      </a>\n  </div>\n</div>\n', '{"language":"Python","stars":36118,"forks":3040,"watchers":36118,"open_issues":158,"topics":["classification","coco","computer-vision","deep-learning","hacktoberfest","image-processing","instance-segmentation","low-code","machine-learning","metrics","object-detection","oriented-bounding-box","pascal-voc","python","pytorch","tensorflow","tracking","video-processing","yolo"],"default_branch":"develop","size_kb":2693269,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:roboflow:notebooks","source_url":"https://github.com/roboflow/notebooks"},{"type":"has_code","target_id":"github:roboflow:inference","source_url":"https://github.com/roboflow/inference"},{"type":"has_code","target_id":"github:autodistill:autodistill","source_url":"https://github.com/autodistill/autodistill"},{"type":"has_code","target_id":"github:roboflow:multimodal-maestro","source_url":"https://github.com/roboflow/multimodal-maestro"},{"type":"has_code","target_id":"github:roboflow:supervision","source_url":"https://github.com/roboflow/supervision"},{"type":"has_code","target_id":"github:roboflow:inference","source_url":"https://github.com/roboflow/inference"},{"type":"has_code","target_id":"github:roboflow:supervision","source_url":"https://github.com/roboflow/supervision"},{"type":"has_code","target_id":"github:roboflow:supervision","source_url":"https://github.com/roboflow/supervision"},{"type":"has_code","target_id":"github:SkalskiP:SkalskiP","source_url":"https://github.com/SkalskiP/SkalskiP"},{"type":"has_code","target_id":"github:SkalskiP:SkalskiP","source_url":"https://github.com/SkalskiP/SkalskiP"},{"type":"has_code","target_id":"github:roboflow:supervision","source_url":"https://github.com/roboflow/supervision"},{"type":"has_code","target_id":"github:roboflow:supervision","source_url":"https://github.com/roboflow/supervision"},{"type":"has_code","target_id":"github:roboflow:supervision","source_url":"https://github.com/roboflow/supervision"},{"type":"has_code","target_id":"github:roboflow:supervision","source_url":"https://github.com/roboflow/supervision"},{"type":"has_code","target_id":"github:roboflow:supervision","source_url":"https://github.com/roboflow/supervision"}]', NULL, 'MIT', 'approved', 80, '2c00741d8d41ce1143c568cdd0f5aa71', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-roboflow-supervision from https://github.com/roboflow.png
Image converted to WebP: data/images/github-roboflow-supervision.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-fengdu78-Coursera-ML-AndrewNg-Notes', 'github--fengdu78--coursera-ml-andrewng-notes', 'Coursera-ML-AndrewNg-Notes', 'fengdu78', '**ÊñØÂù¶Á¶èÂ§ßÂ≠¶2014ÔºàÂê¥ÊÅ©ËææÔºâÊú∫Âô®Â≠¶‰π†ÊïôÁ®ã‰∏≠ÊñáÁ¨îËÆ∞** ËØæÁ®ãÂú∞ÂùÄÔºö<https://www.coursera.org/course/ml> **Á¨îËÆ∞Âú®Á∫øÈòÖËØª** **Machine Learning**(Êú∫Âô®Â≠¶‰π†)ÊòØÁ†îÁ©∂ËÆ°ÁÆóÊú∫ÊÄéÊ†∑Ê®°ÊãüÊàñÂÆûÁé∞‰∫∫Á±ªÁöÑÂ≠¶‰π†Ë°å‰∏∫Ôºå‰ª•Ëé∑ÂèñÊñ∞ÁöÑÁü•ËØÜÊàñÊäÄËÉΩÔºåÈáçÊñ∞ÁªÑÁªáÂ∑≤ÊúâÁöÑÁü•ËØÜÁªìÊûÑ‰Ωø‰πã‰∏çÊñ≠ÊîπÂñÑËá™Ë∫´ÁöÑÊÄßËÉΩ„ÄÇÂÆÉÊòØ‰∫∫Â∑•Êô∫ËÉΩÁöÑÊ†∏ÂøÉÔºåÊòØ‰ΩøËÆ°ÁÆóÊú∫ÂÖ∑ÊúâÊô∫ËÉΩÁöÑÊ†πÊú¨ÈÄîÂæÑÔºåÂÖ∂Â∫îÁî®ÈÅçÂèä‰∫∫Â∑•Êô∫ËÉΩÁöÑÂêÑ‰∏™È¢ÜÂüüÔºåÂÆÉ‰∏ªË¶Å‰ΩøÁî®ÂΩíÁ∫≥„ÄÅÁªºÂêàËÄå‰∏çÊòØÊºîËØë„ÄÇÂú®ËøáÂéªÁöÑÂçÅÂπ¥‰∏≠ÔºåÊú∫Âô®Â≠¶‰π†Â∏ÆÂä©Êàë‰ª¨Ëá™Âä®È©æÈ©∂Ê±ΩËΩ¶ÔºåÊúâÊïàÁöÑËØ≠Èü≥ËØÜÂà´ÔºåÊúâÊïàÁöÑÁΩëÁªúÊêúÁ¥¢ÔºåÂπ∂ÊûÅÂ§ßÂú∞ÊèêÈ´ò‰∫Ü‰∫∫Á±ªÂü∫Âõ†ÁªÑÁöÑËÆ§ËØÜ„ÄÇÊú∫Âô®Â≠¶‰π†ÊòØÂΩì‰ªäÈùûÂ∏∏ÊôÆÈÅçÔºå‰Ω†ÂèØËÉΩ‰ºö‰ΩøÁî®Ëøô‰∏ÄÂ§©Âá†ÂçÅÂÄçËÄå‰∏çËá™Áü•„ÄÇÂæàÂ§öÁ†îÁ©∂ËÄÖ‰πüËÆ§‰∏∫ËøôÊòØÊúÄÂ•ΩÁöÑ‰∫∫Â∑•Êô∫ËÉΩÁöÑÂèñÂæóÊñπÂºè„ÄÇÂú®Êú¨ËØæ‰∏≠ÔºåÊÇ®Â∞ÜÂ≠¶‰π†ÊúÄÊúâÊïàÁöÑÊú∫Âô®Â≠¶‰π†ÊäÄÊúØÔºåÂπ∂Ëé∑ÂæóÂÆûË∑µÔºåËÆ©ÂÆÉ‰ª¨‰∏∫Ëá™Â∑±ÁöÑÂ∑•‰Ωú„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºå‰Ω†‰ºö‰∏ç‰ªÖÂæóÂà∞ÁêÜËÆ∫Âü∫Á°ÄÁöÑÂ≠¶‰π†ÔºåËÄå‰∏îËé∑ÂæóÈÇ£‰∫õÈúÄË¶ÅÂø´ÈÄüÂíåÂº∫Â§ßÁöÑÂ∫îÁî®ÊäÄÊúØËß£ÂÜ≥ÈóÆÈ¢òÁöÑÂÆûÁî®ÊäÄÊúØ„ÄÇÊúÄÂêéÔºå‰Ω†‰ºöÂ≠¶Âà∞‰∏Ä‰∫õÁ°ÖË∞∑Âà©Áî®Êú∫Âô®Â≠¶‰π†Âíå‰∫∫Â∑•Êô∫ËÉΩÁöÑÊúÄ‰Ω≥ÂÆûË∑µÂàõÊñ∞„ÄÇ Êú¨ËØæÁ®ãÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂπøÊ≥õÁöÑ‰ªãÁªçÊú∫Âô®Â≠¶‰π†„ÄÅÊï∞ÊçÆÊåñÊéò„ÄÅÁªüËÆ°Ê®°ÂºèËØÜÂà´ÁöÑËØæÁ®ã„ÄÇ‰∏ªÈ¢òÂåÖÊã¨Ôºö Ôºà‰∏ÄÔºâÁõëÁù£Â≠¶‰π†ÔºàÂèÇÊï∞/ÈùûÂèÇÊï∞ÁÆóÊ≥ïÔºå...', '["coursera","machine-learning","html"]', 'other', 35796, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '**ÊñØÂù¶Á¶èÂ§ßÂ≠¶2014ÔºàÂê¥ÊÅ©ËææÔºâÊú∫Âô®Â≠¶‰π†ÊïôÁ®ã‰∏≠ÊñáÁ¨îËÆ∞**\n\nËØæÁ®ãÂú∞ÂùÄÔºö<https://www.coursera.org/course/ml>\n\n[**Á¨îËÆ∞Âú®Á∫øÈòÖËØª**](http://www.ai-start.com/ml2014)\n\n**Machine Learning**(Êú∫Âô®Â≠¶‰π†)ÊòØÁ†îÁ©∂ËÆ°ÁÆóÊú∫ÊÄéÊ†∑Ê®°ÊãüÊàñÂÆûÁé∞‰∫∫Á±ªÁöÑÂ≠¶‰π†Ë°å‰∏∫Ôºå‰ª•Ëé∑ÂèñÊñ∞ÁöÑÁü•ËØÜÊàñÊäÄËÉΩÔºåÈáçÊñ∞ÁªÑÁªáÂ∑≤ÊúâÁöÑÁü•ËØÜÁªìÊûÑ‰Ωø‰πã‰∏çÊñ≠ÊîπÂñÑËá™Ë∫´ÁöÑÊÄßËÉΩ„ÄÇÂÆÉÊòØ‰∫∫Â∑•Êô∫ËÉΩÁöÑÊ†∏ÂøÉÔºåÊòØ‰ΩøËÆ°ÁÆóÊú∫ÂÖ∑ÊúâÊô∫ËÉΩÁöÑÊ†πÊú¨ÈÄîÂæÑÔºåÂÖ∂Â∫îÁî®ÈÅçÂèä‰∫∫Â∑•Êô∫ËÉΩÁöÑÂêÑ‰∏™È¢ÜÂüüÔºåÂÆÉ‰∏ªË¶Å‰ΩøÁî®ÂΩíÁ∫≥„ÄÅÁªºÂêàËÄå‰∏çÊòØÊºîËØë„ÄÇÂú®ËøáÂéªÁöÑÂçÅÂπ¥‰∏≠ÔºåÊú∫Âô®Â≠¶‰π†Â∏ÆÂä©Êàë‰ª¨Ëá™Âä®È©æÈ©∂Ê±ΩËΩ¶ÔºåÊúâÊïàÁöÑËØ≠Èü≥ËØÜÂà´ÔºåÊúâÊïàÁöÑÁΩëÁªúÊêúÁ¥¢ÔºåÂπ∂ÊûÅÂ§ßÂú∞ÊèêÈ´ò‰∫Ü‰∫∫Á±ªÂü∫Âõ†ÁªÑÁöÑËÆ§ËØÜ„ÄÇÊú∫Âô®Â≠¶‰π†ÊòØÂΩì‰ªäÈùûÂ∏∏ÊôÆÈÅçÔºå‰Ω†ÂèØËÉΩ‰ºö‰ΩøÁî®Ëøô‰∏ÄÂ§©Âá†ÂçÅÂÄçËÄå‰∏çËá™Áü•„ÄÇÂæàÂ§öÁ†îÁ©∂ËÄÖ‰πüËÆ§‰∏∫ËøôÊòØÊúÄÂ•ΩÁöÑ‰∫∫Â∑•Êô∫ËÉΩÁöÑÂèñÂæóÊñπÂºè„ÄÇÂú®Êú¨ËØæ‰∏≠ÔºåÊÇ®Â∞ÜÂ≠¶‰π†ÊúÄÊúâÊïàÁöÑÊú∫Âô®Â≠¶‰π†ÊäÄÊúØÔºåÂπ∂Ëé∑ÂæóÂÆûË∑µÔºåËÆ©ÂÆÉ‰ª¨‰∏∫Ëá™Â∑±ÁöÑÂ∑•‰Ωú„ÄÇÊõ¥ÈáçË¶ÅÁöÑÊòØÔºå‰Ω†‰ºö‰∏ç‰ªÖÂæóÂà∞ÁêÜËÆ∫Âü∫Á°ÄÁöÑÂ≠¶‰π†ÔºåËÄå‰∏îËé∑ÂæóÈÇ£‰∫õÈúÄË¶ÅÂø´ÈÄüÂíåÂº∫Â§ßÁöÑÂ∫îÁî®ÊäÄÊúØËß£ÂÜ≥ÈóÆÈ¢òÁöÑÂÆûÁî®ÊäÄÊúØ„ÄÇÊúÄÂêéÔºå‰Ω†‰ºöÂ≠¶Âà∞‰∏Ä‰∫õÁ°ÖË∞∑Âà©Áî®Êú∫Âô®Â≠¶‰π†Âíå‰∫∫Â∑•Êô∫ËÉΩÁöÑÊúÄ‰Ω≥ÂÆûË∑µÂàõÊñ∞„ÄÇ\n\nÊú¨ËØæÁ®ãÊèê‰æõ‰∫Ü‰∏Ä‰∏™ÂπøÊ≥õÁöÑ‰ªãÁªçÊú∫Âô®Â≠¶‰π†„ÄÅÊï∞ÊçÆÊåñÊéò„ÄÅÁªüËÆ°Ê®°ÂºèËØÜÂà´ÁöÑËØæÁ®ã„ÄÇ‰∏ªÈ¢òÂåÖÊã¨Ôºö\n\nÔºà‰∏ÄÔºâÁõëÁù£Â≠¶‰π†ÔºàÂèÇÊï∞/ÈùûÂèÇÊï∞ÁÆóÊ≥ïÔºåÊîØÊåÅÂêëÈáèÊú∫ÔºåÊ†∏ÂáΩÊï∞ÔºåÁ•ûÁªèÁΩëÁªúÔºâ„ÄÇ\n\nÔºà‰∫åÔºâÊó†ÁõëÁù£Â≠¶‰π†ÔºàËÅöÁ±ªÔºåÈôçÁª¥ÔºåÊé®ËçêÁ≥ªÁªüÔºåÊ∑±ÂÖ•Â≠¶‰π†Êé®ËçêÔºâ„ÄÇ\n\nÔºà‰∏âÔºâÂú®Êú∫Âô®Â≠¶‰π†ÁöÑÊúÄ‰Ω≥ÂÆûË∑µÔºàÂÅèÂ∑Æ/ÊñπÂ∑ÆÁêÜËÆ∫ÔºõÂú®Êú∫Âô®Â≠¶‰π†Âíå‰∫∫Â∑•Êô∫ËÉΩÂàõÊñ∞ËøáÁ®ãÔºâ„ÄÇÊú¨ËØæÁ®ãËøòÂ∞Ü‰ΩøÁî®Â§ßÈáèÁöÑÊ°à‰æãÁ†îÁ©∂ÔºåÊÇ®ËøòÂ∞ÜÂ≠¶‰π†Â¶Ç‰ΩïËøêÁî®Â≠¶‰π†ÁÆóÊ≥ïÊûÑÂª∫Êô∫ËÉΩÊú∫Âô®‰∫∫ÔºàÊÑüÁü•ÔºåÊéßÂà∂ÔºâÔºåÊñáÊú¨ÁöÑÁêÜËß£Ôºà**Web**ÊêúÁ¥¢ÔºåÂèçÂûÉÂúæÈÇÆ‰ª∂ÔºâÔºåËÆ°ÁÆóÊú∫ËßÜËßâÔºåÂåªÁñó‰ø°ÊÅØÔºåÈü≥È¢ëÔºåÊï∞ÊçÆÊåñÊéòÔºåÂíåÂÖ∂‰ªñÈ¢ÜÂüü„ÄÇ\n\nÊú¨ËØæÁ®ãÈúÄË¶Å10Âë®ÂÖ±18ËäÇËØæÔºåÁõ∏ÂØπ‰ª•ÂâçÁöÑÊú∫Âô®Â≠¶‰π†ËßÜÈ¢ëÔºåËøô‰∏™ËßÜÈ¢ëÊõ¥Âä†Ê∏ÖÊô∞ÔºåËÄå‰∏îÊØèËØæÈÉΩÊúâ**ppt**ËØæ‰ª∂ÔºåÊé®ËçêÂ≠¶‰π†„ÄÇ\n\nÊú¨‰∫∫2014Âπ¥‰∏ãÂçäÂπ¥ÂºÄÂßãÁøªËØëÊú¨ËØæÁ®ãÂ≠óÂπïÔºåÂπ∂ÂÜô‰∫ÜËØæÁ®ãÁöÑ‰∏≠ÊñáÁ¨îËÆ∞„ÄÇÁ¨îËÆ∞Ë¢´‰∏ãËΩΩ‰∫ÜÂá†‰∏áÊ¨°ÔºåÂ∫îËØ•Â∏ÆÂä©‰∫Ü‰∏çÂ∞ë‰∫∫Ôºå‰πüÊúâÂæàÂ§ö‰∫∫‰∏ÄÁõ¥Âú®Â∏ÆÂä©ÊàëÔºåÁé∞Âú®ÊàëÊääÁ¨îËÆ∞ÁöÑ**word**ÂéüÁ®øÂíå**markdown**ÂéüÁ®øÂàÜ‰∫´ÁªôÂ§ßÂÆ∂„ÄÇ\n\n**markdown**ÁöÑÁ¨îËÆ∞ÂíåËØæÁ®ã‰∏≠Ëã±ÊñáÂ≠óÂπïÊàëÂ∞ÜÊîæÂú®**github**ÔºåÂ∏åÊúõÂ§ßÂÆ∂ËÉΩÁªßÁª≠ÂÆåÂñÑ„ÄÇ‰∏∫Êñπ‰æøÊï∞Â≠¶ÂÖ¨ÂºèÁöÑÂú®Á∫øÊòæÁ§∫ÔºåÂú®Á∫øËßÇÁúãÁöÑÊòØ**html**Êñá‰ª∂ÔºåÂÖ¨ÂºèÂ∑≤ÁªèË¢´ËΩ¨‰∏∫ÂõæÁâáÔºåÂÖ¨ÂºèÊ∫êÁ†ÅÂú®**markdown**Êñá‰ª∂„ÄÇ\n\n**ÊúÄÂêéÊÉ≥ÂØπÂêÑ‰ΩçÊúãÂèãËØ¥Ôºö**\n**Ëµ†‰∫∫Áé´Áë∞ÔºåÊâãÊúâ‰ΩôÈ¶ôÔºÅ**\n**Âú®‰∫∫Â∑•Êô∫ËÉΩÁöÑÈÅìË∑Ø‰∏äÔºå‰Ω†‰∏çÊòØ‰∏Ä‰∏™‰∫∫Âú®ÊàòÊñóÔºÅ**\n\nÈªÑÊµ∑Âπø\n\n2018-3-26 Â§ú\n\nÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ÔºöÊú∫Âô®Â≠¶‰π†ÂàùÂ≠¶ËÄÖ ![gongzhong](images/gongzhong.jpg)\n[ÊàëÁöÑÁü•‰πé](https://www.zhihu.com/people/fengdu78/activities)\n\nÂèÇËÄÉÔºö\n\n1. https://www.coursera.org/course/ml Êú∫Âô®Â≠¶‰π†ÂÖ¨ÂºÄËØæ\n2. https://mooc.guokr.com/note/12/ [Â∞èÂ∞è‰∫∫_V](https://mooc.guokr.com/user/2133483357/) ÁöÑ‰∏™‰∫∫Á¨îËÆ∞\n\n3. „ÄäÁªüËÆ°Â≠¶‰π†ÊñπÊ≥ï„ÄãÊùéËà™   \n4. „ÄäÊú∫Âô®Â≠¶‰π†ËØæ„ÄãÈÇπÂçö\n\nÂ§áÊ≥®ÔºöÂê¥ÊÅ©ËææËÄÅÂ∏àÁöÑÊ∑±Â∫¶Â≠¶‰π†ËØæÔºàdeepLearning.aiÔºâÁöÑÁ¨îËÆ∞Âú∞ÂùÄÔºöhttps://github.com/fengdu78/deeplearning_ai_books\n\n-----------------------\n\nÊñá‰ª∂Â§πËØ¥ÊòéÔºö\n\n**docx**ÔºöÁ¨îËÆ∞ÁöÑ**word**ÁâàÊú¨\n\n**markdown**ÔºöÁ¨îËÆ∞ÁöÑ**markdown**ÁâàÊú¨\n\n**html**ÔºöÁ¨îËÆ∞ÁöÑ**html**ÁâàÊú¨\n\n**images**ÔºöÁ¨îËÆ∞ÁöÑÂõæÁâá\n\n**ppt**ÔºöËØæÁ®ãÁöÑÂéüÁâàËØæ‰ª∂\n\n**srt**ÔºöËØæÁ®ãÁöÑ‰∏≠Ëã±ÊñáÂ≠óÂπïÔºà**mp4**Êñá‰ª∂ÈúÄË¶ÅÂú®ÁôæÂ∫¶‰∫ë‰∏ãËΩΩÔºåÂ§ßÂÆ∂ÂèØ‰ª•Áî®ËÆ∞‰∫ãÊú¨ÊàñËÄÖÂ≠óÂπïÁºñËæëËΩØ‰ª∂Êù•ÁºñËæëÂ≠óÂπïÔºåÂÖ±ÂêåÂÆåÂñÑ„ÄÇ\n\nÁôæÂ∫¶‰∫ëÈìæÊé•Ôºöhttps://pan.baidu.com/s/1h8QjqBlOm0Exh7orm9teMQ ÂØÜÁ†ÅÔºöd3weÔºå‰∏ãËΩΩÂêéËß£ÂéãÔºâ\n\n**code**ÔºöËØæÁ®ãÁöÑ**python**‰ª£Á†Å\n\nÊú∫Âô®Â≠¶‰π†ËØæÁ®ãËßÜÈ¢ëÔºöhttps://www.bilibili.com/video/BV1W34y1i7xK\n\n[Á¨îËÆ∞Âú®Á∫øÈòÖËØª](http://www.ai-start.com/ml2014)\n\nÁ¨îËÆ∞pdfÁâàÊú¨‰∏ãËΩΩ ÔºöËßÅ**Github**Ê†πÁõÆÂΩï„ÄÇ\n\nÊú∫Âô®Â≠¶‰π†qqÁæ§Ôºö955171419ÔºàÊàë‰ª¨Êúâ13‰∏™Áæ§ÔºåÂä†Ëøá‰∏Ä‰∏™Â∞±‰∏çÈúÄË¶ÅÂä†‰∫ÜÔºâ\n\n-----------------------\n\n# Êú∫Âô®Â≠¶‰π†ÊïôÁ®ã‰∏≠ÊñáÁ¨îËÆ∞ÁõÆÂΩï\n\n- [Á¨¨‰∏ÄÂë®](markdown/week1.md)\n\n‰∏Ä„ÄÅ ÂºïË®Ä(**Introduction**) \n\n1.1 Ê¨¢Ëøé \n\n1.2 Êú∫Âô®Â≠¶‰π†ÊòØ‰ªÄ‰πàÔºü \n\n1.3 ÁõëÁù£Â≠¶‰π† \n\n1.4 Êó†ÁõëÁù£Â≠¶‰π† \n\n‰∫å„ÄÅÂçïÂèòÈáèÁ∫øÊÄßÂõûÂΩí(**Linear Regression with One Variable**) \n\n2.1 Ê®°ÂûãË°®Á§∫ \n\n2.2 ‰ª£‰ª∑ÂáΩÊï∞ \n\n2.3 ‰ª£‰ª∑ÂáΩÊï∞ÁöÑÁõ¥ËßÇÁêÜËß£I \n\n2.4 ‰ª£‰ª∑ÂáΩÊï∞ÁöÑÁõ¥ËßÇÁêÜËß£II \n\n2.5 Ê¢ØÂ∫¶‰∏ãÈôç \n\n2.6 Ê¢ØÂ∫¶‰∏ãÈôçÁöÑÁõ¥ËßÇÁêÜËß£ \n\n2.7 Ê¢ØÂ∫¶‰∏ãÈôçÁöÑÁ∫øÊÄßÂõûÂΩí \n\n2.8 Êé•‰∏ãÊù•ÁöÑÂÜÖÂÆπ \n\n‰∏â„ÄÅÁ∫øÊÄß‰ª£Êï∞ÂõûÈ°æ(**Linear Algebra Review**) \n\n3.1 Áü©ÈòµÂíåÂêëÈáè \n\n3.2 Âä†Ê≥ïÂíåÊ†áÈáè‰πòÊ≥ï \n\n3.3 Áü©ÈòµÂêëÈáè‰πòÊ≥ï \n\n3.4 Áü©Èòµ‰πòÊ≥ï \n\n3.5 Áü©Èòµ‰πòÊ≥ïÁöÑÊÄßË¥® \n\n3.6 ÈÄÜ„ÄÅËΩ¨ÁΩÆ\n\n- [Á¨¨‰∫åÂë®](markdown/week2.md)\n\nÂõõ„ÄÅÂ§öÂèòÈáèÁ∫øÊÄßÂõûÂΩí(**Linear Regression with Multiple Variables**) \n\n4.1 Â§öÁª¥ÁâπÂæÅ \n\n4.2 Â§öÂèòÈáèÊ¢ØÂ∫¶‰∏ãÈôç \n\n4.3 Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÂÆûË∑µ1-ÁâπÂæÅÁº©Êîæ \n\n4.4 Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÂÆûË∑µ2-Â≠¶‰π†Áéá \n\n4.5 ÁâπÂæÅÂíåÂ§öÈ°πÂºèÂõûÂΩí \n\n4.6 Ê≠£ËßÑÊñπÁ®ã \n\n4.7 Ê≠£ËßÑÊñπÁ®ãÂèä‰∏çÂèØÈÄÜÊÄßÔºàÈÄâ‰øÆÔºâ \n\n‰∫î„ÄÅOctaveÊïôÁ®ã(**Octave Tutorial**) \n\n5.1 Âü∫Êú¨Êìç‰Ωú \n\n5.2 ÁßªÂä®Êï∞ÊçÆ \n\n5.3 ËÆ°ÁÆóÊï∞ÊçÆ \n\n5.4 ÁªòÂõæÊï∞ÊçÆ \n\n5.5 ÊéßÂà∂ËØ≠Âè•Ôºö**for**Ôºå**while**Ôºå**if**ËØ≠Âè• \n\n5.6 ÂêëÈáèÂåñ 88\n\n5.7 Â∑•‰ΩúÂíåÊèê‰∫§ÁöÑÁºñÁ®ãÁªÉ‰π† \n\n- [Á¨¨‰∏âÂë®](markdown/week3.md)\n\nÂÖ≠„ÄÅÈÄªËæëÂõûÂΩí(**Logistic Regression**) \n\n6.1 ÂàÜÁ±ªÈóÆÈ¢ò \n\n6.2 ÂÅáËØ¥Ë°®Á§∫ \n\n6.3 Âà§ÂÆöËæπÁïå \n\n6.4 ‰ª£‰ª∑ÂáΩÊï∞ \n\n6.5 ÁÆÄÂåñÁöÑÊàêÊú¨ÂáΩÊï∞ÂíåÊ¢ØÂ∫¶‰∏ãÈôç \n\n6.6 È´òÁ∫ß‰ºòÂåñ \n\n6.7 Â§öÁ±ªÂà´ÂàÜÁ±ªÔºö‰∏ÄÂØπÂ§ö \n\n‰∏É„ÄÅÊ≠£ÂàôÂåñ(**Regularization**) \n\n7.1 ËøáÊãüÂêàÁöÑÈóÆÈ¢ò \n\n7.2 ‰ª£‰ª∑ÂáΩÊï∞ \n\n7.3 Ê≠£ÂàôÂåñÁ∫øÊÄßÂõûÂΩí \n\n7.4 Ê≠£ÂàôÂåñÁöÑÈÄªËæëÂõûÂΩíÊ®°Âûã \n\n- [Á¨¨ÂõõÂë®](markdown/week4.md)\n\nÁ¨¨ÂÖ´„ÄÅÁ•ûÁªèÁΩëÁªúÔºöË°®Ëø∞(**Neural Networks: Representation**) \n\n8.1 ÈùûÁ∫øÊÄßÂÅáËÆæ \n\n8.2 Á•ûÁªèÂÖÉÂíåÂ§ßËÑë \n\n8.3 Ê®°ÂûãË°®Á§∫1 \n\n8.4 Ê®°ÂûãË°®Á§∫2 \n\n8.5 Ê†∑Êú¨ÂíåÁõ¥ËßÇÁêÜËß£1 \n\n8.6 Ê†∑Êú¨ÂíåÁõ¥ËßÇÁêÜËß£II \n\n8.7 Â§öÁ±ªÂàÜÁ±ª \n\n- [Á¨¨‰∫îÂë®](markdown/week5.md)\n\n‰πù„ÄÅÁ•ûÁªèÁΩëÁªúÁöÑÂ≠¶‰π†(**Neural Networks: Learning**) \n\n9.1 ‰ª£‰ª∑ÂáΩÊï∞ \n\n9.2 ÂèçÂêë‰º†Êí≠ÁÆóÊ≥ï \n\n9.3 ÂèçÂêë‰º†Êí≠ÁÆóÊ≥ïÁöÑÁõ¥ËßÇÁêÜËß£ \n\n9.4 ÂÆûÁé∞Ê≥®ÊÑèÔºöÂ±ïÂºÄÂèÇÊï∞ \n\n9.5 Ê¢ØÂ∫¶Ê£ÄÈ™å \n\n9.6 ÈöèÊú∫ÂàùÂßãÂåñ \n\n9.7 ÁªºÂêàËµ∑Êù• \n\n9.8 Ëá™‰∏ªÈ©æÈ©∂ \n\n- [Á¨¨ÂÖ≠Âë®](markdown/week6.md)\n\nÂçÅ„ÄÅÂ∫îÁî®Êú∫Âô®Â≠¶‰π†ÁöÑÂª∫ËÆÆ(**Advice for Applying Machine Learning**) \n\n10.1 ÂÜ≥ÂÆö‰∏ã‰∏ÄÊ≠•ÂÅö‰ªÄ‰πà \n\n10.2 ËØÑ‰º∞‰∏Ä‰∏™ÂÅáËÆæ \n\n10.3 Ê®°ÂûãÈÄâÊã©Âíå‰∫§ÂèâÈ™åËØÅÈõÜ \n\n10.4 ËØäÊñ≠ÂÅèÂ∑ÆÂíåÊñπÂ∑Æ \n\n10.5 Ê≠£ÂàôÂåñÂíåÂÅèÂ∑Æ/ÊñπÂ∑Æ \n\n10.6 Â≠¶‰π†Êõ≤Á∫ø \n\n10.7 ÂÜ≥ÂÆö‰∏ã‰∏ÄÊ≠•ÂÅö‰ªÄ‰πà \n\nÂçÅ‰∏Ä„ÄÅÊú∫Âô®Â≠¶‰π†Á≥ªÁªüÁöÑËÆæËÆ°(**Machine Learning System Design**) \n\n11.1 È¶ñÂÖàË¶ÅÂÅö‰ªÄ‰πà \n\n11.2 ËØØÂ∑ÆÂàÜÊûê \n\n11.3 Á±ªÂÅèÊñúÁöÑËØØÂ∑ÆÂ∫¶Èáè \n\n11.4 Êü•ÂáÜÁéáÂíåÊü•ÂÖ®Áéá‰πãÈó¥ÁöÑÊùÉË°° \n\n11.5 Êú∫Âô®Â≠¶‰π†ÁöÑÊï∞ÊçÆ \n\n[Á¨¨7Âë®](markdown/week7.md)\n\nÂçÅ‰∫å„ÄÅÊîØÊåÅÂêëÈáèÊú∫(**Support Vector Machines**) \n\n12.1 ‰ºòÂåñÁõÆÊ†á \n\n12.2 Â§ßËæπÁïåÁöÑÁõ¥ËßÇÁêÜËß£ \n\n12.3 Êï∞Â≠¶ËÉåÂêéÁöÑÂ§ßËæπÁïåÂàÜÁ±ªÔºàÈÄâ‰øÆÔºâ \n\n12.4 Ê†∏ÂáΩÊï∞1 \n\n12.5 Ê†∏ÂáΩÊï∞2 \n\n12.6 ‰ΩøÁî®ÊîØÊåÅÂêëÈáèÊú∫ \n\n- [Á¨¨ÂÖ´Âë®](markdown/week8.md)\n\nÂçÅ‰∏â„ÄÅËÅöÁ±ª(**Clustering**) \n\n13.1 Êó†ÁõëÁù£Â≠¶‰π†ÔºöÁÆÄ‰ªã \n\n13.2 K-ÂùáÂÄºÁÆóÊ≥ï \n\n13.3 ‰ºòÂåñÁõÆÊ†á \n\n13.4 ÈöèÊú∫ÂàùÂßãÂåñ\n\n13.5 ÈÄâÊã©ËÅöÁ±ªÊï∞ \n\nÂçÅÂõõ„ÄÅÈôçÁª¥(**Dimensionality Reduction**) \n\n14.1 Âä®Êú∫‰∏ÄÔºöÊï∞ÊçÆÂéãÁº© \n\n14.2 Âä®Êú∫‰∫åÔºöÊï∞ÊçÆÂèØËßÜÂåñ \n\n14.3 ‰∏ªÊàêÂàÜÂàÜÊûêÈóÆÈ¢ò \n\n14.4 ‰∏ªÊàêÂàÜÂàÜÊûêÁÆóÊ≥ï \n\n14.5 ÈÄâÊã©‰∏ªÊàêÂàÜÁöÑÊï∞Èáè \n\n14.6 ÈáçÂª∫ÁöÑÂéãÁº©Ë°®Á§∫ \n\n14.7 ‰∏ªÊàêÂàÜÂàÜÊûêÊ≥ïÁöÑÂ∫îÁî®Âª∫ËÆÆ \n\n- [Á¨¨‰πùÂë®](markdown/week9.md)\n\nÂçÅ‰∫î„ÄÅÂºÇÂ∏∏Ê£ÄÊµã(**Anomaly Detection**) \n\n15.1 ÈóÆÈ¢òÁöÑÂä®Êú∫ \n\n15.2 È´òÊñØÂàÜÂ∏É \n\n15.3 ÁÆóÊ≥ï \n\n15.4 ÂºÄÂèëÂíåËØÑ‰ª∑‰∏Ä‰∏™ÂºÇÂ∏∏Ê£ÄÊµãÁ≥ªÁªü \n\n15.5 ÂºÇÂ∏∏Ê£ÄÊµã‰∏éÁõëÁù£Â≠¶‰π†ÂØπÊØî \n\n15.6 ÈÄâÊã©ÁâπÂæÅ \n\n15.7 Â§öÂÖÉÈ´òÊñØÂàÜÂ∏ÉÔºàÈÄâ‰øÆÔºâ \n\n15.8 ‰ΩøÁî®Â§öÂÖÉÈ´òÊñØÂàÜÂ∏ÉËøõË°åÂºÇÂ∏∏Ê£ÄÊµãÔºàÈÄâ‰øÆÔºâ \n\nÂçÅÂÖ≠„ÄÅÊé®ËçêÁ≥ªÁªü(**Recommender Systems**) \n\n16.1 ÈóÆÈ¢òÂΩ¢ÂºèÂåñ \n\n16.2 Âü∫‰∫éÂÜÖÂÆπÁöÑÊé®ËçêÁ≥ªÁªü \n\n16.3 ÂçèÂêåËøáÊª§ \n\n16.4 ÂçèÂêåËøáÊª§ÁÆóÊ≥ï \n\n16.5 ÂêëÈáèÂåñÔºö‰ΩéÁß©Áü©ÈòµÂàÜËß£ \n\n16.6 Êé®Ë°åÂ∑•‰Ωú‰∏äÁöÑÁªÜËäÇÔºöÂùáÂÄºÂΩí‰∏ÄÂåñ \n\n- [Á¨¨ÂçÅÂë®](markdown/week10.md)\n\nÂçÅ‰∏É„ÄÅÂ§ßËßÑÊ®°Êú∫Âô®Â≠¶‰π†(**Large Scale Machine Learning**) \n\n17.1 Â§ßÂûãÊï∞ÊçÆÈõÜÁöÑÂ≠¶‰π† \n\n17.2 ÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ï \n\n17.3 Â∞èÊâπÈáèÊ¢ØÂ∫¶‰∏ãÈôç \n\n17.4 ÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôçÊî∂Êïõ \n\n17.5 Âú®Á∫øÂ≠¶‰π† \n\n17.6 Êò†Â∞ÑÂåñÁÆÄÂíåÊï∞ÊçÆÂπ∂Ë°å \n\nÂçÅÂÖ´„ÄÅÂ∫îÁî®ÂÆû‰æãÔºöÂõæÁâáÊñáÂ≠óËØÜÂà´(**Application Example: Photo OCR**) \n\n18.1 ÈóÆÈ¢òÊèèËø∞ÂíåÊµÅÁ®ãÂõæ\n\n18.2 ÊªëÂä®Á™óÂè£ \n\n18.3 Ëé∑ÂèñÂ§ßÈáèÊï∞ÊçÆÂíå‰∫∫Â∑•Êï∞ÊçÆ \n\n18.4 ‰∏äÈôêÂàÜÊûêÔºöÂì™ÈÉ®ÂàÜÁÆ°ÈÅìÁöÑÊé•‰∏ãÂéªÂÅö \n\nÂçÅ‰πù„ÄÅÊÄªÁªì(**Conclusion**) \n\n19.1 ÊÄªÁªìÂíåËá¥Ë∞¢ \n\n------\n\n\n\n**Êú∫Âô®Â≠¶‰π†qqÁæ§Ôºö955171419ÔºàÊàë‰ª¨Êúâ13‰∏™Áæ§ÔºåÂä†Ëøá‰∏Ä‰∏™Â∞±‰∏çÈúÄË¶ÅÂä†‰∫ÜÔºâ** \n\n', '{"language":"HTML","stars":35796,"forks":11091,"watchers":35796,"open_issues":68,"topics":["coursera","machine-learning"],"default_branch":"master","size_kb":662586,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:fengdu78:deeplearning_ai_books","source_url":"https://github.com/fengdu78/deeplearning_ai_books"}]', NULL, NULL, 'pending', 55, '44dcf2191c08d2352d9f451a89d48c2f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-fengdu78-Coursera-ML-AndrewNg-Notes from https://github.com/fengdu78.png
Image converted to WebP: data/images/github-fengdu78-Coursera-ML-AndrewNg-Notes.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-BVLC-caffe', 'github--bvlc--caffe', 'caffe', 'BVLC', 'Caffe is a deep learning framework made with expression, speed, and modularity in mind. It is developed by Berkeley AI Research (BAIR)/The Berkeley Vision and Learning Center (BVLC) and community contributors. Check out the project site for all the details like - DIY Deep Learning for Vision with Caffe - Tutorial Documentation - BAIR reference models and the community model zoo - Installation instructions and step-by-step examples. - Intel Caffe (Optimized for CPU and support for multi-node),...', '["deep-learning","machine-learning","vision","c++"]', 'other', 34763, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/BVLC/caffe","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Caffe\n\n[![Build Status](https://travis-ci.org/BVLC/caffe.svg?branch=master)](https://travis-ci.org/BVLC/caffe)\n[![License](https://img.shields.io/badge/license-BSD-blue.svg)](LICENSE)\n\nCaffe is a deep learning framework made with expression, speed, and modularity in mind.\nIt is developed by Berkeley AI Research ([BAIR](http://bair.berkeley.edu))/The Berkeley Vision and Learning Center (BVLC) and community contributors.\n\nCheck out the [project site](http://caffe.berkeleyvision.org) for all the details like\n\n- [DIY Deep Learning for Vision with Caffe](https://docs.google.com/presentation/d/1UeKXVgRvvxg9OUdh_UiC5G71UMscNPlvArsWER41PsU/edit#slide=id.p)\n- [Tutorial Documentation](http://caffe.berkeleyvision.org/tutorial/)\n- [BAIR reference models](http://caffe.berkeleyvision.org/model_zoo.html) and the [community model zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo)\n- [Installation instructions](http://caffe.berkeleyvision.org/installation.html)\n\nand step-by-step examples.\n\n## Custom distributions\n\n - [Intel Caffe](https://github.com/BVLC/caffe/tree/intel) (Optimized for CPU and support for multi-node), in particular Intel¬Æ Xeon processors.\n- [OpenCL Caffe](https://github.com/BVLC/caffe/tree/opencl) e.g. for AMD or Intel devices.\n- [Windows Caffe](https://github.com/BVLC/caffe/tree/windows)\n\n## Community\n\n[![Join the chat at https://gitter.im/BVLC/caffe](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/BVLC/caffe?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nPlease join the [caffe-users group](https://groups.google.com/forum/#!forum/caffe-users) or [gitter chat](https://gitter.im/BVLC/caffe) to ask questions and talk about methods and models.\nFramework development discussions and thorough bug reports are collected on [Issues](https://github.com/BVLC/caffe/issues).\n\nHappy brewing!\n\n## License and Citation\n\nCaffe is released under the [BSD 2-Clause license](https://github.com/BVLC/caffe/blob/master/LICENSE).\nThe BAIR/BVLC reference models are released for unrestricted use.\n\nPlease cite Caffe in your publications if it helps your research:\n\n    @article{jia2014caffe,\n      Author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},\n      Journal = {arXiv preprint arXiv:1408.5093},\n      Title = {Caffe: Convolutional Architecture for Fast Feature Embedding},\n      Year = {2014}\n    }\n', '{"language":"C++","stars":34763,"forks":18588,"watchers":34763,"open_issues":1177,"topics":["deep-learning","machine-learning","vision"],"default_branch":"master","size_kb":76170,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:BVLC:caffe","source_url":"https://github.com/BVLC/caffe"},{"type":"has_code","target_id":"github:BVLC:caffe","source_url":"https://github.com/BVLC/caffe"},{"type":"has_code","target_id":"github:BVLC:caffe","source_url":"https://github.com/BVLC/caffe"},{"type":"has_code","target_id":"github:BVLC:caffe","source_url":"https://github.com/BVLC/caffe"},{"type":"has_code","target_id":"github:BVLC:caffe","source_url":"https://github.com/BVLC/caffe"},{"type":"has_code","target_id":"github:BVLC:caffe","source_url":"https://github.com/BVLC/caffe"}]', NULL, 'NOASSERTION', 'approved', 65, 'b66af93ac0b501bf629634a24af30603', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-BVLC-caffe from https://github.com/BVLC.png
Image converted to WebP: data/images/github-BVLC-caffe.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-paperless-ngx-paperless-ngx', 'github--paperless-ngx--paperless-ngx', 'paperless-ngx', 'paperless-ngx', '<p align="center"> <picture> <source media="(prefers-color-scheme: dark)" srcset="https://github.com/paperless-ngx/paperless-ngx/blob/main/resources/logo/web/png/White%20logo%20-%20no%20background.png" width="50%"> <source media="(prefers-color-scheme: light)" srcset="https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png" width="50%"> <img src="https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%2...', '["angular","archiving","django","dms","document-management","document-management-system","hacktoberfest","machine-learning","ocr","optical-character-recognition","pdf","python"]', 'other', 34730, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/paperless-ngx/paperless-ngx","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '[![ci](https://github.com/paperless-ngx/paperless-ngx/workflows/ci/badge.svg)](https://github.com/paperless-ngx/paperless-ngx/actions)\n[![Crowdin](https://badges.crowdin.net/paperless-ngx/localized.svg)](https://crowdin.com/project/paperless-ngx)\n[![Documentation Status](https://img.shields.io/github/deployments/paperless-ngx/paperless-ngx/github-pages?label=docs)](https://docs.paperless-ngx.com)\n[![codecov](https://codecov.io/gh/paperless-ngx/paperless-ngx/branch/main/graph/badge.svg?token=VK6OUPJ3TY)](https://codecov.io/gh/paperless-ngx/paperless-ngx)\n[![Chat on Matrix](https://matrix.to/img/matrix-badge.svg)](https://matrix.to/#/%23paperlessngx%3Amatrix.org)\n[![demo](https://cronitor.io/badges/ve7ItY/production/W5E_B9jkelG9ZbDiNHUPQEVH3MY.svg)](https://demo.paperless-ngx.com)\n\n<p align="center">\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://github.com/paperless-ngx/paperless-ngx/blob/main/resources/logo/web/png/White%20logo%20-%20no%20background.png" width="50%">\n    <source media="(prefers-color-scheme: light)" srcset="https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png" width="50%">\n    <img src="https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png" width="50%">\n  </picture>\n</p>\n\n<!-- omit in toc -->\n\n# Paperless-ngx\n\nPaperless-ngx is a document management system that transforms your physical documents into a searchable online archive so you can keep, well, _less paper_.\n\nPaperless-ngx is the official successor to the original [Paperless](https://github.com/the-paperless-project/paperless) & [Paperless-ng](https://github.com/jonaswinkler/paperless-ng) projects and is designed to distribute the responsibility of advancing and supporting the project among a team of people. [Consider joining us!](#community-support)\n\nThanks to the generous folks at [DigitalOcean](https://m.do.co/c/8d70b916d462), a demo is available at [demo.paperless-ngx.com](https://demo.paperless-ngx.com) using login `demo` / `demo`. _Note: demo content is reset frequently and confidential information should not be uploaded._\n\n- [Features](#features)\n- [Getting started](#getting-started)\n- [Contributing](#contributing)\n  - [Community Support](#community-support)\n  - [Translation](#translation)\n  - [Feature Requests](#feature-requests)\n  - [Bugs](#bugs)\n- [Related Projects](#related-projects)\n- [Important Note](#important-note)\n\n<p align="right">This project is supported by:<br/>\n  <a href="https://m.do.co/c/8d70b916d462" style="padding-top: 4px; display: block;">\n    <picture>\n      <source media="(prefers-color-scheme: dark)" srcset="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_white.svg" width="140px">\n      <source media="(prefers-color-scheme: light)" srcset="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg" width="140px">\n      <img src="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_black_.svg" width="140px">\n    </picture>\n  </a>\n</p>\n\n# Features\n\n<picture>\n  <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards-dark.png">\n  <source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png">\n  <img src="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png">\n</picture>\n\nA full list of [features](https://docs.paperless-ngx.com/#features) and [screenshots](https://docs.paperless-ngx.com/#screenshots) are available in the [documentation](https://docs.paperless-ngx.com/).\n\n# Getting started\n\nThe easiest way to deploy paperless is `docker compose`. The files in the [`/docker/compose` directory](https://github.com/paperless-ngx/paperless-ngx/tree/main/docker/compose) are configured to pull the image from the GitHub container registry.\n\nIf you''d like to jump right in, you can configure a `docker compose` environment with our install script:\n\n```bash\nbash -c "$(curl -L https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/install-paperless-ngx.sh)"\n```\n\nMore details and step-by-step guides for alternative installation methods can be found in [the documentation](https://docs.paperless-ngx.com/setup/#installation).\n\nMigrating from Paperless-ng is easy, just drop in the new docker image! See the [documentation on migrating](https://docs.paperless-ngx.com/setup/#migrating-to-paperless-ngx) for more details.\n\n<!-- omit in toc -->\n\n### Documentation\n\nThe documentation for Paperless-ngx is available at [https://docs.paperless-ngx.com](https://docs.paperless-ngx.com/).\n\n# Contributing\n\nIf you feel like contributing to the project, please do! Bug fixes, enhancements, visual fixes etc. are always welcome. If you want to implement something big: Please start a discussion about that! The [documentation](https://docs.paperless-ngx.com/development/) has some basic information on how to get started.\n\n## Community Support\n\nPeople interested in continuing the work on paperless-ngx are encouraged to reach out here on github and in the [Matrix Room](https://matrix.to/#/#paperless:matrix.org). If you would like to contribute to the project on an ongoing basis there are multiple [teams](https://github.com/orgs/paperless-ngx/people) (frontend, ci/cd, etc) that could use your help so please reach out!\n\n## Translation\n\nPaperless-ngx is available in many languages that are coordinated on Crowdin. If you want to help out by translating paperless-ngx into your language, please head over to https://crowdin.com/project/paperless-ngx, and thank you! More details can be found in [CONTRIBUTING.md](https://github.com/paperless-ngx/paperless-ngx/blob/main/CONTRIBUTING.md#translating-paperless-ngx).\n\n## Feature Requests\n\nFeature requests can be submitted via [GitHub Discussions](https://github.com/paperless-ngx/paperless-ngx/discussions/categories/feature-requests), you can search for existing ideas, add your own and vote for the ones you care about.\n\n## Bugs\n\nFor bugs please [open an issue](https://github.com/paperless-ngx/paperless-ngx/issues) or [start a discussion](https://github.com/paperless-ngx/paperless-ngx/discussions) if you have questions.\n\n# Related Projects\n\nPlease see [the wiki](https://github.com/paperless-ngx/paperless-ngx/wiki/Related-Projects) for a user-maintained list of related projects and software that is compatible with Paperless-ngx.\n\n# Important Note\n\n> Document scanners are typically used to scan sensitive documents like your social insurance number, tax records, invoices, etc. **Paperless-ngx should never be run on an untrusted host** because information is stored in clear text without encryption. No guarantees are made regarding security (but we do try!) and you use the app at your own risk.\n> **The safest way to run Paperless-ngx is on a local server in your own home with backups in place**.\n', '{"language":"Python","stars":34730,"forks":2181,"watchers":34730,"open_issues":17,"topics":["angular","archiving","django","dms","document-management","document-management-system","hacktoberfest","machine-learning","ocr","optical-character-recognition","pdf"],"default_branch":"dev","size_kb":174262,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:the-paperless-project:paperless","source_url":"https://github.com/the-paperless-project/paperless"},{"type":"has_code","target_id":"github:jonaswinkler:paperless-ng","source_url":"https://github.com/jonaswinkler/paperless-ng"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:orgs:paperless-ngx","source_url":"https://github.com/orgs/paperless-ngx"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"}]', NULL, 'GPL-3.0', 'approved', 65, '3bd53fb6e18997b74676fd644fa183ca', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-paperless-ngx-paperless-ngx from https://github.com/paperless-ngx.png
Image converted to WebP: data/images/github-paperless-ngx-paperless-ngx.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-microsoft-qlib', 'github--microsoft--qlib', 'qlib', 'microsoft', 'Recent released features We are excited to announce the release of **RD-Agent**üì¢, a powerful tool that supports automated factor mining and model optimization in quant investment R&D. RD-Agent is now available on GitHub, and we welcome your starüåü! To learn more, please visit our ‚ôæÔ∏èDemo page. Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent. We have prepared several demo videos for you: | Scenario | Demo video (Engli...', '["algorithmic-trading","auto-quant","deep-learning","finance","fintech","investment","machine-learning","paper","platform","python","quant","quant-dataset","quant-models","quantitative-finance","quantitative-trading","research","research-paper","stock-data","python"]', 'other', 34378, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/microsoft/qlib","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&logoColor=white)](https://pypi.org/project/pyqlib/#files)\n[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)\n[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)\n[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)\n[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)\n[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)\n[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)\n[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n## :newspaper: **What''s NEW!** &nbsp;   :sparkling_heart: \n\nRecent released features\n\n### Introducing <a href="https://github.com/microsoft/RD-Agent"><img src="docs/_static/img/rdagent_logo.png" alt="RD_Agent" style="height: 2em"></a>: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&D\n\nWe are excited to announce the release of **RD-Agent**üì¢, a powerful tool that supports automated factor mining and model optimization in quant investment R&D.\n\nRD-Agent is now available on [GitHub](https://github.com/microsoft/RD-Agent), and we welcome your starüåü!\n\nTo learn more, please visit our [‚ôæÔ∏èDemo page](https://rdagent.azurewebsites.net/). Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.\n\nWe have prepared several demo videos for you:\n| Scenario | Demo video (English) | Demo video (‰∏≠Êñá) |\n| --                      | ------    | ------    |\n| Quant Factor Mining | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=zh) |\n| Quant Factor Mining from reports | [Link](https://rdagent.azurewebsites.net/report_factor?lang=en) | [Link](https://rdagent.azurewebsites.net/report_factor?lang=zh) |\n| Quant Model Optimization | [Link](https://rdagent.azurewebsites.net/model_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/model_loop?lang=zh) |\n\n- üìÉ**Paper**: [R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)\n- üëæ**Code**: https://github.com/microsoft/RD-Agent/\n```BibTeX\n@misc{li2025rdagentquant,\n    title={R\&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},\n    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},\n    year={2025},\n    eprint={2505.15155},\n    archivePrefix={arXiv},\n    primaryClass={cs.AI}\n}\n```\n![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)\n\n***\n\n| Feature | Status |\n| --                      | ------    |\n| [R&D-Agent-Quant](https://arxiv.org/abs/2505.15155) Published | Apply R&D-Agent to Qlib for quant trading | \n| BPQP for End-to-end learning | üìàComing soon!([Under review](https://github.com/microsoft/qlib/pull/1863)) |\n| üî•LLM-driven Auto Quant Factoryüî• | üöÄ Released in [‚ôæÔ∏èRD-Agent](https://github.com/microsoft/RD-Agent) on Aug 8, 2024 |\n| KRNN and Sandwich models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1414/) on May 26, 2023 |\n| Release Qlib v0.9.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.9.0) on Dec 9, 2022 |\n| RL Learning Framework | :hammer: :chart_with_upwards_trend: Released on Nov 10, 2022. [#1332](https://github.com/microsoft/qlib/pull/1332), [#1322](https://github.com/microsoft/qlib/pull/1322), [#1316](https://github.com/microsoft/qlib/pull/1316),[#1299](https://github.com/microsoft/qlib/pull/1299),[#1263](https://github.com/microsoft/qlib/pull/1263), [#1244](https://github.com/microsoft/qlib/pull/1244), [#1169](https://github.com/microsoft/qlib/pull/1169), [#1125](https://github.com/microsoft/qlib/pull/1125), [#1076](https://github.com/microsoft/qlib/pull/1076)|\n| HIST and IGMTF models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1040) on Apr 10, 2022 |\n| Qlib [notebook tutorial](https://github.com/microsoft/qlib/tree/main/examples/tutorial) | üìñ [Released](https://github.com/microsoft/qlib/pull/1037) on Apr 7, 2022 | \n| Ibovespa index data | :rice: [Released](https://github.com/microsoft/qlib/pull/990) on Apr 6, 2022 |\n| Point-in-Time database | :hammer: [Released](https://github.com/microsoft/qlib/pull/343) on Mar 10, 2022 |\n| Arctic Provider Backend & Orderbook data example | :hammer: [Released](https://github.com/microsoft/qlib/pull/744) on Jan 17, 2022 |\n| Meta-Learning-based framework & DDG-DA  | :chart_with_upwards_trend:  :hammer: [Released](https://github.com/microsoft/qlib/pull/743) on Jan 10, 2022 | \n| Planning-based portfolio optimization | :hammer: [Released](https://github.com/microsoft/qlib/pull/754) on Dec 28, 2021 | \n| Release Qlib v0.8.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.8.0) on Dec 8, 2021 |\n| ADD model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/704) on Nov 22, 2021 |\n| ADARNN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/689) on Nov 14, 2021 |\n| TCN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/668) on Nov 4, 2021 |\n| Nested Decision Framework | :hammer: [Released](https://github.com/microsoft/qlib/pull/438) on Oct 1, 2021. [Example](https://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/workflow.py) and [Doc](https://qlib.readthedocs.io/en/latest/component/highfreq.html) |\n| Temporal Routing Adaptor (TRA) | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/531) on July 30, 2021 |\n| Transformer & Localformer | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/508) on July 22, 2021 |\n| Release Qlib v0.7.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.7.0) on July 12, 2021 |\n| TCTS Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/491) on July 1, 2021 |\n| Online serving and automatic model rolling | :hammer:  [Released](https://github.com/microsoft/qlib/pull/290) on May 17, 2021 | \n| DoubleEnsemble Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/286) on Mar 2, 2021 | \n| High-frequency data processing example | :hammer: [Released](https://github.com/microsoft/qlib/pull/257) on Feb 5, 2021  |\n| High-frequency trading example | :chart_with_upwards_trend: [Part of code released](https://github.com/microsoft/qlib/pull/227) on Jan 28, 2021  | \n| High-frequency data(1min) | :rice: [Released](https://github.com/microsoft/qlib/pull/221) on Jan 27, 2021 |\n| Tabnet Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/205) on Jan 22, 2021 |\n\nFeatures released before 2021 are not listed here.\n\n<p align="center">\n  <img src="docs/_static/img/logo/1.png" />\n</p>\n\nQlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.\n\nAn increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market''s complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.\n\nIt contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. \nFor more details, please refer to our paper ["Qlib: An AI-oriented Quantitative Investment Platform"](https://arxiv.org/abs/2009.11189).\n\n\n<table>\n  <tbody>\n    <tr>\n      <th>Frameworks, Tutorial, Data & DevOps</th>\n      <th>Main Challenges & Solutions in Quant Research</th>\n    </tr>\n    <tr>\n      <td>\n        <li><a href="#plans"><strong>Plans</strong></a></li>\n        <li><a href="#framework-of-qlib">Framework of Qlib</a></li>\n        <li><a href="#quick-start">Quick Start</a></li>\n          <ul dir="auto">\n            <li type="circle"><a href="#installation">Installation</a> </li>\n            <li type="circle"><a href="#data-preparation">Data Preparation</a></li>\n            <li type="circle"><a href="#auto-quant-research-workflow">Auto Quant Research Workflow</a></li>\n            <li type="circle"><a href="#building-customized-quant-research-workflow-by-code">Building Customized Quant Research Workflow by Code</a></li></ul>\n        <li><a href="#quant-dataset-zoo"><strong>Quant Dataset Zoo</strong></a></li>\n        <li><a href="#learning-framework">Learning Framework</a></li>\n        <li><a href="#more-about-qlib">More About Qlib</a></li>\n        <li><a href="#offline-mode-and-online-mode">Offline Mode and Online Mode</a>\n        <ul>\n          <li type="circle"><a href="#performance-of-qlib-data-server">Performance of Qlib Data Server</a></li></ul>\n        <li><a href="#related-reports">Related Reports</a></li>\n        <li><a href="#contact-us">Contact Us</a></li>\n        <li><a href="#contributing">Contributing</a></li>\n      </td>\n      <td valign="baseline">\n        <li><a href="#main-challenges--solutions-in-quant-research">Main Challenges &amp; Solutions in Quant Research</a>\n          <ul>\n            <li type="circle"><a href="#forecasting-finding-valuable-signalspatterns">Forecasting: Finding Valuable Signals/Patterns</a>\n              <ul>\n                <li type="disc"><a href="#quant-model-paper-zoo"><strong>Quant Model (Paper) Zoo</strong></a>\n                  <ul>\n                    <li type="circle"><a href="#run-a-single-model">Run a Single Model</a></li>\n                    <li type="circle"><a href="#run-multiple-models">Run Multiple Models</a></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n          <li type="circle"><a href="#adapting-to-market-dynamics">Adapting to Market Dynamics</a></li>\n          <li type="circle"><a href="#reinforcement-learning-modeling-continuous-decisions">Reinforcement Learning: modeling continuous decisions</a></li>\n          </ul>\n        </li>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n# Plans\nNew features under development(order by estimated release time).\nYour feedbacks about the features are very important.\n<!-- | Feature                        | Status      | -->\n<!-- | --                      | ------    | -->\n\n# Framework of Qlib\n\n<div style="align: center">\n<img src="docs/_static/img/framework-abstract.jpg" />\n</div>\n\nThe high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib''s design when getting into nitty gritty).\nThe components are designed as loose-coupled modules, and each component could be used stand-alone.\n\nQlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.\nA strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).\nBy modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).\nAt last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.\n\n\n# Quick Start\n\nThis quick start guide tries to demonstrate\n1. It''s very easy to build a complete Quant research workflow and try your ideas with _Qlib_.\n2. Though with *public data* and *simple models*, machine learning technologies **work very well** in practical Quant investment.\n\nHere is a quick **[demo](https://terminalizer.com/view/3f24561a4470)** shows how to install ``Qlib``, and run LightGBM with ``qrun``. **But**, please make sure you have already prepared the data following the [instruction](#data-preparation).\n\n\n## Installation\n\nThis table demonstrates the supported Python version of `Qlib`:\n|               | install with pip      | install from source  |        plot        |\n| ------------- |:---------------------:|:--------------------:|:------------------:|\n| Python 3.8    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.9    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.10   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.11   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.12   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n\n**Note**: \n1. **Conda** is suggested for managing your Python environment. In some cases, using Python outside of a `conda` environment may result in missing header files, causing the installation failure of certain packages.\n2. Please pay attention that installing cython in Python 3.6 will raise some error when installing ``Qlib`` from source. If users use Python 3.6 on their machines, it is recommended to *upgrade* Python to version 3.8 or higher, or use `conda`''s Python to install ``Qlib`` from source.\n\n### Install with pip\nUsers can easily install ``Qlib`` by pip according to the following command.\n\n```bash\n  pip install pyqlib\n```\n\n**Note**: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.\n\n### Install from source\nAlso, users can install the latest dev version ``Qlib`` by the source code according to the following steps:\n\n* Before installing ``Qlib`` from source, users need to install some dependencies:\n\n  ```bash\n  pip install numpy\n  pip install --upgrade cython\n  ```\n\n* Clone the repository and install ``Qlib`` as follows.\n    ```bash\n    git clone https://github.com/microsoft/qlib.git && cd qlib\n    pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst\n    ```\n\n**Tips**: If you fail to install `Qlib` or run the examples in your environment,  comparing your steps and the [CI workflow](.github/workflows/test_qlib_from_source.yml) may help you find the problem.\n\n**Tips for Mac**: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with ``brew install libomp`` and then run ``pip install .`` to build it successfully. \n\n## Data Preparation\n‚ùó Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.cli.data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.cli.data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can''t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the "qlib" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)\n\n### Checking the health of the data\n  * We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not.\n    ```\n    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data\n    ```\n  * Of course, you can also add some parameters to adjust the test results, such as this.\n    ```\n    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data --missing_data_num 30055 --large_step_threshold_volume 94485 --large_step_threshold_price 20\n    ```\n  * If you want more information about `check_data_health`, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/component/data.html#checking-the-health-of-the-data).\n\n<!-- \n- Run the initialization code and get stock data:\n\n  ```python\n  import qlib\n  from qlib.data import D\n  from qlib.constant import REG_CN\n\n  # Initialization\n  mount_path = "~/.qlib/qlib_data/cn_data"  # target_dir\n  qlib.init(mount_path=mount_path, region=REG_CN)\n\n  # Get stock data by Qlib\n  # Load trading calendar with the given time range and frequency\n  print(D.calendar(start_time=''2010-01-01'', end_time=''2017-12-31'', freq=''day'')[:2])\n\n  # Parse a given market name into a stockpool config\n  instruments = D.instruments(''csi500'')\n  print(D.list_instruments(instruments=instruments, start_time=''2010-01-01'', end_time=''2017-12-31'', as_list=True)[:6])\n\n  # Load features of certain instruments in given time range\n  instruments = [''SH600000'']\n  fields = [''$close'', ''$volume'', ''Ref($close, 1)'', ''Mean($close, 3)'', ''$high-$low'']\n  print(D.features(instruments, fields, start_time=''2010-01-01'', end_time=''2017-12-31'', freq=''day'').head())\n  ```\n -->\n\n## Docker images\n1. Pulling a docker image from a docker hub repository\n    ```bash\n    docker pull pyqlib/qlib_image_stable:stable\n    ```\n2. Start a new Docker container\n    ```bash\n    docker run -it --name <container name> -v <Mounted local directory>:/app pyqlib/qlib_image_stable:stable\n    ```\n3. At this point you are in the docker environment and can run the qlib scripts. An example:\n    ```bash\n    >>> python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn\n    >>> python qlib/cli/run.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n4. Exit the container\n    ```bash\n    >>> exit\n    ```\n5. Restart the container\n    ```bash\n    docker start -i -a <container name>\n    ```\n6. Stop the container\n    ```bash\n    docker stop <container name>\n    ```\n7. Delete the container\n    ```bash\n    docker rm <container name>\n    ```\n8. If you want to know more information, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/developer/how_to_build_image.html).\n\n## Auto Quant Research Workflow\nQlib provides a tool named `qrun` to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps: \n\n1. Quant Research Workflow: Run  `qrun` with lightgbm workflow config ([workflow_config_lightgbm_Alpha158.yaml](examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml) as following.\n    ```bash\n      cd examples  # Avoid running program under the directory contains `qlib`\n      qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    If users want to use `qrun` under debug mode, please use the following command:\n    ```bash\n    python -m pdb qlib/cli/run.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    The result of `qrun` is as follows, please refer to [docs](https://qlib.readthedocs.io/en/latest/component/strategy.html#result) for more explanations about the result. \n\n    ```bash\n\n    ''The following are analysis results of the excess return without cost.''\n                           risk\n    mean               0.000708\n    std                0.005626\n    annualized_return  0.178316\n    information_ratio  1.996555\n    max_drawdown      -0.081806\n    ''The following are analysis results of the excess return with cost.''\n                           risk\n    mean               0.000512\n    std                0.005626\n    annualized_return  0.128982\n    information_ratio  1.444287\n    max_drawdown      -0.091078\n    ```\n    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).\n\n2. Graphical Reports Analysis: First, run `python -m pip install .[analysis]` to install the required dependencies. Then run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports. \n    - Forecasting signal (model prediction) analysis\n      - Cumulative Return of groups\n      ![Cumulative Return](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_cumulative_return.png)\n      - Return distribution\n      ![long_short](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_long_short.png)\n      - Information Coefficient (IC)\n      ![Information Coefficient](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_IC.png)\n      ![Monthly IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_monthly_IC.png)\n      ![IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_NDQ.png)\n      - Auto Correlation of forecasting signal (model prediction)\n      ![Auto Correlation](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_auto_correlation.png)\n\n    - Portfolio analysis\n      - Backtest return\n      ![Report](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/report.png)\n      <!-- \n      - Score IC\n      ![Score IC](docs/_static/img/score_ic.png)\n      - Cumulative Return\n      ![Cumulative Return](docs/_static/img/cumulative_return.png)\n      - Risk Analysis\n      ![Risk Analysis](docs/_static/img/risk_analysis.png)\n      - Rank Label\n      ![Rank Label](docs/_static/img/rank_label.png)\n      -->\n   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results\n\n## Building Customized Quant Research Workflow by Code\nThe automatic workflow may not suit the research workflow of all Quant researchers. To support a flexible Quant research workflow, Qlib also provides a modularized interface to allow researchers to build their own workflow by code. [Here](examples/workflow_by_code.ipynb) is a demo for customized Quant research workflow by code.\n\n# Main Challenges & Solutions in Quant Research\nQuant investment is a very unique scenario with lots of key challenges to be solved.\nCurrently, Qlib provides some solutions for several of them.\n\n## Forecasting: Finding Valuable Signals/Patterns\nAccurate forecasting of the stock price trend is a very important part to construct profitable portfolios.\nHowever, huge amount of data with various formats in the financial market which make it challenging to build forecasting models.\n\nAn increasing number of SOTA Quant research works/papers, which focus on building forecasting models to mine valuable signals/patterns in complex financial data, are released in `Qlib`\n\n\n### [Quant Model (Paper) Zoo](examples/benchmarks)\n\nHere is a list of models built on `Qlib`.\n- [GBDT based on XGBoost (Tianqi Chen, et al. KDD 2016)](examples/benchmarks/XGBoost/)\n- [GBDT based on LightGBM (Guolin Ke, et al. NIPS 2017)](examples/benchmarks/LightGBM/)\n- [GBDT based on Catboost (Liudmila Prokhorenkova, et al. NIPS 2018)](examples/benchmarks/CatBoost/)\n- [MLP based on pytorch](examples/benchmarks/MLP/)\n- [LSTM based on pytorch (Sepp Hochreiter, et al. Neural computation 1997)](examples/benchmarks/LSTM/)\n- [GRU based on pytorch (Kyunghyun Cho, et al. 2014)](examples/benchmarks/GRU/)\n- [ALSTM based on pytorch (Yao Qin, et al. IJCAI 2017)](examples/benchmarks/ALSTM)\n- [GATs based on pytorch (Petar Velickovic, et al. 2017)](examples/benchmarks/GATs/)\n- [SFM based on pytorch (Liheng Zhang, et al. KDD 2017)](examples/benchmarks/SFM/)\n- [TFT based on tensorflow (Bryan Lim, et al. International Journal of Forecasting 2019)](examples/benchmarks/TFT/)\n- [TabNet based on pytorch (Sercan O. Arik, et al. AAAI 2019)](examples/benchmarks/TabNet/)\n- [DoubleEnsemble based on LightGBM (Chuheng Zhang, et al. ICDM 2020)](examples/benchmarks/DoubleEnsemble/)\n- [TCTS based on pytorch (Xueqing Wu, et al. ICML 2021)](examples/benchmarks/TCTS/)\n- [Transformer based on pytorch (Ashish Vaswani, et al. NeurIPS 2017)](examples/benchmarks/Transformer/)\n- [Localformer based on pytorch (Juyong Jiang, et al.)](examples/benchmarks/Localformer/)\n- [TRA based on pytorch (Hengxu, Dong, et al. KDD 2021)](examples/benchmarks/TRA/)\n- [TCN based on pytorch (Shaojie Bai, et al. 2018)](examples/benchmarks/TCN/)\n- [ADARNN based on pytorch (YunTao Du, et al. 2021)](examples/benchmarks/ADARNN/)\n- [ADD based on pytorch (Hongshun Tang, et al.2020)](examples/benchmarks/ADD/)\n- [IGMTF based on pytorch (Wentao Xu, et al.2021)](examples/benchmarks/IGMTF/)\n- [HIST based on pytorch (Wentao Xu, et al.2021)](examples/benchmarks/HIST/)\n- [KRNN based on pytorch](examples/benchmarks/KRNN/)\n- [Sandwich based on pytorch](examples/benchmarks/Sandwich/)\n\nYour PR of new Quant models is highly welcomed.\n\nThe performance of each model on the `Alpha158` and `Alpha360` datasets can be found [here](examples/benchmarks/README.md).\n\n### Run a single model\nAll the models listed above are runnable with ``Qlib``. Users can find the config files we provide and some details about the model through the [benchmarks](examples/benchmarks) folder. More information can be retrieved at the model files listed above.\n\n`Qlib` provides three different ways to run a single model, users can pick the one that fits their cases best:\n- Users can use the tool `qrun` mentioned above to run a model''s workflow based from a config file.\n- Users can create a `workflow_by_code` python script based on the [one](examples/workflow_by_code.py) listed in the `examples` folder.\n\n- Users can use the script [`run_all_model.py`](examples/run_all_model.py) listed in the `examples` folder to run a model. Here is an example of the specific shell command to be used: `python run_all_model.py run --models=lightgbm`, where the `--models` arguments can take any number of models listed above(the available models can be found  in [benchmarks](examples/benchmarks/)). For more use cases, please refer to the file''s [docstrings](examples/run_all_model.py).\n    - **NOTE**: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of `tensorflow==1.15.0`)\n\n### Run multiple models\n`Qlib` also provides a script [`run_all_model.py`](examples/run_all_model.py) which can run multiple models for several iterations. (**Note**: the script only support *Linux* for now. Other OS will be supported in the future. Besides, it doesn''t support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)\n\nThe script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as `IC` and `backtest` results will be generated and stored.\n\nHere is an example of running all the models for 10 iterations:\n```python\npython run_all_model.py run 10\n```\n\nIt also provides the API to run specific models at once. For more use cases, please refer to the file''s [docstrings](examples/run_all_model.py). \n\n### Break change\nIn `pandas`, `group_key` is one of the parameters of the `groupby` method. From version 1.5 to 2.0 of `pandas`, the default value of `group_key` has been changed from `no default` to `True`, which will cause qlib to report an error during operation. So we set `group_key=False`, but it doesn''t guarantee that some programmes will run correctly, including:\n* qlib\examples\rl_order_execution\scripts\gen_training_orders.py\n* qlib\examples\benchmarks\TRA\src\dataset.MTSDatasetH.py\n* qlib\examples\benchmarks\TFT\tft.py\n\n\n\n## [Adapting to Market Dynamics](examples/benchmarks_dynamic)\n\nDue to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data.\nSo adapting the forecasting models/strategies to market dynamics is very important to the model/strategies'' performance.\n\nHere is a list of solutions built on `Qlib`.\n- [Rolling Retraining](examples/benchmarks_dynamic/baseline/)\n- [DDG-DA on pytorch (Wendi, et al. AAAI 2022)](examples/benchmarks_dynamic/DDG-DA/)\n\n##  Reinforcement Learning: modeling continuous decisions\nQlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.\n\nHere is a list of solutions built on `Qlib` categorized by scenarios.\n\n### [RL for order execution](examples/rl_order_execution)\n[Here](https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution) is the introduction of this scenario.  All the methods below are compared [here](examples/rl_order_execution).\n- [TWAP](examples/rl_order_execution/exp_configs/backtest_twap.yml)\n- [PPO: "An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization", IJCAL 2020](examples/rl_order_execution/exp_configs/backtest_ppo.yml)\n- [OPDS: "Universal Trading for Order Execution with Oracle Policy Distillation", AAAI 2021](examples/rl_order_execution/exp_configs/backtest_opds.yml)\n\n# Quant Dataset Zoo\nDataset plays a very important role in Quant. Here is a list of the datasets built on `Qlib`:\n\n| Dataset                                    | US Market | China Market |\n| --                                         | --        | --           |\n| [Alpha360](./qlib/contrib/data/handler.py) |  ‚àö        |  ‚àö           |\n| [Alpha158](./qlib/contrib/data/handler.py) |  ‚àö        |  ‚àö           |\n\n[Here](https://qlib.readthedocs.io/en/latest/advanced/alpha.html) is a tutorial to build dataset with `Qlib`.\nYour PR to build new Quant dataset is highly welcomed.\n\n\n# Learning Framework\nQlib is high customizable and a lot of its components are learnable.\nThe learnable components are instances of `Forecast Model` and `Trading Agent`. They are learned based on the `Learning Framework` layer and then applied to multiple scenarios in `Workflow` layer.\nThe learning framework leverages the `Workflow` layer as well(e.g. sharing `Information Extractor`, creating environments based on `Execution Env`).\n\nBased on learning paradigms, they can be categorized into reinforcement learning and supervised learning.\n- For supervised learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/model.html).\n- For reinforcement learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/rl.html). Qlib''s RL learning framework leverages `Execution Env` in `Workflow` layer to create environments.  It''s worth noting that `NestedExecutor` is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).\n\n\n# More About Qlib\nIf you want to have a quick glance at the most frequently used components of qlib, you can try notebooks [here](examples/tutorial/).\n\nThe detailed documents are organized in [docs](docs/).\n[Sphinx](http://www.sphinx-doc.org) and the readthedocs theme is required to build the documentation in html formats. \n```bash\ncd docs/\nconda install sphinx sphinx_rtd_theme -y\n# Otherwise, you can install them with pip\n# pip install sphinx sphinx_rtd_theme\nmake html\n```\nYou can also view the [latest document](http://qlib.readthedocs.io/) online directly.\n\nQlib is in active and continuing development. Our plan is in the roadmap, which is managed as a [github project](https://github.com/microsoft/qlib/projects/1).\n\n\n\n# Offline Mode and Online Mode\nThe data server of Qlib can either deployed as `Offline` mode or `Online` mode. The default mode is offline mode.\n\nUnder `Offline` mode, the data will be deployed locally. \n\nUnder `Online` mode, the data will be deployed as a shared data service. The data and their cache will be shared by all the clients. The data retrieval performance is expected to be improved due to a higher rate of cache hits. It will consume less disk space, too. The documents of the online mode can be found in [Qlib-Server](https://qlib-server.readthedocs.io/). The online mode can be deployed automatically with [Azure CLI based scripts](https://qlib-server.readthedocs.io/en/latest/build.html#one-click-deployment-in-azure). The source code of online data server can be found in [Qlib-Server repository](https://github.com/microsoft/qlib-server).\n\n## Performance of Qlib Data Server\nThe performance of data processing is important to data-driven methods like AI technologies. As an AI-oriented platform, Qlib provides a solution for data storage and data processing. To demonstrate the performance of Qlib data server, we\ncompare it with several other data storage solutions. \n\nWe evaluate the performance of several storage solutions by finishing the same task,\nwhich creates a dataset (14 features/factors) from the basic OHLCV daily data of a stock market (800 stocks each day from 2007 to 2020). The task involves data queries and processing.\n\n|                         | HDF5      | MySQL     | MongoDB   | InfluxDB  | Qlib -E -D  | Qlib +E -D   | Qlib +E +D  |\n| --                      | ------    | ------    | --------  | --------- | ----------- | ------------ | ----------- |\n| Total (1CPU) (seconds)  | 184.4¬±3.7 | 365.3¬±7.5 | 253.6¬±6.7 | 368.2¬±3.6 | 147.0¬±8.8   | 47.6¬±1.0     | **7.4¬±0.3** |\n| Total (64CPU) (seconds) |           |           |           |           | 8.8¬±0.6     | **4.2¬±0.2**  |             |\n* `+(-)E` indicates with (out) `ExpressionCache`\n* `+(-)D` indicates with (out) `DatasetCache`\n\nMost general-purpose databases take too much time to load data. After looking into the underlying implementation, we find that data go through too many layers of interfaces and unnecessary format transformations in general-purpose database solutions.\nSuch overheads greatly slow down the data loading process.\nQlib data are stored in a compact format, which is efficient to be combined into arrays for scientific computation.\n\n# Related Reports\n- [Guide To Qlib: Microsoft‚Äôs AI Investment Platform](https://analyticsindiamag.com/qlib/)\n- [ÂæÆËΩØ‰πüÊêûAIÈáèÂåñÂπ≥Âè∞ÔºüËøòÊòØÂºÄÊ∫êÁöÑÔºÅ](https://mp.weixin.qq.com/s/47bP5YwxfTp2uTHjUBzJQQ)\n- [ÂæÆÁüøQlibÔºö‰∏öÂÜÖÈ¶ñ‰∏™AIÈáèÂåñÊäïËµÑÂºÄÊ∫êÂπ≥Âè∞](https://mp.weixin.qq.com/s/vsJv7lsgjEi-ALYUz4CvtQ)\n\n# Contact Us\n- If you have any issues, please create issue [here](https://github.com/microsoft/qlib/issues/new/choose) or send messages in [gitter](https://gitter.im/Microsoft/qlib).\n- If you want to make contributions to `Qlib`, please [create pull requests](https://github.com/microsoft/qlib/compare). \n- For other reasons, you are welcome to contact us by email([qlib@microsoft.com](mailto:qlib@microsoft.com)).\n  - We are recruiting new members(both FTEs and interns), your resumes are welcome!\n\nJoin IM discussion groups:\n|[Gitter](https://gitter.im/Microsoft/qlib)|\n|----|\n|![image](https://github.com/microsoft/qlib/blob/main/docs/_static/img/qrcode/gitter_qr.png)|\n\n# Contributing\nWe appreciate all contributions and thank all the contributors!\n<a href="https://github.com/microsoft/qlib/graphs/contributors"><img src="https://contrib.rocks/image?repo=microsoft/qlib" /></a>\n\nBefore we released Qlib as an open-source project on Github in Sep 2020, Qlib is an internal project in our group. Unfortunately, the internal commit history is not kept. A lot of members in our group have also contributed a lot to Qlib, which includes Ruihua Wang, Yinda Zhang, Haisu Yu, Shuyu Wang, Bochen Pang, and [Dong Zhou](https://github.com/evanzd/evanzd). Especially thanks to [Dong Zhou](https://github.com/evanzd/evanzd) due to his initial version of Qlib.\n\n## Guidance\n\nThis project welcomes contributions and suggestions.  \n**Here are some \n[code standards and development guidance](docs/developer/code_standard_and_dev_guide.rst) for submiting a pull request.**\n\nMaking contributions is not a hard thing. Solving an issue(maybe just answering a question raised in [issues list](https://github.com/microsoft/qlib/issues) or [gitter](https://gitter.im/Microsoft/qlib)), fixing/issuing a bug, improving the documents and even fixing a typo are important contributions to Qlib.\n\nFor example, if you want to contribute to Qlib''s document/code, you can follow the steps in the figure below.\n<p align="center">\n  <img src="https://github.com/demon143/qlib/blob/main/docs/_static/img/change%20doc.gif" />\n</p>\n\nIf you don''t know how to start to contribute, you can refer to the following examples.\n| Type | Examples |\n| -- | -- |\n| Solving issues | [Answer a question](https://github.com/microsoft/qlib/issues/749);  [issuing](https://github.com/microsoft/qlib/issues/765) or [fixing](https://github.com/microsoft/qlib/pull/792) a bug |\n| Docs | [Improve docs quality](https://github.com/microsoft/qlib/pull/797/files) ;  [Fix a typo](https://github.com/microsoft/qlib/pull/774) | \n| Feature |  Implement a [requested feature](https://github.com/microsoft/qlib/projects) like [this](https://github.com/microsoft/qlib/pull/754); [Refactor interfaces](https://github.com/microsoft/qlib/pull/539/files) |\n| Dataset | [Add a dataset](https://github.com/microsoft/qlib/pull/733) | \n| Models |  [Implement a new model](https://github.com/microsoft/qlib/pull/689), [some instructions to contribute models](https://github.com/microsoft/qlib/tree/main/examples/benchmarks#contributing) |\n\n[Good first issues](https://github.com/microsoft/qlib/labels/good%20first%20issue) are labelled to indicate that they are easy to start your contributions.\n\nYou can find some impefect implementation in Qlib by  `rg ''TODO|FIXME'' qlib`\n \nIf you would like to become one of Qlib''s maintainers to contribute more (e.g. help merge PR, triage issues), please contact us by email([qlib@microsoft.com](mailto:qlib@microsoft.com)).  We are glad to help to upgrade your permission.\n\n## License\nMost contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe right to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n', '{"language":"Python","stars":34378,"forks":5342,"watchers":34378,"open_issues":309,"topics":["algorithmic-trading","auto-quant","deep-learning","finance","fintech","investment","machine-learning","paper","platform","python","quant","quant-dataset","quant-models","quantitative-finance","quantitative-trading","research","research-paper","stock-data"],"default_branch":"main","size_kb":17928,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:RD-Agent\"><img","source_url":"https://github.com/microsoft/RD-Agent\"><img"},{"type":"has_code","target_id":"github:microsoft:RD-Agent","source_url":"https://github.com/microsoft/RD-Agent"},{"type":"has_code","target_id":"github:microsoft:RD-Agent","source_url":"https://github.com/microsoft/RD-Agent"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:RD-Agent","source_url":"https://github.com/microsoft/RD-Agent"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib.git","source_url":"https://github.com/microsoft/qlib.git"},{"type":"has_code","target_id":"github:chenditc:investment_data","source_url":"https://github.com/chenditc/investment_data"},{"type":"has_code","target_id":"github:chenditc:investment_data","source_url":"https://github.com/chenditc/investment_data"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib-server","source_url":"https://github.com/microsoft/qlib-server"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:evanzd:evanzd","source_url":"https://github.com/evanzd/evanzd"},{"type":"has_code","target_id":"github:evanzd:evanzd","source_url":"https://github.com/evanzd/evanzd"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:demon143:qlib","source_url":"https://github.com/demon143/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"}]', NULL, 'MIT', 'approved', 80, '269f3f31daa446f6617316d22dab7592', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-microsoft-qlib from https://github.com/microsoft.png
Image converted to WebP: data/images/github-microsoft-qlib.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-CMU-Perceptual-Computing-Lab-openpose', 'github--cmu-perceptual-computing-lab--openpose', 'openpose', 'CMU-Perceptual-Computing-Lab', '<div align="center"> <img src=".github/Logo_main_black.png" width="300"> </div> ----------------- | **Build Type** | | | | | :---: | :---: | :---: | :---: | | **Build Status** | | | | **OpenPose** has represented the **first real-time multi-person system to jointly detect human body, hand, facial, and foot keypoints (in total 135 keypoints) on single images**. It is **authored by** **Gin√©s Hidalgo**, **Zhe Cao**, **Tomas Simon**, **Shih-En Wei**, **Yaadhav Raaj**, **Hanbyul Joo**, **and** **Y...', '["caffe","computer-vision","cpp","cvpr-2017","deep-learning","face","foot-estimation","hand-estimation","human-behavior-understanding","human-pose","human-pose-estimation","keypoint-detection","keypoints","machine-learning","multi-person","opencv","openpose","pose","pose-estimation","real-time","c++"]', 'other', 33527, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n    <img src=".github/Logo_main_black.png" width="300">\n</div>\n\n-----------------\n\n| **Build Type**   |`Linux`           |`MacOS`           |`Windows`         |\n| :---:            | :---:            | :---:            | :---:            |\n| **Build Status** | [![Status](https://github.com/CMU-Perceptual-Computing-Lab/openpose/workflows/CI/badge.svg)](https://github.com/CMU-Perceptual-Computing-Lab/openpose/actions) | [![Status](https://github.com/CMU-Perceptual-Computing-Lab/openpose/workflows/CI/badge.svg)](https://github.com/CMU-Perceptual-Computing-Lab/openpose/actions) | [![Status](https://ci.appveyor.com/api/projects/status/5leescxxdwen77kg/branch/master?svg=true)](https://ci.appveyor.com/project/gineshidalgo99/openpose/branch/master) |\n\n[**OpenPose**](https://github.com/CMU-Perceptual-Computing-Lab/openpose) has represented the **first real-time multi-person system to jointly detect human body, hand, facial, and foot keypoints (in total 135 keypoints) on single images**.\n\nIt is **authored by** [**Gin√©s Hidalgo**](https://www.gineshidalgo.com), [**Zhe Cao**](https://people.eecs.berkeley.edu/~zhecao), [**Tomas Simon**](http://www.cs.cmu.edu/~tsimon), [**Shih-En Wei**](https://scholar.google.com/citations?user=sFQD3k4AAAAJ&hl=en), [**Yaadhav Raaj**](https://www.raaj.tech), [**Hanbyul Joo**](https://jhugestar.github.io), **and** [**Yaser Sheikh**](http://www.cs.cmu.edu/~yaser). It is **maintained by** [**Gin√©s Hidalgo**](https://www.gineshidalgo.com) **and** [**Yaadhav Raaj**](https://www.raaj.tech). OpenPose would not be possible without the [**CMU Panoptic Studio dataset**](http://domedb.perception.cs.cmu.edu). We would also like to thank all the people who [have helped OpenPose in any way](doc/09_authors_and_contributors.md).\n\n\n\n<p align="center">\n    <img src=".github/media/pose_face_hands.gif" width="480">\n    <br>\n    <sup>Authors <a href="https://www.gineshidalgo.com" target="_blank">Gin√©s Hidalgo</a> (left) and <a href="https://jhugestar.github.io" target="_blank">Hanbyul Joo</a> (right) in front of the <a href="http://domedb.perception.cs.cmu.edu" target="_blank">CMU Panoptic Studio</a></sup>\n</p>\n\n\n\n## Contents\n1. [Results](#results)\n2. [Features](#features)\n3. [Related Work](#related-work)\n4. [Installation](#installation)\n5. [Quick Start Overview](#quick-start-overview)\n6. [Send Us Feedback!](#send-us-feedback)\n7. [Citation](#citation)\n8. [License](#license)\n\n\n\n## Results\n### Whole-body (Body, Foot, Face, and Hands) 2D Pose Estimation\n<p align="center">\n    <img src=".github/media/dance_foot.gif" width="300">\n    <img src=".github/media/pose_face.gif" width="300">\n    <img src=".github/media/pose_hands.gif" width="300">\n    <br>\n    <sup>Testing OpenPose: (Left) <a href="https://www.youtube.com/watch?v=2DiQUX11YaY" target="_blank"><i>Crazy Uptown Funk flashmob in Sydney</i></a> video sequence. (Center and right) Authors <a href="https://www.gineshidalgo.com" target="_blank">Gin√©s Hidalgo</a> and <a href="http://www.cs.cmu.edu/~tsimon" target="_blank">Tomas Simon</a> testing face and hands</sup>\n</p>\n\n### Whole-body 3D Pose Reconstruction and Estimation\n<p align="center">\n    <img src=".github/media/openpose3d.gif" width="360">\n    <br>\n    <sup><a href="https://ziutinyat.github.io/" target="_blank">Tianyi Zhao</a> testing the OpenPose 3D Module</a></sup>\n</p>\n\n### Unity Plugin\n<p align="center">\n    <img src=".github/media/unity_main.png" width="300">\n    <img src=".github/media/unity_body_foot.png" width="300">\n    <img src=".github/media/unity_hand_face.png" width="300">\n    <br>\n    <sup><a href="https://ziutinyat.github.io/" target="_blank">Tianyi Zhao</a> and <a href="https://www.gineshidalgo.com" target="_blank">Gin√©s Hidalgo</a> testing the <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin" target="_blank">OpenPose Unity Plugin</a></sup>\n</p>\n\n### Runtime Analysis\nWe show an inference time comparison between the 3 available pose estimation libraries (same hardware and conditions): OpenPose, Alpha-Pose (fast Pytorch version), and Mask R-CNN. The OpenPose runtime is constant, while the runtime of Alpha-Pose and Mask R-CNN grow linearly with the number of people. More details [**here**](https://arxiv.org/abs/1812.08008).\n\n<p align="center">\n    <img src=".github/media/openpose_vs_competition.png" width="360">\n</p>\n\n\n\n## Features\n**Main Functionality**:\n- **2D real-time multi-person keypoint detection**:\n    - 15, 18 or **25-keypoint body/foot keypoint estimation**, including **6 foot keypoints**. **Runtime invariant to number of detected people**.\n    - **2x21-keypoint hand keypoint estimation**. **Runtime depends on number of detected people**. See [**OpenPose Training**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_train) for a runtime invariant alternative.\n    - **70-keypoint face keypoint estimation**. **Runtime depends on number of detected people**. See [**OpenPose Training**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_train) for a runtime invariant alternative.\n- [**3D real-time single-person keypoint detection**](doc/advanced/3d_reconstruction_module.md):\n    - 3D triangulation from multiple single views.\n    - Synchronization of Flir cameras handled.\n    - Compatible with Flir/Point Grey cameras.\n- [**Calibration toolbox**](doc/advanced/calibration_module.md): Estimation of distortion, intrinsic, and extrinsic camera parameters.\n- **Single-person tracking** for further speedup or visual smoothing.\n\n**Input**: Image, video, webcam, Flir/Point Grey, IP camera, and support to add your own custom input source (e.g., depth camera).\n\n**Output**: Basic image + keypoint display/saving (PNG, JPG, AVI, ...), keypoint saving (JSON, XML, YML, ...), keypoints as array class, and support to add your own custom output code (e.g., some fancy UI).\n\n**OS**: Ubuntu (20, 18, 16, 14), Windows (10, 8), Mac OSX, Nvidia TX2.\n\n**Hardware compatibility**: CUDA (Nvidia GPU), OpenCL (AMD GPU), and non-GPU (CPU-only) versions.\n\n**Usage Alternatives**:\n- [**Command-line demo**](doc/01_demo.md) for built-in functionality.\n- [**C++ API**](doc/04_cpp_api.md/) and [**Python API**](doc/03_python_api.md) for custom functionality. E.g., adding your custom inputs, pre-processing, post-posprocessing, and output steps.\n\nFor further details, check the [major released features](doc/07_major_released_features.md) and [release notes](doc/08_release_notes.md) docs.\n\n\n\n## Related Work\n- [**OpenPose training code**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_train)\n- [**OpenPose foot dataset**](https://cmu-perceptual-computing-lab.github.io/foot_keypoint_dataset/)\n- [**OpenPose Unity Plugin**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin)\n- OpenPose papers published in **IEEE TPAMI and CVPR**. Cite them in your publications if OpenPose helps your research! (Links and more details in the [Citation](#citation) section below).\n\n\n\n## Installation\nIf you want to use OpenPose without installing or writing any code, simply [download and use the latest Windows portable version of OpenPose](doc/installation/0_index.md#windows-portable-demo)!\n\nOtherwise, you could [build OpenPose from source](doc/installation/0_index.md#compiling-and-running-openpose-from-source). See the [installation doc](doc/installation/0_index.md) for all the alternatives.\n\n\n\n## Quick Start Overview\nSimply use the OpenPose Demo from your favorite command-line tool (e.g., Windows PowerShell or Ubuntu Terminal). E.g., this example runs OpenPose on your webcam and displays the body keypoints:\n```\n# Ubuntu\n./build/examples/openpose/openpose.bin\n```\n```\n:: Windows - Portable Demo\nbin\OpenPoseDemo.exe --video examples\media\video.avi\n```\n\nYou can also add any of the available flags in any order. E.g., the following example runs on a video (`--video {PATH}`), enables face (`--face`) and hands (`--hand`), and saves the output keypoints on JSON files on disk (`--write_json {PATH}`).\n```\n# Ubuntu\n./build/examples/openpose/openpose.bin --video examples/media/video.avi --face --hand --write_json output_json_folder/\n```\n```\n:: Windows - Portable Demo\nbin\OpenPoseDemo.exe --video examples\media\video.avi --face --hand --write_json output_json_folder/\n```\n\nOptionally, you can also extend OpenPose''s functionality from its Python and C++ APIs. After [installing](doc/installation/0_index.md) OpenPose, check its [official doc](doc/00_index.md) for a quick overview of all the alternatives and tutorials.\n\n\n\n## Send Us Feedback!\nOur library is open source for research purposes, and we want to improve it! So let us know (create a new GitHub issue or pull request, email us, etc.) if you...\n1. Find/fix any bug (in functionality or speed) or know how to speed up or improve any part of OpenPose.\n2. Want to add/show some cool functionality/demo/project made on top of OpenPose. We can add your project link to our [Community-based Projects](doc/10_community_projects.md) section or even integrate it with OpenPose!\n\n\n\n## Citation\nPlease cite these papers in your publications if OpenPose helps your research. All of OpenPose is based on [OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields](https://arxiv.org/abs/1812.08008), while the hand and face detectors also use [Hand Keypoint Detection in Single Images using Multiview Bootstrapping](https://arxiv.org/abs/1704.07809) (the face detector was trained using the same procedure as the hand detector).\n\n    @article{8765346,\n      author = {Z. {Cao} and G. {Hidalgo Martinez} and T. {Simon} and S. {Wei} and Y. A. {Sheikh}},\n      journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n      title = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},\n      year = {2019}\n    }\n\n    @inproceedings{simon2017hand,\n      author = {Tomas Simon and Hanbyul Joo and Iain Matthews and Yaser Sheikh},\n      booktitle = {CVPR},\n      title = {Hand Keypoint Detection in Single Images using Multiview Bootstrapping},\n      year = {2017}\n    }\n\n    @inproceedings{cao2017realtime,\n      author = {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},\n      booktitle = {CVPR},\n      title = {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},\n      year = {2017}\n    }\n\n    @inproceedings{wei2016cpm,\n      author = {Shih-En Wei and Varun Ramakrishna and Takeo Kanade and Yaser Sheikh},\n      booktitle = {CVPR},\n      title = {Convolutional pose machines},\n      year = {2016}\n    }\n\nPaper links:\n- OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields:\n    - [IEEE TPAMI](https://ieeexplore.ieee.org/document/8765346)\n    - [ArXiv](https://arxiv.org/abs/1812.08008)\n- [Hand Keypoint Detection in Single Images using Multiview Bootstrapping](https://arxiv.org/abs/1704.07809)\n- [Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields](https://arxiv.org/abs/1611.08050)\n- [Convolutional Pose Machines](https://arxiv.org/abs/1602.00134)\n\n\n\n## License\nOpenPose is freely available for free non-commercial use, and may be redistributed under these conditions. Please, see the [license](./LICENSE) for further details. Interested in a commercial license? Check this [FlintBox link](https://cmu.flintbox.com/#technologies/b820c21d-8443-4aa2-a49f-8919d93a8740). For commercial queries, use the `Contact` section from the [FlintBox link](https://cmu.flintbox.com/#technologies/b820c21d-8443-4aa2-a49f-8919d93a8740) and also send a copy of that message to [Yaser Sheikh](mailto:yaser@cs.cmu.edu).\n', '{"language":"C++","stars":33527,"forks":8046,"watchers":33527,"open_issues":360,"topics":["caffe","computer-vision","cpp","cvpr-2017","deep-learning","face","foot-estimation","hand-estimation","human-behavior-understanding","human-pose","human-pose-estimation","keypoint-detection","keypoints","machine-learning","multi-person","opencv","openpose","pose","pose-estimation","real-time"],"default_branch":"master","size_kb":86496,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose"},{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose"},{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose"},{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose"},{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose"},{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose_unity_plugin\"","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin\""},{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose_train","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose_train"},{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose_train","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose_train"},{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose_train","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose_train"},{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose_unity_plugin","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin"}]', NULL, 'NOASSERTION', 'approved', 80, '0688fa6a237c82539265bb9257f66a6f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-CMU-Perceptual-Computing-Lab-openpose from https://github.com/CMU-Perceptual-Computing-Lab.png
Image converted to WebP: data/images/github-CMU-Perceptual-Computing-Lab-openpose.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-TheAlgorithms-C-Plus-Plus', 'github--thealgorithms--c-plus-plus', 'C-Plus-Plus', 'TheAlgorithms', '<!-- the suffix in the above line is required for doxygen to consider this as the index page of the generated documentation site --> !GitHub repo size This repository is a collection of open-source implementation of a variety of algorithms implemented in C++ and licensed under MIT License. These algorithms span a variety of topics from computer science, mathematics and statistics, data science, machine learning, engineering, etc.. The implementations and the associated documentation are meant...', '["algorithm","algorithm-competitions","algorithms-implemented","artificial-intelligence-algorithms","computer-science","cpp","data-structures","educational","instructor-materials","interview-preparation","interview-questions","machine-learning","machine-learning-algorithms","mathematics","search","sort","c++"]', 'other', 33486, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# The Algorithms - C++ # {#mainpage}\n\n<!-- the suffix in the above line is required for doxygen to consider this as the index page of the generated documentation site -->\n\n[![Gitpod Ready-to-Code](https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/TheAlgorithms/C-Plus-Plus)\n[![CodeQL CI](https://github.com/TheAlgorithms/C-Plus-Plus/actions/workflows/codeql.yml/badge.svg)](https://github.com/TheAlgorithms/C-Plus-Plus/actions/workflows/codeql.yml)\n[![Gitter chat](https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&logo=gitter&style=flat-square)](https://gitter.im/TheAlgorithms)\n[![contributions welcome](https://img.shields.io/static/v1.svg?label=Contributions&message=Welcome&color=0059b3&style=flat-square)](https://github.com/TheAlgorithms/C-Plus-Plus/blob/master/CONTRIBUTING.md)\n![GitHub repo size](https://img.shields.io/github/repo-size/TheAlgorithms/C-Plus-Plus?color=red&style=flat-square)\n[![Doxygen CI](https://github.com/TheAlgorithms/C-Plus-Plus/workflows/Doxygen%20CI/badge.svg)](https://TheAlgorithms.github.io/C-Plus-Plus)\n[![Awesome CI](https://github.com/TheAlgorithms/C-Plus-Plus/workflows/Awesome%20CI%20Workflow/badge.svg)](https://github.com/TheAlgorithms/C-Plus-Plus/actions?query=workflow%3A%22Awesome+CI+Workflow%22)\n[![Income](https://img.shields.io/liberapay/receives/TheAlgorithms.svg?logo=liberapay)](https://liberapay.com/TheAlgorithms)\n[![Discord chat](https://img.shields.io/discord/808045925556682782.svg?logo=discord&colorB=5865F2)](https://the-algorithms.com/discord/)\n[![Donate](https://liberapay.com/assets/widgets/donate.svg)](https://liberapay.com/TheAlgorithms/donate)\n\n## Overview\n\nThis repository is a collection of open-source implementation of a variety of algorithms implemented in C++ and licensed under [MIT License](https://github.com/TheAlgorithms/C-Plus-Plus/blob/master/LICENSE). These algorithms span a variety of topics from computer science, mathematics and statistics, data science, machine learning, engineering, etc.. The implementations and the associated documentation are meant to provide a learning resource for educators and students. Hence, one may find more than one implementation for the same objective but using a different algorithm strategies and optimizations.\n\n## Features\n\n- The repository provides implementations of various algorithms in one of the most fundamental general purpose languages - [C++](https://en.wikipedia.org/wiki/C%2B%2B).\n- Well documented source code with detailed explanations provide a valuable resource for educators and students alike.\n- Each source code is atomic using [STL classes](https://en.wikipedia.org/wiki/Standard_Template_Library) and _no external libraries_ are required for their compilation and execution. Thus, the fundamentals of the algorithms can be studied in much depth.\n- Source codes are [compiled and tested](https://github.com/TheAlgorithms/C-Plus-Plus/actions?query=workflow%3A%22Awesome+CI+Workflow%22) for every commit on the latest versions of three major operating systems viz., Windows, MacOS, and Ubuntu (Linux) using MSVC 19 2022, AppleClang 15.0.15, and GNU 13.3.0 respectively.\n- Strict adherence to [C++17](https://en.wikipedia.org/wiki/C%2B%2B17) standard ensures portability of code to embedded systems as well like [ESP32](https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-guides/cplusplus.html#c-language-standard), [ARM Cortex](https://developer.arm.com/documentation/101458/2404/Standards-support/Supported-C-C---standards-in-Arm-C-C---Compiler), etc. with little to no changes.\n- Self-checks within programs ensure correct implementations with confidence.\n- Modular implementations and OpenSource licensing enable the functions to be utilized conveniently in other applications.\n\n## Documentation\n\n[Online Documentation](https://TheAlgorithms.github.io/C-Plus-Plus) is generated from the repository source codes directly. The documentation contains all resources including source code snippets, details on execution of the programs, diagrammatic representation of program flow, and links to external resources where necessary. The documentation also introduces interactive source code with links to documentation for C++ STL library functions used.\nClick on [Files menu](https://TheAlgorithms.github.io/C-Plus-Plus/files.html) to see the list of all the files documented with the code.\n\n[Documentation of Algorithms in C++](https://thealgorithms.github.io/C-Plus-Plus) by [The Algorithms Contributors](https://github.com/TheAlgorithms/C-Plus-Plus/graphs/contributors) is licensed under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/?ref=chooser-v1)<br/>\n<a href="https://creativecommons.org/licenses/by-sa/4.0"><img alt="Creative Commons License" style="height:22px!important;margin-left: 3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg" /><img  alt="Credit must be given to the creator" style="height:22px!important;margin-left: 3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg" /><img alt="Adaptations must be shared under the same terms" style="height:22px!important;margin-left: 3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg" /></a>\n\n## Contributions\n\nAs a community developed and maintained repository, we welcome new un-plagiarized quality contributions. Please read our [Contribution Guidelines](https://github.com/TheAlgorithms/C-Plus-Plus/blob/master/CONTRIBUTING.md).\n', '{"language":"C++","stars":33486,"forks":7657,"watchers":33486,"open_issues":32,"topics":["algorithm","algorithm-competitions","algorithms-implemented","artificial-intelligence-algorithms","computer-science","cpp","data-structures","educational","instructor-materials","interview-preparation","interview-questions","machine-learning","machine-learning-algorithms","mathematics","search","sort"],"default_branch":"master","size_kb":140614,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"}]', NULL, 'MIT', 'approved', 65, 'ba0cc19e36b5c085e81d9fc62fe742be', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-TheAlgorithms-C-Plus-Plus from https://github.com/TheAlgorithms.png
Image converted to WebP: data/images/github-TheAlgorithms-C-Plus-Plus.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-explosion-spaCy', 'github--explosion--spacy', 'spaCy', 'explosion', '<a href="https://explosion.ai"><img src="https://explosion.ai/assets/img/logo.svg" width="125" height="125" align="right" /></a> spaCy is a library for **advanced Natural Language Processing** in Python and Cython. It''s built on the very latest research, and was designed from day one to be used in real products. spaCy comes with pretrained pipelines and currently supports tokenization and training for **70+ languages**. It features state-of-the-art speed and **neural network models** for tagg...', '["ai","artificial-intelligence","cython","data-science","deep-learning","entity-linking","machine-learning","named-entity-recognition","natural-language-processing","neural-network","neural-networks","nlp","nlp-library","python","spacy","text-classification","tokenization","python"]', 'other', 32917, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/explosion/spaCy","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<a href="https://explosion.ai"><img src="https://explosion.ai/assets/img/logo.svg" width="125" height="125" align="right" /></a>\n\n# spaCy: Industrial-strength NLP\n\nspaCy is a library for **advanced Natural Language Processing** in Python and\nCython. It''s built on the very latest research, and was designed from day one to\nbe used in real products.\n\nspaCy comes with [pretrained pipelines](https://spacy.io/models) and currently\nsupports tokenization and training for **70+ languages**. It features\nstate-of-the-art speed and **neural network models** for tagging, parsing,\n**named entity recognition**, **text classification** and more, multi-task\nlearning with pretrained **transformers** like BERT, as well as a\nproduction-ready [**training system**](https://spacy.io/usage/training) and easy\nmodel packaging, deployment and workflow management. spaCy is commercial\nopen-source software, released under the\n[MIT license](https://github.com/explosion/spaCy/blob/master/LICENSE).\n\nüí´ **Version 3.8 out now!**\n[Check out the release notes here.](https://github.com/explosion/spaCy/releases)\n\n[![tests](https://github.com/explosion/spaCy/actions/workflows/tests.yml/badge.svg)](https://github.com/explosion/spaCy/actions/workflows/tests.yml)\n[![Current Release Version](https://img.shields.io/github/release/explosion/spacy.svg?style=flat-square&logo=github)](https://github.com/explosion/spaCy/releases)\n[![pypi Version](https://img.shields.io/pypi/v/spacy.svg?style=flat-square&logo=pypi&logoColor=white)](https://pypi.org/project/spacy/)\n[![conda Version](https://img.shields.io/conda/vn/conda-forge/spacy.svg?style=flat-square&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/spacy)\n[![Python wheels](https://img.shields.io/badge/wheels-%E2%9C%93-4c1.svg?longCache=true&style=flat-square&logo=python&logoColor=white)](https://github.com/explosion/wheelwright/releases)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/ambv/black)\n<br />\n[![PyPi downloads](https://static.pepy.tech/personalized-badge/spacy?period=total&units=international_system&left_color=grey&right_color=orange&left_text=pip%20downloads)](https://pypi.org/project/spacy/)\n[![Conda downloads](https://img.shields.io/conda/dn/conda-forge/spacy?label=conda%20downloads)](https://anaconda.org/conda-forge/spacy)\n\n## üìñ Documentation\n\n| Documentation                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                              |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| ‚≠êÔ∏è **[spaCy 101]**                                                                                                                                                                                                       | New to spaCy? Here''s everything you need to know!                                                                                                                                                                                                                                                                                            |\n| üìö **[Usage Guides]**                                                                                                                                                                                                     | How to use spaCy and its features.                                                                                                                                                                                                                                                                                                           |\n| üöÄ **[New in v3.0]**                                                                                                                                                                                                      | New features, backwards incompatibilities and migration guide.                                                                                                                                                                                                                                                                               |\n| ü™ê **[Project Templates]**                                                                                                                                                                                                | End-to-end workflows you can clone, modify and run.                                                                                                                                                                                                                                                                                          |\n| üéõ **[API Reference]**                                                                                                                                                                                                     | The detailed reference for spaCy''s API.                                                                                                                                                                                                                                                                                                      |\n| ‚è© **[GPU Processing]**                                                                                                                                                                                                    | Use spaCy with CUDA-compatible GPU processing.                                                                                                                                                                                                                                                                                               |\n| üì¶ **[Models]**                                                                                                                                                                                                           | Download trained pipelines for spaCy.                                                                                                                                                                                                                                                                                                        |\n| ü¶ô **[Large Language Models]**                                                                                                                                                                                            | Integrate LLMs into spaCy pipelines.                                                                                                                                                                                                                                                                                                        |\n| üåå **[Universe]**                                                                                                                                                                                                         | Plugins, extensions, demos and books from the spaCy ecosystem.                                                                                                                                                                                                                                                                               |\n| ‚öôÔ∏è **[spaCy VS Code Extension]**                                                                                                                                                                                          | Additional tooling and features for working with spaCy''s config files.                                                                                                                                                                                                                                                                       |\n| üë©‚Äçüè´ **[Online Course]**                                                                                                                                                                                                    | Learn spaCy in this free and interactive online course.                                                                                                                                                                                                                                                                                      |\n| üì∞ **[Blog]**                                                                                                                                                                                                             | Read about current spaCy and Prodigy development, releases, talks and more from Explosion.                                                                                                                                                                                                                 |\n| üì∫ **[Videos]**                                                                                                                                                                                                           | Our YouTube channel with video tutorials, talks and more.                                                                                                                                                                                                                                                                                    |\n| üî¥ **[Live Stream]**                                                                                                                                                                                                       | Join Matt as he works on spaCy and chat about NLP, live every week.                                                                                                                                                                                                                                                                         |\n| üõ† **[Changelog]**                                                                                                                                                                                                         | Changes and version history.                                                                                                                                                                                                                                                                                                                 |\n| üíù **[Contribute]**                                                                                                                                                                                                       | How to contribute to the spaCy project and code base.                                                                                                                                                                                                                                                                                        |\n| üëï **[Swag]**                                                                                                                                                                                                             | Support us and our work with unique, custom-designed swag!                                                                                                                                                                                                                                                                                   |\n| <a href="https://explosion.ai/tailored-solutions"><img src="https://github.com/explosion/spaCy/assets/13643239/36d2a42e-98c0-4599-90e1-788ef75181be" width="150" alt="Tailored Solutions"/></a> | Custom NLP consulting, implementation and strategic advice by spaCy‚Äôs core development team. Streamlined, production-ready, predictable and maintainable. Send us an email or take our 5-minute questionnaire, and well''be in touch! **[Learn more &rarr;](https://explosion.ai/tailored-solutions)**                 |\n\n[spacy 101]: https://spacy.io/usage/spacy-101\n[new in v3.0]: https://spacy.io/usage/v3\n[usage guides]: https://spacy.io/usage/\n[api reference]: https://spacy.io/api/\n[gpu processing]: https://spacy.io/usage#gpu\n[models]: https://spacy.io/models\n[large language models]: https://spacy.io/usage/large-language-models\n[universe]: https://spacy.io/universe\n[spacy vs code extension]: https://github.com/explosion/spacy-vscode\n[videos]: https://www.youtube.com/c/ExplosionAI\n[live stream]: https://www.youtube.com/playlist?list=PLBmcuObd5An5_iAxNYLJa_xWmNzsYce8c\n[online course]: https://course.spacy.io\n[blog]: https://explosion.ai\n[project templates]: https://github.com/explosion/projects\n[changelog]: https://spacy.io/usage#changelog\n[contribute]: https://github.com/explosion/spaCy/blob/master/CONTRIBUTING.md\n[swag]: https://explosion.ai/merch\n\n## üí¨ Where to ask questions\n\nThe spaCy project is maintained by the [spaCy team](https://explosion.ai/about).\nPlease understand that we won''t be able to provide individual support via email.\nWe also believe that help is much more valuable if it''s shared publicly, so that\nmore people can benefit from it.\n\n| Type                            | Platforms                               |\n| ------------------------------- | --------------------------------------- |\n| üö® **Bug Reports**              | [GitHub Issue Tracker]                  |\n| üéÅ **Feature Requests & Ideas** | [GitHub Discussions] ¬∑ [Live Stream]    |\n| üë©‚Äçüíª **Usage Questions**          | [GitHub Discussions] ¬∑ [Stack Overflow] |\n| üóØ **General Discussion**        | [GitHub Discussions] ¬∑ [Live Stream]   |\n\n[github issue tracker]: https://github.com/explosion/spaCy/issues\n[github discussions]: https://github.com/explosion/spaCy/discussions\n[stack overflow]: https://stackoverflow.com/questions/tagged/spacy\n[live stream]: https://www.youtube.com/playlist?list=PLBmcuObd5An5_iAxNYLJa_xWmNzsYce8c\n\n## Features\n\n- Support for **70+ languages**\n- **Trained pipelines** for different languages and tasks\n- Multi-task learning with pretrained **transformers** like BERT\n- Support for pretrained **word vectors** and embeddings\n- State-of-the-art speed\n- Production-ready **training system**\n- Linguistically-motivated **tokenization**\n- Components for named **entity recognition**, part-of-speech-tagging,\n  dependency parsing, sentence segmentation, **text classification**,\n  lemmatization, morphological analysis, entity linking and more\n- Easily extensible with **custom components** and attributes\n- Support for custom models in **PyTorch**, **TensorFlow** and other frameworks\n- Built in **visualizers** for syntax and NER\n- Easy **model packaging**, deployment and workflow management\n- Robust, rigorously evaluated accuracy\n\nüìñ **For more details, see the\n[facts, figures and benchmarks](https://spacy.io/usage/facts-figures).**\n\n## ‚è≥ Install spaCy\n\nFor detailed installation instructions, see the\n[documentation](https://spacy.io/usage).\n\n- **Operating system**: macOS / OS X ¬∑ Linux ¬∑ Windows (Cygwin, MinGW, Visual\n  Studio)\n- **Python version**: Python >=3.7, <3.13 (only 64 bit)\n- **Package managers**: [pip] ¬∑ [conda] (via `conda-forge`)\n\n[pip]: https://pypi.org/project/spacy/\n[conda]: https://anaconda.org/conda-forge/spacy\n\n### pip\n\nUsing pip, spaCy releases are available as source packages and binary wheels.\nBefore you install spaCy and its dependencies, make sure that your `pip`,\n`setuptools` and `wheel` are up to date.\n\n```bash\npip install -U pip setuptools wheel\npip install spacy\n```\n\nTo install additional data tables for lemmatization and normalization you can\nrun `pip install spacy[lookups]` or install\n[`spacy-lookups-data`](https://github.com/explosion/spacy-lookups-data)\nseparately. The lookups package is needed to create blank models with\nlemmatization data, and to lemmatize in languages that don''t yet come with\npretrained models and aren''t powered by third-party libraries.\n\nWhen using pip it is generally recommended to install packages in a virtual\nenvironment to avoid modifying system state:\n\n```bash\npython -m venv .env\nsource .env/bin/activate\npip install -U pip setuptools wheel\npip install spacy\n```\n\n### conda\n\nYou can also install spaCy from `conda` via the `conda-forge` channel. For the\nfeedstock including the build recipe and configuration, check out\n[this repository](https://github.com/conda-forge/spacy-feedstock).\n\n```bash\nconda install -c conda-forge spacy\n```\n\n### Updating spaCy\n\nSome updates to spaCy may require downloading new statistical models. If you''re\nrunning spaCy v2.0 or higher, you can use the `validate` command to check if\nyour installed models are compatible and if not, print details on how to update\nthem:\n\n```bash\npip install -U spacy\npython -m spacy validate\n```\n\nIf you''ve trained your own models, keep in mind that your training and runtime\ninputs must match. After updating spaCy, we recommend **retraining your models**\nwith the new version.\n\nüìñ **For details on upgrading from spaCy 2.x to spaCy 3.x, see the\n[migration guide](https://spacy.io/usage/v3#migrating).**\n\n## üì¶ Download model packages\n\nTrained pipelines for spaCy can be installed as **Python packages**. This means\nthat they''re a component of your application, just like any other module. Models\ncan be installed using spaCy''s [`download`](https://spacy.io/api/cli#download)\ncommand, or manually by pointing pip to a path or URL.\n\n| Documentation              |                                                                  |\n| -------------------------- | ---------------------------------------------------------------- |\n| **[Available Pipelines]**  | Detailed pipeline descriptions, accuracy figures and benchmarks. |\n| **[Models Documentation]** | Detailed usage and installation instructions.                    |\n| **[Training]**             | How to train your own pipelines on your data.                    |\n\n[available pipelines]: https://spacy.io/models\n[models documentation]: https://spacy.io/usage/models\n[training]: https://spacy.io/usage/training\n\n```bash\n# Download best-matching version of specific model for your spaCy installation\npython -m spacy download en_core_web_sm\n\n# pip install .tar.gz archive or .whl from path or URL\npip install /Users/you/en_core_web_sm-3.0.0.tar.gz\npip install /Users/you/en_core_web_sm-3.0.0-py3-none-any.whl\npip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n```\n\n### Loading and using models\n\nTo load a model, use [`spacy.load()`](https://spacy.io/api/top-level#spacy.load)\nwith the model name or a path to the model data directory.\n\n```python\nimport spacy\nnlp = spacy.load("en_core_web_sm")\ndoc = nlp("This is a sentence.")\n```\n\nYou can also `import` a model directly via its full name and then call its\n`load()` method with no arguments.\n\n```python\nimport spacy\nimport en_core_web_sm\n\nnlp = en_core_web_sm.load()\ndoc = nlp("This is a sentence.")\n```\n\nüìñ **For more info and examples, check out the\n[models documentation](https://spacy.io/docs/usage/models).**\n\n## ‚öí Compile from source\n\nThe other way to install spaCy is to clone its\n[GitHub repository](https://github.com/explosion/spaCy) and build it from\nsource. That is the common way if you want to make changes to the code base.\nYou''ll need to make sure that you have a development environment consisting of a\nPython distribution including header files, a compiler,\n[pip](https://pip.pypa.io/en/latest/installing/),\n[virtualenv](https://virtualenv.pypa.io/en/latest/) and\n[git](https://git-scm.com) installed. The compiler part is the trickiest. How to\ndo that depends on your system.\n\n| Platform    |                                                                                                                                                                                                                                                                     |\n| ----------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **Ubuntu**  | Install system-level dependencies via `apt-get`: `sudo apt-get install build-essential python-dev git` .                                                                                                                                                            |\n| **Mac**     | Install a recent version of [XCode](https://developer.apple.com/xcode/), including the so-called "Command Line Tools". macOS and OS X ship with Python and git preinstalled.                                                                                        |\n| **Windows** | Install a version of the [Visual C++ Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/) or [Visual Studio Express](https://visualstudio.microsoft.com/vs/express/) that matches the version that was used to compile your Python interpreter. |\n\nFor more details and instructions, see the documentation on\n[compiling spaCy from source](https://spacy.io/usage#source) and the\n[quickstart widget](https://spacy.io/usage#section-quickstart) to get the right\ncommands for your platform and Python version.\n\n```bash\ngit clone https://github.com/explosion/spaCy\ncd spaCy\n\npython -m venv .env\nsource .env/bin/activate\n\n# make sure you are using the latest pip\npython -m pip install -U pip setuptools wheel\n\npip install -r requirements.txt\npip install --no-build-isolation --editable .\n```\n\nTo install with extras:\n\n```bash\npip install --no-build-isolation --editable .[lookups,cuda102]\n```\n\n## üö¶ Run tests\n\nspaCy comes with an [extensive test suite](spacy/tests). In order to run the\ntests, you''ll usually want to clone the repository and build spaCy from source.\nThis will also install the required development dependencies and test utilities\ndefined in the [`requirements.txt`](requirements.txt).\n\nAlternatively, you can run `pytest` on the tests from within the installed\n`spacy` package. Don''t forget to also install the test utilities via spaCy''s\n[`requirements.txt`](requirements.txt):\n\n```bash\npip install -r requirements.txt\npython -m pytest --pyargs spacy\n```\n', '{"language":"Python","stars":32917,"forks":4636,"watchers":32917,"open_issues":183,"topics":["ai","artificial-intelligence","cython","data-science","deep-learning","entity-linking","machine-learning","named-entity-recognition","natural-language-processing","neural-network","neural-networks","nlp","nlp-library","python","spacy","text-classification","tokenization"],"default_branch":"master","size_kb":203340,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:wheelwright","source_url":"https://github.com/explosion/wheelwright"},{"type":"has_code","target_id":"github:ambv:black","source_url":"https://github.com/ambv/black"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spacy-vscode","source_url":"https://github.com/explosion/spacy-vscode"},{"type":"has_code","target_id":"github:explosion:projects","source_url":"https://github.com/explosion/projects"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spacy-lookups-data","source_url":"https://github.com/explosion/spacy-lookups-data"},{"type":"has_code","target_id":"github:conda-forge:spacy-feedstock","source_url":"https://github.com/conda-forge/spacy-feedstock"},{"type":"has_code","target_id":"github:explosion:spacy-models","source_url":"https://github.com/explosion/spacy-models"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"}]', NULL, 'MIT', 'approved', 80, '1c94d9d3cb9a56ec01b4f2ed0f361c05', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-explosion-spaCy from https://github.com/explosion.png
Image converted to WebP: data/images/github-explosion-spaCy.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-google-ai-edge-mediapipe', 'github--google-ai-edge--mediapipe', 'mediapipe', 'google-ai-edge', '--- layout: forward target: https://developers.google.com/mediapipe title: Home nav_order: 1 --- ---- **Attention:** *We have moved to https://developers.google.com/mediapipe as the primary developer documentation site for MediaPipe as of April 3, 2023.* !MediaPipe **Attention**: MediaPipe Solutions Preview is an early release. Learn more. **On-device machine learning for everyone** Delight your customers with innovative machine learning features. MediaPipe contains everything that you need t...', '["android","audio-processing","c-plus-plus","calculator","computer-vision","deep-learning","framework","graph-based","graph-framework","inference","machine-learning","mediapipe","mobile-development","perception","pipeline-framework","stream-processing","video-processing","c++"]', 'other', 32330, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/google-ai-edge/mediapipe","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '---\nlayout: forward\ntarget: https://developers.google.com/mediapipe\ntitle: Home\nnav_order: 1\n---\n\n----\n\n**Attention:** *We have moved to\n[https://developers.google.com/mediapipe](https://developers.google.com/mediapipe)\nas the primary developer documentation site for MediaPipe as of April 3, 2023.*\n\n![MediaPipe](https://developers.google.com/static/mediapipe/images/home/hero_01_1920.png)\n\n**Attention**: MediaPipe Solutions Preview is an early release. [Learn\nmore](https://developers.google.com/mediapipe/solutions/about#notice).\n\n**On-device machine learning for everyone**\n\nDelight your customers with innovative machine learning features. MediaPipe\ncontains everything that you need to customize and deploy to mobile (Android,\niOS), web, desktop, edge devices, and IoT, effortlessly.\n\n*   [See demos](https://goo.gle/mediapipe-studio)\n*   [Learn more](https://developers.google.com/mediapipe/solutions)\n\n## Get started\n\nYou can get started with MediaPipe Solutions by by checking out any of the\ndeveloper guides for\n[vision](https://developers.google.com/mediapipe/solutions/vision/object_detector),\n[text](https://developers.google.com/mediapipe/solutions/text/text_classifier),\nand\n[audio](https://developers.google.com/mediapipe/solutions/audio/audio_classifier)\ntasks. If you need help setting up a development environment for use with\nMediaPipe Tasks, check out the setup guides for\n[Android](https://developers.google.com/mediapipe/solutions/setup_android), [web\napps](https://developers.google.com/mediapipe/solutions/setup_web), and\n[Python](https://developers.google.com/mediapipe/solutions/setup_python).\n\n## Solutions\n\nMediaPipe Solutions provides a suite of libraries and tools for you to quickly\napply artificial intelligence (AI) and machine learning (ML) techniques in your\napplications. You can plug these solutions into your applications immediately,\ncustomize them to your needs, and use them across multiple development\nplatforms. MediaPipe Solutions is part of the MediaPipe [open source\nproject](https://github.com/google/mediapipe), so you can further customize the\nsolutions code to meet your application needs.\n\nThese libraries and resources provide the core functionality for each MediaPipe\nSolution:\n\n*   **MediaPipe Tasks**: Cross-platform APIs and libraries for deploying\n    solutions. [Learn\n    more](https://developers.google.com/mediapipe/solutions/tasks).\n*   **MediaPipe models**: Pre-trained, ready-to-run models for use with each\n    solution.\n\nThese tools let you customize and evaluate solutions:\n\n*   **MediaPipe Model Maker**: Customize models for solutions with your data.\n    [Learn more](https://developers.google.com/mediapipe/solutions/model_maker).\n*   **MediaPipe Studio**: Visualize, evaluate, and benchmark solutions in your\n    browser. [Learn\n    more](https://developers.google.com/mediapipe/solutions/studio).\n\n### Legacy solutions\n\nWe have ended support for [these MediaPipe Legacy Solutions](https://developers.google.com/mediapipe/solutions/guide#legacy)\nas of March 1, 2023. All other MediaPipe Legacy Solutions will be upgraded to\na new MediaPipe Solution. See the [Solutions guide](https://developers.google.com/mediapipe/solutions/guide#legacy)\nfor details. The [code repository](https://github.com/google/mediapipe/tree/master/mediapipe)\nand prebuilt binaries for all MediaPipe Legacy Solutions will continue to be\nprovided on an as-is basis.\n\nFor more on the legacy solutions, see the [documentation](https://github.com/google/mediapipe/tree/master/docs/solutions).\n\n## Framework\n\nTo start using MediaPipe Framework, [install MediaPipe\nFramework](https://developers.google.com/mediapipe/framework/getting_started/install)\nand start building example applications in C++, Android, and iOS.\n\n[MediaPipe Framework](https://developers.google.com/mediapipe/framework) is the\nlow-level component used to build efficient on-device machine learning\npipelines, similar to the premade MediaPipe Solutions.\n\nBefore using MediaPipe Framework, familiarize yourself with the following key\n[Framework\nconcepts](https://developers.google.com/mediapipe/framework/framework_concepts/overview.md):\n\n*   [Packets](https://developers.google.com/mediapipe/framework/framework_concepts/packets.md)\n*   [Graphs](https://developers.google.com/mediapipe/framework/framework_concepts/graphs.md)\n*   [Calculators](https://developers.google.com/mediapipe/framework/framework_concepts/calculators.md)\n\n## Community\n\n*   [Slack community](https://mediapipe.page.link/joinslack) for MediaPipe\n    users.\n*   [Discuss](https://groups.google.com/forum/#!forum/mediapipe) - General\n    community discussion around MediaPipe.\n*   [Awesome MediaPipe](https://mediapipe.page.link/awesome-mediapipe) - A\n    curated list of awesome MediaPipe related frameworks, libraries and\n    software.\n\n## Contributing\n\nWe welcome contributions. Please follow these\n[guidelines](https://github.com/google/mediapipe/blob/master/CONTRIBUTING.md).\n\nWe use GitHub issues for tracking requests and bugs. Please post questions to\nthe MediaPipe Stack Overflow with a `mediapipe` tag.\n\n## Resources\n\n### Publications\n\n*   [Bringing artworks to life with AR](https://developers.googleblog.com/2021/07/bringing-artworks-to-life-with-ar.html)\n    in Google Developers Blog\n*   [Prosthesis control via Mirru App using MediaPipe hand tracking](https://developers.googleblog.com/2021/05/control-your-mirru-prosthesis-with-mediapipe-hand-tracking.html)\n    in Google Developers Blog\n*   [SignAll SDK: Sign language interface using MediaPipe is now available for\n    developers](https://developers.googleblog.com/2021/04/signall-sdk-sign-language-interface-using-mediapipe-now-available.html)\n    in Google Developers Blog\n*   [MediaPipe Holistic - Simultaneous Face, Hand and Pose Prediction, on\n    Device](https://ai.googleblog.com/2020/12/mediapipe-holistic-simultaneous-face.html)\n    in Google AI Blog\n*   [Background Features in Google Meet, Powered by Web ML](https://ai.googleblog.com/2020/10/background-features-in-google-meet.html)\n    in Google AI Blog\n*   [MediaPipe 3D Face Transform](https://developers.googleblog.com/2020/09/mediapipe-3d-face-transform.html)\n    in Google Developers Blog\n*   [Instant Motion Tracking With MediaPipe](https://developers.googleblog.com/2020/08/instant-motion-tracking-with-mediapipe.html)\n    in Google Developers Blog\n*   [BlazePose - On-device Real-time Body Pose Tracking](https://ai.googleblog.com/2020/08/on-device-real-time-body-pose-tracking.html)\n    in Google AI Blog\n*   [MediaPipe Iris: Real-time Eye Tracking and Depth Estimation](https://ai.googleblog.com/2020/08/mediapipe-iris-real-time-iris-tracking.html)\n    in Google AI Blog\n*   [MediaPipe KNIFT: Template-based feature matching](https://developers.googleblog.com/2020/04/mediapipe-knift-template-based-feature-matching.html)\n    in Google Developers Blog\n*   [Alfred Camera: Smart camera features using MediaPipe](https://developers.googleblog.com/2020/03/alfred-camera-smart-camera-features-using-mediapipe.html)\n    in Google Developers Blog\n*   [Real-Time 3D Object Detection on Mobile Devices with MediaPipe](https://ai.googleblog.com/2020/03/real-time-3d-object-detection-on-mobile.html)\n    in Google AI Blog\n*   [AutoFlip: An Open Source Framework for Intelligent Video Reframing](https://ai.googleblog.com/2020/02/autoflip-open-source-framework-for.html)\n    in Google AI Blog\n*   [MediaPipe on the Web](https://developers.googleblog.com/2020/01/mediapipe-on-web.html)\n    in Google Developers Blog\n*   [Object Detection and Tracking using MediaPipe](https://developers.googleblog.com/2019/12/object-detection-and-tracking-using-mediapipe.html)\n    in Google Developers Blog\n*   [On-Device, Real-Time Hand Tracking with MediaPipe](https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html)\n    in Google AI Blog\n*   [MediaPipe: A Framework for Building Perception Pipelines](https://arxiv.org/abs/1906.08172)\n\n### Videos\n\n*   [YouTube Channel](https://www.youtube.com/c/MediaPipe)\n', '{"language":"C++","stars":32330,"forks":5637,"watchers":32330,"open_issues":621,"topics":["android","audio-processing","c-plus-plus","calculator","computer-vision","deep-learning","framework","graph-based","graph-framework","inference","machine-learning","mediapipe","mobile-development","perception","pipeline-framework","stream-processing","video-processing"],"default_branch":"master","size_kb":605811,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:google:mediapipe","source_url":"https://github.com/google/mediapipe"},{"type":"has_code","target_id":"github:google:mediapipe","source_url":"https://github.com/google/mediapipe"},{"type":"has_code","target_id":"github:google:mediapipe","source_url":"https://github.com/google/mediapipe"},{"type":"has_code","target_id":"github:google:mediapipe","source_url":"https://github.com/google/mediapipe"}]', NULL, 'Apache-2.0', 'approved', 65, 'e1b087e75e015a74ce76b7a137389649', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-google-ai-edge-mediapipe from https://github.com/google-ai-edge.png
Image converted to WebP: data/images/github-google-ai-edge-mediapipe.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-lutzroeder-netron', 'github--lutzroeder--netron', 'netron', 'lutzroeder', '<div align="center"> <img width="400px" height="100px" src="https://github.com/lutzroeder/netron/raw/main/.github/logo-light.svg#gh-light-mode-only"> <img width="400px" height="100px" src="https://github.com/lutzroeder/netron/raw/main/.github/logo-dark.svg#gh-dark-mode-only"> </div> Netron is a viewer for neural network, deep learning and machine learning models. Netron supports ONNX, TensorFlow Lite, Core ML, Keras, Caffe, Darknet, PyTorch, TensorFlow.js, Safetensors and NumPy. Netron has ex...', '["ai","coreml","deep-learning","deeplearning","keras","machine-learning","machinelearning","ml","neural-network","numpy","onnx","pytorch","safetensors","tensorflow","tensorflow-lite","visualizer","javascript"]', 'other', 31961, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/lutzroeder/netron","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<div align="center">\n<img width="400px" height="100px" src="https://github.com/lutzroeder/netron/raw/main/.github/logo-light.svg#gh-light-mode-only">\n<img width="400px" height="100px" src="https://github.com/lutzroeder/netron/raw/main/.github/logo-dark.svg#gh-dark-mode-only">\n</div>\n\nNetron is a viewer for neural network, deep learning and machine learning models.\n\nNetron supports ONNX, TensorFlow Lite, Core ML, Keras, Caffe, Darknet, PyTorch, TensorFlow.js, Safetensors and NumPy.\n\nNetron has experimental support for TorchScript, torch.export, ExecuTorch, TensorFlow, OpenVINO, RKNN, ncnn, MNN, PaddlePaddle, GGUF and scikit-learn.\n\n<p align=''center''><a href=''https://www.lutzroeder.com/ai''><img src=''.github/screenshot.png'' width=''800''></a></p>\n\n## Install\n\n**Browser**: [**Start**](https://netron.app) the browser version.\n\n**macOS**: [**Download**](https://github.com/lutzroeder/netron/releases/latest) the `.dmg` file or run `brew install --cask netron`.\n\n**Linux**: [**Download**](https://github.com/lutzroeder/netron/releases/latest) the `.deb` or `.rpm` file.\n\n**Windows**: [**Download**](https://github.com/lutzroeder/netron/releases/latest) the `.exe` installer or run `winget install -s winget netron`.\n\n**Python**: `pip install netron`, then run `netron [FILE]` or `netron.start(''[FILE]'')`.\n\n## Models\n\nSample model files to download or open using the browser version:\n\n * **ONNX**: [squeezenet](https://github.com/onnx/models/raw/main/validated/vision/classification/squeezenet/model/squeezenet1.0-3.onnx) [[open](https://netron.app?url=https://github.com/onnx/models/raw/main/validated/vision/classification/squeezenet/model/squeezenet1.0-3.onnx)]\n * **TorchScript**: [traced_online_pred_layer](https://github.com/ApolloAuto/apollo/raw/master/modules/prediction/data/traced_online_pred_layer.pt) [[open](https://netron.app?url=https://github.com/ApolloAuto/apollo/raw/master/modules/prediction/data/traced_online_pred_layer.pt)]\n * **TensorFlow Lite**: [yamnet](https://huggingface.co/thelou1s/yamnet/resolve/main/lite-model_yamnet_tflite_1.tflite) [[open](https://netron.app?url=https://huggingface.co/thelou1s/yamnet/blob/main/lite-model_yamnet_tflite_1.tflite)]\n * **TensorFlow**: [chessbot](https://github.com/srom/chessbot/raw/master/model/chessbot.pb) [[open](https://netron.app?url=https://github.com/srom/chessbot/raw/master/model/chessbot.pb)]\n * **Keras**: [mobilenet](https://github.com/aio-libs/aiohttp-demos/raw/master/demos/imagetagger/tests/data/mobilenet.h5) [[open](https://netron.app?url=https://github.com/aio-libs/aiohttp-demos/raw/master/demos/imagetagger/tests/data/mobilenet.h5)]\n * **Core ML**: [exermote](https://github.com/Lausbert/Exermote/raw/master/ExermoteInference/ExermoteCoreML/ExermoteCoreML/Model/Exermote.mlmodel) [[open](https://netron.app?url=https://github.com/Lausbert/Exermote/raw/master/ExermoteInference/ExermoteCoreML/ExermoteCoreML/Model/Exermote.mlmodel)]\n * **Darknet**: [yolo](https://github.com/AlexeyAB/darknet/raw/master/cfg/yolo.cfg) [[open](https://netron.app?url=https://github.com/AlexeyAB/darknet/raw/master/cfg/yolo.cfg)]\n', '{"language":"JavaScript","stars":31961,"forks":3040,"watchers":31961,"open_issues":21,"topics":["ai","coreml","deep-learning","deeplearning","keras","machine-learning","machinelearning","ml","neural-network","numpy","onnx","pytorch","safetensors","tensorflow","tensorflow-lite","visualizer"],"default_branch":"main","size_kb":71209,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:lutzroeder:netron","source_url":"https://github.com/lutzroeder/netron"},{"type":"has_code","target_id":"github:lutzroeder:netron","source_url":"https://github.com/lutzroeder/netron"},{"type":"has_code","target_id":"github:lutzroeder:netron","source_url":"https://github.com/lutzroeder/netron"},{"type":"has_code","target_id":"github:lutzroeder:netron","source_url":"https://github.com/lutzroeder/netron"},{"type":"has_code","target_id":"github:lutzroeder:netron","source_url":"https://github.com/lutzroeder/netron"},{"type":"has_code","target_id":"github:onnx:models","source_url":"https://github.com/onnx/models"},{"type":"has_code","target_id":"github:onnx:models","source_url":"https://github.com/onnx/models"},{"type":"has_code","target_id":"github:ApolloAuto:apollo","source_url":"https://github.com/ApolloAuto/apollo"},{"type":"has_code","target_id":"github:ApolloAuto:apollo","source_url":"https://github.com/ApolloAuto/apollo"},{"type":"has_code","target_id":"github:srom:chessbot","source_url":"https://github.com/srom/chessbot"},{"type":"has_code","target_id":"github:srom:chessbot","source_url":"https://github.com/srom/chessbot"},{"type":"has_code","target_id":"github:aio-libs:aiohttp-demos","source_url":"https://github.com/aio-libs/aiohttp-demos"},{"type":"has_code","target_id":"github:aio-libs:aiohttp-demos","source_url":"https://github.com/aio-libs/aiohttp-demos"},{"type":"has_code","target_id":"github:Lausbert:Exermote","source_url":"https://github.com/Lausbert/Exermote"},{"type":"has_code","target_id":"github:Lausbert:Exermote","source_url":"https://github.com/Lausbert/Exermote"},{"type":"has_code","target_id":"github:AlexeyAB:darknet","source_url":"https://github.com/AlexeyAB/darknet"},{"type":"has_code","target_id":"github:AlexeyAB:darknet","source_url":"https://github.com/AlexeyAB/darknet"}]', NULL, 'MIT', 'approved', 65, 'fb3042121c5294e9e993246abef63e68', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-lutzroeder-netron from https://github.com/lutzroeder.png
Image converted to WebP: data/images/github-lutzroeder-netron.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-openai-CLIP', 'github--openai--clip', 'CLIP', 'openai', '[[Blog]](https://openai.com/blog/clip/) [[Paper]](https://arxiv.org/abs/2103.00020) [[Model Card]](model-card.md) [[Colab]](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb) CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to th...', '["deep-learning","machine-learning","jupyter notebook"]', 'other', 31883, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/openai/CLIP","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# CLIP\n\n[[Blog]](https://openai.com/blog/clip/) [[Paper]](https://arxiv.org/abs/2103.00020) [[Model Card]](model-card.md) [[Colab]](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb)\n\nCLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3. We found CLIP matches the performance of the original ResNet50 on ImageNet ‚Äúzero-shot‚Äù without using any of the original 1.28M labeled examples, overcoming several major challenges in computer vision.\n\n\n\n## Approach\n\n![CLIP](CLIP.png)\n\n\n\n## Usage\n\nFirst, [install PyTorch 1.7.1](https://pytorch.org/get-started/locally/) (or later) and torchvision, as well as small additional dependencies, and then install this repo as a Python package. On a CUDA GPU machine, the following will do the trick:\n\n```bash\n$ conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n$ pip install ftfy regex tqdm\n$ pip install git+https://github.com/openai/CLIP.git\n```\n\nReplace `cudatoolkit=11.0` above with the appropriate CUDA version on your machine or `cpuonly` when installing on a machine without a GPU.\n\n```python\nimport torch\nimport clip\nfrom PIL import Image\n\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\nmodel, preprocess = clip.load("ViT-B/32", device=device)\n\nimage = preprocess(Image.open("CLIP.png")).unsqueeze(0).to(device)\ntext = clip.tokenize(["a diagram", "a dog", "a cat"]).to(device)\n\nwith torch.no_grad():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    \n    logits_per_image, logits_per_text = model(image, text)\n    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n\nprint("Label probs:", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]\n```\n\n\n## API\n\nThe CLIP module `clip` provides the following methods:\n\n#### `clip.available_models()`\n\nReturns the names of the available CLIP models.\n\n#### `clip.load(name, device=..., jit=False)`\n\nReturns the model and the TorchVision transform needed by the model, specified by the model name returned by `clip.available_models()`. It will download the model as necessary. The `name` argument can also be a path to a local checkpoint.\n\nThe device to run the model can be optionally specified, and the default is to use the first CUDA device if there is any, otherwise the CPU. When `jit` is `False`, a non-JIT version of the model will be loaded.\n\n#### `clip.tokenize(text: Union[str, List[str]], context_length=77)`\n\nReturns a LongTensor containing tokenized sequences of given text input(s). This can be used as the input to the model\n\n---\n\nThe model returned by `clip.load()` supports the following methods:\n\n#### `model.encode_image(image: Tensor)`\n\nGiven a batch of images, returns the image features encoded by the vision portion of the CLIP model.\n\n#### `model.encode_text(text: Tensor)`\n\nGiven a batch of text tokens, returns the text features encoded by the language portion of the CLIP model.\n\n#### `model(image: Tensor, text: Tensor)`\n\nGiven a batch of images and a batch of text tokens, returns two Tensors, containing the logit scores corresponding to each image and text input. The values are cosine similarities between the corresponding image and text features, times 100.\n\n\n\n## More Examples\n\n### Zero-Shot Prediction\n\nThe code below performs zero-shot prediction using CLIP, as shown in Appendix B in the paper. This example takes an image from the [CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), and predicts the most likely labels among the 100 textual labels from the dataset.\n\n```python\nimport os\nimport clip\nimport torch\nfrom torchvision.datasets import CIFAR100\n\n# Load the model\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\nmodel, preprocess = clip.load(''ViT-B/32'', device)\n\n# Download the dataset\ncifar100 = CIFAR100(root=os.path.expanduser("~/.cache"), download=True, train=False)\n\n# Prepare the inputs\nimage, class_id = cifar100[3637]\nimage_input = preprocess(image).unsqueeze(0).to(device)\ntext_inputs = torch.cat([clip.tokenize(f"a photo of a {c}") for c in cifar100.classes]).to(device)\n\n# Calculate features\nwith torch.no_grad():\n    image_features = model.encode_image(image_input)\n    text_features = model.encode_text(text_inputs)\n\n# Pick the top 5 most similar labels for the image\nimage_features /= image_features.norm(dim=-1, keepdim=True)\ntext_features /= text_features.norm(dim=-1, keepdim=True)\nsimilarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\nvalues, indices = similarity[0].topk(5)\n\n# Print the result\nprint("\nTop predictions:\n")\nfor value, index in zip(values, indices):\n    print(f"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%")\n```\n\nThe output will look like the following (the exact numbers may be slightly different depending on the compute device):\n\n```\nTop predictions:\n\n           snake: 65.31%\n          turtle: 12.29%\n    sweet_pepper: 3.83%\n          lizard: 1.88%\n       crocodile: 1.75%\n```\n\nNote that this example uses the `encode_image()` and `encode_text()` methods that return the encoded features of given inputs.\n\n\n### Linear-probe evaluation\n\nThe example below uses [scikit-learn](https://scikit-learn.org/) to perform logistic regression on image features.\n\n```python\nimport os\nimport clip\nimport torch\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR100\nfrom tqdm import tqdm\n\n# Load the model\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\nmodel, preprocess = clip.load(''ViT-B/32'', device)\n\n# Load the dataset\nroot = os.path.expanduser("~/.cache")\ntrain = CIFAR100(root, download=True, train=True, transform=preprocess)\ntest = CIFAR100(root, download=True, train=False, transform=preprocess)\n\n\ndef get_features(dataset):\n    all_features = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n            features = model.encode_image(images.to(device))\n\n            all_features.append(features)\n            all_labels.append(labels)\n\n    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n\n# Calculate the image features\ntrain_features, train_labels = get_features(train)\ntest_features, test_labels = get_features(test)\n\n# Perform logistic regression\nclassifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\nclassifier.fit(train_features, train_labels)\n\n# Evaluate using the logistic regression classifier\npredictions = classifier.predict(test_features)\naccuracy = np.mean((test_labels == predictions).astype(float)) * 100.\nprint(f"Accuracy = {accuracy:.3f}")\n```\n\nNote that the `C` value should be determined via a hyperparameter sweep using a validation split.\n\n\n## See Also\n\n* [OpenCLIP](https://github.com/mlfoundations/open_clip): includes larger and independently trained CLIP models up to ViT-G/14\n* [Hugging Face implementation of CLIP](https://huggingface.co/docs/transformers/model_doc/clip): for easier integration with the HF ecosystem\n', '{"language":"Jupyter Notebook","stars":31883,"forks":3858,"watchers":31883,"open_issues":265,"topics":["deep-learning","machine-learning"],"default_branch":"main","size_kb":9140,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:openai:CLIP.git","source_url":"https://github.com/openai/CLIP.git"},{"type":"has_code","target_id":"github:mlfoundations:open_clip","source_url":"https://github.com/mlfoundations/open_clip"}]', NULL, 'MIT', 'approved', 65, '4a11535c9dd86fc0584b456ad8f4118a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-openai-CLIP from https://github.com/openai.png
Image converted to WebP: data/images/github-openai-CLIP.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-Lightning-AI-pytorch-lightning', 'github--lightning-ai--pytorch-lightning', 'pytorch-lightning', 'Lightning-AI', '<div align="center"> <img alt="Lightning" src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/ptl_banner.png" width="800px" style="max-width: 100%;"> <br/> <br/> **The deep learning framework to pretrain and finetune AI models.** **Deploying models?** Check out LitServe, the PyTorch Lightning for inference engines ______________________________________________________________________ <p align="center"> <a href="#quick-start" style="margin: 0 10px;">Quick start</a> ‚Ä¢ <a href="#ex...', '["ai","artificial-intelligence","data-science","deep-learning","machine-learning","python","pytorch","python"]', 'other', 30554, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/Lightning-AI/pytorch-lightning","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<div align="center">\n\n<img alt="Lightning" src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/ptl_banner.png" width="800px" style="max-width: 100%;">\n\n<br/>\n<br/>\n\n**The deep learning framework to pretrain and finetune AI models.**\n\n**Deploying models?** Check out [LitServe](https://github.com/Lightning-AI/litserve?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme), the PyTorch Lightning for inference engines\n\n______________________________________________________________________\n\n<p align="center">\n    <a href="#quick-start" style="margin: 0 10px;">Quick start</a> ‚Ä¢\n  <a href="#examples">Examples</a> ‚Ä¢\n  <a href="#why-pytorch-lightning">PyTorch Lightning</a> ‚Ä¢\n  <a href="#lightning-fabric-expert-control">Fabric</a> ‚Ä¢\n  <a href="https://lightning.ai/?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme">Lightning Cloud</a> ‚Ä¢   \n  <a href="#community">Community</a> ‚Ä¢\n  <a href="https://pytorch-lightning.readthedocs.io/en/stable/">Docs</a>\n</p>\n\n<!-- DO NOT ADD CONDA DOWNLOADS... README CHANGES MUST BE APPROVED BY EDEN OR WILL -->\n\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/pytorch-lightning)](https://pypi.org/project/pytorch-lightning/)\n[![PyPI Status](https://badge.fury.io/py/pytorch-lightning.svg)](https://badge.fury.io/py/pytorch-lightning)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/pytorch-lightning)](https://pepy.tech/project/pytorch-lightning)\n[![Conda](https://img.shields.io/conda/v/conda-forge/lightning?label=conda&color=success)](https://anaconda.org/conda-forge/lightning)\n[![codecov](https://codecov.io/gh/Lightning-AI/pytorch-lightning/graph/badge.svg?token=SmzX8mnKlA)](https://codecov.io/gh/Lightning-AI/pytorch-lightning)\n\n[![Discord](https://img.shields.io/discord/1077906959069626439?style=plastic)](https://discord.gg/VptPCZkGNa)\n![GitHub commit activity](https://img.shields.io/github/commit-activity/w/lightning-ai/lightning)\n[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/Lightning-AI/pytorch-lightning/blob/master/LICENSE)\n\n<!--\n[![CodeFactor](https://www.codefactor.io/repository/github/Lightning-AI/lightning/badge)](https://www.codefactor.io/repository/github/Lightning-AI/lightning)\n-->\n\n</div>\n\n<div align="center">\n  \n<p align="center">\n\n&nbsp;\n\n<a target="_blank" href="https://lightning.ai/docs/pytorch/latest/starter/introduction.html?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme#define-a-lightningmodule">\n  <img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/get-started-badge.svg" height="36px" alt="Get started"/>\n</a>\n\n</p>\n\n</div>\n\n&nbsp;\n\n# Looking for GPUs?\nOver 340,000 developers use [Lightning Cloud](https://lightning.ai/?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) - purpose-built for PyTorch and PyTorch Lightning. \n- [GPUs](https://lightning.ai/pricing?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) from $0.19.   \n- [Clusters](https://lightning.ai/clusters?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme): frontier-grade training/inference clusters.   \n- [AI Studio (vibe train)](https://lightning.ai/studios?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme): workspaces where AI helps you debug, tune and vibe train.\n- [AI Studio (vibe deploy)](https://lightning.ai/studios?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme): workspaces where AI helps you optimize, and deploy models.     \n- [Notebooks](https://lightning.ai/notebooks?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme): Persistent GPU workspaces where AI helps you code and analyze.\n- [Inference](https://lightning.ai/deploy?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme): Deploy models as inference APIs.   \n\n<a id="why-pytorch-lightning"></a>\n# Why PyTorch Lightning?   \n\nTraining models in plain PyTorch is tedious and error-prone - you have to manually handle things like backprop, mixed precision, multi-GPU, and distributed training, often rewriting code for every new project. PyTorch Lightning organizes PyTorch code to automate those complexities so you can focus on your model and data, while keeping full control and scaling from CPU to multi-node without changing your core code. But if you want control of those things, you can still opt into [expert-level control](#lightning-fabric-expert-control).   \n\nFun analogy: If PyTorch is Javascript, PyTorch Lightning is ReactJS or NextJS.\n\n# Lightning has 2 core packages\n\n[PyTorch Lightning: Train and deploy PyTorch at scale](#why-pytorch-lightning).\n<br/>\n[Lightning Fabric: Expert control](#lightning-fabric-expert-control).\n\nLightning gives you granular control over how much abstraction you want to add over PyTorch.\n\n<div align="center">\n    <img src="https://pl-public-data.s3.amazonaws.com/assets_lightning/continuum.png" width="80%">\n</div>\n\n&nbsp;\n\n# Quick start\nInstall Lightning:\n\n```bash\npip install lightning\n```\n\n<!-- following section will be skipped from PyPI description -->\n\n<details>\n  <summary>Advanced install options</summary>\n    <!-- following section will be skipped from PyPI description -->\n\n#### Install with optional dependencies\n\n```bash\npip install lightning[''extra'']\n```\n\n#### Conda\n\n```bash\nconda install lightning -c conda-forge\n```\n\n#### Install stable version\n\nInstall future release from the source\n\n```bash\npip install https://github.com/Lightning-AI/lightning/archive/refs/heads/release/stable.zip -U\n```\n\n#### Install bleeding-edge\n\nInstall nightly from the source (no guarantees)\n\n```bash\npip install https://github.com/Lightning-AI/lightning/archive/refs/heads/master.zip -U\n```\n\nor from testing PyPI\n\n```bash\npip install -iU https://test.pypi.org/simple/ pytorch-lightning\n```\n\n</details>\n<!-- end skipping PyPI description -->\n\n### PyTorch Lightning example\nDefine the training workflow. Here''s a toy example ([explore real examples](https://lightning.ai/lightning-ai/studios?view=public&section=featured&query=pytorch+lightning&utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme)):\n\n```python\n# main.py\n# ! pip install torchvision\nimport torch, torch.nn as nn, torch.utils.data as data, torchvision as tv, torch.nn.functional as F\nimport lightning as L\n\n# --------------------------------\n# Step 1: Define a LightningModule\n# --------------------------------\n# A LightningModule (nn.Module subclass) defines a full *system*\n# (ie: an LLM, diffusion model, autoencoder, or simple image classifier).\n\n\nclass LitAutoEncoder(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))\n        self.decoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))\n\n    def forward(self, x):\n        # in lightning, forward defines the prediction/inference actions\n        embedding = self.encoder(x)\n        return embedding\n\n    def training_step(self, batch, batch_idx):\n        # training_step defines the train loop. It is independent of forward\n        x, _ = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = F.mse_loss(x_hat, x)\n        self.log("train_loss", loss)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer\n\n\n# -------------------\n# Step 2: Define data\n# -------------------\ndataset = tv.datasets.MNIST(".", download=True, transform=tv.transforms.ToTensor())\ntrain, val = data.random_split(dataset, [55000, 5000])\n\n# -------------------\n# Step 3: Train\n# -------------------\nautoencoder = LitAutoEncoder()\ntrainer = L.Trainer()\ntrainer.fit(autoencoder, data.DataLoader(train), data.DataLoader(val))\n```\n\nRun the model on your terminal\n\n```bash\npip install torchvision\npython main.py\n```\n\n&nbsp;\n\n\n# Convert from PyTorch to PyTorch Lightning\n\nPyTorch Lightning is just organized PyTorch - Lightning disentangles PyTorch code to decouple the science from the engineering.\n\n![PT to PL](docs/source-pytorch/_static/images/general/pl_quick_start_full_compressed.gif)\n\n&nbsp;\n\n----\n\n### Examples\nExplore various types of training possible with PyTorch Lightning. Pretrain and finetune ANY kind of model to perform ANY task like classification, segmentation, summarization and more:    \n\n| Task | Description | Run |\n|------|--------------|-----|\n| [Hello world](https://lightning.ai/lightning-ai/studios/pytorch-lightning-hello-world?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Pretrain - Hello world example | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/pytorch-lightning-hello-world?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [Image classification](https://lightning.ai/lightning-ai/studios/image-classification-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Finetune - ResNet-34 model to classify images of cars | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/image-classification-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [Image segmentation](https://lightning.ai/lightning-ai/studios/image-segmentation-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Finetune - ResNet-50 model to segment images | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/image-segmentation-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [Object detection](https://lightning.ai/lightning-ai/studios/object-detection-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Finetune - Faster R-CNN model to detect objects | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/object-detection-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [Text classification](https://lightning.ai/lightning-ai/studios/text-classification-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Finetune - text classifier (BERT model) | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/text-classification-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [Text summarization](https://lightning.ai/lightning-ai/studios/text-summarization-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Finetune - text summarization (Hugging Face transformer model) | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/text-summarization-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [Audio generation](https://lightning.ai/lightning-ai/studios/finetune-a-personal-ai-music-generator?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Finetune - audio generator (transformer model) | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/finetune-a-personal-ai-music-generator?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [LLM finetuning](https://lightning.ai/lightning-ai/studios/finetune-an-llm-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Finetune - LLM (Meta Llama 3.1 8B) | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/finetune-an-llm-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [Image generation](https://lightning.ai/lightning-ai/studios/train-a-diffusion-model-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Pretrain - Image generator (diffusion model) | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/train-a-diffusion-model-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [Recommendation system](https://lightning.ai/lightning-ai/studios/recommendation-system-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Train - recommendation system (factorization and embedding) | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/recommendation-system-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [Time-series forecasting](https://lightning.ai/lightning-ai/studios/time-series-forecasting-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Train - Time-series forecasting with LSTM | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/time-series-forecasting-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n\n\n______________________________________________________________________\n\n## Advanced features\n\nLightning has over [40+ advanced features](https://lightning.ai/docs/pytorch/stable/common/trainer.html?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme#trainer-flags)\ndesigned for professional AI research at scale.\n\nHere are some examples:\n\n<div align="center">\n    <img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/features_2.jpg" max-height="600px">\n  </div>\n\n<details>\n  <summary>Train on 1000s of GPUs without code changes</summary>\n\n```python\n# 8 GPUs\n# no code changes needed\ntrainer = Trainer(accelerator="gpu", devices=8)\n\n# 256 GPUs\ntrainer = Trainer(accelerator="gpu", devices=8, num_nodes=32)\n```\n\n</details>\n\n<details>\n  <summary>Train on other accelerators like TPUs without code changes</summary>\n\n```python\n# no code changes needed\ntrainer = Trainer(accelerator="tpu", devices=8)\n```\n\n</details>\n\n<details>\n  <summary>16-bit precision</summary>\n\n```python\n# no code changes needed\ntrainer = Trainer(precision=16)\n```\n\n</details>\n\n<details>\n  <summary>Experiment managers</summary>\n\n```python\nfrom lightning import loggers\n\n# tensorboard\ntrainer = Trainer(logger=TensorBoardLogger("logs/"))\n\n# weights and biases\ntrainer = Trainer(logger=loggers.WandbLogger())\n\n# comet\ntrainer = Trainer(logger=loggers.CometLogger())\n\n# mlflow\ntrainer = Trainer(logger=loggers.MLFlowLogger())\n\n# neptune\ntrainer = Trainer(logger=loggers.NeptuneLogger())\n\n# ... and dozens more\n```\n\n</details>\n\n<details>\n\n<summary>Early Stopping</summary>\n\n```python\nes = EarlyStopping(monitor="val_loss")\ntrainer = Trainer(callbacks=[es])\n```\n\n</details>\n\n<details>\n  <summary>Checkpointing</summary>\n\n```python\ncheckpointing = ModelCheckpoint(monitor="val_loss")\ntrainer = Trainer(callbacks=[checkpointing])\n```\n\n</details>\n\n<details>\n  <summary>Export to torchscript (JIT) (production use)</summary>\n\n```python\n# torchscript\nautoencoder = LitAutoEncoder()\ntorch.jit.save(autoencoder.to_torchscript(), "model.pt")\n```\n\n</details>\n\n<details>\n  <summary>Export to ONNX (production use)</summary>\n\n```python\n# onnx\nwith tempfile.NamedTemporaryFile(suffix=".onnx", delete=False) as tmpfile:\n    autoencoder = LitAutoEncoder()\n    input_sample = torch.randn((1, 64))\n    autoencoder.to_onnx(tmpfile.name, input_sample, export_params=True)\n    os.path.isfile(tmpfile.name)\n```\n\n</details>\n\n______________________________________________________________________\n\n## Advantages over unstructured PyTorch\n\n- Models become hardware agnostic\n- Code is clear to read because engineering code is abstracted away\n- Easier to reproduce\n- Make fewer mistakes because lightning handles the tricky engineering\n- Keeps all the flexibility (LightningModules are still PyTorch modules), but removes a ton of boilerplate\n- Lightning has dozens of integrations with popular machine learning tools.\n- [Tested rigorously with every new PR](https://github.com/Lightning-AI/lightning/tree/master/tests). We test every combination of PyTorch and Python supported versions, every OS, multi GPUs and even TPUs.\n- Minimal running speed overhead (about 300 ms per epoch compared with pure PyTorch).\n\n______________________________________________________________________\n\n<div align="center">\n    <a href="https://lightning.ai/docs/pytorch/stable/?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme">Read the PyTorch Lightning docs</a>\n</div>\n\n______________________________________________________________________\n\n&nbsp;\n&nbsp;\n\n# Lightning Fabric: Expert control\n\nRun on any device at any scale with expert-level control over PyTorch training loop and scaling strategy. You can even write your own Trainer.\n\nFabric is designed for the most complex models like foundation model scaling, LLMs, diffusion, transformers, reinforcement learning, active learning. Of any size.\n\n<table>\n<tr>\n<th>What to change</th>\n<th>Resulting Fabric Code (copy me!)</th>\n</tr>\n<tr>\n<td>\n<sub>\n\n```diff\n+ import lightning as L\n  import torch; import torchvision as tv\n\n dataset = tv.datasets.CIFAR10("data", download=True,\n                               train=True,\n                               transform=tv.transforms.ToTensor())\n\n+ fabric = L.Fabric()\n+ fabric.launch()\n\n  model = tv.models.resnet18()\n  optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n- device = "cuda" if torch.cuda.is_available() else "cpu"\n- model.to(device)\n+ model, optimizer = fabric.setup(model, optimizer)\n\n  dataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\n+ dataloader = fabric.setup_dataloaders(dataloader)\n\n  model.train()\n  num_epochs = 10\n  for epoch in range(num_epochs):\n      for batch in dataloader:\n          inputs, labels = batch\n-         inputs, labels = inputs.to(device), labels.to(device)\n          optimizer.zero_grad()\n          outputs = model(inputs)\n          loss = torch.nn.functional.cross_entropy(outputs, labels)\n-         loss.backward()\n+         fabric.backward(loss)\n          optimizer.step()\n          print(loss.data)\n```\n\n</sub>\n<td>\n<sub>\n\n```Python\nimport lightning as L\nimport torch; import torchvision as tv\n\ndataset = tv.datasets.CIFAR10("data", download=True,\n                              train=True,\n                              transform=tv.transforms.ToTensor())\n\nfabric = L.Fabric()\nfabric.launch()\n\nmodel = tv.models.resnet18()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\nmodel, optimizer = fabric.setup(model, optimizer)\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\ndataloader = fabric.setup_dataloaders(dataloader)\n\nmodel.train()\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        inputs, labels = batch\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = torch.nn.functional.cross_entropy(outputs, labels)\n        fabric.backward(loss)\n        optimizer.step()\n        print(loss.data)\n```\n\n</sub>\n</td>\n</tr>\n</table>\n\n## Key features\n\n<details>\n  <summary>Easily switch from running on CPU to GPU (Apple Silicon, CUDA, ‚Ä¶), TPU, multi-GPU or even multi-node training</summary>\n\n```python\n# Use your available hardware\n# no code changes needed\nfabric = Fabric()\n\n# Run on GPUs (CUDA or MPS)\nfabric = Fabric(accelerator="gpu")\n\n# 8 GPUs\nfabric = Fabric(accelerator="gpu", devices=8)\n\n# 256 GPUs, multi-node\nfabric = Fabric(accelerator="gpu", devices=8, num_nodes=32)\n\n# Run on TPUs\nfabric = Fabric(accelerator="tpu")\n```\n\n</details>\n\n<details>\n  <summary>Use state-of-the-art distributed training strategies (DDP, FSDP, DeepSpeed) and mixed precision out of the box</summary>\n\n```python\n# Use state-of-the-art distributed training techniques\nfabric = Fabric(strategy="ddp")\nfabric = Fabric(strategy="deepspeed")\nfabric = Fabric(strategy="fsdp")\n\n# Switch the precision\nfabric = Fabric(precision="16-mixed")\nfabric = Fabric(precision="64")\n```\n\n</details>\n\n<details>\n  <summary>All the device logic boilerplate is handled for you</summary>\n\n```diff\n  # no more of this!\n- model.to(device)\n- batch.to(device)\n```\n\n</details>\n\n<details>\n  <summary>Build your own custom Trainer using Fabric primitives for training checkpointing, logging, and more</summary>\n\n```python\nimport lightning as L\n\n\nclass MyCustomTrainer:\n    def __init__(self, accelerator="auto", strategy="auto", devices="auto", precision="32-true"):\n        self.fabric = L.Fabric(accelerator=accelerator, strategy=strategy, devices=devices, precision=precision)\n\n    def fit(self, model, optimizer, dataloader, max_epochs):\n        self.fabric.launch()\n\n        model, optimizer = self.fabric.setup(model, optimizer)\n        dataloader = self.fabric.setup_dataloaders(dataloader)\n        model.train()\n\n        for epoch in range(max_epochs):\n            for batch in dataloader:\n                input, target = batch\n                optimizer.zero_grad()\n                output = model(input)\n                loss = loss_fn(output, target)\n                self.fabric.backward(loss)\n                optimizer.step()\n```\n\nYou can find a more extensive example in our [examples](examples/fabric/build_your_own_trainer)\n\n</details>\n\n______________________________________________________________________\n\n<div align="center">\n    <a href="https://lightning.ai/docs/fabric/stable/?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme">Read the Lightning Fabric docs</a>\n</div>\n\n______________________________________________________________________\n\n&nbsp;\n&nbsp;\n\n## Examples\n\n###### Self-supervised Learning\n\n- [CPC transforms](https://lightning-bolts.readthedocs.io/en/stable/transforms/self_supervised.html#cpc-transforms)\n- [Moco v2 transforms](https://lightning-bolts.readthedocs.io/en/stable/transforms/self_supervised.html#moco-v2-transforms)\n- [SimCLR transforms](https://lightning-bolts.readthedocs.io/en/stable/transforms/self_supervised.html#simclr-transforms)\n\n###### Convolutional Architectures\n\n- [GPT-2](https://lightning-bolts.readthedocs.io/en/stable/models/convolutional.html#gpt-2)\n- [UNet](https://lightning-bolts.readthedocs.io/en/stable/models/convolutional.html#unet)\n\n###### Reinforcement Learning\n\n- [DQN Loss](https://lightning-bolts.readthedocs.io/en/stable/losses.html#dqn-loss)\n- [Double DQN Loss](https://lightning-bolts.readthedocs.io/en/stable/losses.html#double-dqn-loss)\n- [Per DQN Loss](https://lightning-bolts.readthedocs.io/en/stable/losses.html#per-dqn-loss)\n\n###### GANs\n\n- [Basic GAN](https://lightning-bolts.readthedocs.io/en/stable/models/gans.html#basic-gan)\n- [DCGAN](https://lightning-bolts.readthedocs.io/en/stable/models/gans.html#dcgan)\n\n###### Classic ML\n\n- [Logistic Regression](https://lightning-bolts.readthedocs.io/en/stable/models/classic_ml.html#logistic-regression)\n- [Linear Regression](https://lightning-bolts.readthedocs.io/en/stable/models/classic_ml.html#linear-regression)\n\n&nbsp;\n&nbsp;\n\n## Continuous Integration\n\nLightning is rigorously tested across multiple CPUs, GPUs and TPUs and against major Python and PyTorch versions.\n\n###### \*Codecov is > 90%+ but build delays may show less\n\n<details>\n  <summary>Current build statuses</summary>\n\n<center>\n\n|       System / PyTorch ver.        | 1.13                                                                                                                                                                                                                            | 2.0                                                                                                                                                                                                                             |                                                                                                               2.1                                                                                                               |\n| :--------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n|        Linux py3.9 \[GPUs\]        |  |  | [![Build Status](https://dev.azure.com/Lightning-AI/lightning/_apis/build/status%2Fpytorch-lightning%20%28GPUs%29?branchName=master)](https://dev.azure.com/Lightning-AI/lightning/_build/latest?definitionId=24&branchName=master) |\n|  Linux (multiple Python versions)  | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                                 | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                                 |                 [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                 |\n|   OSX (multiple Python versions)   | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                                 | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                                 |                 [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                 |\n| Windows (multiple Python versions) | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                                 | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                                 |                 [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                 |\n\n</center>\n</details>\n\n&nbsp;\n&nbsp;\n\n## Community\n\nThe lightning community is maintained by\n\n- [10+ core contributors](https://lightning.ai/docs/pytorch/latest/community/governance.html) who are all a mix of professional engineers, Research Scientists, and Ph.D. students from top AI labs.\n- 800+ community contributors.\n\nWant to help us build Lightning and reduce boilerplate for thousands of researchers? [Learn how to make your first contribution here](https://lightning.ai/docs/pytorch/stable/generated/CONTRIBUTING.html?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme)\n\nLightning is also part of the [PyTorch ecosystem](https://pytorch.org/ecosystem/) which requires projects to have solid testing, documentation and support.\n\n### Asking for help\n\nIf you have any questions please:\n\n1. [Read the docs](https://lightning.ai/docs?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme).\n1. [Search through existing Discussions](https://github.com/Lightning-AI/lightning/discussions), or [add a new question](https://github.com/Lightning-AI/lightning/discussions/new)\n1. [Join our discord](https://discord.com/invite/tfXFetEZxv).\n', '{"language":"Python","stars":30554,"forks":3619,"watchers":30554,"open_issues":920,"topics":["ai","artificial-intelligence","data-science","deep-learning","machine-learning","python","pytorch"],"default_branch":"master","size_kb":132374,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:Lightning-AI:litserve","source_url":"https://github.com/Lightning-AI/litserve?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"},{"type":"has_code","target_id":"github:Lightning-AI:pytorch-lightning","source_url":"https://github.com/Lightning-AI/pytorch-lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"}]', NULL, 'Apache-2.0', 'approved', 80, '500f6ac54ba616a0e2a7c606810b9b64', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-Lightning-AI-pytorch-lightning from https://github.com/Lightning-AI.png
Image converted to WebP: data/images/github-Lightning-AI-pytorch-lightning.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-AMAI-GmbH-AI-Expert-Roadmap', 'github--amai-gmbh--ai-expert-roadmap', 'AI-Expert-Roadmap', 'AMAI-GmbH', '<p align="center"> <a href="https://github.com/AMAI-GmbH/AI-Expert-Roadmap"> <img src="https://uploads-ssl.webflow.com/58e6a2b25c28230d367487ad/5c32232ecb585fcc5c4645e1_icon_machine-learning.svg" alt="Developer Roadmap" width="96" height="96"> </a> <h2 align="center">i.am.ai<br>AI Expert Roadmap</h2> <p align="center">Roadmap to becoming an Artificial Intelligence Expert in 2022</p> <p align="center"> <a href="https://twitter.com/home?status=https://i.am.ai/roadmap Roadmap to becoming an Arti...', '["ai","ai-roadmap","artificial-intelligence","data-analysis","data-science","deep-learning","machine-learning","neural-network","roadmap","study-plan","javascript"]', 'other', 30531, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/AMAI-GmbH/AI-Expert-Roadmap","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <a href="https://github.com/AMAI-GmbH/AI-Expert-Roadmap">\n    <img src="https://uploads-ssl.webflow.com/58e6a2b25c28230d367487ad/5c32232ecb585fcc5c4645e1_icon_machine-learning.svg" alt="Developer Roadmap" width="96" height="96">\n  </a>\n  <h2 align="center">i.am.ai<br>AI Expert Roadmap</h2>\n  <p align="center">Roadmap to becoming an Artificial Intelligence Expert in 2022</p>\n  <p align="center">\n      <a href="https://twitter.com/home?status=https://i.am.ai/roadmap Roadmap to becoming an Artificial Intelligence Expert in 2022" target="_blank"><img src="https://img.shields.io/badge/tweet-blue.svg?logo=twitter&logoColor=white" style="display: inherit;"/></a>\n      <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://i.am.ai/roadmap&title=&summary=Roadmap to becoming an Artificial Intelligence Expert in 2022&source=" target="_blank"><img src="https://img.shields.io/badge/post-blue.svg?logo=linkedin&logoColor=white" style="display: inherit;"/></a>\n      <a href="https://github.com/AMAI-GmbH/AI-Expert-Roadmap"><img src="https://img.shields.io/badge/Roadmap-2022-yellowgreen.svg" style="display: inherit;"/></a>\n      <a href="https://am.ai?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Badge" target="_blank"><img alt="AMAI GmbH" src="https://img.shields.io/badge/Author-AMAI GmbH-blue.svg" style="display: inherit;"/></a>\n<a href="https://opensource.org/licenses/MIT/" target="_blank"><img alt="MIT License" src="https://img.shields.io/badge/License-MIT-blue.svg" style="display: inherit;"/></a>\n  </p>\n  <br>\n</p>\n\nBelow you find a set of charts demonstrating the paths that you can take and the technologies that you would want to adopt in order to become a data scientist, machine learning or an AI expert. We made these charts for our new employees to make them AI Experts but we wanted to share them here to help the community.\n\nIf you are interested to become an AI EXPERT at [AMAI](https://www.linkedin.com/company/amai-gmbh/?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Become+Expert) in Germany, or you want to [hire an AI Expert](https://am.ai?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Hire+Expert), please say [hi@am.ai](mailto:hi@am.ai).\n\n## Note\n\nüëâ An **interactive version with links to follow** about each bullet of the list can be found at [i.am.ai/roadmap](https://i.am.ai/roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Interactive) üëà\n\nTo receive updates [star :star:](https://github.com/AMAI-GmbH/AI-Expert-Roadmap/stargazers) and watch :eyes: the [GitHub Repo](https://github.com/AMAI-GmbH/AI-Expert-Roadmap/) to get notified, when we add new content to stay on the top of the most recent research.\n\nFollow our [AI Newsletter](https://i.am.ai/newsletter?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Newsletter) to stay up to date with the latest developments in AI. We cover new use cases and research topics.\n\n## Disclaimer\n\nThe purpose of these roadmaps is to give you an idea about the landscape and to guide you if you are confused about what to learn next and not to encourage you to pick what is hip and trendy. You should grow some understanding of why one tool would be better suited for some cases than the other and remember hip and trendy never means best suited for the job.\n\n## Introduction\n\n<p align="center">\n  <a href="https://i.am.ai/roadmap#introduction?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Introduction" target="_blank">\n      <img src="./images/intro.svg"/>\n  </a>\n</p>\n\n## Fundamentals\n\n<p align="center">\n  <a href="https://i.am.ai/roadmap#fundamentals?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Fundamentals" target="_blank">\n      <img src="./images/fundamentals.svg"/>\n  </a>\n</p>\n\n## Data Science Roadmap\n\n<p align="center">\n  <a href="https://i.am.ai/roadmap#data-science-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+DataScience" target="_blank">\n      <img src="./images/datascience.svg"/>\n  </a>\n</p>\n\n## Machine Learning Roadmap\n\n<p align="center">\n  <a href="https://i.am.ai/roadmap#machine-learning-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+MachineLearning" target="_blank">\n      <img src="./images/machine_learning.svg"/>\n  </a>\n</p>\n\n## Deep Learning Roadmap\n\n<p align="center">\n  <a href="https://i.am.ai/roadmap#deep-learning-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+DeepLearning" target="_blank">\n      <img src="./images/deep_learning.svg"/>\n  </a>\n</p>\n\n## Data Engineer Roadmap\n\n<p align="center">\n  <a href="https://i.am.ai/roadmap#data-engineer-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+DataEngineer" target="_blank">\n      <img src="./images/data_engineer.svg"/>\n  </a>\n</p>\n\n## Big Data Engineer Roadmap\n\n<p align="center">\n  <a href="https://i.am.ai/roadmap#big-data-engineer-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+BigDataEngineer" target="_blank">\n      <img src="./images/big_data_engineer.svg"/>\n  </a>\n</p>\n\n## üö¶ Wrap Up\n\nIf you think any of the roadmaps can be improved, please do open a PR with any updates and submit any issues. Also, we will continue to improve this, so you might want to watch/star this repository to revisit.\n\n## üôå Contribution\n\n> Have a look at the [contribution docs](./contributing.md) for how to update any of the roadmaps\n\n* Open pull request with improvements\n* Discuss ideas in issues\n* Spread the word\n* Reach out with any feedback\n\n## Supported By\n\n<a href="https://www.linkedin.com/company/amai-gmbh/?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+SupportedBy" target="_blank"><img alt="AMAI GmbH" src="./images/logos/amai.svg" style="display: inherit;max-width: 150px;"/></a>\n<a href="https://digitalhub-ai.de?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap" target="_blank"><img alt="AMAI GmbH" src="./images/logos/de-hub.svg" style="display: inherit; max-width: 150px;"/></a>\n', '{"language":"JavaScript","stars":30531,"forks":2549,"watchers":30531,"open_issues":15,"topics":["ai","ai-roadmap","artificial-intelligence","data-analysis","data-science","deep-learning","machine-learning","neural-network","roadmap","study-plan"],"default_branch":"main","size_kb":414,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:AMAI-GmbH:AI-Expert-Roadmap\">","source_url":"https://github.com/AMAI-GmbH/AI-Expert-Roadmap\">"},{"type":"has_code","target_id":"github:AMAI-GmbH:AI-Expert-Roadmap\"><img","source_url":"https://github.com/AMAI-GmbH/AI-Expert-Roadmap\"><img"},{"type":"has_code","target_id":"github:AMAI-GmbH:AI-Expert-Roadmap","source_url":"https://github.com/AMAI-GmbH/AI-Expert-Roadmap"},{"type":"has_code","target_id":"github:AMAI-GmbH:AI-Expert-Roadmap","source_url":"https://github.com/AMAI-GmbH/AI-Expert-Roadmap"}]', NULL, 'MIT', 'approved', 65, '5f1a62a38943510c58736e39d645c2b1', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-AMAI-GmbH-AI-Expert-Roadmap from https://github.com/AMAI-GmbH.png
Image converted to WebP: data/images/github-AMAI-GmbH-AI-Expert-Roadmap.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-iperov-DeepFaceLive', 'github--iperov--deepfacelive', 'DeepFaceLive', 'iperov', '<table align="center" border="0"> <tr><td colspan=2 align="center"> </td></tr> </table> <table align="center" border="0"> <tr><td colspan=2 align="center"> You can swap your face from a webcam or the face in the video using trained face models. Here is a list of available ready-to-use public face models. These persons do not exists. Similarities with real people are accidental. Except Keanu Reeves. He exists, and he''s breathtaking! </td></tr> <tr><td colspan=2 align="center"> <table align="ce...', '["deepfake","faceswap","machine-learning","real-time","streaming","videocall","webcam","python"]', 'other', 30200, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/iperov/DeepFaceLive","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<table align="center" border="0">\n\n<tr><td colspan=2 align="center">\n\n![](doc/deepfacelive_intro.png)\n\n![](doc/logo_onnx.png)![](doc/logo_directx.png)![](doc/logo_python.png)\n\n</td></tr>\n</table>\n<table align="center" border="0">\n\n<tr><td colspan=2 align="center">\n\n## Face Swap (DFM)\n\nYou can swap your face from a webcam or the face in the video using trained face models.\n\nHere is a list of available ready-to-use public face models.\n\nThese persons do not exists. Similarities with real people are accidental. Except Keanu Reeves. He exists, and he''s breathtaking!\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\n<table align="center" border="0">\n<tr><td align="center">\nKeanu Reeves\n\n<img src="doc/celebs/Keanu_Reeves/Keanu_Reeves.png" width=128></img>\n\n<a href="doc/celebs/Keanu_Reeves/examples.md">examples</a>\n</td><td align="center">\nIrina Arty\n\n<img src="doc/celebs/Irina_Arty/Irina_Arty.png" width=128></img>\n\nexamples\n</td><td align="center">\nMillie Park\n\n<img src="doc/celebs/Millie_Park/Millie_Park.png" width=128></img>\n\nexamples\n</td><td align="center">\nRob Doe\n\n<img src="doc/celebs/Rob_Doe/Rob_Doe.png" width=128></img>\n\n<a href="doc/celebs/Rob_Doe/examples.md">examples</a>\n</td><td align="center">\nJesse Stat\n\n<img src="doc/celebs/Jesse_Stat/Jesse_Stat.png" width=128></img>\n\nexamples\n</td></tr>\n\n</table>\n\n<table align="center" border="0">\n<tr><td align="center">\nBryan Greynolds\n\n<img src="doc/celebs/Bryan_Greynolds/Bryan_Greynolds.png" width=128></img>\n\n<a href="doc/celebs/Bryan_Greynolds/examples.md">examples</a>\n</td><td align="center">\nMr. Bean\n\n<img src="doc/celebs/Mr_Bean/Mr_Bean.png" width=128></img>\n\nexamples\n</td><td align="center">\nEwon Spice\n\n<img src="doc/celebs/Ewon_Spice/Ewon_Spice.png" width=128></img>\n\n<a href="doc/celebs/Ewon_Spice/examples.md">examples</a>\n\n</td><td align="center">\nNatasha Former\n\n<img src="doc/celebs/Natasha_Former/Natasha_Former.png" width=128></img>\n\n<a href="doc/celebs/Natasha_Former/examples.md">examples</a>\n\n</td><td align="center">\nEmily Winston\n\n<img src="doc/celebs/Emily_Winston/Emily_Winston.png" width=128></img>\n\n<a href="doc/celebs/Emily_Winston/examples.md">examples</a>\n\n</td></tr></table>\n<table align="center" border="0">\n<tr><td align="center">\nAva de Addario\n\n<img src="doc/celebs/Ava_de_Addario/Ava_de_Addario.png" width=128></img>\n\n<a href="doc/celebs/Ava_de_Addario/examples.md">examples</a>\n</td><td align="center">\nDilraba Dilmurat\n\n<img src="doc/celebs/Dilraba_Dilmurat/Dilraba_Dilmurat.png" width=128></img>\n\nexamples\n</td><td align="center">\nMatilda Bobbie\n\n<img src="doc/celebs/Matilda_Bobbie/Matilda_Bobbie.png" width=128></img>\n\n<a href="doc/celebs/Matilda_Bobbie/examples.md">examples</a>\n</td><td align="center">\nYohanna Coralson\n\n<img src="doc/celebs/Yohanna_Coralson/Yohanna_Coralson.png" width=128></img>\n\n<a href="doc/celebs/Yohanna_Coralson/examples.md">examples</a>\n\n</td><td align="center">\nAmber Song\n\n<img src="doc/celebs/Amber_Song/Amber_Song.png" width=128></img>\n\nexamples\n\n</td></tr></table>\n<table align="center" border="0">\n<tr align="center"><td align="center">\nKim Jarrey\n\n<img src="doc/celebs/Kim_Jarrey/Kim_Jarrey.png" width=128></img>\n\n<a href="doc/celebs/Kim_Jarrey/examples.md">examples</a>\n</td><td align="center">\nDavid Kovalniy\n\n<img src="doc/celebs/David_Kovalniy/David_Kovalniy.png" width=128></img>\n\n<a href="doc/celebs/David_Kovalniy/examples.md">examples</a>\n</td><td align="center">\nJackie Chan\n\n<img src="doc/celebs/Jackie_Chan/Jackie_Chan.png" width=128></img>\n\nexamples\n</td><td align="center">\nNicola Badge\n\n<img src="doc/celebs/Nicola_Badge/Nicola_Badge.png" width=128></img>\n\n<a href="doc/celebs/Nicola_Badge/examples.md">examples</a>\n</td><td align="center">\nJoker\n\n<img src="doc/celebs/Joker/Joker.png" width=128></img>\n\nexamples\n</td></tr></table>\n<table align="center" border="0">\n<tr align="center"><td>\nDean Wiesel\n\n<img src="doc/celebs/Dean_Wiesel/Dean_Wiesel.png" width=128></img>\n\n<a href="doc/celebs/Dean_Wiesel/examples.md">examples</a>\n</td><td align="center">\nSilwan Stillwone\n\n<img src="doc/celebs/Silwan_Stillwone/Silwan_Stillwone.png" width=128></img>\n\n<a href="doc/celebs/Silwan_Stillwone/examples.md">examples</a>\n</td><td align="center">\nTim Chrys\n\n<img src="doc/celebs/Tim_Chrys/Tim_Chrys.png" width=128></img>\n\n<a href="doc/celebs/Tim_Chrys/examples.md">examples</a>\n\n</td><td align="center">\nZahar Lupin\n\n<img src="doc/celebs/Zahar_Lupin/Zahar_Lupin.png" width=128></img>\n\n<a href="doc/celebs/Zahar_Lupin/examples.md">examples</a>\n</td><td align="center">\nTim Norland\n\n<img src="doc/celebs/Tim_Norland/Tim_Norland.png" width=128></img>\n\n<a href="doc/celebs/Tim_Norland/examples.md">examples</a>\n</td></tr></table>\n\n\n<table align="center" border="0">\n<tr align="center"><td>\nNatalie Fatman\n\n<img src="doc/celebs/Natalie_Fatman/Natalie_Fatman.png" width=128></img>\n\n<a href="doc/celebs/Natalie_Fatman/examples.md">examples</a>\n</td><td align="center">\nLiu Lice\n\n<img src="doc/celebs/Liu_Lice/Liu_Lice.png" width=128></img>\n\n<a href="doc/celebs/Liu_Lice/examples.md">examples</a>\n</td><td align="center">\nAlbica Johns\n\n<img src="doc/celebs/Albica_Johns/Albica_Johns.png" width=128></img>\n\n<a href="doc/celebs/Albica_Johns/examples.md">examples</a>\n\n</td><td align="center">\nMeggie Merkel\n\n<img src="doc/celebs/Meggie_Merkel/Meggie_Merkel.png" width=128></img>\n\n<a href="doc/celebs/Meggie_Merkel/examples.md">examples</a>\n</td><td align="center">\nTina Shift\n\n<img src="doc/celebs/Tina_Shift/Tina_Shift.png" width=128></img>\n\n<a href="doc/celebs/Tina_Shift/examples.md">examples</a>\n</td></tr></table>\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\nIf you want a higher quality or better face match, you can train your own face model using <a href="https://github.com/iperov/DeepFaceLab">DeepFaceLab</a>\n\nHere is an <a href="https://www.tiktok.com/@arnoldschwarzneggar/video/6995538782204300545">example</a> of Arnold Schwarzneggar trained on a particular face and used in a video call. Read the FAQ for more information.\n\n</td></tr>\n\n</table>\n<table align="center" border="0">\n\n<tr><td colspan=2 align="center">\n\n## Face Swap (Insight)\n\nYou can swap your face from a webcam or the face in the video using your own single photo.\n\n<img src="doc/lukashenko.png" width=128></img>\n\n<img src="doc/insight_faceswap_example.gif"></img>\n\n</td></tr>\n\n</table>\n<table align="center" border="0">\n\n<tr><td colspan=2 align="center">\n\n## Face Animator\n\nThere is also a Face Animator module in DeepFaceLive app. You can control a static face picture using video or your own face from the camera. The quality is not the best, and requires fine face matching and tuning parameters for every face pair, but enough for funny videos and memes or real-time streaming at 25 fps using 35 TFLOPS GPU.\n\n<img src="doc/face_animator_example.gif"></img>\n\n[![Stranger Things theme intro acapella](doc/Ng1C78Ceyxg_screenshot.png)](https://www.youtube.com/watch?v=Ng1C78Ceyxg)\n\nHere is a [mini video](doc/FaceAnimator_tutor.webm?raw=true) showing the process of setting up the Face Animator for Obama controlling Kim Chen''s face.\n\n</td></tr>\n\n</table>\n\n<table align="center" border="0">\n\n<tr><td colspan=2 align="center">\n\n## System requirements\n\nany DirectX12 compatible graphics card\n\n(Recommended RTX 2070+ / Radeon RX 5700 XT+ )\n\nModern CPU with AVX instructions\n\n4GB RAM, 32GB+ paging file\n\nWindows 10\n\n</td></tr>\n<tr><td colspan=2 align="center">\n\n## Documentation\n\n</td></tr>\n<tr><td align="right">\nWindows\n</td><td align="left">\n\n<a href="doc/windows/main_setup.md">Main setup</a>\n\n- <a href="doc/windows/for_streaming.md">additional setup for streaming</a>\n\n- <a href="doc/windows/for_video_calls.md">additional setup for video calls</a>       \n\n<a href="doc/windows/using_android_phone_camera.md">Using Android phone camera</a>  \n\n</td></tr>\n<tr><td align="right">\nLinux\n</td><td align="left">\n<a href="build/linux">Build info</a>\n</td></tr>\n<tr><td align="right">\nFrequently asked questions\n</td><td align="left">\n<a href="doc/user_faq/user_faq.md">for User</a>\n\n<a href="doc/developer_faq/developer_faq.md">for Developer</a>\n</td></tr>\n<tr><td colspan=2 align="center">\n\n## Releases\n\n</td></tr>\n<tr><td align="right">\n\n<a href="https://disk.yandex.ru/d/7i5XTKIKVg5UUg">Windows 10 x64 (yandex.ru)</a>\n\n<a href="https://mega.nz/folder/m10iELBK#Y0H6BflF9C4k_clYofC7yA">Windows 10 x64 (mega.nz)</a>\n\n\n</td><td align="left">\nContains stand-alone zero-dependency all-in-one ready-to-use portable self-extracting folder! You don''t need to install anything other than video drivers.\n<br><br>\nDirectX12 build : NVIDIA, AMD, Intel videocards.\n<br><br>\nNVIDIA build : NVIDIA cards only, GT730 and higher. Works faster than DX12. FaceMerger can work also on AMD/Intel.\n</td></tr>\n<tr><td colspan=2 align="center">\n\n## Communication groups\n\n<tr><td align="right">\n<a href="https://discord.gg/rxa7h9M6rH">Discord</a>\n</td><td align="left">Official discord channel. English / Russian.</td></tr>\n\n<tr><td align="right">\nQQÁæ§124500433\n</td><td align="left">‰∏≠Êñá‰∫§ÊµÅQQÁæ§ÔºåÂïÜÂä°Âêà‰ΩúÊâæÁæ§‰∏ª</td></tr>\n\n</td></tr>\n<tr><td colspan=2 align="center">\n\n## How can I help the project?\n\n</td></tr>\n<tr><td colspan=2 align="center">\nTrain your own face model by following the recommendations in the FAQ section and share it on Discord. If the model fits the quality, it will be added to the public library.\n</td></tr>\n<tr><td colspan=2 align="center">\nRegister github account and push "Star" button.\n</td></tr>\n<!--<tr><td colspan=2 align="center">\n<a href="https://www.paypal.com/paypalme/DeepFaceLab">Donate via Paypal</a>\n</td></tr>-->\n<tr><td colspan=2 align="center">\n<a href="https://yoomoney.ru/to/41001142318065">Donate via Yoomoney</a>\n</td></tr>\n<tr><td colspan=2 align="center">\nbitcoin:bc1qewl062v70rszulml3f0mjdjrys8uxdydw3v6rq\n</td></tr>\n<tr><td colspan=2 align="center">\n\n\n<!--\n    <a href="https://br-stone.online"><img src="doc/logo_barclay_stone.png"></img></a><a href="https://exmo.com"><img src="doc/logo_exmo.png"></img></a>\n\n    presents\n\n    <tr><td align="right">\n\n\n    <a href="">Windows (magnet link)</a>\n    </td><td align="center">Latest release. Use torrent client to download.</td></tr>\n    </tr>\n-->\n\n</table>\n\n\n\n', '{"language":"Python","stars":30200,"forks":1019,"watchers":30200,"open_issues":1,"topics":["deepfake","faceswap","machine-learning","real-time","streaming","videocall","webcam"],"default_branch":"master","size_kb":1540184,"archived":true,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:iperov:DeepFaceLab\">DeepFaceLab<","source_url":"https://github.com/iperov/DeepFaceLab\">DeepFaceLab<"}]', NULL, 'GPL-3.0', 'approved', 80, '132e8c6be66745b1d9e6c0ed9a1f2dcf', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-iperov-DeepFaceLive from https://github.com/iperov.png
Image converted to WebP: data/images/github-iperov-DeepFaceLive.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-eriklindernoren-ML-From-Scratch', 'github--eriklindernoren--ml-from-scratch', 'ML-From-Scratch', 'eriklindernoren', 'Python implementations of some of the fundamental Machine Learning models and algorithms from scratch. The purpose of this project is not to produce as optimized and computationally efficient algorithms as possible but rather to present the inner workings of them in a transparent and accessible way. - Machine Learning From Scratch * About * Table of Contents * Installation * Examples + Polynomial Regression + Classification With CNN + Density-Based Clustering + Generating Handwritten Digits +...', '["data-mining","data-science","deep-learning","deep-reinforcement-learning","genetic-algorithm","machine-learning","machine-learning-from-scratch","python"]', 'other', 29764, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/eriklindernoren/ML-From-Scratch","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '# Machine Learning From Scratch\n\n## About\nPython implementations of some of the fundamental Machine Learning models and algorithms from scratch.\n\nThe purpose of this project is not to produce as optimized and computationally efficient algorithms as possible\nbut rather to present the inner workings of them in a transparent and accessible way.\n\n## Table of Contents\n- [Machine Learning From Scratch](#machine-learning-from-scratch)\n  * [About](#about)\n  * [Table of Contents](#table-of-contents)\n  * [Installation](#installation)\n  * [Examples](#examples)\n    + [Polynomial Regression](#polynomial-regression)\n    + [Classification With CNN](#classification-with-cnn)\n    + [Density-Based Clustering](#density-based-clustering)\n    + [Generating Handwritten Digits](#generating-handwritten-digits)\n    + [Deep Reinforcement Learning](#deep-reinforcement-learning)\n    + [Image Reconstruction With RBM](#image-reconstruction-with-rbm)\n    + [Evolutionary Evolved Neural Network](#evolutionary-evolved-neural-network)\n    + [Genetic Algorithm](#genetic-algorithm)\n    + [Association Analysis](#association-analysis)\n  * [Implementations](#implementations)\n    + [Supervised Learning](#supervised-learning)\n    + [Unsupervised Learning](#unsupervised-learning)\n    + [Reinforcement Learning](#reinforcement-learning)\n    + [Deep Learning](#deep-learning)\n  * [Contact](#contact)\n\n## Installation\n    $ git clone https://github.com/eriklindernoren/ML-From-Scratch\n    $ cd ML-From-Scratch\n    $ python setup.py install\n\n## Examples\n### Polynomial Regression\n    $ python mlfromscratch/examples/polynomial_regression.py\n\n<p align="center">\n    <img src="http://eriklindernoren.se/images/p_reg.gif" width="640"\>\n</p>\n<p align="center">\n    Figure: Training progress of a regularized polynomial regression model fitting <br>\n    temperature data measured in Link√∂ping, Sweden 2016.\n</p>\n\n### Classification With CNN\n    $ python mlfromscratch/examples/convolutional_neural_network.py\n\n    +---------+\n    | ConvNet |\n    +---------+\n    Input Shape: (1, 8, 8)\n    +----------------------+------------+--------------+\n    | Layer Type           | Parameters | Output Shape |\n    +----------------------+------------+--------------+\n    | Conv2D               | 160        | (16, 8, 8)   |\n    | Activation (ReLU)    | 0          | (16, 8, 8)   |\n    | Dropout              | 0          | (16, 8, 8)   |\n    | BatchNormalization   | 2048       | (16, 8, 8)   |\n    | Conv2D               | 4640       | (32, 8, 8)   |\n    | Activation (ReLU)    | 0          | (32, 8, 8)   |\n    | Dropout              | 0          | (32, 8, 8)   |\n    | BatchNormalization   | 4096       | (32, 8, 8)   |\n    | Flatten              | 0          | (2048,)      |\n    | Dense                | 524544     | (256,)       |\n    | Activation (ReLU)    | 0          | (256,)       |\n    | Dropout              | 0          | (256,)       |\n    | BatchNormalization   | 512        | (256,)       |\n    | Dense                | 2570       | (10,)        |\n    | Activation (Softmax) | 0          | (10,)        |\n    +----------------------+------------+--------------+\n    Total Parameters: 538570\n\n    Training: 100% [------------------------------------------------------------------------] Time: 0:01:55\n    Accuracy: 0.987465181058\n\n<p align="center">\n    <img src="http://eriklindernoren.se/images/mlfs_cnn1.png" width="640">\n</p>\n<p align="center">\n    Figure: Classification of the digit dataset using CNN.\n</p>\n\n### Density-Based Clustering\n    $ python mlfromscratch/examples/dbscan.py\n\n<p align="center">\n    <img src="http://eriklindernoren.se/images/mlfs_dbscan.png" width="640">\n</p>\n<p align="center">\n    Figure: Clustering of the moons dataset using DBSCAN.\n</p>\n\n### Generating Handwritten Digits\n    $ python mlfromscratch/unsupervised_learning/generative_adversarial_network.py\n\n    +-----------+\n    | Generator |\n    +-----------+\n    Input Shape: (100,)\n    +------------------------+------------+--------------+\n    | Layer Type             | Parameters | Output Shape |\n    +------------------------+------------+--------------+\n    | Dense                  | 25856      | (256,)       |\n    | Activation (LeakyReLU) | 0          | (256,)       |\n    | BatchNormalization     | 512        | (256,)       |\n    | Dense                  | 131584     | (512,)       |\n    | Activation (LeakyReLU) | 0          | (512,)       |\n    | BatchNormalization     | 1024       | (512,)       |\n    | Dense                  | 525312     | (1024,)      |\n    | Activation (LeakyReLU) | 0          | (1024,)      |\n    | BatchNormalization     | 2048       | (1024,)      |\n    | Dense                  | 803600     | (784,)       |\n    | Activation (TanH)      | 0          | (784,)       |\n    +------------------------+------------+--------------+\n    Total Parameters: 1489936\n\n    +---------------+\n    | Discriminator |\n    +---------------+\n    Input Shape: (784,)\n    +------------------------+------------+--------------+\n    | Layer Type             | Parameters | Output Shape |\n    +------------------------+------------+--------------+\n    | Dense                  | 401920     | (512,)       |\n    | Activation (LeakyReLU) | 0          | (512,)       |\n    | Dropout                | 0          | (512,)       |\n    | Dense                  | 131328     | (256,)       |\n    | Activation (LeakyReLU) | 0          | (256,)       |\n    | Dropout                | 0          | (256,)       |\n    | Dense                  | 514        | (2,)         |\n    | Activation (Softmax)   | 0          | (2,)         |\n    +------------------------+------------+--------------+\n    Total Parameters: 533762\n\n\n<p align="center">\n    <img src="http://eriklindernoren.se/images/gan_mnist5.gif" width="640">\n</p>\n<p align="center">\n    Figure: Training progress of a Generative Adversarial Network generating <br>\n    handwritten digits.\n</p>\n\n### Deep Reinforcement Learning\n    $ python mlfromscratch/examples/deep_q_network.py\n\n    +----------------+\n    | Deep Q-Network |\n    +----------------+\n    Input Shape: (4,)\n    +-------------------+------------+--------------+\n    | Layer Type        | Parameters | Output Shape |\n    +-------------------+------------+--------------+\n    | Dense             | 320        | (64,)        |\n    | Activation (ReLU) | 0          | (64,)        |\n    | Dense             | 130        | (2,)         |\n    +-------------------+------------+--------------+\n    Total Parameters: 450\n\n<p align="center">\n    <img src="http://eriklindernoren.se/images/mlfs_dql1.gif" width="640">\n</p>\n<p align="center">\n    Figure: Deep Q-Network solution to the CartPole-v1 environment in OpenAI gym.\n</p>\n\n### Image Reconstruction With RBM\n    $ python mlfromscratch/examples/restricted_boltzmann_machine.py\n\n<p align="center">\n    <img src="http://eriklindernoren.se/images/rbm_digits1.gif" width="640">\n</p>\n<p align="center">\n    Figure: Shows how the network gets better during training at reconstructing <br>\n    the digit 2 in the MNIST dataset.\n</p>\n\n### Evolutionary Evolved Neural Network\n    $ python mlfromscratch/examples/neuroevolution.py\n\n    +---------------+\n    | Model Summary |\n    +---------------+\n    Input Shape: (64,)\n    +----------------------+------------+--------------+\n    | Layer Type           | Parameters | Output Shape |\n    +----------------------+------------+--------------+\n    | Dense                | 1040       | (16,)        |\n    | Activation (ReLU)    | 0          | (16,)        |\n    | Dense                | 170        | (10,)        |\n    | Activation (Softmax) | 0          | (10,)        |\n    +----------------------+------------+--------------+\n    Total Parameters: 1210\n\n    Population Size: 100\n    Generations: 3000\n    Mutation Rate: 0.01\n\n    [0 Best Individual - Fitness: 3.08301, Accuracy: 10.5%]\n    [1 Best Individual - Fitness: 3.08746, Accuracy: 12.0%]\n    ...\n    [2999 Best Individual - Fitness: 94.08513, Accuracy: 98.5%]\n    Test set accuracy: 96.7%\n\n<p align="center">\n    <img src="http://eriklindernoren.se/images/evo_nn4.png" width="640">\n</p>\n<p align="center">\n    Figure: Classification of the digit dataset by a neural network which has<br>\n    been evolutionary evolved.\n</p>\n\n### Genetic Algorithm\n    $ python mlfromscratch/examples/genetic_algorithm.py\n\n    +--------+\n    |   GA   |\n    +--------+\n    Description: Implementation of a Genetic Algorithm which aims to produce\n    the user specified target string. This implementation calculates each\n    candidate''s fitness based on the alphabetical distance between the candidate\n    and the target. A candidate is selected as a parent with probabilities proportional\n    to the candidate''s fitness. Reproduction is implemented as a single-point\n    crossover between pairs of parents. Mutation is done by randomly assigning\n    new characters with uniform probability.\n\n    Parameters\n    ----------\n    Target String: ''Genetic Algorithm''\n    Population Size: 100\n    Mutation Rate: 0.05\n\n    [0 Closest Candidate: ''CJqlJguPlqzvpoJmb'', Fitness: 0.00]\n    [1 Closest Candidate: ''MCxZxdr nlfiwwGEk'', Fitness: 0.01]\n    [2 Closest Candidate: ''MCxZxdm nlfiwwGcx'', Fitness: 0.01]\n    [3 Closest Candidate: ''SmdsAklMHn kBIwKn'', Fitness: 0.01]\n    [4 Closest Candidate: ''  lotneaJOasWfu Z'', Fitness: 0.01]\n    ...\n    [292 Closest Candidate: ''GeneticaAlgorithm'', Fitness: 1.00]\n    [293 Closest Candidate: ''GeneticaAlgorithm'', Fitness: 1.00]\n    [294 Answer: ''Genetic Algorithm'']\n\n### Association Analysis\n    $ python mlfromscratch/examples/apriori.py\n    +-------------+\n    |   Apriori   |\n    +-------------+\n    Minimum Support: 0.25\n    Minimum Confidence: 0.8\n    Transactions:\n        [1, 2, 3, 4]\n        [1, 2, 4]\n        [1, 2]\n        [2, 3, 4]\n        [2, 3]\n        [3, 4]\n        [2, 4]\n    Frequent Itemsets:\n        [1, 2, 3, 4, [1, 2], [1, 4], [2, 3], [2, 4], [3, 4], [1, 2, 4], [2, 3, 4]]\n    Rules:\n        1 -> 2 (support: 0.43, confidence: 1.0)\n        4 -> 2 (support: 0.57, confidence: 0.8)\n        [1, 4] -> 2 (support: 0.29, confidence: 1.0)\n\n\n## Implementations\n### Supervised Learning\n- [Adaboost](mlfromscratch/supervised_learning/adaboost.py)\n- [Bayesian Regression](mlfromscratch/supervised_learning/bayesian_regression.py)\n- [Decision Tree](mlfromscratch/supervised_learning/decision_tree.py)\n- [Elastic Net](mlfromscratch/supervised_learning/regression.py)\n- [Gradient Boosting](mlfromscratch/supervised_learning/gradient_boosting.py)\n- [K Nearest Neighbors](mlfromscratch/supervised_learning/k_nearest_neighbors.py)\n- [Lasso Regression](mlfromscratch/supervised_learning/regression.py)\n- [Linear Discriminant Analysis](mlfromscratch/supervised_learning/linear_discriminant_analysis.py)\n- [Linear Regression](mlfromscratch/supervised_learning/regression.py)\n- [Logistic Regression](mlfromscratch/supervised_learning/logistic_regression.py)\n- [Multi-class Linear Discriminant Analysis](mlfromscratch/supervised_learning/multi_class_lda.py)\n- [Multilayer Perceptron](mlfromscratch/supervised_learning/multilayer_perceptron.py)\n- [Naive Bayes](mlfromscratch/supervised_learning/naive_bayes.py)\n- [Neuroevolution](mlfromscratch/supervised_learning/neuroevolution.py)\n- [Particle Swarm Optimization of Neural Network](mlfromscratch/supervised_learning/particle_swarm_optimization.py)\n- [Perceptron](mlfromscratch/supervised_learning/perceptron.py)\n- [Polynomial Regression](mlfromscratch/supervised_learning/regression.py)\n- [Random Forest](mlfromscratch/supervised_learning/random_forest.py)\n- [Ridge Regression](mlfromscratch/supervised_learning/regression.py)\n- [Support Vector Machine](mlfromscratch/supervised_learning/support_vector_machine.py)\n- [XGBoost](mlfromscratch/supervised_learning/xgboost.py)\n\n### Unsupervised Learning\n- [Apriori](mlfromscratch/unsupervised_learning/apriori.py)\n- [Autoencoder](mlfromscratch/unsupervised_learning/autoencoder.py)\n- [DBSCAN](mlfromscratch/unsupervised_learning/dbscan.py)\n- [FP-Growth](mlfromscratch/unsupervised_learning/fp_growth.py)\n- [Gaussian Mixture Model](mlfromscratch/unsupervised_learning/gaussian_mixture_model.py)\n- [Generative Adversarial Network](mlfromscratch/unsupervised_learning/generative_adversarial_network.py)\n- [Genetic Algorithm](mlfromscratch/unsupervised_learning/genetic_algorithm.py)\n- [K-Means](mlfromscratch/unsupervised_learning/k_means.py)\n- [Partitioning Around Medoids](mlfromscratch/unsupervised_learning/partitioning_around_medoids.py)\n- [Principal Component Analysis](mlfromscratch/unsupervised_learning/principal_component_analysis.py)\n- [Restricted Boltzmann Machine](mlfromscratch/unsupervised_learning/restricted_boltzmann_machine.py)\n\n### Reinforcement Learning\n- [Deep Q-Network](mlfromscratch/reinforcement_learning/deep_q_network.py)\n\n### Deep Learning\n  + [Neural Network](mlfromscratch/deep_learning/neural_network.py)\n  + [Layers](mlfromscratch/deep_learning/layers.py)\n    * Activation Layer\n    * Average Pooling Layer\n    * Batch Normalization Layer\n    * Constant Padding Layer\n    * Convolutional Layer\n    * Dropout Layer\n    * Flatten Layer\n    * Fully-Connected (Dense) Layer\n    * Fully-Connected RNN Layer\n    * Max Pooling Layer\n    * Reshape Layer\n    * Up Sampling Layer\n    * Zero Padding Layer\n  + Model Types\n    * [Convolutional Neural Network](mlfromscratch/examples/convolutional_neural_network.py)\n    * [Multilayer Perceptron](mlfromscratch/examples/multilayer_perceptron.py)\n    * [Recurrent Neural Network](mlfromscratch/examples/recurrent_neural_network.py)\n\n## Contact\nIf there''s some implementation you would like to see here or if you''re just feeling social,\nfeel free to [email](mailto:eriklindernoren@gmail.com) me or connect with me on [LinkedIn](https://www.linkedin.com/in/eriklindernoren/).\n', '{"language":"Python","stars":29764,"forks":5055,"watchers":29764,"open_issues":68,"topics":["data-mining","data-science","deep-learning","deep-reinforcement-learning","genetic-algorithm","machine-learning","machine-learning-from-scratch"],"default_branch":"master","size_kb":553,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:eriklindernoren:ML-From-Scratch","source_url":"https://github.com/eriklindernoren/ML-From-Scratch"}]', NULL, 'MIT', 'approved', 80, 'cbc0ae71030004da1e92540e114f9703', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-eriklindernoren-ML-From-Scratch from https://github.com/eriklindernoren.png
Image converted to WebP: data/images/github-eriklindernoren-ML-From-Scratch.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-ashishpatel26-500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code', 'github--ashishpatel26--500-ai-machine-learning-deep-learning-computer-vision-nlp-projects-with-code', '500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code', 'ashishpatel26', '***500 AI Machine learning Deep learning Computer vision NLP Projects with code* !!!** Follow me on LinkedIn : ***This list is continuously updated.*** - You can take pull requests and contribute. All Links are tested and working fine. Please ping if any link doesn''t work | Sr No | Name | Link | | ----- | ------------------------------------------------------------ | ------------------------------------------------------------ | | 1 | 365 Days Computer Vision Learning | üëÜ | | 2 | 125+ NLP La...', '["artificial-intelligence","artificial-intelligence-projects","awesome","computer-vision","computer-vision-project","data-science","deep-learning","deep-learning-project","machine-learning","machine-learning-projects","nlp","nlp-projects","python"]', 'other', 29464, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/ashishpatel26/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '## 500 + ùóîùóøùòÅùó∂ùó≥ùó∂ùó∞ùó∂ùóÆùóπ ùóúùóªùòÅùó≤ùóπùóπùó∂ùó¥ùó≤ùóªùó∞ùó≤ ùó£ùóøùóºùó∑ùó≤ùó∞ùòÅ ùóüùó∂ùòÄùòÅ ùòÑùó∂ùòÅùóµ ùó∞ùóºùó±ùó≤\n\n***500 AI Machine learning Deep learning Computer vision NLP Projects with code* !!!**\n\n![](https://raw.githubusercontent.com/ashishpatel26/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code/main/images/Colorful%20Futuristic%20Technology%20Poster.gif)\n\nFollow me on LinkedIn : [![](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/ashishpatel2604/)\n\n***This list is continuously updated.*** - You can take pull requests and contribute. All Links are tested and working fine. Please ping if any link doesn''t work\n\n| Sr No | Name                                                         | Link                                                         |\n| ----- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| 1     | 365 Days Computer Vision Learning                            | [üëÜ](https://github.com/ashishpatel26/365-Days-Computer-Vision-Learning-Linkedin-Post) |\n| 2     | 125+ NLP Language Models Treasure of Transformers            | [üëÜ](https://github.com/ashishpatel26/Treasure-of-Transformers) |\n| 3     | Andrew NG ML notes                                           | [üëÜ](https://github.com/ashishpatel26/Andrew-NG-Notes)        |\n| 4     | 10 Machine Learning Projects on Time Series Forecasting      | [üëÜ](https://medium.com/coders-camp/10-machine-learning-projects-on-time-seri%20es-forecasting-ee0368420ccd) |\n| 5     | 20 Deep Learning Projects Solved and Explained with Python   | [üëÜ](https://thecleverprogrammer.com/2020/11/22/deep-learning-projects-with-python/) |\n| 6     | 20 Machine learning Project                                  | [üëÜ](https://amankharwal.medium.com/20-machine-learning-projects-for-portfolio-81e3dbd167b1) |\n| 7     | 30 Python Project Solved and Explained                       | [üëÜ](https://amankharwal.medium.com/30-python-projects-solved-and-explained-563fd7473003) |\n| 8     | Machine learning Course for Free                             | [üëÜ](https://thecleverprogrammer.com/2020/09/24/machine-learning-course/) |\n| 9     | 5 Web Scraping Projects with Python                          | [üëÜ](https://amankharwal.medium.com/5-web-scraping-projects-with-python-4bcc25ff039) |\n| 10    | 20 Machine Learning Projects on Future Prediction with Python | [üëÜ](https://amankharwal.medium.com/20-machine-learning-projects-on-future-prediction-with-python-93932d9a7f7f) |\n| 11    | 4 Chatbot Project With Python                                | [üëÜ](https://amankharwal.medium.com/4-chatbot-projects-with-python-5b32fd84af37) |\n| 12    | 7 Python Gui project                                         | [üëÜ](https://amankharwal.medium.com/7-python-gui-projects-for-beginners-87ae2c695d78) |\n| 13    | All Unsupervised learning Projects                           | [üëÜ](https://amankharwal.medium.com/all-unsupervised-machine-learning-algorithms-explained-aecf1ba95d8b) |\n| 14    | 10 Machine learning Projects for Regression Analysis         | [üëÜ](https://amankharwal.medium.com/10-machine-learning-projects-on-regression-with-python-e5494615a0d0) |\n| 15    | 10 Machine learning Project for Classification with Python   | [üëÜ](https://medium.datadriveninvestor.com/10-machine-learning-projects-on-classification-with-python-9261add2e8a7) |\n| 16    | 6 Sentimental Analysis Projects with python                  | [üëÜ](https://amankharwal.medium.com/6-sentiment-analysis-projects-with-python-1fdd3d43d90f) |\n| 17    | 4 Recommendations Projects with Python                       | [üëÜ](https://medium.com/coders-camp/4-recommendation-system-projects-with-python-5934de32ba7d) |\n| 18    | 20 Deep learning Project with python                         | [üëÜ](https://medium.com/coders-camp/20-deep-learning-projects-with-python-3c56f7e6a721) |\n| 19    | 5 COVID19 Projects with Python                               | [üëÜ](https://amankharwal.medium.com/5-covid-19-projects-with-python-and-machine-learning-63d51cde96e2) |\n| 20    | 9 Computer Vision Project with python                        | [üëÜ](https://becominghuman.ai/computer-vision-projects-with-python-ecfac58ded18) |\n| 21    | 8 Neural Network Project with python                         | [üëÜ](https://medium.datadriveninvestor.com/8-neural-networks-projects-solved-and-explained-a4f142bc10c) |\n| 22    | 5 Machine learning Project for healthcare                    | [üëÜ](https://medium.datadriveninvestor.com/5-machine-learning-projects-for-healthcare-bbd0eac57b4a) |\n| 23    | 5 NLP Project with Python                                    | [üëÜ](https://medium.datadriveninvestor.com/5-nlp-projects-for-machine-learning-72d3234381d4) |\n| 24    | 47 Machine Learning Projects for 2021                        | [üëÜ](https://data-flair.training/blogs/machine-learning-project-ideas/) |\n| 25    | 19 Artificial Intelligence Projects for 2021                 | [üëÜ](https://data-flair.training/blogs/artificial-intelligence-project-ideas/) |\n| 26    | 28 Machine learning Projects for 2021                        | [üëÜ](https://data-flair.training/blogs/machine-learning-project-ideas/) |\n| 27    | 16 Data Science Projects with Source Code for 2021           | [üëÜ](https://data-flair.training/blogs/data-science-project-ideas/) |\n| 28    | 23 Deep learning Projects with Source Code for 2021          | [üëÜ](https://data-flair.training/blogs/deep-learning-project-ideas/) |\n| 29    | 25 Computer Vision Projects with Source Code for 2021        | [üëÜ](https://data-flair.training/blogs/computer-vision-project-ideas/) |\n| 30    | 23 Iot Projects with Source Code for 2021                    | [üëÜ](https://data-flair.training/blogs/iot-project-ideas/)    |\n| 31    | 27 Django Projects with Source Code for 2021                 | [üëÜ](https://data-flair.training/blogs/django-project-ideas/) |\n| 32    | 37 Python Fun Projects with Code for 2021                    | [üëÜ](https://data-flair.training/blogs/python-project-ideas/) |\n| 33    | 500 + Top Deep learning Codes                                | [üëÜ](https://github.com/aymericdamien/TopDeepLearning)        |\n| 34    | 500 + Machine learning Codes                                 | [üëÜ](https://github.com/josephmisiti/awesome-machine-learning) |\n| 35    | 20+ Machine Learning Datasets & Project Ideas                | [üëÜ](https://www.kdnuggets.com/2020/03/20-machine-learning-datasets-project-ideas.html) |\n| 36    | 1000+ Computer vision codes                                  | [üëÜ](https://github.com/jbhuang0604/awesome-computer-vision)  |\n| 37    | 300 + Industry wise Real world projects with code            | [üëÜ](https://github.com/ashishpatel26/Real-time-ML-Project)   |\n| 38    | 1000 + Python Project Codes                                  | [üëÜ](https://github.com/vinta/awesome-python)                 |\n| 39    | 363 + NLP Project with Code                                  | [üëÜ](https://github.com/fighting41love/funNLP)                |\n| 40    | 50 + Code ML Models (For iOS 11) Projects                    | [üëÜ](https://github.com/likedan/Awesome-CoreML-Models)        |\n| 41    | 360+ Pretrained Model Projects for Image, text, Audio and Video | [üëÜ](https://github.com/PaddlePaddle/PaddleHub)               |\n| 42    | 50 + Graph Classification Project List                       | [üëÜ](https://github.com/benedekrozemberczki/awesome-graph-classification) |\n| 43    | 100 + Sentence Embedding(NLP Resources)                      | [üëÜ](https://github.com/Separius/awesome-sentence-embedding)  |\n| 44    | 100 + Production Machine learning Projects                   | [üëÜ](https://github.com/EthicalML/awesome-production-machine-learning) |\n| 45    | 300 + Machine Learning Resources Collection                  | [üëÜ](https://github.com/ujjwalkarn/Machine-Learning-Tutorials) |\n| 46    | 70 + Awesome AI                                              | [üëÜ](https://github.com/NirantK/awesome-project-ideas)        |\n| 47    | 150 + Machine learning Project Ideas with code               | [üëÜ](https://github.com/src-d/awesome-machine-learning-on-source-code) |\n| 48    | 100 + AutoML Projects with code                              | [üëÜ](https://github.com/hibayesian/awesome-automl-papers)     |\n| 49    | 100 + Machine Learning Model Interpretability Code Frameworks | [üëÜ](https://github.com/jphall663/awesome-machine-learning-interpretability) |\n| 50    | 120 + Multi Model Machine learning Code Projects             | [üëÜ](https://github.com/pliang279/awesome-multimodal-ml)      |\n| 51    | Awesome Chatbot Projects                                     | [üëÜ](https://github.com/fendouai/Awesome-Chatbot)             |\n| 52    | Awesome ML Demo Project with iOS                             | [üëÜ](https://github.com/tucan9389/awesome-ml-demos-with-ios)  |\n| 53    | 100 + Python based Machine learning Application Projects     | [üëÜ](https://github.com/prateekiiest/Code-Sleep-Python)       |\n| 54    | 100 + Reproducible Research Projects of ML and DL            | [üëÜ](https://github.com/leipzig/awesome-reproducible-research) |\n| 55    | 25 + Python Projects                                         | [üëÜ](https://github.com/saadhaxxan/Awesome-Python-Projects)   |\n| 56    | 8 + OpenCV Projects                                          | [üëÜ](https://github.com/moadmmh/Awesome-OpenCV)               |\n| 57    | 1000 + Awesome Deep learning Collection                      | [üëÜ](https://github.com/ChristosChristofidis/awesome-deep-learning) |\n| 58    | 200 + Awesome NLP learning Collection                        | [üëÜ](https://github.com/keon/awesome-nlp)                     |\n| 59    | 200 + The Super Duper NLP Repo                               | [üëÜ](https://notebooks.quantumstat.com/)                      |\n| 60    | 100 + NLP dataset for your Projects                          | [üëÜ](https://index.quantumstat.com/#dataset)                  |\n| 61    | 364 + Machine Learning Projects definition                   | [üëÜ](https://projectworlds.in/free-projects/machine-learning-projects-with-source-code/) |\n| 62    | 300+ Google Earth Engine Jupyter Notebooks to Analyze Geospatial Data | [üëÜ](https://github.com/giswqs/earthengine-py-notebooks)      |\n| 63    | 1000 + Machine learning Projects Information                 | [üëÜ](https://1000projects.org/projects/machine-learning-projects) |\n| 64.   | 11 Computer Vision Projects with code                        | [üëÜ](https://github.com/akshaybhatia10/ComputerVision-Projects) |\n| 65.   | 13 Computer Vision Projects with Code                        | [üëÜ](https://github.com/anuragreddygv323/computer-vision-projects) |\n| 66.   | 13 Cool Computer Vision GitHub Projects To Inspire You       | [üëÜ](https://machinelearningknowledge.ai/cool-computer-vision-github-projects-to-inspire-you/) |\n| 67.   | Open-Source Computer Vision Projects (With Tutorials)        | [üëÜ](https://www.theclickreader.com/open-source-computer-vision-projects-with-tutorials/) |\n| 68.   | OpenCV Computer Vision Projects with Python                  | [üëÜ](https://github.com/PacktPublishing/OpenCV-Computer-Vision-Projects-with-Python) |\n| 69.   | 100 + Computer vision Algorithm Implementation               | [üëÜ](https://github.com/gmalivenko/awesome-computer-vision-models) |\n| 70.   | 80 + Computer vision Learning code                           | [üëÜ](https://github.com/spmallick/learnopencv)                |\n| 71.   | Deep learning Treasure                                       | [üëÜ](https://github.com/kmario23/deep-learning-drizzle)       |\n| 72    | Data Analysis and Machine learning Projects                  | [üëÜ](https://github.com/rhiever/Data-Analysis-and-Machine-Learning-Projects) |\n| 73    | AI Projects                                                  | [üëÜ](https://github.com/StevenLei2017/AI_projects)            |\n| 74    | Kaggle projects collection                                   | [üëÜ](https://github.com/alexattia/Data-Science-Projects)      |\n| 75    | Unique AI projects                                           | [üëÜ](https://github.com/robsalgado/personal_data_science_projects) |\n| 76    | Data Science Project Collection                              | [üëÜ](https://github.com/tuangauss/DataScienceProjects)        |\n| 77    | Advance Data Science Projects                                | [üëÜ](https://github.com/TheCodex-Me/Projects)                 |\n| 78    | Deep and Machine learning Projects                           | [üëÜ](https://github.com/nitinkaushik01/Deep_and_Machine_Learning_Projects) |\n| 79    | Data Science Projects kaggle                                 | [üëÜ](https://github.com/alexattia/Data-Science-Projects)      |\n| 80    | Auto Deep learning Project                                   | [üëÜ](https://github.com/D-X-Y/AutoDL-Projects)                |\n| 81    | 180 Machine learning Project                                 | [üëÜ](https://medium.com/coders-camp/180-data-science-and-machine-learning-projects-with-python-6191bc7b9db9) |\n| 82    | Amazing Hackthon Project Collection                          | [üëÜ](https://github.com/analyticsindiamagazine/MachineHack/tree/master/Hackathon_Solutions) |\n| 83    | Awesome NLP Project Ideas                                    | [üëÜ](https://nirantk.com/awesome-project-ideas/)              |\n| 84    | 12 NLP Projects                                              | [üëÜ](https://github.com/gaoisbest/NLP-Projects)               |\n| 85    | Advance NLP Projects                                         | [üëÜ](https://github.com/PacktPublishing/Advanced-NLP-Projects-with-TensorFlow-2.0) |\n| 86    | 6 Amazing NLP Projects                                       | [üëÜ](https://github.com/anujvyas/Natural-Language-Processing-Projects) |\n| 87    | NLP Beginner Projects                                        | [üëÜ](https://github.com/positivepeng/nlp-beginner-projects)   |\n| 88    | Paper with Code by PwC Collection                            | [üëÜ](https://github.com/zziz/pwc)                             |\n| 89    | SOTA Models(State of the Art Results)                        | [üëÜ](https://github.com/RedditSota/state-of-the-art-result-for-machine-learning-problems) |\n| 90    | Best AI Papers                                               | [üëÜ](https://github.com/louisfb01/Best_AI_paper_2020)         |\n| 91    | Generative Adversarial nets                                  | [üëÜ](https://github.com/zhangqianhui/AdversarialNetsPapers)   |\n| 92    | Computer Vision Paper with Code                              | [üëÜ](https://github.com/DWCTOD/CVPR2022-Papers-with-Code-Demo) |\n| 93    | NILMS Paper with code                                        | [üëÜ](https://github.com/klemenjak/nilm-papers-with-code)      |\n| 94    | 3D Computer Vision Research Projects                         | [üëÜ](https://github.com/Tom-Hardy-3D-Vision-Workshop/awesome-3D-vision) |\n| 95    | NLP and Computer Vision Project Collection                   | [üëÜ](https://github.com/NELSONZHAO/zhihu)                     |\n| 96    | Udacity Collection of Computer Vision Projects               | [üëÜ](https://github.com/Bjarten/computer-vision-ND)           |\n| 97    | Zero to Hero Tensorflow Tutorial                             | [üëÜ](https://github.com/mrdbourke/tensorflow-deep-learning)   |\n| 98    | Deep learning in Production                                  | [üëÜ](https://github.com/The-AI-Summer/Deep-Learning-In-Production) |\n| 99    | GANs Collection                                              | [üëÜ](https://github.com/The-AI-Summer/GANs-in-Computer-Vision) |\n| 100   | Time Series Projects Code                                    | [üëÜ](https://github.com/deshpandenu/Time-Series-Forecasting-of-Amazon-Stock-Prices-using-Neural-Networks-LSTM-and-GAN-) |\n| 101   | 12 Machine learning Object Detection                         | [üëÜ](https://amankharwal.medium.com/12-machine-learning-projects-on-object-detection-46b32adc3c37) |\n| 102   | 20 NLP Project with Python                                   | [üëÜ](https://medium.com/coders-camp/20-machine-learning-projects-on-nlp-582effe73b9c) |\n| 103   | Learning Material for Deep Learning, ML, Computer Vision and NLP   | [üëÜ](https://github.com/kmario23/deep-learning-drizzle) |\n***More Projects list is coming...!!!***\n\n---\n\n', '{"language":null,"stars":29464,"forks":6527,"watchers":29464,"open_issues":59,"topics":["artificial-intelligence","artificial-intelligence-projects","awesome","computer-vision","computer-vision-project","data-science","deep-learning","deep-learning-project","machine-learning","machine-learning-projects","nlp","nlp-projects","python"],"default_branch":"main","size_kb":879,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:ashishpatel26:365-Days-Computer-Vision-Learning-Linkedin-Post","source_url":"https://github.com/ashishpatel26/365-Days-Computer-Vision-Learning-Linkedin-Post"},{"type":"has_code","target_id":"github:ashishpatel26:Treasure-of-Transformers","source_url":"https://github.com/ashishpatel26/Treasure-of-Transformers"},{"type":"has_code","target_id":"github:ashishpatel26:Andrew-NG-Notes","source_url":"https://github.com/ashishpatel26/Andrew-NG-Notes"},{"type":"has_code","target_id":"github:aymericdamien:TopDeepLearning","source_url":"https://github.com/aymericdamien/TopDeepLearning"},{"type":"has_code","target_id":"github:josephmisiti:awesome-machine-learning","source_url":"https://github.com/josephmisiti/awesome-machine-learning"},{"type":"has_code","target_id":"github:jbhuang0604:awesome-computer-vision","source_url":"https://github.com/jbhuang0604/awesome-computer-vision"},{"type":"has_code","target_id":"github:ashishpatel26:Real-time-ML-Project","source_url":"https://github.com/ashishpatel26/Real-time-ML-Project"},{"type":"has_code","target_id":"github:vinta:awesome-python","source_url":"https://github.com/vinta/awesome-python"},{"type":"has_code","target_id":"github:fighting41love:funNLP","source_url":"https://github.com/fighting41love/funNLP"},{"type":"has_code","target_id":"github:likedan:Awesome-CoreML-Models","source_url":"https://github.com/likedan/Awesome-CoreML-Models"},{"type":"has_code","target_id":"github:PaddlePaddle:PaddleHub","source_url":"https://github.com/PaddlePaddle/PaddleHub"},{"type":"has_code","target_id":"github:benedekrozemberczki:awesome-graph-classification","source_url":"https://github.com/benedekrozemberczki/awesome-graph-classification"},{"type":"has_code","target_id":"github:Separius:awesome-sentence-embedding","source_url":"https://github.com/Separius/awesome-sentence-embedding"},{"type":"has_code","target_id":"github:EthicalML:awesome-production-machine-learning","source_url":"https://github.com/EthicalML/awesome-production-machine-learning"},{"type":"has_code","target_id":"github:ujjwalkarn:Machine-Learning-Tutorials","source_url":"https://github.com/ujjwalkarn/Machine-Learning-Tutorials"},{"type":"has_code","target_id":"github:NirantK:awesome-project-ideas","source_url":"https://github.com/NirantK/awesome-project-ideas"},{"type":"has_code","target_id":"github:src-d:awesome-machine-learning-on-source-code","source_url":"https://github.com/src-d/awesome-machine-learning-on-source-code"},{"type":"has_code","target_id":"github:hibayesian:awesome-automl-papers","source_url":"https://github.com/hibayesian/awesome-automl-papers"},{"type":"has_code","target_id":"github:jphall663:awesome-machine-learning-interpretability","source_url":"https://github.com/jphall663/awesome-machine-learning-interpretability"},{"type":"has_code","target_id":"github:pliang279:awesome-multimodal-ml","source_url":"https://github.com/pliang279/awesome-multimodal-ml"},{"type":"has_code","target_id":"github:fendouai:Awesome-Chatbot","source_url":"https://github.com/fendouai/Awesome-Chatbot"},{"type":"has_code","target_id":"github:tucan9389:awesome-ml-demos-with-ios","source_url":"https://github.com/tucan9389/awesome-ml-demos-with-ios"},{"type":"has_code","target_id":"github:prateekiiest:Code-Sleep-Python","source_url":"https://github.com/prateekiiest/Code-Sleep-Python"},{"type":"has_code","target_id":"github:leipzig:awesome-reproducible-research","source_url":"https://github.com/leipzig/awesome-reproducible-research"},{"type":"has_code","target_id":"github:saadhaxxan:Awesome-Python-Projects","source_url":"https://github.com/saadhaxxan/Awesome-Python-Projects"},{"type":"has_code","target_id":"github:moadmmh:Awesome-OpenCV","source_url":"https://github.com/moadmmh/Awesome-OpenCV"},{"type":"has_code","target_id":"github:ChristosChristofidis:awesome-deep-learning","source_url":"https://github.com/ChristosChristofidis/awesome-deep-learning"},{"type":"has_code","target_id":"github:keon:awesome-nlp","source_url":"https://github.com/keon/awesome-nlp"},{"type":"has_code","target_id":"github:giswqs:earthengine-py-notebooks","source_url":"https://github.com/giswqs/earthengine-py-notebooks"},{"type":"has_code","target_id":"github:akshaybhatia10:ComputerVision-Projects","source_url":"https://github.com/akshaybhatia10/ComputerVision-Projects"},{"type":"has_code","target_id":"github:anuragreddygv323:computer-vision-projects","source_url":"https://github.com/anuragreddygv323/computer-vision-projects"},{"type":"has_code","target_id":"github:PacktPublishing:OpenCV-Computer-Vision-Projects-with-Python","source_url":"https://github.com/PacktPublishing/OpenCV-Computer-Vision-Projects-with-Python"},{"type":"has_code","target_id":"github:gmalivenko:awesome-computer-vision-models","source_url":"https://github.com/gmalivenko/awesome-computer-vision-models"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:kmario23:deep-learning-drizzle","source_url":"https://github.com/kmario23/deep-learning-drizzle"},{"type":"has_code","target_id":"github:rhiever:Data-Analysis-and-Machine-Learning-Projects","source_url":"https://github.com/rhiever/Data-Analysis-and-Machine-Learning-Projects"},{"type":"has_code","target_id":"github:StevenLei2017:AI_projects","source_url":"https://github.com/StevenLei2017/AI_projects"},{"type":"has_code","target_id":"github:alexattia:Data-Science-Projects","source_url":"https://github.com/alexattia/Data-Science-Projects"},{"type":"has_code","target_id":"github:robsalgado:personal_data_science_projects","source_url":"https://github.com/robsalgado/personal_data_science_projects"},{"type":"has_code","target_id":"github:tuangauss:DataScienceProjects","source_url":"https://github.com/tuangauss/DataScienceProjects"},{"type":"has_code","target_id":"github:TheCodex-Me:Projects","source_url":"https://github.com/TheCodex-Me/Projects"},{"type":"has_code","target_id":"github:nitinkaushik01:Deep_and_Machine_Learning_Projects","source_url":"https://github.com/nitinkaushik01/Deep_and_Machine_Learning_Projects"},{"type":"has_code","target_id":"github:alexattia:Data-Science-Projects","source_url":"https://github.com/alexattia/Data-Science-Projects"},{"type":"has_code","target_id":"github:D-X-Y:AutoDL-Projects","source_url":"https://github.com/D-X-Y/AutoDL-Projects"},{"type":"has_code","target_id":"github:analyticsindiamagazine:MachineHack","source_url":"https://github.com/analyticsindiamagazine/MachineHack"},{"type":"has_code","target_id":"github:gaoisbest:NLP-Projects","source_url":"https://github.com/gaoisbest/NLP-Projects"},{"type":"has_code","target_id":"github:PacktPublishing:Advanced-NLP-Projects-with-TensorFlow-2.0","source_url":"https://github.com/PacktPublishing/Advanced-NLP-Projects-with-TensorFlow-2.0"},{"type":"has_code","target_id":"github:anujvyas:Natural-Language-Processing-Projects","source_url":"https://github.com/anujvyas/Natural-Language-Processing-Projects"},{"type":"has_code","target_id":"github:positivepeng:nlp-beginner-projects","source_url":"https://github.com/positivepeng/nlp-beginner-projects"},{"type":"has_code","target_id":"github:zziz:pwc","source_url":"https://github.com/zziz/pwc"},{"type":"has_code","target_id":"github:RedditSota:state-of-the-art-result-for-machine-learning-problems","source_url":"https://github.com/RedditSota/state-of-the-art-result-for-machine-learning-problems"},{"type":"has_code","target_id":"github:louisfb01:Best_AI_paper_2020","source_url":"https://github.com/louisfb01/Best_AI_paper_2020"},{"type":"has_code","target_id":"github:zhangqianhui:AdversarialNetsPapers","source_url":"https://github.com/zhangqianhui/AdversarialNetsPapers"},{"type":"has_code","target_id":"github:DWCTOD:CVPR2022-Papers-with-Code-Demo","source_url":"https://github.com/DWCTOD/CVPR2022-Papers-with-Code-Demo"},{"type":"has_code","target_id":"github:klemenjak:nilm-papers-with-code","source_url":"https://github.com/klemenjak/nilm-papers-with-code"},{"type":"has_code","target_id":"github:Tom-Hardy-3D-Vision-Workshop:awesome-3D-vision","source_url":"https://github.com/Tom-Hardy-3D-Vision-Workshop/awesome-3D-vision"},{"type":"has_code","target_id":"github:NELSONZHAO:zhihu","source_url":"https://github.com/NELSONZHAO/zhihu"},{"type":"has_code","target_id":"github:Bjarten:computer-vision-ND","source_url":"https://github.com/Bjarten/computer-vision-ND"},{"type":"has_code","target_id":"github:mrdbourke:tensorflow-deep-learning","source_url":"https://github.com/mrdbourke/tensorflow-deep-learning"},{"type":"has_code","target_id":"github:The-AI-Summer:Deep-Learning-In-Production","source_url":"https://github.com/The-AI-Summer/Deep-Learning-In-Production"},{"type":"has_code","target_id":"github:The-AI-Summer:GANs-in-Computer-Vision","source_url":"https://github.com/The-AI-Summer/GANs-in-Computer-Vision"},{"type":"has_code","target_id":"github:deshpandenu:Time-Series-Forecasting-of-Amazon-Stock-Prices-using-Neural-Networks-LSTM-and-GAN-","source_url":"https://github.com/deshpandenu/Time-Series-Forecasting-of-Amazon-Stock-Prices-using-Neural-Networks-LSTM-and-GAN-"},{"type":"has_code","target_id":"github:kmario23:deep-learning-drizzle","source_url":"https://github.com/kmario23/deep-learning-drizzle"}]', NULL, NULL, 'pending', 70, '98811fcd8b39ade4742dddc3e16be02d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-ashishpatel26-500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code from https://github.com/ashishpatel26.png
Image converted to WebP: data/images/github-ashishpatel26-500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-donnemartin-data-science-ipython-notebooks', 'github--donnemartin--data-science-ipython-notebooks', 'data-science-ipython-notebooks', 'donnemartin', '<br/> <p align="center"> <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/README_1200x800.gif"> </p> <p align="center"> <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/coversmall_alt.png"> <br/> </p> * deep-learning * tensorflow * theano * keras * caffe * scikit-learn * statistical-inference-scipy * pandas * matplotlib * numpy * python-data * kaggle-and-business-analyses * spark * mapreduc...', '["aws","big-data","caffe","data-science","deep-learning","hadoop","kaggle","keras","machine-learning","mapreduce","matplotlib","numpy","pandas","python","scikit-learn","scipy","spark","tensorflow","theano","python"]', 'other', 28738, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/donnemartin/data-science-ipython-notebooks","fetched_at":"2025-12-08T10:39:52.042Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/README_1200x800.gif">\n</p>\n\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/coversmall_alt.png">\n  <br/>\n</p>\n\n# data-science-ipython-notebooks\n\n## Index\n\n* [deep-learning](#deep-learning)\n    * [tensorflow](#tensor-flow-tutorials)\n    * [theano](#theano-tutorials)\n    * [keras](#keras-tutorials)\n    * [caffe](#deep-learning-misc)\n* [scikit-learn](#scikit-learn)\n* [statistical-inference-scipy](#statistical-inference-scipy)\n* [pandas](#pandas)\n* [matplotlib](#matplotlib)\n* [numpy](#numpy)\n* [python-data](#python-data)\n* [kaggle-and-business-analyses](#kaggle-and-business-analyses)\n* [spark](#spark)\n* [mapreduce-python](#mapreduce-python)\n* [amazon web services](#aws)\n* [command lines](#commands)\n* [misc](#misc)\n* [notebook-installation](#notebook-installation)\n* [credits](#credits)\n* [contributing](#contributing)\n* [contact-info](#contact-info)\n* [license](#license)\n\n<br/>\n<p align="center">\n  <img src="http://i.imgur.com/ZhKXrKZ.png">\n</p>\n\n## deep-learning\n\nIPython Notebook(s) demonstrating deep learning functionality.\n\n<br/>\n<p align="center">\n  <img src="https://avatars0.githubusercontent.com/u/15658638?v=3&s=100">\n</p>\n\n### tensor-flow-tutorials\n\nAdditional TensorFlow tutorials:\n\n* [pkmital/tensorflow_tutorials](https://github.com/pkmital/tensorflow_tutorials)\n* [nlintz/TensorFlow-Tutorials](https://github.com/nlintz/TensorFlow-Tutorials)\n* [alrojo/tensorflow-tutorial](https://github.com/alrojo/tensorflow-tutorial)\n* [BinRoot/TensorFlow-Book](https://github.com/BinRoot/TensorFlow-Book)\n* [tuanavu/tensorflow-basic-tutorials](https://github.com/tuanavu/tensorflow-basic-tutorials)\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [tsf-basics](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/1_intro/basic_operations.ipynb) | Learn basic operations in TensorFlow, a library for various kinds of perceptual and language understanding tasks from Google. |\n| [tsf-linear](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/2_basic_classifiers/linear_regression.ipynb) | Implement linear regression in TensorFlow. |\n| [tsf-logistic](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/2_basic_classifiers/logistic_regression.ipynb) | Implement logistic regression in TensorFlow. |\n| [tsf-nn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/2_basic_classifiers/nearest_neighbor.ipynb) | Implement nearest neighboars in TensorFlow. |\n| [tsf-alex](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/alexnet.ipynb) | Implement AlexNet in TensorFlow. |\n| [tsf-cnn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/convolutional_network.ipynb) | Implement convolutional neural networks in TensorFlow. |\n| [tsf-mlp](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/multilayer_perceptron.ipynb) | Implement multilayer perceptrons in TensorFlow. |\n| [tsf-rnn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/recurrent_network.ipynb) | Implement recurrent neural networks in TensorFlow. |\n| [tsf-gpu](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/4_multi_gpu/multigpu_basics.ipynb) | Learn about basic multi-GPU computation in TensorFlow. |\n| [tsf-gviz](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/5_ui/graph_visualization.ipynb) | Learn about graph visualization in TensorFlow. |\n| [tsf-lviz](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/5_ui/loss_visualization.ipynb) | Learn about loss visualization in TensorFlow. |\n\n### tensor-flow-exercises\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [tsf-not-mnist](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/1_notmnist.ipynb) | Learn simple data curation by creating a pickle with formatted datasets for training, development and testing in TensorFlow. |\n| [tsf-fully-connected](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/2_fullyconnected.ipynb) | Progressively train deeper and more accurate models using logistic regression and neural networks in TensorFlow. |\n| [tsf-regularization](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/3_regularization.ipynb) | Explore regularization techniques by training fully connected networks to classify notMNIST characters in TensorFlow. |\n| [tsf-convolutions](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/4_convolutions.ipynb) | Create convolutional neural networks in TensorFlow. |\n| [tsf-word2vec](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/5_word2vec.ipynb) | Train a skip-gram model over Text8 data in TensorFlow. |\n| [tsf-lstm](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/6_lstm.ipynb) | Train a LSTM character model over Text8 data in TensorFlow. |\n\n<br/>\n<p align="center">\n  <img src="http://www.deeplearning.net/software/theano/_static/theano_logo_allblue_200x46.png">\n</p>\n\n### theano-tutorials\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [theano-intro](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/intro_theano/intro_theano.ipynb) | Intro to Theano, which allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It can use GPUs and perform efficient symbolic differentiation. |\n| [theano-scan](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/scan_tutorial/scan_tutorial.ipynb) | Learn scans, a mechanism to perform loops in a Theano graph. |\n| [theano-logistic](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/intro_theano/logistic_regression.ipynb) | Implement logistic regression in Theano. |\n| [theano-rnn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/rnn_tutorial/simple_rnn.ipynb) | Implement recurrent neural networks in Theano. |\n| [theano-mlp](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/theano_mlp/theano_mlp.ipynb) | Implement multilayer perceptrons in Theano. |\n\n<br/>\n<p align="center">\n  <img src="http://i.imgur.com/L45Q8c2.jpg">\n</p>\n\n### keras-tutorials\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| keras | Keras is an open source neural network library written in Python. It is capable of running on top of either Tensorflow or Theano. |\n| [setup](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/0.%20Preamble.ipynb) | Learn about the tutorial goals and how to set up your Keras environment. |\n| [intro-deep-learning-ann](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/1.1%20Introduction%20-%20Deep%20Learning%20and%20ANN.ipynb) | Get an intro to deep learning with Keras and Artificial Neural Networks (ANN). |\n| [theano](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/1.2%20Introduction%20-%20Theano.ipynb) | Learn about Theano by working with weights matrices and gradients. |\n| [keras-otto](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/1.3%20Introduction%20-%20Keras.ipynb) | Learn about Keras by looking at the Kaggle Otto challenge. |\n| [ann-mnist](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/1.4%20%28Extra%29%20A%20Simple%20Implementation%20of%20ANN%20for%20MNIST.ipynb) | Review a simple implementation of ANN for MNIST using Keras. |\n| [conv-nets](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/2.1%20Supervised%20Learning%20-%20ConvNets.ipynb) | Learn about Convolutional Neural Networks (CNNs) with Keras. |\n| [conv-net-1](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/2.2.1%20Supervised%20Learning%20-%20ConvNet%20HandsOn%20Part%20I.ipynb) | Recognize handwritten digits from MNIST using Keras - Part 1. |\n| [conv-net-2](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/2.2.2%20Supervised%20Learning%20-%20ConvNet%20HandsOn%20Part%20II.ipynb) | Recognize handwritten digits from MNIST using Keras - Part 2. |\n| [keras-models](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/2.3%20Supervised%20Learning%20-%20Famous%20Models%20with%20Keras.ipynb) | Use pre-trained models such as VGG16, VGG19, ResNet50, and Inception v3 with Keras. |\n| [auto-encoders](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/3.1%20Unsupervised%20Learning%20-%20AutoEncoders%20and%20Embeddings.ipynb) | Learn about Autoencoders with Keras. |\n| [rnn-lstm](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/3.2%20RNN%20and%20LSTM.ipynb) | Learn about Recurrent Neural Networks (RNNs) with Keras. |\n| [lstm-sentence-gen](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/3.3%20%28Extra%29%20LSTM%20for%20Sentence%20Generation.ipynb) |  Learn about RNNs using Long Short Term Memory (LSTM) networks with Keras. |\n\n### deep-learning-misc\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [deep-dream](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/deep-dream/dream.ipynb) | Caffe-based computer vision program which uses a convolutional neural network to find and enhance patterns in images. |\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/scikitlearn.png">\n</p>\n\n## scikit-learn\n\nIPython Notebook(s) demonstrating scikit-learn functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [intro](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-intro.ipynb) | Intro notebook to scikit-learn.  Scikit-learn adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays. |\n| [knn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-intro.ipynb#K-Nearest-Neighbors-Classifier) | Implement k-nearest neighbors in scikit-learn. |\n| [linear-reg](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-linear-reg.ipynb) | Implement linear regression in scikit-learn. |\n| [svm](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-svm.ipynb) | Implement support vector machine classifiers with and without kernels in scikit-learn. |\n| [random-forest](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-random-forest.ipynb) | Implement random forest classifiers and regressors in scikit-learn. |\n| [k-means](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-k-means.ipynb) | Implement k-means clustering in scikit-learn. |\n| [pca](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-pca.ipynb) | Implement principal component analysis in scikit-learn. |\n| [gmm](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-gmm.ipynb) | Implement Gaussian mixture models in scikit-learn. |\n| [validation](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-validation.ipynb) | Implement validation and model selection in scikit-learn. |\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/scipy.png">\n</p>\n\n## statistical-inference-scipy\n\nIPython Notebook(s) demonstrating statistical inference with SciPy functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| scipy | SciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data. |\n| [effect-size](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scipy/effect_size.ipynb) | Explore statistics that quantify effect size by analyzing the difference in height between men and women.  Uses data from the Behavioral Risk Factor Surveillance System (BRFSS) to estimate the mean and standard deviation of height for adult women and men in the United States. |\n| [sampling](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scipy/sampling.ipynb) | Explore random sampling by analyzing the average weight of men and women in the United States using BRFSS data. |\n| [hypothesis](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scipy/hypothesis.ipynb) | Explore hypothesis testing by analyzing the difference of first-born babies compared with others. |\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/pandas.png">\n</p>\n\n## pandas\n\nIPython Notebook(s) demonstrating pandas functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [pandas](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/pandas.ipynb) | Software library written for data manipulation and analysis in Python. Offers data structures and operations for manipulating numerical tables and time series. |\n| [github-data-wrangling](https://github.com/donnemartin/viz/blob/master/githubstats/data_wrangling.ipynb) | Learn how to load, clean, merge, and feature engineer by analyzing GitHub data from the [`Viz`](https://github.com/donnemartin/viz) repo. |\n| [Introduction-to-Pandas](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.00-Introduction-to-Pandas.ipynb) | Introduction to Pandas. |\n| [Introducing-Pandas-Objects](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.01-Introducing-Pandas-Objects.ipynb) | Learn about Pandas objects. |\n| [Data Indexing and Selection](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.02-Data-Indexing-and-Selection.ipynb) | Learn about data indexing and selection in Pandas. |\n| [Operations-in-Pandas](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.03-Operations-in-Pandas.ipynb) | Learn about operating on data in Pandas. |\n| [Missing-Values](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.04-Missing-Values.ipynb) | Learn about handling missing data in Pandas. |\n| [Hierarchical-Indexing](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.05-Hierarchical-Indexing.ipynb) | Learn about hierarchical indexing in Pandas. |\n| [Concat-And-Append](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.06-Concat-And-Append.ipynb) | Learn about combining datasets: concat and append in Pandas. |\n| [Merge-and-Join](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.07-Merge-and-Join.ipynb) | Learn about combining datasets: merge and join in Pandas. |\n| [Aggregation-and-Grouping](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.08-Aggregation-and-Grouping.ipynb) | Learn about aggregation and grouping in Pandas. |\n| [Pivot-Tables](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.09-Pivot-Tables.ipynb) | Learn about pivot tables in Pandas. |\n| [Working-With-Strings](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.10-Working-With-Strings.ipynb) | Learn about vectorized string operations in Pandas. |\n| [Working-with-Time-Series](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.11-Working-with-Time-Series.ipynb) | Learn about working with time series in pandas. |\n| [Performance-Eval-and-Query](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.12-Performance-Eval-and-Query.ipynb) | Learn about high-performance Pandas: eval() and query() in Pandas. |\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/matplotlib.png">\n</p>\n\n## matplotlib\n\nIPython Notebook(s) demonstrating matplotlib functionality.\n\n| Notebook | Description |\n|-----------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [matplotlib](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/matplotlib.ipynb) | Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. |\n| [matplotlib-applied](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/matplotlib-applied.ipynb) | Apply matplotlib visualizations to Kaggle competitions for exploratory data analysis.  Learn how to create bar plots, histograms, subplot2grid, normalized plots, scatter plots, subplots, and kernel density estimation plots. |\n| [Introduction-To-Matplotlib](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.00-Introduction-To-Matplotlib.ipynb) | Introduction to Matplotlib. |\n| [Simple-Line-Plots](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.01-Simple-Line-Plots.ipynb) | Learn about simple line plots in Matplotlib. |\n| [Simple-Scatter-Plots](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.02-Simple-Scatter-Plots.ipynb) | Learn about simple scatter plots in Matplotlib. |\n| [Errorbars.ipynb](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.03-Errorbars.ipynb) | Learn about visualizing errors in Matplotlib. |\n| [Density-and-Contour-Plots](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.04-Density-and-Contour-Plots.ipynb) | Learn about density and contour plots in Matplotlib. |\n| [Histograms-and-Binnings](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.05-Histograms-and-Binnings.ipynb) | Learn about histograms, binnings, and density in Matplotlib. |\n| [Customizing-Legends](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.06-Customizing-Legends.ipynb) | Learn about customizing plot legends in Matplotlib. |\n| [Customizing-Colorbars](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.07-Customizing-Colorbars.ipynb) | Learn about customizing colorbars in Matplotlib. |\n| [Multiple-Subplots](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.08-Multiple-Subplots.ipynb) | Learn about multiple subplots in Matplotlib. |\n| [Text-and-Annotation](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.09-Text-and-Annotation.ipynb) | Learn about text and annotation in Matplotlib. |\n| [Customizing-Ticks](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.10-Customizing-Ticks.ipynb) | Learn about customizing ticks in Matplotlib. |\n| [Settings-and-Stylesheets](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.11-Settings-and-Stylesheets.ipynb) | Learn about customizing Matplotlib: configurations and stylesheets. |\n| [Three-Dimensional-Plotting](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.12-Three-Dimensional-Plotting.ipynb) | Learn about three-dimensional plotting in Matplotlib. |\n| [Geographic-Data-With-Basemap](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.13-Geographic-Data-With-Basemap.ipynb) | Learn about geographic data with basemap in Matplotlib. |\n| [Visualization-With-Seaborn](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.14-Visualization-With-Seaborn.ipynb) | Learn about visualization with Seaborn. |\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/numpy.png">\n</p>\n\n## numpy\n\nIPython Notebook(s) demonstrating NumPy functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [numpy](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/numpy.ipynb) | Adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays. |\n| [Introduction-to-NumPy](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.00-Introduction-to-NumPy.ipynb) | Introduction to NumPy. |\n| [Understanding-Data-Types](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.01-Understanding-Data-Types.ipynb) | Learn about data types in Python. |\n| [The-Basics-Of-NumPy-Arrays](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.02-The-Basics-Of-NumPy-Arrays.ipynb) | Learn about the basics of NumPy arrays. |\n| [Computation-on-arrays-ufuncs](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.03-Computation-on-arrays-ufuncs.ipynb) | Learn about computations on NumPy arrays: universal functions. |\n| [Computation-on-arrays-aggregates](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.04-Computation-on-arrays-aggregates.ipynb) | Learn about aggregations: min, max, and everything in between in NumPy. |\n| [Computation-on-arrays-broadcasting](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.05-Computation-on-arrays-broadcasting.ipynb) | Learn about computation on arrays: broadcasting in NumPy. |\n| [Boolean-Arrays-and-Masks](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.06-Boolean-Arrays-and-Masks.ipynb) | Learn about comparisons, masks, and boolean logic in NumPy. |\n| [Fancy-Indexing](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.07-Fancy-Indexing.ipynb) | Learn about fancy indexing in NumPy. |\n| [Sorting](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.08-Sorting.ipynb) | Learn about sorting arrays in NumPy. |\n| [Structured-Data-NumPy](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.09-Structured-Data-NumPy.ipynb) | Learn about structured data: NumPy''s structured arrays. |\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/python.png">\n</p>\n\n## python-data\n\nIPython Notebook(s) demonstrating Python functionality geared towards data analysis.\n\n| Notebook | Description |\n|-----------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|\n| [data structures](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/structs.ipynb) | Learn Python basics with tuples, lists, dicts, sets. |\n| [data structure utilities](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/structs_utils.ipynb) | Learn Python operations such as slice, range, xrange, bisect, sort, sorted, reversed, enumerate, zip, list comprehensions. |\n| [functions](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/functions.ipynb) | Learn about more advanced Python features: Functions as objects, lambda functions, closures, *args, **kwargs currying, generators, generator expressions, itertools. |\n| [datetime](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/datetime.ipynb) | Learn how to work with Python dates and times: datetime, strftime, strptime, timedelta. |\n| [logging](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/logs.ipynb) | Learn about Python logging with RotatingFileHandler and TimedRotatingFileHandler. |\n| [pdb](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/pdb.ipynb) | Learn how to debug in Python with the interactive source code debugger. |\n| [unit tests](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/unit_tests.ipynb) | Learn how to test in Python with Nose unit tests. |\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/kaggle.png">\n</p>\n\n## kaggle-and-business-analyses\n\nIPython Notebook(s) used in [kaggle](https://www.kaggle.com/) competitions and business analyses.\n\n| Notebook | Description |\n|-------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\n| [titanic](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb) | Predict survival on the Titanic.  Learn data cleaning, exploratory data analysis, and machine learning. |\n| [churn-analysis](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/analyses/churn.ipynb) | Predict customer churn.  Exercise logistic regression, gradient boosting classifers, support vector machines, random forests, and k-nearest-neighbors.  Includes discussions of confusion matrices, ROC plots, feature importances, prediction probabilities, and calibration/descrimination.|\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/spark.png">\n</p>\n\n## spark\n\nIPython Notebook(s) demonstrating spark and HDFS functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|\n| [spark](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/spark/spark.ipynb) | In-memory cluster computing framework, up to 100 times faster for certain applications and is well suited for machine learning algorithms. |\n| [hdfs](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/spark/hdfs.ipynb) | Reliably stores very large files across machines in a large cluster. |\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/mrjob.png">\n</p>\n\n## mapreduce-python\n\nIPython Notebook(s) demonstrating Hadoop MapReduce with mrjob functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|\n| [mapreduce-python](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/mapreduce/mapreduce-python.ipynb) | Runs MapReduce jobs in Python, executing jobs locally or on Hadoop clusters. Demonstrates Hadoop Streaming in Python code with unit test and [mrjob](https://github.com/Yelp/mrjob) config file to analyze Amazon S3 bucket logs on Elastic MapReduce.  [Disco](https://github.com/discoproject/disco/) is another python-based alternative.|\n\n<br/>\n\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/aws.png">\n</p>\n\n## aws\n\nIPython Notebook(s) demonstrating Amazon Web Services (AWS) and AWS tools functionality.\n\n\nAlso check out:\n\n* [SAWS](https://github.com/donnemartin/saws): A Supercharged AWS command line interface (CLI).\n* [Awesome AWS](https://github.com/donnemartin/awesome-aws): A curated list of libraries, open source repos, guides, blogs, and other resources.\n\n| Notebook | Description |\n|------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [boto](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#Boto) | Official AWS SDK for Python. |\n| [s3cmd](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#s3cmd) | Interacts with S3 through the command line. |\n| [s3distcp](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#s3distcp) | Combines smaller files and aggregates them together by taking in a pattern and target file.  S3DistCp can also be used to transfer large volumes of data from S3 to your Hadoop cluster. |\n| [s3-parallel-put](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#s3-parallel-put) | Uploads multiple files to S3 in parallel. |\n| [redshift](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#redshift) | Acts as a fast data warehouse built on top of technology from massive parallel processing (MPP). |\n| [kinesis](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#kinesis) | Streams data in real time with the ability to process thousands of data streams per second. |\n| [lambda](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#lambda) | Runs code in response to events, automatically managing compute resources. |\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/commands.png">\n</p>\n\n## commands\n\nIPython Notebook(s) demonstrating various command lines for Linux, Git, etc.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [linux](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/linux.ipynb) | Unix-like and mostly POSIX-compliant computer operating system.  Disk usage, splitting files, grep, sed, curl, viewing running processes, terminal syntax highlighting, and Vim.|\n| [anaconda](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#anaconda) | Distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing, that aims to simplify package management and deployment. |\n| [ipython notebook](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#ipython-notebook) | Web-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document. |\n| [git](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#git) | Distributed revision control system with an emphasis on speed, data integrity, and support for distributed, non-linear workflows. |\n| [ruby](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#ruby) | Used to interact with the AWS command line and for Jekyll, a blog framework that can be hosted on GitHub Pages. |\n| [jekyll](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#jekyll) | Simple, blog-aware, static site generator for personal, project, or organization sites.  Renders Markdown or Textile and Liquid templates, and produces a complete, static website ready to be served by Apache HTTP Server, Nginx or another web server. |\n| [pelican](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#pelican) | Python-based alternative to Jekyll. |\n| [django](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#django) | High-level Python Web framework that encourages rapid development and clean, pragmatic design. It can be useful to share reports/analyses and for blogging. Lighter-weight alternatives include [Pyramid](https://github.com/Pylons/pyramid), [Flask](https://github.com/pallets/flask), [Tornado](https://github.com/tornadoweb/tornado), and [Bottle](https://github.com/bottlepy/bottle).\n\n## misc\n\nIPython Notebook(s) demonstrating miscellaneous functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [regex](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/misc/regex.ipynb) | Regular expression cheat sheet useful in data wrangling.|\n[algorithmia](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/misc/Algorithmia.ipynb) | Algorithmia is a marketplace for algorithms. This notebook showcases 4 different algorithms: Face Detection, Content Summarizer, Latent Dirichlet Allocation and Optical Character Recognition.|\n\n## notebook-installation\n\n### anaconda\n\nAnaconda is a free distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing that aims to simplify package management and deployment.\n\nFollow instructions to install [Anaconda](https://docs.continuum.io/anaconda/install) or the more lightweight [miniconda](http://conda.pydata.org/miniconda.html).\n\n### dev-setup\n\nFor detailed instructions, scripts, and tools to set up your development environment for data analysis, check out the [dev-setup](https://github.com/donnemartin/dev-setup) repo.\n\n### running-notebooks\n\nTo view interactive content or to modify elements within the IPython notebooks, you must first clone or download the repository then run the notebook.  More information on IPython Notebooks can be found [here.](http://ipython.org/notebook.html)\n\n    $ git clone https://github.com/donnemartin/data-science-ipython-notebooks.git\n    $ cd data-science-ipython-notebooks\n    $ jupyter notebook\n\nNotebooks tested with Python 2.7.x.\n\n## credits\n\n* [Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython](http://www.amazon.com/Python-Data-Analysis-Wrangling-IPython/dp/1449319793) by Wes McKinney\n* [PyCon 2015 Scikit-learn Tutorial](https://github.com/jakevdp/sklearn_pycon2015) by Jake VanderPlas\n* [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook) by Jake VanderPlas\n* [Parallel Machine Learning with scikit-learn and IPython](https://github.com/ogrisel/parallel_ml_tutorial) by Olivier Grisel\n* [Statistical Interference Using Computational Methods in Python](https://github.com/AllenDowney/CompStats) by Allen Downey\n* [TensorFlow Examples](https://github.com/aymericdamien/TensorFlow-Examples) by Aymeric Damien\n* [TensorFlow Tutorials](https://github.com/pkmital/tensorflow_tutorials) by Parag K Mital\n* [TensorFlow Tutorials](https://github.com/nlintz/TensorFlow-Tutorials) by Nathan Lintz\n* [TensorFlow Tutorials](https://github.com/alrojo/tensorflow-tutorial) by Alexander R Johansen\n* [TensorFlow Book](https://github.com/BinRoot/TensorFlow-Book) by Nishant Shukla\n* [Summer School 2015](https://github.com/mila-udem/summerschool2015) by mila-udem\n* [Keras tutorials](https://github.com/leriomaggio/deep-learning-keras-tensorflow) by Valerio Maggio\n* [Kaggle](https://www.kaggle.com/)\n* [Yhat Blog](http://blog.yhat.com/)\n\n## contributing\n\nContributions are welcome!  For bug reports or requests please [submit an issue](https://github.com/donnemartin/data-science-ipython-notebooks/issues).\n\n## contact-info\n\nFeel free to contact me to discuss any issues, questions, or comments.\n\n* Email: [donne.martin@gmail.com](mailto:donne.martin@gmail.com)\n* Twitter: [@donne_martin](https://twitter.com/donne_martin)\n* GitHub: [donnemartin](https://github.com/donnemartin)\n* LinkedIn: [donnemartin](https://www.linkedin.com/in/donnemartin)\n* Website: [donnemartin.com](http://donnemartin.com)\n\n## license\n\nThis repository contains a variety of content; some developed by Donne Martin, and some from third-parties.  The third-party content is distributed under the license provided by those parties.\n\nThe content developed by Donne Martin is distributed under the following license:\n\n*I am providing code and resources in this repository to you under an open source license.  Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).*\n\n    Copyright 2015 Donne Martin\n\n    Licensed under the Apache License, Version 2.0 (the "License");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an "AS IS" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n', '{"language":"Python","stars":28738,"forks":8042,"watchers":28738,"open_issues":40,"topics":["aws","big-data","caffe","data-science","deep-learning","hadoop","kaggle","keras","machine-learning","mapreduce","matplotlib","numpy","pandas","python","scikit-learn","scipy","spark","tensorflow","theano"],"default_branch":"master","size_kb":49025,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:pkmital:tensorflow_tutorials","source_url":"https://github.com/pkmital/tensorflow_tutorials"},{"type":"has_code","target_id":"github:nlintz:TensorFlow-Tutorials","source_url":"https://github.com/nlintz/TensorFlow-Tutorials"},{"type":"has_code","target_id":"github:alrojo:tensorflow-tutorial","source_url":"https://github.com/alrojo/tensorflow-tutorial"},{"type":"has_code","target_id":"github:BinRoot:TensorFlow-Book","source_url":"https://github.com/BinRoot/TensorFlow-Book"},{"type":"has_code","target_id":"github:tuanavu:tensorflow-basic-tutorials","source_url":"https://github.com/tuanavu/tensorflow-basic-tutorials"},{"type":"has_code","target_id":"github:donnemartin:viz","source_url":"https://github.com/donnemartin/viz"},{"type":"has_code","target_id":"github:donnemartin:viz","source_url":"https://github.com/donnemartin/viz"},{"type":"has_code","target_id":"github:Yelp:mrjob","source_url":"https://github.com/Yelp/mrjob"},{"type":"has_code","target_id":"github:discoproject:disco","source_url":"https://github.com/discoproject/disco"},{"type":"has_code","target_id":"github:donnemartin:saws","source_url":"https://github.com/donnemartin/saws"},{"type":"has_code","target_id":"github:donnemartin:awesome-aws","source_url":"https://github.com/donnemartin/awesome-aws"},{"type":"has_code","target_id":"github:Pylons:pyramid","source_url":"https://github.com/Pylons/pyramid"},{"type":"has_code","target_id":"github:pallets:flask","source_url":"https://github.com/pallets/flask"},{"type":"has_code","target_id":"github:tornadoweb:tornado","source_url":"https://github.com/tornadoweb/tornado"},{"type":"has_code","target_id":"github:bottlepy:bottle","source_url":"https://github.com/bottlepy/bottle"},{"type":"has_code","target_id":"github:donnemartin:dev-setup","source_url":"https://github.com/donnemartin/dev-setup"},{"type":"has_code","target_id":"github:donnemartin:data-science-ipython-notebooks.git","source_url":"https://github.com/donnemartin/data-science-ipython-notebooks.git"},{"type":"has_code","target_id":"github:jakevdp:sklearn_pycon2015","source_url":"https://github.com/jakevdp/sklearn_pycon2015"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:ogrisel:parallel_ml_tutorial","source_url":"https://github.com/ogrisel/parallel_ml_tutorial"},{"type":"has_code","target_id":"github:AllenDowney:CompStats","source_url":"https://github.com/AllenDowney/CompStats"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:pkmital:tensorflow_tutorials","source_url":"https://github.com/pkmital/tensorflow_tutorials"},{"type":"has_code","target_id":"github:nlintz:TensorFlow-Tutorials","source_url":"https://github.com/nlintz/TensorFlow-Tutorials"},{"type":"has_code","target_id":"github:alrojo:tensorflow-tutorial","source_url":"https://github.com/alrojo/tensorflow-tutorial"},{"type":"has_code","target_id":"github:BinRoot:TensorFlow-Book","source_url":"https://github.com/BinRoot/TensorFlow-Book"},{"type":"has_code","target_id":"github:mila-udem:summerschool2015","source_url":"https://github.com/mila-udem/summerschool2015"},{"type":"has_code","target_id":"github:leriomaggio:deep-learning-keras-tensorflow","source_url":"https://github.com/leriomaggio/deep-learning-keras-tensorflow"},{"type":"has_code","target_id":"github:donnemartin:data-science-ipython-notebooks","source_url":"https://github.com/donnemartin/data-science-ipython-notebooks"}]', NULL, 'NOASSERTION', 'approved', 80, '37dba8dda7f80f6172b4d2987fcc6473', NULL, NULL, CURRENT_TIMESTAMP);
