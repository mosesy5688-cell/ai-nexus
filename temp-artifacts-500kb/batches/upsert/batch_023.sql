/* LOGS:
Downloading image for github-dair-ai-ML-YouTube-Courses from https://github.com/dair-ai.png
Image converted to WebP: data/images/github-dair-ai-ML-YouTube-Courses.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-Mikoto10032-DeepLearning', 'github--mikoto10032--deeplearning', 'DeepLearning', 'Mikoto10032', '**完备的 AI 学习路线，最详细的中英文资源整理** :star: AiLearning: 机器学习 - MachineLearning - ML、深度学习 - DeepLearning - DL、自然语言处理 NL Machine-Learning * 矩阵微积分 * 机器学习的数学基础 * CS229线性代数与概率论基础 * 机器学习算法地图 * 机器学习 吴恩达 Coursera个人笔记 && 视频（含官方笔记） * CS229 课程讲义中文翻译 && 机器学习 吴恩达 cs229个人笔记 && 官网（笔记） && 视频（中文字幕） * 百页机器学习 * 《统计学习方法》李航 && 《统计学习方法》各章节笔记 && 《统计学习方法》各章节笔记 && 推荐答案：statistical-learning-method-solutions-manual 《统计学习方法》各章节笔记 && 《统计学习方法》各章节代码实现与课后习题参考解答 * 《模式识别与机器学习》 Christopher Bishop * 《机器学习》 周志华 && 南瓜书：pumpkin-book * 《机器学...', '["cnn","deep-learning","deeplearning","gan","gcn","kaggle","machine-learning","machinelearning","mxnet","pytorch","rnn","tensorflow","jupyter notebook"]', 'other', 16940, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/Mikoto10032/DeepLearning","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# 		DeepLearning Tutorial\n## 一. 入门资料\n\n[**完备的 AI 学习路线，最详细的中英文资源整理**](https://zhuanlan.zhihu.com/p/64052743) :star:\n\n[AiLearning: 机器学习 - MachineLearning - ML、深度学习 - DeepLearning - DL、自然语言处理 NL](https://github.com/apachecn/AiLearning)\n\n[Machine-Learning](https://github.com/shunliz/Machine-Learning)\n\n### 数学基础\n\n![](notes/Images/MathematicalBasis.png)\n\n* [矩阵微积分](https://zh.wikipedia.org/wiki/%E7%9F%A9%E9%98%B5%E5%BE%AE%E7%A7%AF%E5%88%86)\n* [机器学习的数学基础](https://github.com/fengdu78/Data-Science-Notes/tree/master/0.math/0.basic)\n* [CS229线性代数与概率论基础](https://github.com/fengdu78/Data-Science-Notes/tree/master/0.math/1.CS229)\n\n### 机器学习基础\n\n#### 快速入门\n* [机器学习算法地图](http://www.tensorinfinity.com/paper_18.html)\n* [机器学习 吴恩达 Coursera个人笔记](https://github.com/Mikoto10032/DeepLearning/blob/master/books/%5BML-Coursera%5D%5B2014%5D%5BAndrew%20Ng%5D/%5B2014%5D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AA%E4%BA%BA%E7%AC%94%E8%AE%B0%E5%AE%8C%E6%95%B4%E7%89%88v5.1.pdf)  && [视频（含官方笔记）](https://www.coursera.org/learn/machine-learning)  \n* [CS229 课程讲义中文翻译](https://kivy-cn.github.io/Stanford-CS-229-CN/#/) && [机器学习 吴恩达 cs229个人笔记](https://github.com/Mikoto10032/DeepLearning/blob/master/books/%5BML-CS229%5D%5B2011%5D%5BAndrew%20NG%5D/%5B2011%5D%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E4%B8%AA%E4%BA%BA%E7%AC%94.pdf) && [官网（笔记）](http://cs229.stanford.edu/)  && [视频（中文字幕）](http://open.163.com/newview/movie/free?pid=M6SGF6VB4&mid=M6SGHFBMC)   \n* [百页机器学习](http://themlbook.com/wiki/doku.php)\n\n#### 深入理解\n* [《统计学习方法》李航](https://github.com/Mikoto10032/DeepLearning/tree/master/books/%E6%9D%8E%E8%88%AA-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0) && [《统计学习方法》各章节笔记](https://www.cnblogs.com/YongSun/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/) && [《统计学习方法》各章节笔记](https://zhuanlan.zhihu.com/c_1213397558586257408) && [推荐答案：statistical-learning-method-solutions-manual](https://github.com/datawhalechina/statistical-learning-method-solutions-manual)  [《统计学习方法》各章节笔记](https://www.cnblogs.com/liaohuiqiang/category/1039314.html) &&  [《统计学习方法》各章节代码实现与课后习题参考解答](https://blog.csdn.net/breeze_blows/article/details/85469944)\n* [《模式识别与机器学习》 Christopher Bishop](https://github.com/Mikoto10032/DeepLearning/blob/master/books/模式识别与机器学习PRML_Chinese_vision.pdf)\n* [《机器学习》 周志华](https://github.com/Mikoto10032/DeepLearning/blob/master/books/机器学习周志华.pdf)    && [南瓜书：pumpkin-book](https://github.com/datawhalechina/pumpkin-book)\n* [《机器学习实战》 PelerHarrington](https://github.com/Mikoto10032/DeepLearning/blob/master/books/机器学习实战%20中文双页版.pdf)\n* [机器学习与深度学习书单](https://mp.weixin.qq.com/s?__biz=MzAxMjcyNjE5MQ==&mid=2650488718&idx=1&sn=815a79d27d500f0fb8db1fe1fc6cfe48&chksm=83a2e54eb4d56c58a0989654f920d64ad2784ce52e4b2bc6883974257cf475c9983f05fb88c1&scene=0&xtrack=1&ascene=14&devicetype=android-28&version=27000339&nettype=WIFI&abtest_cookie=AwABAAoACwATAAQAI5ceAFaZHgDQmR4A3JkeAAAA&lang=zh_CN&pass_ticket=oEB1108Pes6HkdxEITmBjTb2Glju5%2BEGqHZKz50fMg0rgK4l9Fodlbe%2FDm96iX57&wx_header=1)\n\n### 深度学习基础\n\n#### 快速入门\n* [深度学习思维导图](https://github.com/dformoso/deeplearning-mindmap)	&& [深度学习算法地图](http://www.tensorinfinity.com/paper_158.html)\n* [《斯坦福大学深度学习基础教程》 Andrew Ng（吴恩达）](https://github.com/Mikoto10032/DeepLearning/blob/master/books/斯坦福大学-深度学习基础教程.pdf)  \n* [深度学习 吴恩达 个人笔记](http://www.ai-start.com/dl2017/)  && [视频](http://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n* [MIT深度学习基础-2019视频课程](https://deeplearning.mit.edu/)\n* [台湾大学（NTU）李宏毅教授课程](http://speech.ee.ntu.edu.tw/~tlkagk/index.html) && [[leeml-notes](https://github.com/datawhalechina/leeml-notes)\n* [图解深度学习_Grokking-Deep-Learning](https://github.com/iamtrask/Grokking-Deep-Learning)\n* [《神经网络与深度学习》 Michael Nielsen](https://github.com/Mikoto10032/DeepLearning/blob/master/books/神经网络和深度学习neural%20networks%20and%20deep-learning-中文_ALL.pdf)     \n* [ CS321-Hinton](http://www.cs.toronto.edu/~tijmen/csc321/)\n* [ CS230: Deep Learning](https://web.stanford.edu/class/cs230/)		\n* [ CS294-112](http://rail.eecs.berkeley.edu/deeprlcourse/resources/)\n\n##### 计算机视觉\n* [CS231 李飞飞 已授权个人翻译笔记](https://zhuanlan.zhihu.com/p/21930884) && [视频](http://study.163.com/course/courseMain.htm?courseId=1003223001)\n* [计算机视觉研究方向](https://mp.weixin.qq.com/s/WNkzfvYtEO5zJoe_-yAPow)\n\n##### 自然语言处理\n* [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/index.html)\n* [NLP上手教程](https://github.com/FudanNLP/nlp-beginner)\n* [NLP入门推荐书目（2019版）](https://zhuanlan.zhihu.com/p/58874484)\n\n##### 深度强化学习\n* [CS234: Reinforcement Learning](http://web.stanford.edu/class/cs234/index.html)\n\n#### 深入理解\n* [《深度学习》 Yoshua Bengio.Ian GoodFellow](https://github.com/Mikoto10032/DeepLearning/blob/master/books/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.Yoshua%20Bengio%2BIan%20GoodFellow.pdf):star:       \n* [《自然语言处理》Jacob Eisenstein](https://github.com/Mikoto10032/DeepLearning/blob/master/books/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.Jacob%20Eisenstein.pdf)		\n* [《强化学习》](https://github.com/Mikoto10032/DeepLearning/blob/master/books/Reinforcement%20Learning.Sutton.pdf) && [第二版](http://incompleteideas.net/book/RLbook2018trimmed.pdf)	\n* [hangdong的深度学习博客,论文推荐](https://handong1587.github.io/categories.html#deep_learning-ref)\n* [Practical Deep Learning for Coders, v3](https://course.fast.ai/)\n* [《Tensorflow实战Google深度学习框架》 郑泽宇 顾思宇](https://github.com/Mikoto10032/DeepLearning/blob/master/books/Tensorflow%20实战Google深度学习框架.pdf)\n\n#### 一些书单\n\n* [2019年最新-深度学习、生成对抗、Pytorch优秀教材推荐](https://zhuanlan.zhihu.com/p/63784033)\n\n### 工程能力\n\n![](https://pic4.zhimg.com/v2-009013278688f520c070b27910255cb1_r.jpg)\n\n* [如何系统地学习算法？](https://www.zhihu.com/question/20588261/answer/798928056) && [LeetCode](https://leetcode.com/) && [leetcode题解](https://github.com/azl397985856/leetcode) && [《算法导论》中算法的C++实现](https://github.com/huaxz1986/cplusplus-_Implementation_Of_Introduction_to_Algorithms)\n* [机器学习算法实战](#机器学习实战篇)\n* [深度学习框架](#深度学习框架)\n* [如何成为一名算法工程师](https://mp.weixin.qq.com/s/YMtnBAVDZepsMTO4h-VRtQ) && [从小白到入门算法，我的经验分享给你～](https://mp.weixin.qq.com/s?__biz=MzAxMjcyNjE5MQ==&mid=2650488786&idx=1&sn=68b9536d0b0b3105ab8d79f8efcb0a4b&chksm=83a2e512b4d56c045c6ab0349108842e6a5b26e8f3e507ff5d19ee50e3bd63ef149a36d23eef&scene=0&xtrack=1&ascene=14&devicetype=android-28&version=27000437&nettype=WIFI&abtest_cookie=BAABAAoACwASABMABgAjlx4AVpkeANCZHgDcmR4A8ZkeAAOaHgAAAA%3D%3D&lang=zh_CN&pass_ticket=4yovfEr0v09yZCvvQ1NEy12qGIonnRpGi774X09Mh5EZD2oL%2BRz6FTtX9R5gALB1&wx_header=1) && [我的研究生这三年](https://zhuanlan.zhihu.com/p/54161673) :star:\n* [编程面试的题目分类](https://zhuanlan.zhihu.com/p/89392459)\n* [《AI算法工程师手册》](http://www.huaxiaozhuan.com/)\n* [如何准备算法工程师面试，斩获一线互联网公司机器学习岗offer？](https://zhuanlan.zhihu.com/p/76827460)\n* [【完结】深度学习CV算法工程师从入门到初级面试有多远，大概是25篇文章的距离](https://mp.weixin.qq.com/s/HZ3Cd2jHuikyFN9ydvcMTw)\n* [ 计算机相关技术面试必备](https://github.com/CyC2018/CS-Notes) && [CS-WiKi](https://veal98.gitee.io/cs-wiki/#/) && [计算机基础面试问题全面总结](https://github.com/wolverinn/Waking-Up) && [TeachYourselfCS-CN](https://github.com/keithnull/TeachYourselfCS-CN) && [面试算法笔记-中文](https://github.com/imhuay/Algorithm_for_Interview-Chinese) \n* [算法工程师面试](https://github.com/DarLiner/Algorithm_Interview_Notes-Chinese)\n* [深度学习面试题目](https://github.com/ShanghaiTechAIClub/DLInterview)\n* [深度学习500问](https://github.com/scutan90/DeepLearning-500-questions)\n* [AI算法岗求职攻略](https://github.com/amusi/AI-Job-Notes#Strategy)\n* [Kaggle实战]()\n  * 常用算法：\n    * Feature Engineering：continue variable && categorical variable\n    * Classic machine learning algorithm：LR, KNN, SVM, Random Forest, GBDT(XGBoost&&LightGBM), Factorization Machine, Field-aware Factorization Machine, Neural Network\n    * Cross validation, model selection：grid search, random search, hyper-opt\n    * Ensemble learning\n  * [kaggle竞赛宝典第一章-竞赛框架篇！:star:](https://mp.weixin.qq.com/s/EGiFG6u9BYr1aBdq0a0wIQ)\n  * [Kaggle 项目实战（教程） = 文档 + 代码 + 视频](https://github.com/apachecn/kaggle)\n  * [Kaggle入门系列：（一）机器学习环境搭建](https://zhuanlan.zhihu.com/p/29086448) && [Kaggle入门系列：（二）Kaggle简介](https://zhuanlan.zhihu.com/p/29417603) && [Kaggle入门系列（三）Titanic初试身手](https://zhuanlan.zhihu.com/p/29086614)\n  * [从 0 到 1 走进 Kaggle](https://zhuanlan.zhihu.com/p/61660061) \n  * [Kaggle 入门指南](https://zhuanlan.zhihu.com/p/25742261) \n  * [一个框架解决几乎所有机器学习问题](https://zhuanlan.zhihu.com/p/61657532) && [Approaching (Almost) Any Machine Learning Problem | Abhishek Thakur](http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/)\n  * [分分钟带你杀入Kaggle Top 1%](https://zhuanlan.zhihu.com/p/27424282)\n  * [如何达到Kaggle竞赛top 2%？这里有一篇特征探索经验帖](https://zhuanlan.zhihu.com/p/48758045) \n  * [如何在 Kaggle 首战中进入前 10%？](https://zhuanlan.zhihu.com/p/27486736)\n  * [Kaggle 首战 Top 2%, APTOS 2019 复盘总结 + 机器学习竞赛通用流程归纳](http://bbs.cvmart.net/topics/1717)\n  * [kaggle的riiid比赛里关于数据处理时间空间优化的笔记](https://zhuanlan.zhihu.com/p/344388290)\n* [大数据&机器学习相关竞赛推荐](https://blog.csdn.net/weixin_33739541/article/details/87565983)\n\n## 二. 神经网络模型概览		\n\n* [1. 一文看懂25个神经网络模型](https://blog.csdn.net/qq_35082030/article/details/73368962)\n* [2. DNN概述论文：详解前馈、卷积和循环神经网络技术](https://zhuanlan.zhihu.com/p/29141828)\n* [3. colah''s blog](http://colah.github.io/)\n* [4. Model Zoom](https://modelzoo.co/)\n* [5. DNN概述](https://zhuanlan.zhihu.com/p/29141828)\n* [GitHub上的机器学习/深度学习综述项目合集](https://zhuanlan.zhihu.com/p/60245227)\n* [AlphaTree-graphic-deep-neural-network](https://github.com/weslynn/AlphaTree-graphic-deep-neural-network)\n\n### CNN\n\n#### 发展史\n\n* [94页论文综述卷积神经网络：从基础技术到研究前景](https://zhuanlan.zhihu.com/p/35388569)\n\n##### 图像分类\n\n* [从LeNet-5到DenseNet](https://zhuanlan.zhihu.com/p/31006686)      \n* [深度学习笔记（十一）网络 Inception, Xception, MobileNet, ShuffeNet, ResNeXt, SqueezeNet, EfficientNet, MixConv](https://www.cnblogs.com/xuanyuyt/p/11329998.html)\n* [CNN网络结构的发展](https://zhuanlan.zhihu.com/p/68411179)\n* [Awesome - Image Classification：论文&&代码大全](https://github.com/weiaicunzai/awesome-image-classification)\n* [pytorch-image-models](https://github.com/rwightman/pytorch-image-models)\n\n##### 目标检测\n\n- [深度学习之目标检测的前世今生（Mask R-CNN）](https://zhuanlan.zhihu.com/p/32830206)      \n- [深度学习目标检测模型全面综述：Faster R-CNN、R-FCN和SSD](https://zhuanlan.zhihu.com/p/29434605)      \n- [从RCNN到SSD，这应该是最全的一份目标检测算法盘点](https://zhuanlan.zhihu.com/p/36184131)    \n- [目标检测算法综述三部曲](https://zhuanlan.zhihu.com/p/40047760)\n  - [基于深度学习的目标检测算法综述（一）](https://zhuanlan.zhihu.com/p/40047760)\n  - [基于深度学习的目标检测算法综述（二）](https://zhuanlan.zhihu.com/p/40020809)\n  - [基于深度学习的目标检测算法综述（三）](https://zhuanlan.zhihu.com/p/40102001)\n- [From RCNN to YOLOv3]()：[上](https://zhuanlan.zhihu.com/p/35724768)，[下](https://zhuanlan.zhihu.com/p/35731743)\n- [后 R-CNN时代， Faster R-CNN、SSD、YOLO 各类变体统治下的目标检测综述：Faster R-CNN系列胜了吗？](https://zhuanlan.zhihu.com/p/38709522)\n- [目标检测进化史](https://zhuanlan.zhihu.com/p/60590369)\n- [CVPR2019目标检测方法进展综述](https://zhuanlan.zhihu.com/p/59376548)\n- [一文看尽21篇目标检测最新论文（腾讯/Google/商汤/旷视/清华/浙大/CMU/华科/中科院等](https://zhuanlan.zhihu.com/p/61080508)\n- [我这两年的目标检测](https://zhuanlan.zhihu.com/p/82491218)\n- [Anchor-Free目标检测算法](): [第一篇：arxiv2015_baidu_DenseBox](https://zhuanlan.zhihu.com/p/40221183)， [如何评价最新的anchor-free目标检测模型FoveaBox？](https://www.zhihu.com/question/319605567/answer/647844997), [FCOS: 最新的one-stage逐像素目标检测算法](https://zhuanlan.zhihu.com/p/61644900) && [最新的Anchor-Free目标检测模型FCOS，现已开源！](https://zhuanlan.zhihu.com/p/62198865) && [中科院牛津华为诺亚提出CenterNet，one-stage detector可达47AP，已开源！](https://zhuanlan.zhihu.com/p/62789701) && [AnchorFreeDetection](https://github.com/VCBE123/AnchorFreeDetection)\n- [Anchor free深度学习的目标检测方法](https://zhuanlan.zhihu.com/p/64563186)\n- [聊聊Anchor的"前世今生"（上）](https://zhuanlan.zhihu.com/p/63273342)&&[聊聊Anchor的"前世今生"（下）](https://zhuanlan.zhihu.com/p/68291859)\n- [目标检测算法综述之FPN优化篇](https://zhuanlan.zhihu.com/p/62975854) && [一文看尽物体检测中的各种FPN](https://zhuanlan.zhihu.com/p/148738276)\n- [awesome-object-detection：论文&&代码](https://github.com/amusi/awesome-object-detection)\n- [deep_learning_object_detection](https://github.com/hoya012/deep_learning_object_detection)\n- [ObjectDetectionImbalance](https://github.com/kemaloksuz/ObjectDetectionImbalance)\n\n##### 图像分割（语义分割、实例分割、全景分割）\n\n* [图像语义分割(Semantic segmentation) Survey](https://zhuanlan.zhihu.com/p/36801104)\n* [干货 | 一文概览主要语义分割网络](https://blog.csdn.net/qq_20084101/article/details/80432960)\n* [语义分割 发展综述](https://zhuanlan.zhihu.com/p/37618829)       \n* [9102年了，语义分割的入坑指南和最新进展都是什么样的](https://zhuanlan.zhihu.com/p/76603228)\n* [实例分割最新最全面综述：从Mask R-CNN到BlendMask](https://zhuanlan.zhihu.com/p/110132002)\n* [语义分割综述：深度学习背景下的语义分割的发展状况【推荐】](https://zhuanlan.zhihu.com/p/133212654)\n* [Awesome Semantic Segmentation：论文&&代码](https://github.com/mrgloom/awesome-semantic-segmentation)\n* [一篇看完就懂的最新语义分割综述](https://zhuanlan.zhihu.com/p/110123136)\n* [基于深度学习的语义分割综述](https://zhuanlan.zhihu.com/p/142451150)\n\n##### 轻量化卷积神经网络\n\n- [纵览轻量化卷积神经网络：SqueezeNet、MobileNet、ShuffleNet、Xception](https://zhuanlan.zhihu.com/p/32746221)   \n\n##### 人脸相关\n\n* [如何走近深度学习人脸识别？你需要这篇超长综述 | 附开源代码](https://zhuanlan.zhihu.com/p/35295839)    \n* [人脸检测和识别算法综述]()      \n    * [人脸检测算法综述 ](https://zhuanlan.zhihu.com/p/36621308)          \n    * [人脸检测背景介绍和发展现状](https://zhuanlan.zhihu.com/p/32702868)\n    * [人脸识别算法演化史](https://zhuanlan.zhihu.com/p/36416906)\n    * [CascadeCNN](https://blog.csdn.net/shuzfan/article/details/50358809)  \n    * [MTCNN](https://blog.csdn.net/qq_14845119/article/details/52680940)\n    * [awesome-Face_Recognition](https://github.com/ChanChiChoi/awesome-Face_Recognition)\n    * [异质人脸识别研究综述](https://zhuanlan.zhihu.com/p/64191484)\n    * [老板来了：人脸识别+手机推送，老板来了你立刻知道。](https://zhuanlan.zhihu.com/p/26431250)&& [手把手教你用Python实现人脸识别](https://zhuanlan.zhihu.com/p/33456076) && [人脸识别项目，网络模型，损失函数，数据集相关总结](https://www.jianshu.com/p/e57205edc364)\n    * [基于深度学习的人脸识别技术综述](https://zhuanlan.zhihu.com/p/24816781) && [如何走近深度学习人脸识别？你需要这篇超长综述](https://zhuanlan.zhihu.com/p/35295839) && [人脸识别损失函数综述（附开源实现）](https://zhuanlan.zhihu.com/p/51324547) && [Face Recognition Loss on Mnist with Pytorch](https://zhuanlan.zhihu.com/p/64427565) && [人脸识别的LOSS（上）](https://zhuanlan.zhihu.com/p/34404607) && [人脸识别的LOSS（下）](https://zhuanlan.zhihu.com/p/34436551)\n* [人脸关键点检测]()\n    * [【每周CV论文推荐】 初学深度学习人脸关键点检测必读文章](https://zhuanlan.zhihu.com/p/88344339)\n    * [从传统方法到深度学习，人脸关键点检测方法综述](https://mp.weixin.qq.com/s/CvdeV5xgUF0kStJQdRst0w)\n    * [人脸关键点检测综述](https://zhuanlan.zhihu.com/p/42968117)\n    * [人脸专集4 | 遮挡、光照等因素的人脸关键点检测](https://zhuanlan.zhihu.com/p/62824113)\n    * [【Face key point detection】人脸关键点检测实现](https://zhuanlan.zhihu.com/p/52525598)\n    * [OpenCV实战：人脸关键点检测（FaceMark）](https://zhuanlan.zhihu.com/p/35390012)\n    * [CenterFace+TensorRT部署人脸和关键点检测400fps](https://zhuanlan.zhihu.com/p/106774468)\n\n##### 图像超分辨率\n\n* [深度学习图像超分辨率综述](https://zhuanlan.zhihu.com/p/57564211)\n* [从SRCNN到EDSR，总结深度学习端到端超分辨率方法发展历程](https://zhuanlan.zhihu.com/p/31664818)\n\n##### 行人重识别\n\n* [【CVPR2019正式公布】行人重识别论文](https://zhuanlan.zhihu.com/p/62843442)\n* [【CVPR2019正式公布】行人重识别论文](https://zhuanlan.zhihu.com/p/62843442)，[2019 行人再识别年度进展回顾](https://zhuanlan.zhihu.com/p/64004977)\n\n##### 图像着色\n\n* [Awesome-Image-Colorization](https://github.com/MarkMoHR/Awesome-Image-Colorization)\n\n##### 边检测\n\n* [Awesome-Edge-Detection-Papers](https://github.com/MarkMoHR/Awesome-Edge-Detection-Papers)\n\n##### OCR&&文本检测\n\n* [2019CVPR文本检测综述](https://zhuanlan.zhihu.com/p/67319122)\n* [OCR文字处理](https://zhuanlan.zhihu.com/p/65707543)\n* [自然场景文本检测识别技术综述](https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&mid=2247485142&idx=1&sn=c0e01da30eb5e750be453eabe4be2bf4&chksm=fdb69b41cac11257ae22c7dac395e9651dab628fc35dd6d3c02d9566a8c7f5f2b56353d58a64&token=1065243837&lang=zh_CN#rd)\n\n##### 点云\n\n* [awesome-point-cloud-analysis](https://zhuanlan.zhihu.com/p/65690433)\n\n##### 细粒度图像分类\n\n* [超全深度学习细粒度图像分析：项目、综述、教程一网打尽](https://zhuanlan.zhihu.com/p/73542103)\n\n##### 图像检索\n\n* 图像检索的十年[上](https://mp.weixin.qq.com/s/sM78DCOK3fuG2JrP2QaSZA)、[下](https://mp.weixin.qq.com/s/yzVMDEpwbXVS0y-CwWSBEA)\n\n##### 人群计数\n\n* [人群计数](http://chuansong.me/n/443237851736), [1](https://www.cnblogs.com/wmr95/p/8134692.html), [2](https://blog.csdn.net/u011285477/article/details/51954989), [3](https://blog.csdn.net/qingqingdeaini/article/details/79922549)\n\n#### 教程\n\n##### 前馈神经网络\n\n* [从基本原理到梯度下降，小白都能看懂的神经网络教程](https://zhuanlan.zhihu.com/p/59385110)\n\n##### 激活函数\n\n* [激活函数一览](https://zhuanlan.zhihu.com/p/30567264) && [深度学习中几种常见的激活函数理解与总结](https://www.cnblogs.com/XDU-Lakers/p/10557496.html)\n* [一个激活函数需要具有哪些必要的属性](https://www.zhihu.com/question/67366051)\n\n##### 反向传播算法\n\n* [反向传播算法（过程及公式推导）](https://blog.csdn.net/u014313009/article/details/51039334)\n* [通俗理解神经网络BP传播算法](https://zhuanlan.zhihu.com/p/24801814)\n\n##### 优化问题\n\n* [神经网络训练中的梯度消失与梯度爆炸](https://zhuanlan.zhihu.com/p/25631496)\n* [梯度消失和梯度爆炸问题详解](https://www.jianshu.com/p/3f35e555d5ba)\n* [详解深度学习中的梯度消失、爆炸原因及其解决方法](https://zhuanlan.zhihu.com/p/33006526) && [神经网络梯度消失和梯度爆炸及解决办法](https://blog.csdn.net/program_developer/article/details/80032376)\n\n##### 卷积层\n\n* [A Comprehensive Introduction to Different Types of Convolutions in Deep Learning](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215) && 翻译：[上](https://www.leiphone.com/news/201902/D2Mkv61w9IPq9qGh.html)、[下](https://www.leiphone.com/news/201902/biIqSBpehsaXFwpN.html?uniqueCode=OTEsp9649VqJfUcO)\n* [卷积有多少种？一文读懂深度学习中的各种卷积](https://zhuanlan.zhihu.com/p/57575810)\n* [各种卷积](https://www.cnblogs.com/cvtoEyes/p/8848815.html)\n* [Convolution Network及其变种（反卷积、扩展卷积、因果卷积、图卷积）](https://www.cnblogs.com/yangperasd/p/7071657.html)\n* [深度学习基础--卷积类型](https://zhuanlan.zhihu.com/p/59839551)\n* [变形卷积核、可分离卷积](https://zhuanlan.zhihu.com/p/28749411)\n* [对深度可分离卷积、分组卷积、扩张卷积、转置卷积（反卷积）的理解](https://blog.csdn.net/chaolei3/article/details/79374563)\n* [反卷积](https://buptldy.github.io/2016/10/29/2016-10-29-deconv/) \n* [Dilated/Atrous conv 空洞卷积/多孔卷积](https://blog.csdn.net/silence2015/article/details/79748729)\n* [卷积层输出大小尺寸计算及 “SAME” 和 “VALID”](https://blog.csdn.net/weixin_37697191/article/details/89527315) && [卷积的三种模式full, same, valid以及padding的same, valid](https://zhuanlan.zhihu.com/p/62760780)\n* [正常卷积与空洞卷积输出特征图与感受野大小的计算](https://blog.csdn.net/qq_43232545/article/details/103317773)\n* [【Tensorflow】tf.nn.depthwise_conv2d如何实现深度卷积?](https://blog.csdn.net/mao_xiao_feng/article/details/78003476)	\n* [【Tensorflow】tf.nn.atrous_conv2d如何实现空洞卷积？](https://blog.csdn.net/mao_xiao_feng/article/details/78003730)\n* [【Tensorflow】tf.nn.separable_conv2d如何实现深度可分卷积?](https://blog.csdn.net/mao_xiao_feng/article/details/78002811)	\n* [【TensorFlow】tf.nn.conv2d_transpose是怎样实现反卷积的？](https://blog.csdn.net/mao_xiao_feng/article/details/71713358)\n\n##### 池化层\n\n* [卷积神经网络中的各种池化操作](https://zhuanlan.zhihu.com/p/112216409)\n\n##### 卷积神经网络\n\n- [卷积神经网络工作原理](https://www.zhihu.com/question/39022858)\n- [「七夕的礼物」: 一日搞懂卷积神经网络](https://zhuanlan.zhihu.com/p/28863709)\n- [一文读懂卷积神经网络中的1x1卷积核](https://zhuanlan.zhihu.com/p/40050371)\n- [如何理解神经网络中通过add和concate的方式融合特征？](https://blog.csdn.net/xiaojiajia007/article/details/86008415) && [神经网络中对需要concat的特征进行线性变换然后相加是否好于直接concat?](https://www.zhihu.com/question/389912594/answer/1178054600)\n- [CNN 模型所需的计算力（flops）和参数（parameters）数量是怎么计算的？](https://www.zhihu.com/question/65305385) && [深度学习中卷积的参数量和计算量](https://www.cnblogs.com/hejunlin1992/p/12978988.html)\n\n##### 图像分类网络详解\n\n* [经典CNN模型LeNet解读](https://zhuanlan.zhihu.com/p/41736894)\n* [机器学习进阶笔记之三 | 深入理解Alexnet](https://zhuanlan.zhihu.com/p/22659166)\n* [一文读懂VGG网络](https://zhuanlan.zhihu.com/p/41423739)\n* [Inception V1,V2,V3,V4 模型总结](https://zhuanlan.zhihu.com/p/52802896)\n* [ResNet解析](https://blog.csdn.net/lanran2/article/details/79057994)\n* [一文简述ResNet及其多种变体](https://zhuanlan.zhihu.com/p/35985680)\n* [CapsNet入门系列](http://mp.weixin.qq.com/s?__biz=MzI3ODkxODU3Mg==&mid=2247484099&idx=1&sn=97e209f1a9860c8d8c51e81d98fc8a0a&chksm=eb4ee600dc396f16624a33cdfc0ead905e62ae9447b49b20146020e6cbd7d71f089101512a40&scene=21#wechat_redirect)\n  - [CapsNet入门系列之一：胶囊网络背后的直觉](http://mp.weixin.qq.com/s?__biz=MzI3ODkxODU3Mg==&mid=2247484099&idx=1&sn=97e209f1a9860c8d8c51e81d98fc8a0a&chksm=eb4ee600dc396f16624a33cdfc0ead905e62ae9447b49b20146020e6cbd7d71f089101512a40&scene=21#wechat_redirect)\n  - [CapsNet入门系列之二：胶囊如何工作](http://mp.weixin.qq.com/s?__biz=MzI3ODkxODU3Mg==&mid=2247484165&idx=1&sn=0ca679e3a5f499f8d8addb405fe3df83&chksm=eb4ee7c6dc396ed0a330fcac12690110bcaf9a8a10794dbc5e1a326c69ecbb140140f55fd6ba&scene=21#wechat_redirect)\n  - [CapsNet入门系列之三：囊间动态路由算法](http://mp.weixin.qq.com/s?__biz=MzI3ODkxODU3Mg==&mid=2247484433&idx=1&sn=3afe4605bc2501eebbc41c6dd1af9572&chksm=eb4ee0d2dc3969c4619d6c1097d5c949c76c6c854e60d36eba4388da2c3855747818d062c90a&scene=21#wechat_redirect)\n  - [CapsNet入门系列之四：胶囊网络架构](https://mp.weixin.qq.com/s/6CRSen8P6zKaMGtX8IRfqw)\n* [深入剖析MobileNet和它的变种（例如：ShuffleNet）为什么会变快？](https://zhuanlan.zhihu.com/p/158591662)\n* [CNN模型之ShuffleNet](https://zhuanlan.zhihu.com/p/32304419)\n* [ShuffleNet V2和四个网络架构设计准则](https://zhuanlan.zhihu.com/p/40980942)\n* [ResNeXt 深入解读与模型实现](https://zhuanlan.zhihu.com/p/78019001)\n* [如何评价Momenta ImageNet 2017夺冠架构SENet?](https://www.zhihu.com/question/63460684)\n* [CBAM：卷积块注意力模块](https://zhuanlan.zhihu.com/p/79419670) && [CBAM: Convolutional Block Attention Module](https://zhuanlan.zhihu.com/p/65529934)\n* [SKNet——SENet孪生兄弟篇](https://zhuanlan.zhihu.com/p/59690223)\n* [GCNet：当Non-local遇见SENet](https://zhuanlan.zhihu.com/p/64988633)\n* [深度学习笔记（十一）网络 Inception, Xception, MobileNet, ShuffeNet, ResNeXt, SqueezeNet, EfficientNet, MixConv](https://www.cnblogs.com/xuanyuyt/p/11329998.html)\n* [如何评价最新的Octave Convolution？](https://www.zhihu.com/question/320462422)\n* [ResNeSt 之语义分割](https://zhuanlan.zhihu.com/p/136105870) && [关于ResNeSt的点滴疑惑](https://zhuanlan.zhihu.com/p/133805433) && [ResNeSt在刷榜之后被ECCV2020 strong reject](https://zhuanlan.zhihu.com/p/143214871)\n\n##### 目标检测网络详解\n\n* [目标检测的性能评价指标](https://zhuanlan.zhihu.com/p/70306015) && [NMS和计算mAP时的置信度阈值和IoU阈值 ](https://zhuanlan.zhihu.com/p/75348108) && [白话mAP](https://zhuanlan.zhihu.com/p/60834912) && [目标检测模型的评估指标mAP详解(附代码）](https://zhuanlan.zhihu.com/p/37910324)\n* [深度学习中IU、IoU(Intersection over Union)](https://blog.csdn.net/iamoldpan/article/details/78799857)\n* [Selective Search for Object Detection ](https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/)[（译文）](https://blog.csdn.net/guoyunfei20/article/details/78723646)\n* [Region Proposal Network(RPN)](https://zhuanlan.zhihu.com/p/106192020)\n* [边框回归(Bounding Box Regression)详解](https://blog.csdn.net/zijin0802034/article/details/77685438)\n* [NMS——非极大值抑制](https://blog.csdn.net/shuzfan/article/details/52711706) && [非极大值抑制NMS的python实现](https://zhuanlan.zhihu.com/p/128125301)\n* [一文打尽目标检测NMS——精度提升篇](https://zhuanlan.zhihu.com/p/151914931) && [一文打尽目标检测NMS——效率提升篇](https://zhuanlan.zhihu.com/p/157900024)\n* [目标检测回归损失函数简介：SmoothL1/IoU/GIoU/DIoU/CIoU Loss](https://zhuanlan.zhihu.com/p/104236411)\n* [将CNN引入目标检测的开山之作：R-CNN](https://zhuanlan.zhihu.com/p/23006190)\n* [R-CNN论文详解](https://blog.csdn.net/u014696921/article/details/52824097)\n* [深度学习（十八）基于R-CNN的物体检测](https://blog.csdn.net/hjimce/article/details/50187029)\n* [Fast R-CNN](https://zhuanlan.zhihu.com/p/24780395)\n* [深度学习（六十四）Faster R-CNN物体检测](https://blog.csdn.net/hjimce/article/details/73382553) && [你真的学会RoI Pooling了吗?](https://zhuanlan.zhihu.com/p/59692298)\n* [目标检测论文阅读：Feature Pyramid Networks for Object Detection](https://zhuanlan.zhihu.com/p/36461718)\n* [SSD](https://zhuanlan.zhihu.com/p/24954433)\n* [实例分割--Mask RCNN详解(ROI Align / Loss Function)](https://www.codetd.com/article/2554465) && [令人拍案称奇的Mask RCNN](https://zhuanlan.zhihu.com/p/37998710)\n* [何恺明大神的「Focal Loss」，如何更好地理解？](https://zhuanlan.zhihu.com/p/32423092) && [FocalLoss 对样本不平衡的权重调节和减低损失值](https://zhuanlan.zhihu.com/p/82148525) && [focal_loss 多类别和二分类 Pytorch代码实现](https://blog.csdn.net/qq_33278884/article/details/91572173) && [多分类focal loss及其tensorflow实现](https://blog.csdn.net/qq_39012149/article/details/96184383)\n* [堪比Focal Loss！解决目标检测中样本不平衡的无采样方法](https://zhuanlan.zhihu.com/p/93658728)\n* [目标检测正负样本区分策略和平衡策略总结(一)](https://zhuanlan.zhihu.com/p/138824387) && [目标检测正负样本区分策略和平衡策略总结(二)](https://zhuanlan.zhihu.com/p/138828372) && [目标检测正负样本区分策略和平衡策略总结(三）](https://zhuanlan.zhihu.com/p/144659734)\n* [YOLO](http://www.mamicode.com/info-detail-2314392.html) && [目标检测|YOLO原理与实现](https://zhuanlan.zhihu.com/p/32525231) && [图解YOLO](https://zhuanlan.zhihu.com/p/24916786) && [【论文解读】Yolo三部曲解读——Yolov1](https://zhuanlan.zhihu.com/p/70387154)\n* [目标检测|YOLOv2原理与实现(附YOLOv3)](https://zhuanlan.zhihu.com/p/35325884?group_id=966229905398362112) && [YOLO2](https://zhuanlan.zhihu.com/p/25167153) && [【论文解读】Yolo三部曲解读——Yolov2](https://zhuanlan.zhihu.com/p/74540100)\n* [<机器爱学习>YOLO v3深入理解](https://zhuanlan.zhihu.com/p/49556105) && [【论文解读】Yolo三部曲解读——Yolov3](https://zhuanlan.zhihu.com/p/76802514)\n* [YOLOv4](https://zhuanlan.zhihu.com/p/138510087)\n* [目标检测之CornerNet](https://arxiv.org/abs/1808.01244), [1](https://zhuanlan.zhihu.com/p/41825737), [2](https://blog.csdn.net/Hibercraft/article/details/81637451), [3](https://zhuanlan.zhihu.com/p/41759548)\n* [目标检测小tricks--样本不均衡处理](https://zhuanlan.zhihu.com/p/60612064)\n\n##### 图像分割网络详解\n\n* [超像素、语义分割、实例分割、全景分割 傻傻分不清 ](https://zhuanlan.zhihu.com/p/50996404) && [语义分割、实例分割和全景分割的区别](https://blog.csdn.net/u013066730/article/details/103613154)\n* [语义分割卷积神经网络快速入门](https://blog.csdn.net/qq_20084101/article/details/80455877)          \n* [图像语义分割入门+FCN/U-Net网络解析](https://zhuanlan.zhihu.com/p/31428783) && [深入理解深度学习分割网络Ｕnet](https://blog.csdn.net/Formlsl/article/details/80373200)\n* [Unet神经网络为什么会在医学图像分割表现好？](https://www.zhihu.com/question/269914775)\n* [图像语义分割的工作原理和CNN架构变迁](https://zhuanlan.zhihu.com/p/38033032)\n* [语义分割中的Attention和低秩重建](https://zhuanlan.zhihu.com/p/77834369)\n* [打通多个视觉任务的全能Backbone:HRNet](https://zhuanlan.zhihu.com/p/134253318)\n\n##### 注意力机制\n\n* [深度学习中的注意力模型（2017版）](https://zhuanlan.zhihu.com/p/37601161)\n* [Attention Model（mechanism） 的 套路](https://blog.csdn.net/bvl10101111/article/details/78470716)\n* [计算机视觉中的注意力机制（推荐）](https://zhuanlan.zhihu.com/p/146130215) \n* [More About Attention（推荐）](https://zhuanlan.zhihu.com/p/106662375)\n* [计算机视觉中的注意力机制](https://zhuanlan.zhihu.com/p/32928645)\n* [NLP中的Attention Mechanism](https://zhuanlan.zhihu.com/p/31547842)\n* [Transformer中的Attention](https://mp.weixin.qq.com/s/k8PdZAld2ANVoekuyQxI3w)\n* [综述：图像处理中的注意力机制](https://bbs.cvmart.net/topics/2581)\n\n##### 特征融合\n\n* [盘点目标检测中的特征融合技巧（根据YOLO v4总结）](https://zhuanlan.zhihu.com/p/141685352)\n* [多尺度融合介绍](https://zhuanlan.zhihu.com/p/147820687)\n\n#### Action\n\n* [PyTorch官方实现ResNet](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py) && [pytorch_resnet_cifar10](https://github.com/akamaster/pytorch_resnet_cifar10)\n* [PyTorch 63.Coding for FLOPs, Params and Latency](https://zhuanlan.zhihu.com/p/268816646)\n* [先读懂CapsNet架构然后用TensorFlow实现](https://zhuanlan.zhihu.com/p/30753326)\n* [目标检测-20种模型的原味代码汇总](https://zhuanlan.zhihu.com/p/37056927)     \n* [TensorFlow Object Detection API 教程](https://blog.csdn.net/qq_36148847/article/details/79306762)\n  * [TensorFlow 对象检测 API 教程1](https://blog.csdn.net/qq_36148847/article/details/79306762)\n  * [TensorFlow 对象检测 API 教程2](https://blog.csdn.net/qq_36148847/article/details/79307598)\n  * [TensorFlow 对象检测 API 教程3](https://blog.csdn.net/qq_36148847/article/details/79307751)\n  * [TensorFlow 对象检测 API 教程 4](https://blog.csdn.net/qq_36148847/article/details/79307931)\n  * [TensorFlow 对象检测 API 教程5](https://blog.csdn.net/qq_36148847/article/details/79307933)\n* [在TensorFlow+Keras环境下使用RoI池化一步步实现注意力机制](https://zhuanlan.zhihu.com/p/65327747)\n* [mxnet如何查看参数数量](https://discuss.gluon.ai/t/topic/7216) && [mxnet查看FLOPS](https://github.com/likelyzhao/CalFLOPS-Mxnet)\n* [Pytorch-UNet](https://github.com/milesial/Pytorch-UNet)\n* [segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch)\n\n### GAN\n\n#### 发展史      \n\n* [千奇百怪的GAN变体](https://zhuanlan.zhihu.com/p/26491601)      \n* [苏剑林博客，讲解得淋漓尽致](https://kexue.fm/tag/GAN/)\n* [The GAN Landscape：Losses, Architectures, Regularization, and Normalization](https://arxiv.org/pdf/1807.04720.pdf)\n* [深度学习新星：GAN的基本原理、应用和走向](https://www.leiphone.com/news/201701/Kq6FvnjgbKK8Lh8N.html)\n* [GAN生成图像综述](https://zhuanlan.zhihu.com/p/62746494)\n* [2017年GAN 计算机视觉相关paper汇总](https://zhuanlan.zhihu.com/p/29882709)\n* [必读的10篇关于GAN的论文](https://zhuanlan.zhihu.com/p/72745900)\n\n#### 教程     \n\n* [GAN原理学习笔记](https://zhuanlan.zhihu.com/p/27295635)\n* [GAN万字长文综述](https://zhuanlan.zhihu.com/p/58812258)\n* [极端图像压缩的对抗生成网络](https://zhuanlan.zhihu.com/p/35783437?group_id=969598777652420608)\n* [台湾大学李宏毅GAN教程](https://www.youtube.com/watch?v=0CKeqXl5IY0&feature=youtu.be)\n    * [Basic](https://github.com/Mikoto10032/DeepLearning/blob/master/books/GAN-Basic%20Idea%20(2017.04.21).pdf)\n    * [Improving](https://github.com/Mikoto10032/DeepLearning/blob/master/books/GAN-Improving%20GAN%20(2017.05.05).pdf)\n* [CycleGAN：图片风格，想换就换 | ICCV 2017论文解读](https://zhuanlan.zhihu.com/p/34711316)\n* [Wasserstein GAN](https://zhuanlan.zhihu.com/p/25071913) && [GAN：两者分布不重合JS散度为log2的数学证明](https://blog.csdn.net/Invokar/article/details/88917214)\n* [用变分推断统一理解生成模型（VAE、GAN、AAE、ALI）](https://zhuanlan.zhihu.com/p/40105143)\n#### Action     \n\n* [GAN学习指南：从原理入门到制作生成Demo](https://zhuanlan.zhihu.com/p/24767059)    \n* [机器之心GitHub项目：GAN完整理论推导与实现](https://zhuanlan.zhihu.com/p/29837245)   \n* [在Keras上实现GAN：构建消除图片模糊的应用](https://zhuanlan.zhihu.com/p/35030377)  \n\n### RNN      \n\n#### 发展史      \n\n* [从90年代的SRNN开始，纵览循环神经网络27年的研究进展](https://zhuanlan.zhihu.com/p/32668465)       \n\n#### 教程     \n\n* [Awesome-Chinese-NLP](https://github.com/crownpku/Awesome-Chinese-NLP)\n* [nlp-pytorch-zh](https://github.com/apachecn/nlp-pytorch-zh)\n* [完全图解RNN、RNN变体、Seq2Seq、Attention机制](https://zhuanlan.zhihu.com/p/28054589)\n* [循环神经网络(RNN, Recurrent Neural Networks)介绍](https://blog.csdn.net/heyongluoyao8/article/details/48636251)\n* [RNN以及LSTM的介绍和公式梳理](https://blog.csdn.net/Dark_Scope/article/details/47056361)\n* [（译）理解长短期记忆(LSTM) 神经网络](https://zhuanlan.zhihu.com/p/24018768)\n* [ 一文读懂LSTM和RNN](https://zhuanlan.zhihu.com/p/35878575?group_id=970350175025385472)\n* [探索LSTM：基本概念到内部结构](https://zhuanlan.zhihu.com/p/27345523)\n* [ 翻译：深入理解LSTM系列](https://blog.csdn.net/matrix_space/article/details/53374040)                      \n* [深入理解 LSTM 网络 (一)](https://blog.csdn.net/matrix_space/article/details/53374040)\n* [深入理解 LSTM 网络 (二)](https://blog.csdn.net/matrix_space/article/details/53376870)\n* [LSTM](https://zhuanlan.zhihu.com/p/32085405)\n* [深度学习其五 循环神经网络](https://zybuluo.com/hanbingtao/note/541458)                      \n* [用循环神经网络进行文件无损压缩：斯坦福大学提出DeepZip](https://zhuanlan.zhihu.com/p/32582764)         \n* [吴恩达序列建模课程]()\n    * [Coursera吴恩达《序列模型》课程笔记（1）-- 循环神经网络（RNN）](https://zhuanlan.zhihu.com/p/34309635)\n    * [Coursera吴恩达《序列模型》课程笔记（2）-- NLP & Word Embeddings](https://zhuanlan.zhihu.com/p/34975871)\n    * [Coursera吴恩达《序列模型》课程笔记（3）-- Sequence models & Attention mechanism](https://zhuanlan.zhihu.com/p/35532553)\n* word2vec\n\n    - 原理\n      - [NLP 秒懂词向量Word2vec的本质](https://zhuanlan.zhihu.com/p/26306795)\n      - [一篇通俗易懂的word2vec](https://zhuanlan.zhihu.com/p/35500923)\n      - [YJango的Word Embedding--介绍](https://zhuanlan.zhihu.com/p/27830489)\n      - [nlp中的词向量对比：word2vec/glove/fastText/elmo/GPT/bert](https://zhuanlan.zhihu.com/p/56382372)\n      - [词嵌入（word2vec）](https://zh.diveintodeeplearning.org/chapter_natural-language-processing/word2vec.html)\n      - [谈谈谷歌word2vec的原理](https://blog.csdn.net/wangyangzhizhou/article/details/77073023)\n      - [Word2Vec中为什么使用负采样？](https://zhuanlan.zhihu.com/p/67117737)\n    - 训练词向量\n      - [练习-word2vec](https://zhuanlan.zhihu.com/p/29200034)\n      - [word2vec方法的实现和应用](https://zhuanlan.zhihu.com/p/31886824)\n      - [自然语言处理入门 word2vec 使用tensorflow自己训练词向量](https://blog.csdn.net/wzdjsgf/article/details/79541492)\n      - [使用tensorflow实现word2vec中文词向量的训练](https://zhuanlan.zhihu.com/p/28979653)\n      - [如何用TensorFlow训练词向量](https://blog.csdn.net/wangyangzhizhou/article/details/77530479?locationNum=1&fps=1)\n* [聊聊 Transformer](https://zhuanlan.zhihu.com/p/47812375)\n* [基于Transform的机器翻译系统](https://zhuanlan.zhihu.com/p/144825330)\n* [基于word2vec训练词向量(一)](https://zhuanlan.zhihu.com/p/35648927)\n* [基于word2vec训练词向量(二)](https://zhuanlan.zhihu.com/p/35889385)\n* [自然语言处理中的自注意力机制（Self-Attention Mechanism）](https://zhuanlan.zhihu.com/p/35041012)    \n* [自然语言处理中注意力机制综述](https://zhuanlan.zhihu.com/p/54491016)\n* [YJango的Word Embedding--介绍](https://zhuanlan.zhihu.com/p/27830489)          \n\n#### Action     \n\n* [推荐：nlp-tutorial](https://github.com/graykode/nlp-tutorial)\n* [nlp-tutorial](https://github.com/lyeoni/nlp-tutorial)\n* [tensorflow中RNNcell源码分析以及自定义RNNCell的方法](https://blog.csdn.net/liuchonge/article/details/78405185?locationNum=8&fps=1)     \n* [TensorFlow中RNN实现的正确打开方式](https://zhuanlan.zhihu.com/p/28196873)      \n* [TensorFlow RNN 代码](https://zhuanlan.zhihu.com/p/27906426)\n* [Tensorflow实现的深度NLP模型集锦](https://zhuanlan.zhihu.com/p/67031035)\n* [用tensorflow LSTM如何预测股票价格](https://zhuanlan.zhihu.com/p/33186759)\n* [TensorFlow的多层LSTM实践](https://zhuanlan.zhihu.com/p/29797089)\n* [《安娜卡列尼娜》文本生成——利用TensorFlow构建LSTM模型](https://zhuanlan.zhihu.com/p/27087310)\n\n### GNN\n\n#### 发展史\n\n* [Graph Neural Network（GNN）综述](https://zhuanlan.zhihu.com/p/65539782)\n* [深度学习时代的图模型，清华发文综述图网络](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650754422&idx=4&sn=0dc881487f362322a875b4ce06e645f7&chksm=871a8908b06d001ef7386ccc752827c20711877a4a23d6a8318978095dd241d118257c607b22&scene=21#wechat_redirect)\n* [清华大学图神经网络综述：模型与应用](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650754558&idx=2&sn=7d79191b9ed30679d5d40e22d9cabdf8&chksm=871a8980b06d00962e0dbe984e1d3469214db31cb402b4725a0dfe330249a830b45cb26932b5&scene=21#wechat_redirect)\n* [图神经网络概述第三弹：来自IEEE Fellow的GNN综述](https://zhuanlan.zhihu.com/p/54241746)\n* [GNN最全文献资料整理](https://github.com/DeepGraphLearning/LiteratureDL4Graph) && [Awesome-Graph-Neural-Networks](https://github.com/nnzhan/Awesome-Graph-Neural-Networks)\n\n#### 教程\n\n* [如何理解 Graph Convolutional Network（GCN）](https://www.zhihu.com/question/54504471)\n* [图卷积网络(GCN)新手村完全指南](https://zhuanlan.zhihu.com/p/54505069)\n* [何时能懂你的心——图卷积神经网络（GCN）](https://zhuanlan.zhihu.com/p/71200936)\n* [图卷积网络GCN的理解与介绍](https://zhuanlan.zhihu.com/p/90470499)\n* [一文读懂图卷积GCN](https://zhuanlan.zhihu.com/p/89503068)\n* [2020 年 GNN 开卷有益与再谈图卷积](https://zhuanlan.zhihu.com/p/101310106)\n* [【GCN】万字长文带你入门 GCN](https://zhuanlan.zhihu.com/p/120311352)\n* [如何解决图神经网络（GNN）训练中过度平滑的问题？](https://www.zhihu.com/question/346942899/answer/848298494)\n* [全连接的图卷积网络(GCN)和self-attention这些机制有什么区别联系](https://www.zhihu.com/question/366088445) && [CNN与GCN的区别、联系及融合](https://zhuanlan.zhihu.com/p/147654689)\n\n#### Action\n\n* [图卷积网络到底怎么做，这是一份极简的Numpy实现](https://zhuanlan.zhihu.com/p/57235377)\n* [DGL](https://docs.dgl.ai/index.html)\n\n## 三. 深度模型的优化与正则化\n\n* [1. 优化算法纵览](http://fa.bianp.net/teaching/2018/eecs227at/)\n* [2. 从梯度下降到Adam](https://zhuanlan.zhihu.com/p/27449596)\n* [3. 从梯度下降到拟牛顿法：盘点训练神经网络的五大学习算法](https://zhuanlan.zhihu.com/p/25703402)\n* [4. 正则化技术总结](https://zhuanlan.zhihu.com/p/35429054?group_id=966442942538444800)\n  * [史上最全面的正则化技术总结与分析--part1](https://zhuanlan.zhihu.com/p/35429054?group_id=966442942538444800)\n  * [史上最全面的正则化技术总结与分析--part2](https://zhuanlan.zhihu.com/p/35432128?group_id=966443101011738624)\n* [权重衰减（weight decay）与学习率衰减（learning rate decay）](https://zhuanlan.zhihu.com/p/38709373) && [pytorch必须掌握的的4种学习率衰减策略](https://zhuanlan.zhihu.com/p/93624972)\n* [5. 最优化算法系列（math）](https://blog.csdn.net/chunyun0716/article/category/6188191/2)\n* [6. 神经网络训练中的梯度消失与梯度爆炸](https://zhuanlan.zhihu.com/p/25631496)        \n* [7. 神经网络的优化及训练](https://zhuanlan.zhihu.com/p/36050743)\n* [8. 通俗讲解查全率和查准率](https://zhuanlan.zhihu.com/p/35888543) && [全面梳理：准确率,精确率,召回率,查准率,查全率,假阳性,真阳性,PRC,ROC,AUC,F1](https://zhuanlan.zhihu.com/p/34079183) && [机器学习之类别不平衡问题 (1) —— 各种评估指标](https://zhuanlan.zhihu.com/p/34473430) && [机器学习之类别不平衡问题 (2) —— ROC和PR曲线](https://zhuanlan.zhihu.com/p/34655990) && [AUC详解与python实现](https://zhuanlan.zhihu.com/p/84035782) && [微平均和宏平均](https://zhuanlan.zhihu.com/p/78628437)  && [机器学习中的性能度量](https://zhuanlan.zhihu.com/p/74980268) && [精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么](https://www.zhihu.com/question/30643044)\n* [激活函数一览](https://zhuanlan.zhihu.com/p/30567264) && [深度学习中几种常见的激活函数理解与总结](https://www.cnblogs.com/XDU-Lakers/p/10557496.html)\n* [深度学习笔记(三)：激活函数和损失函数](https://blog.csdn.net/u014595019/article/details/52562159)\n* [激活函数/损失函数汇总](https://zhuanlan.zhihu.com/p/30385380)\n* [机器学习中常见的损失函数及其应用场景](https://blog.csdn.net/zuolixiangfisher/article/details/88649110) && [PyTorch的十八个损失函数](https://zhuanlan.zhihu.com/p/61379965)\n* [深度度量学习中的损失函数](https://zhuanlan.zhihu.com/p/82199561)\n* [反向传播算法（过程及公式推导）](https://blog.csdn.net/u014313009/article/details/51039334)\n* [通俗理解神经网络BP传播算法](https://zhuanlan.zhihu.com/p/24801814)\n* [10. Coursera吴恩达《优化深度神经网络》课程笔记（3）-- 超参数调试、Batch正则化和编程框架](https://zhuanlan.zhihu.com/p/30922689)\n* [11. 机器学习各种熵](https://zhuanlan.zhihu.com/p/35423404)\n* [12. 距离和相似性度量](https://zhuanlan.zhihu.com/p/27305237)\n* [13. 机器学习里的黑色艺术：normalization, standardization, regularization](https://zhuanlan.zhihu.com/p/29974820) && [数据标准化/归一化normalization](https://blog.csdn.net/pipisorry/article/details/52247379) && [特征工程中的「归一化」有什么作用？](https://www.zhihu.com/question/20455227)\n* [14. LSTM系列的梯度问题](https://zhuanlan.zhihu.com/p/36101196)\n* [15. 损失函数整理](https://zhuanlan.zhihu.com/p/35027284)\n* [16. 详解残差块为何有助于解决梯度弥散问题](https://zhuanlan.zhihu.com/p/28124810)\n* [17. FAIR何恺明等人提出组归一化：替代批归一化，不受批量大小限制](https://zhuanlan.zhihu.com/p/34858971)\n* [18. Batch Normalization（BN）]():[1 ](https://zhuanlan.zhihu.com/p/26702482),[2 ](https://blog.csdn.net/hjimce/article/details/50866313),[3 ](https://bbs.cvmart.net/topics/576),[4 ](https://blog.csdn.net/edogawachia/article/details/80040456), [5](https://zhuanlan.zhihu.com/p/38176412), [6](https://www.zhihu.com/question/38102762), [7](https://zhuanlan.zhihu.com/p/52132614)\n* [19. 详解深度学习中的Normalization，不只是BN](https://zhuanlan.zhihu.com/p/33173246) && [如何区分并记住常见的几种 Normalization 算法](https://zhuanlan.zhihu.com/p/69659844)\n* [20. BFGS](https://blog.csdn.net/philosophyatmath/article/details/70173128)\n* [21. 详解深度学习中的梯度消失、爆炸原因及其解决方法](https://zhuanlan.zhihu.com/p/33006526) && [神经网络梯度消失和梯度爆炸及解决办法](https://blog.csdn.net/program_developer/article/details/80032376)\n* [22. Dropout](https://arxiv.org/pdf/1207.0580.pdf), [1](https://blog.csdn.net/stdcoutzyx/article/details/49022443), [2](https://blog.csdn.net/hjimce/article/details/50413257), [3](https://blog.csdn.net/shuzfan/article/details/50580915)，[系列解读Dropout](https://blog.csdn.net/shuzfan/article/details/50580915)\n* [23.谱归一化（Spectral Normalization）的理解](https://blog.csdn.net/StreamRock/article/details/83590347)，[常见向量范数和矩阵范数](https://blog.csdn.net/left_la/article/details/9159949)，[谱范数正则（Spectral Norm Regularization）的理解](https://blog.csdn.net/StreamRock/article/details/83539937)\n* [24.L1正则化与L2正则化](https://zhuanlan.zhihu.com/p/35356992) && [深入理解L1、L2正则化](https://zhuanlan.zhihu.com/p/29360425) && [L2正则=Weight Decay？并不是这样](https://zhuanlan.zhihu.com/p/40814046) && [都9102年了，别再用Adam + L2 regularization](https://zhuanlan.zhihu.com/p/63982470)\n* [25.为什么选用交叉熵而不是MSE](https://zhuanlan.zhihu.com/p/61944055) &&[为什么使用交叉熵作为损失函数](https://zhuanlan.zhihu.com/p/63731947) &&[二元分类为什么不能用MSE做为损失函数？](http://sofasofa.io/forum_main_post.php?postid=1001792)&& [为什么平方损失函数不适用分类问题？](https://www.zhihu.com/question/319865092)\n* [浅谈神经网络中的梯度爆炸问题](https://zhuanlan.zhihu.com/p/32154263)\n* [为什么weight decay能够防止过拟合](https://www.zhihu.com/question/65626362)\n* [交叉熵代价函数（作用及公式推导）](https://blog.csdn.net/u014313009/article/details/51043064) && [交叉熵损失的来源、说明、求导与pytorch实现](https://zhuanlan.zhihu.com/p/67782576) && [Softmax函数与交叉熵](https://zhuanlan.zhihu.com/p/27223959) && [极大似然估计与最小化交叉熵损失或者KL散度为什么等价](https://zhuanlan.zhihu.com/p/84764177)\n* [梯度下降优化算法纵览](http://ruder.io/optimizing-gradient-descent/), [1](https://blog.csdn.net/qq_23269761/article/details/80901411), [2](https://www.cnblogs.com/guoyaohua/p/8542554.html), [几种优化算法的比较（BGD、SGD、Adam、RMSPROP）](https://blog.csdn.net/qq_32172681/article/details/100979476)\n* **Softmax**：[详解softmax函数以及相关求导过程](https://zhuanlan.zhihu.com/p/25723112)  &&  [softmax的log似然代价函数（公式求导）](https://blog.csdn.net/u014313009/article/details/51045303) && [【技术综述】一文道尽softmax loss及其变种](https://zhuanlan.zhihu.com/p/34044634)\n* [从最优化的角度看待Softmax损失函数](https://zhuanlan.zhihu.com/p/45014864)  && [Softmax理解之二分类与多分类](https://zhuanlan.zhihu.com/p/45368976) && [Softmax理解之Smooth程度控制](https://zhuanlan.zhihu.com/p/49939159) && [Softmax理解之margin](https://zhuanlan.zhihu.com/p/52108088)\n* **权重初始化**\n  * [神经网络中的权重初始化一览：从基础到Kaiming](https://zhuanlan.zhihu.com/p/62850258)\n  * [深度学习中常见的权重初始化方法](https://zhuanlan.zhihu.com/p/138064188)\n  * [深度学习中神经网络的几种权重初始化方法](https://blog.csdn.net/u012328159/article/details/80025785)\n  * [谈谈神经网络权重为什么不能初始化为0](https://zhuanlan.zhihu.com/p/75879624)\n  * [神经网络中的偏置（bias）究竟有这么用？](https://www.zhihu.com/question/305340182)\n  * [深度学习里面的偏置为什么不加正则？](https://www.zhihu.com/question/66894061)\n* [为什么说bagging是减少variance，而boosting是减少bias?](https://www.zhihu.com/question/26760839)\n\n## 四. 炼丹术士那些事\n\n### 调参经验\n* [训练的神经网络不工作？一文带你跨过这37个坑](https://blog.csdn.net/jiandanjinxin/article/details/77190687)\n* [Deep Learning 之 训练过程中出现NaN问题](https://blog.csdn.net/BVL10101111/article/details/76086344)\n* [神经网络训练trick](https://zhuanlan.zhihu.com/p/59918821)\n* [你有哪些deep learning（rnn、cnn）调参的经验？](https://www.zhihu.com/question/41631631)\n* [GAN的一些小trick](https://zhuanlan.zhihu.com/p/27725664)\n* [深度学习与计算机视觉系列(8)_神经网络训练与注意点](https://blog.csdn.net/han_xiaoyang/article/details/50521064)\n* [神经网络训练loss不下降原因集合](https://blog.csdn.net/liuweiyuxiang/article/details/80856991) && [ loss不下降的解决方法](https://blog.csdn.net/zongza/article/details/89185852)\n* [深度学习：欠拟合问题的几种解决方案](https://blog.csdn.net/u014038273/article/details/84108688) &&[过拟合和欠拟合问题](https://blog.csdn.net/mzpmzk/article/details/79741682)\n* [机器学习：如何找到最优学习率](https://blog.csdn.net/whut_ldz/article/details/78882871)及[实现](https://github.com/L1aoXingyu/torchlib)\n* [神经网络中 warmup 策略为什么有效](https://www.zhihu.com/question/338066667)\n* [不平衡数据集处理方法](): [其一](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/), [其二](https://www.zhihu.com/question/285824343), [其三](https://blog.csdn.net/songhk0209/article/details/71484469) && [Awesome Imbalanced Learning](https://github.com/ZhiningLiu1998/awesome-imbalanced-learning) && [Class-balanced-loss-pytorch](https://github.com/vandit15/Class-balanced-loss-pytorch)\n* [同一个神经网络使用不同激活函数的表达能力是否一致](https://www.zhihu.com/question/41841299)\n* [论文笔记之数据增广：mixup](https://blog.csdn.net/ly244855983/article/details/78938667#%E8%AE%A8%E8%AE%BA)\n* [避坑指南：数据科学家新手常犯的13个错误](https://zhuanlan.zhihu.com/p/44331706)	\n* [凭什么相信CNN的结果?--可视化](https://bindog.github.io/blog/2018/02/10/model-explanation/)				\n  * [凭什么相信你，我的CNN模型？（篇一：CAM和Grad-CAM)](https://bindog.github.io/blog/2018/02/10/model-explanation/) && [pytorch-grad-cam](https://github.com/jacobgil/pytorch-grad-cam) && [Grad-CAM-tensorflow](https://github.com/insikk/Grad-CAM-tensorflow) && [grad-cam.tensorflow](https://github.com/Ankush96/grad-cam.tensorflow) && [cnn_visualization](https://github.com/js-fan/mxnet/tree/d2b802e2d2af3dae5b4ac941354602630d2ef1c7/example/cnn_visualization)\n  * [凭什么相信你，我的CNN模型？（篇二：万金油LIME)](http://bindog.github.io/blog/2018/02/11/model-explanation-2/)\n  * [论文笔记:Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://www.jianshu.com/p/294ad9ae2e50)\n  * [CV：基于Keras利用训练好的hdf5模型进行目标检测实现输出模型中的表情或性别的gradcam(可视化)](https://blog.csdn.net/qq_41185868/article/details/80323646)\n* [大卷积核还是小卷积核?]() [1](https://www.jianshu.com/p/d75375dd7ebd), [2](https://blog.csdn.net/kuangtun9713/article/details/79475457)	\n* [模型可解释性差？你考虑了各种不确定性了吗？](https://baijiahao.baidu.com/s?id=1608193373391996908)\n* [炼丹笔记系列]()\n  * [炼丹笔记一：样本不平衡问题](https://zhuanlan.zhihu.com/p/56882616)\n  * [炼丹笔记二：数据清洗](https://zhuanlan.zhihu.com/p/56022212)\n  * [炼丹笔记三：数据增强](https://zhuanlan.zhihu.com/p/56139575)\n  * [炼丹笔记四：小样本问题](https://zhuanlan.zhihu.com/p/56365469)\n  * [炼丹笔记五：数据标注](https://zhuanlan.zhihu.com/p/56443169)\n  * [炼丹笔记六 : 调参技巧](https://zhuanlan.zhihu.com/p/56745640)\n  * [炼丹笔记七：卷积神经网络模型设计](https://zhuanlan.zhihu.com/p/57738934)\n\n### 刷排行榜的小技巧\n\n* [Kaggle 六大比赛最全面解析（上）](https://www.leiphone.com/news/201803/XBjvQriKTyTMPLcz.html)\n\n* [Kaggle 六大比赛最全面解析（下）](https://www.leiphone.com/news/201803/chz1DNHqgVWNEm5t.html)\n\n#### 图像分类\n\n* [炼丹笔记三：数据增强](https://zhuanlan.zhihu.com/p/56139575) && [数据增强(Data Augmentation)](https://zhuanlan.zhihu.com/p/41679153)\n* [【技术综述】 深度学习中的数据增强（上）](https://zhuanlan.zhihu.com/p/38345420) && [【技术综述】深度学习中的数据增强（下）](https://zhuanlan.zhihu.com/p/38437739)\n* [深度学习数据增广技术一览](https://zhuanlan.zhihu.com/p/144921458)\n* [《Bag of Tricks for Image Classification with CNN》](https://zhuanlan.zhihu.com/p/53324148)&& [pdf](https://arxiv.org/pdf/1812.01187.pdf)\n* [深度神经网络模型训练中的最新tricks总结【原理与代码汇总】](https://zhuanlan.zhihu.com/p/66080948) && [神经网络训练trick](https://zhuanlan.zhihu.com/p/59918821)\n* [Kaggle解决方案分享]()\n  * [从0上手Kaggle图像分类挑战：冠军解决方案详解](https://www.itcodemonkey.com/article/4898.html)\n  * [Kaggle 冰山图像分类大赛近日落幕，看冠军团队方案有何亮点](https://www.leiphone.com/news/201803/u40cjEZWArBfFaBm.html)\n  * [【Kaggle冠军分享】图像识别和分类竞赛，数据增强及优化算法](https://mp.weixin.qq.com/s/_S8EBBJ-u9g_fHp7I3ChMQ?)\n  * [识别座头鲸，Kaggle竞赛第一名解决方案解读](https://zhuanlan.zhihu.com/p/58496385)\n  * [kaggle 首战拿金牌总结](https://zhuanlan.zhihu.com/p/60953933)\n  * [16岁高中生夺冠Kaggle地标检索挑战赛！而且竟然是Kaggle老兵](https://zhuanlan.zhihu.com/p/37522227)\n  * [6次Kaggle计算机视觉类比赛赛后感](https://zhuanlan.zhihu.com/p/37663895)\n  * [Kaggle首战斩获第三-卫星图像识别](https://zhuanlan.zhihu.com/p/63275166)\n\n#### 目标检测\n\n* ensemble\n* deformable\n* sync bn\n* ms train/test\n* [目标检测任务的优化策略tricks](https://zhuanlan.zhihu.com/p/56792817)\n* [目标检测小tricks--样本不均衡处理](https://zhuanlan.zhihu.com/p/60612064)\n* [汇总|目标检测中的数据增强、backbone、head、neck、损失函数](https://zhuanlan.zhihu.com/p/137769687)\n* [目标检测算法中的常见trick](https://zhuanlan.zhihu.com/p/39262769)\n* [Bag of Freebies —— 提升目标检测模型性能的免费tricks](https://zhuanlan.zhihu.com/p/141878389)\n* [目标检测比赛中的tricks（已更新更多代码解析）](https://zhuanlan.zhihu.com/p/102817180)\n* [Kaggle：肺癌自动诊断系统3D Deep Leaky Noisy-or Network 论文阅读](https://www.jianshu.com/p/50158f8daf0d)\n* [干货|大神教你如何参加kaggle比赛——根据CT扫描图预测肺癌](https://yq.aliyun.com/articles/89312)\n\n## 五. 年度总结\n* [新年大礼包：机器之心2018高分教程合集](https://zhuanlan.zhihu.com/p/53717510)   \n* [收藏、退出一气呵成，2019年机器之心干货教程都在这里了](https://zhuanlan.zhihu.com/p/104022144)\n\n## 六. 科研相关\n\n### 深度学习框架\n\n#### Python3.x(先修)		\n\n* [The Python Tutorial](https://docs.python.org/3/tutorial/)\n* [廖雪峰Python教程](https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000)\n* [菜鸟教程](http://www.runoob.com/python3/python3-tutorial.html)     \n* [给深度学习入门者的Python快速教程 - 基础篇](https://zhuanlan.zhihu.com/p/24162430)\n* [Python - 100天从新手到大师](https://github.com/jackfrued/Python-100-Days)\n* [Python中读取,显示,保存图片的方法](https://blog.csdn.net/u010472607/article/details/78855816) && [Python的图像打开保存显示的几种方式](https://blog.csdn.net/weixin_37619439/article/details/86559239)\n\n#### Numpy(先修)\n\n* [Quickstart tutorial](https://www.numpy.org/devdocs/user/quickstart.html)\n\n* [Numpy快速入门(Numpy 1.14 官方文档中文翻译)](https://www.jianshu.com/p/3e566f09a0cf)\n* [Numpy中文文档](https://www.numpy.org.cn/index.html)\n* [给深度学习入门者的Python快速教程 - numpy和Matplotlib篇](https://zhuanlan.zhihu.com/p/24309547)\n\n#### Opencv-python\n\n* [OpenCV-Python Tutorials](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html)\n* [OpenCV官方教程中文版（For Python）](https://www.cnblogs.com/Undo-self-blog/p/8423851.html)\n* [数字图像处理系列](https://blog.csdn.net/feilong_csdn/article/category/8037591)\n* [python+OpenCV图像处理](https://blog.csdn.net/qq_40962368/article/category/7688903)\n* [给深度学习入门者的Python快速教程 - 番外篇之Python-OpenCV](https://zhuanlan.zhihu.com/p/24425116)\n\n#### Pandas\n\n* [Python 数据科学入门教程：Pandas](https://www.jianshu.com/p/d9774cf1fea5?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation)\n\n#### Tensorflow		\n\n* [如何高效地学习 TensorFlow 代码](https://www.zhihu.com/question/41667903)\n* [中文教程](http://www.tensorfly.cn/tfdoc/tutorials/overview.html)\n* [TensorFlow官方文档](https://www.w3cschool.cn/tensorflow_python/)\n* [CS20:Tensorflow for DeepLearning Research](http://web.stanford.edu/class/cs20si/syllabus.html)\n* [吴恩达TensorFlow专项课程](https://zhuanlan.zhihu.com/p/62981537)\n* [【干货】史上最全的Tensorflow学习资源汇总](https://zhuanlan.zhihu.com/p/35515805?group_id=967136289941897216)\n* [《21个项目玩转深度学习———基于TensorFlow的实践详解》](https://github.com/hzy46/Deep-Learning-21-Examples)  \n* [最全Tensorflow2.0 入门教程持续更新](https://zhuanlan.zhihu.com/p/59507137)\n* [Github优秀开源教程](https://github.com/search?o=desc&q=tensorflow+tutorial&s=&type=Repositories)\n\n#### MXNet		\n* [Gluon](http://zh.gluon.ai/#)\n* [GluonCV](https://gluon-cv.mxnet.io/index.html#)\n* [GluonNLP](http://gluon-nlp.mxnet.io/)\n\n#### PyTorch\n\n* [Pytorch版动手学深度学习](https://github.com/ShusenTang/Dive-into-DL-PyTorch)\n* [PyTorch中文文档](https://pytorch-cn.readthedocs.io/zh/latest/)\n* [WELCOME TO PYTORCH TUTORIALS](https://pytorch.org/tutorials/index.html)\n* [史上最全的PyTorch学习资源汇总](https://zhuanlan.zhihu.com/p/64895011)\n* [【干货】史上最全的PyTorch学习资源汇总](https://github.com/INTERMT/Awesome-PyTorch-Chinese)\n* [Hands-on tour to deep learning with PyTorch](https://mlelarge.github.io/dataflowr-web/cea_edf_inria.html)\n* [pytorch学习(五)—图像的加载/读取方式](https://www.jianshu.com/p/cfca9c4338e7) && [PyTorch—ImageFolder/自定义类 读取图片数据](https://blog.csdn.net/wsp_1138886114/article/details/83620869)\n\n### 深度学习常用命令\n\n* [command_for_deeplearning](https://github.com/Stephenfang51/command_for_deeplearning/blob/master/command%20for%20deeplearning.md)\n* [Shell编程](https://zhuanlan.zhihu.com/p/102176365)\n\n### Python可视化\n\n* [Top 50 matplotlib Visualizations – The Master Plots (with full python code)](https://www.machinelearningplus.com/plots/top-50-matplotlib-visualizations-the-master-plots-python/)\n* [Python之MatPlotLib使用教程](https://blog.csdn.net/zhw864680355/article/details/102500263)\n* [十分钟上手matplotlib，开启你的python可视化](https://mp.weixin.qq.com/s/UfvEdzr-ZGmyT08yKDOchA)\n* [给深度学习入门者的Python快速教程 - numpy和Matplotlib篇](https://zhuanlan.zhihu.com/p/24309547)\n\n### 标注工具\n* 目标检测标注工具\n	* [labelImg](https://github.com/tzutalin/labelImg)\n* 语义分割标注工具\n	* [labelme](https://github.com/wkentaro/labelme)\n\n### 数据集  		\n* [1. 25个深度学习相关公开数据集](https://zhuanlan.zhihu.com/p/35449783)\n* [2. 自然语言处理（NLP）数据集](https://zhuanlan.zhihu.com/p/35423943)\n* [3.全唐诗(43030首)](https://pan.baidu.com/s/1o7QlUhO)\n* [4. 伯克利大学公开数据集](https://people.eecs.berkeley.edu/~taesung_park/)\n* [5. ACL 2018资源：100+ 预训练的中文词向量](https://zhuanlan.zhihu.com/p/36835964)\n* [6. 预训练中文词向量](https://github.com/Embedding/Chinese-Word-Vectors)\n* [7. 公开数据集种子库](http://academictorrents.com)\n* [8. 计算机视觉，深度学习，数据挖掘数据集整理](https://blog.csdn.net/c20081052/article/details/79814082)\n* [9. 计算机视觉著名数据集CV Datasets](https://blog.csdn.net/accepthjp/article/details/51831026)\n* [10. 计算机视觉相关数据集和比赛](https://blog.csdn.net/NNNNNNNNNNNNY/article/details/68485160)\n* [11. 这是一份非常全面的开源数据集，你，真的不想要吗？](https://zhuanlan.zhihu.com/p/43846002)\n* [12. 人群密度估计现有主要数据集特点及其比较](https://blog.csdn.net/weixin_40516558/article/details/81564464)\n* [13. DANBOORU2017: A LARGE-SCALE CROWDSOURCED AND TAGGED ANIME ILLUSTRATION DATASET](https://www.gwern.net/Danbooru2017)\n* [14. 行人重识别数据集](http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html)\n* [15. 自然语言处理常见数据集、论文最全整理分享](https://zhuanlan.zhihu.com/p/56144877)\n* [16. paper, code, sota](https://paperswithcode.com/)\n* [17. 旷视RPC大型商品数据集发布！](https://zhuanlan.zhihu.com/p/55627416)\n* [18. CVPR 2019「准满分」论文：英伟达推出首个跨摄像头汽车跟踪数据集(汽车Re-ID)](https://zhuanlan.zhihu.com/p/60617001)\n* [19.【OCR技术】大批量生成文字训练集](https://zhuanlan.zhihu.com/p/59052013)\n* [20. 语义分析数据集-MSRA](https://github.com/msra-nlc/MSParS)\n* [IEEE DataPort](https://ieee-dataport.org/)\n* [数据集市](http://www.shujujishi.com/)\n* [医疗/医学图像数据集]()：[Medical Data for Machine Learning](https://github.com/beamandrew/medical-data) && [医疗领域图像挑战赛数据集](https://grand-challenge.org/challenges/) && [【医学影像系列：一】数据集合集 最新最全](https://blog.csdn.net/qq_31622015/article/details/90573874) && [medical-imaging-datasets](https://github.com/sfikas/medical-imaging-datasets) && [【数据集】一文道尽医学图像数据集与竞赛](https://zhuanlan.zhihu.com/p/50615907) && [医学图像数据集汇总](https://zhuanlan.zhihu.com/p/102855802)\n\n### 记笔记工具\n\n* [Markdown编辑器：Typora介绍](https://zhuanlan.zhihu.com/p/67153848) \n* [Markdown语法介绍（常用）](https://zhuanlan.zhihu.com/p/47897214)\n* [Markdown 语法手册 （完整整理版）](https://blog.csdn.net/witnessai1/article/details/52551362)\n* [Markdown中Latex 数学公式基本语法](https://blog.csdn.net/u014630987/article/details/70156489)\n\n### 会议期刊列表\n\n* [国际会议日期表](https://github.com/JackieTseng/conference_call_for_paper)\n* [ai-deadlines](https://github.com/abhshkdz/ai-deadlines/)\n* [Keep Up With New Trends](https://handong1587.github.io/deep_learning/2017/12/18/keep-up-with-new-trends.html)\n* [计算机会议排名等级](https://blog.csdn.net/cserchen/article/details/40508181)\n* [中国计算机学会(CCF)推荐国际学术刊物和会议](https://www.ccf.org.cn/Academic_Evaluation/By_category/)\n\n### 论文写作工具\n* [Windows: Texlive+Texstudio](https://jingyan.baidu.com/article/b2c186c83c9b40c46ff6ff4f.html)\n* [Ubuntu: Texlive+Texmaker](https://jingyan.baidu.com/article/7c6fb4280b024180642c90e4.html)\n* [Latex：基本用法、表格、公式、算法](https://blog.csdn.net/quiet_girl/article/details/72847208)\n* [LaTeX 各种命令，符号](https://blog.csdn.net/garfielder007/article/details/51646604)\n\n### 论文画图工具\n* [Visio2016](https://msdn.itellyou.cn/)\n* [Matplotlib](#Python可视化)\n\n### 论文写作教程\n* [刘知远_如何写一篇合格的NLP论文](https://zhuanlan.zhihu.com/p/58752815)\n* [刘洋_如何写论文_V7](http://nlp.csai.tsinghua.edu.cn/~ly/talks/cwmt14_tut.pdf)\n* [如何端到端地写科研论文-邱锡鹏](https://xpqiu.github.io/slides/20181019-PaperWriting.pdf)\n* [论文Introduction写作其一](https://zhuanlan.zhihu.com/p/33876355), [论文Introduction写作其二](https://zhuanlan.zhihu.com/p/52494933), [论文Introduction写作其三](https://zhuanlan.zhihu.com/p/52494879)\n* [毕业论文怎么写](https://zhuanlan.zhihu.com/c_179195484)\n* [浅谈学术论文rebuttal](https://zhuanlan.zhihu.com/p/104298923) && [学术论文投稿与返修（Rebuttal）分享](https://zhuanlan.zhihu.com/p/344008879)\n* [研之成理写作实验室](https://zhuanlan.zhihu.com/rationalscience-writing-lab)\n* [智源论坛·论文写作专题报告会]()：[《论文写作小白的成长之路》](https://zhuanlan.zhihu.com/p/135989892) && [《谈如何写一篇合格的国际学术论文》](https://zhuanlan.zhihu.com/p/136005095) && [《计算机视觉会议论文从投稿到接收》](https://zhuanlan.zhihu.com/p/139571199)\n\n### ResearchGos\n\n* [ResearchGo:研究生活第一帖——文献检索与管理](https://zhuanlan.zhihu.com/p/22323250?refer=wjdml)\n* [ResearchGo:研究生活第二贴——文献阅读](https://zhuanlan.zhihu.com/p/22402393?refer=wjdml)\n* [ResearchGo:研究生活第三帖——阅读辅助](https://zhuanlan.zhihu.com/p/22622502?refer=wjdml)\n* [ResearchGo:研究生活第四帖——文献调研](https://zhuanlan.zhihu.com/p/23178836?refer=wjdml)\n* [ResearchGo:研究生活第五帖——文献综述](https://zhuanlan.zhihu.com/p/23356843?refer=wjdml)\n* [ResearchGo:研究生活第六帖——如何讲论文](https://zhuanlan.zhihu.com/p/23872063?refer=wjdml)\n* [ResearchGo:研究生活第七帖——专利检索与申请](https://zhuanlan.zhihu.com/p/25191025)\n* [ResearchGo:研究生活第八帖——写论文、做PPT、写文档必备工具集锦](https://zhuanlan.zhihu.com/p/62100815)\n\n### 毕业论文排版\n\n* [吐血推荐收藏的学位论文排版教程（完整版）](https://zhuanlan.zhihu.com/p/52495345)\n* [论文怎么写——如何修改毕业论文格式](https://zhuanlan.zhihu.com/p/35951260)\n\n_____\n\n______\n\n## 信号处理\n\n### 傅里叶变换\n\n* [傅里叶分析之掐死教程（完整版）更新于2014.06.06](https://zhuanlan.zhihu.com/p/19763358)\n* [如何简明的总结傅里叶变换？](https://www.zhihu.com/question/34899574/answer/612923473)\n* [从连续时间傅里叶级数到快速傅里叶变换](https://blog.csdn.net/clover13/article/details/79469851)\n* [十分简明易懂的FFT（快速傅里叶变换）](https://blog.csdn.net/enjoy_pascal/article/details/81478582)\n* [傅里叶级数推导过程](https://blog.csdn.net/hanxiaohu88/article/details/8245687)\n\n### 小波变换\n\n* [形象易懂讲解算法I——小波变换](https://zhuanlan.zhihu.com/p/22450818)\n\n* [小波变换完美通俗讲解系列之 （一）](https://zhuanlan.zhihu.com/p/44215123) && [小波变换完美通俗讲解系列之 （二）](https://zhuanlan.zhihu.com/p/44217268)\n\n#### 实战\n* [MWCNN中使用的haar小波变换 pytorch](https://www.cnblogs.com/wanghui-garcia/p/12524515.html)\n* [【小波变换】小波变换入门----haar小波](https://blog.csdn.net/baidu_27643275/article/details/84826773)\n* [（3）小波变换原理及应用](https://blog.csdn.net/hhaowang/article/details/82909332)\n* [图像处理-小波变换](https://blog.csdn.net/qq_30815237/article/details/89704855)\n\n## 机器学习理论与实战\n\n* [机器学习原理](https://github.com/shunliz/Machine-Learning):star:\n* [ID3、C4.5、CART、随机森林、bagging、boosting、Adaboost、GBDT、xgboost算法总结](https://zhuanlan.zhihu.com/p/34534004)\n* [数据挖掘十大算法简要说明](http://www.cnblogs.com/en-heng/p/5013995.html)，[机器学习十大经典算法入门](https://blog.csdn.net/qq_42379006/article/details/80741808) && [【算法模型】轻松看懂机器学习十大常用算法](https://www.cnblogs.com/ljt1412451704/p/9678248.html)\n* [AdaBoost到GBDT系列]()\n  * [当我们在谈论GBDT：从 AdaBoost 到 Gradient Boosting](https://zhuanlan.zhihu.com/p/25096501?refer=data-miner)\n  * [当我们在谈论GBDT：Gradient Boosting 用于分类与回归](https://zhuanlan.zhihu.com/p/25257856?refer=data-miner)\n  * [当我们在谈论GBDT：其他 Ensemble Learning 算法](https://zhuanlan.zhihu.com/p/25443980)\n\n### 机器学习理论篇之经典算法\n\n#### 信息论\n* [1. 机器学习中的各种熵](https://zhuanlan.zhihu.com/p/35423404)    \n* [2. 从香农熵到手推KL散度：纵览机器学习中的信息论](https://zhuanlan.zhihu.com/p/32985487)\n\n#### 多层感知机(MLP)\n* [多层感知机（MLP）学习与总结博客](https://blog.csdn.net/baidu_33718858/article/details/84972537)\n* [多层感知机：Multi-Layer Perceptron](https://blog.csdn.net/xholes/article/details/78461164)\n* [神经网络基础-多层感知器(MLP)](https://blog.csdn.net/weixin_38206214/article/details/81137911)\n\n#### k近邻(KNN)\n\n* [机器学习之KNN（k近邻）算法详解](https://blog.csdn.net/sinat_30353259/article/details/80901746)\n\n#### k均值(K-means)\n\n* [Kmeans聚类算法详解](https://blog.csdn.net/qq_32892383/article/details/80107795)\n\n#### 朴素贝叶斯(Naive Bayesian)\n\n* [一个例子搞清楚（先验分布/后验分布/似然估计）](https://blog.csdn.net/qq_23947237/article/details/78265026)\n* [朴素贝叶斯分类器（Naive Bayesian Classifier）](https://blog.csdn.net/qq_32690999/article/details/78737393)\n* [朴素贝叶斯分类器 详细解析](https://blog.csdn.net/qq_17073497/article/details/81076250)\n\n#### 决策树(Decision Tree)\n* [最常见核心的决策树算法详细介绍，含ID3、C4.5、CART:star:](https://mp.weixin.qq.com/s/lXaPZyNrgG9LBv-JHdGm9A) && [最常用的决策树算法！Random Forest、Adaboost、GBDT 算法:star:](https://mp.weixin.qq.com/s/Nl_-PdF0nHBq8yGp6AdI-Q) && [终于有人把XGBoost 和 LightGBM 讲明白了，项目中最主流的集成算法！:star:](https://mp.weixin.qq.com/s/LoX987dypDg8jbeTJMpEPQ)\n* [为什么xgboost要用泰勒展开，优势在哪里](http://blog.itblood.com/4082.html)\n* [Python3《机器学习实战》学习笔记（二）：决策树基础篇之让我们从相亲说起](https://blog.csdn.net/c406495762/article/details/75663451)\n* [Python3《机器学习实战》学习笔记（三）：决策树实战篇之为自己配个隐形眼镜](https://blog.csdn.net/c406495762/article/details/76262487)\n* [机器学习实战教程（十三）：树回归基础篇之CART算法与树剪枝](http://cuijiahua.com/blog/2017/12/ml_13_regtree_1.html)\n* [《机器学习实战》基于信息论的三种决策树算法(ID3,C4.5,CART)](https://blog.csdn.net/gamer_gyt/article/details/51242815)\n* [说说决策树剪枝算法](https://zhuanlan.zhihu.com/p/31404571)\n* [机器学习实战 第九章 树回归](https://blog.csdn.net/namelessml/article/details/52595066)\n* [决策树值ID3、C4.5实现](https://blog.csdn.net/u014688145/article/details/53212112)\n* [决策树之CART实现](https://blog.csdn.net/u014688145/article/details/53326910)\n\n#### 随机森林(Random Forest)\n* [随机森林和GBDT的区别](https://blog.csdn.net/login_sonata/article/details/73929426)\n* [随机森林（Random Forest）入门与实战](https://blog.csdn.net/sb19931201/article/details/52601058)\n* [随机森林之特征选择](https://www.cnblogs.com/justcxtoworld/p/3447231.html)\n\n#### 线性回归（Linear Regression）\n\n* [线性回归最小二乘法和最大似然估计](https://blog.csdn.net/lt793843439/article/details/91392646)\n* [【从入门到放弃】线性回归](https://zhuanlan.zhihu.com/p/147297924)\n* [线性回归(频率学派-最大似然估计)与岭回归(贝叶斯角度-最大后验估计)的概率解释](https://blog.csdn.net/z_feng12489/article/details/101388745)\n* [机器学习笔记四：线性回归回顾与logistic回归](https://blog.csdn.net/xierhacker/article/details/53316138)\n\n#### 逻辑回归(Logistic Regression)\n\n* [【机器学习面试总结】—— LR（逻辑回归）](https://zhuanlan.zhihu.com/p/100763009)\n* [【机器学习面试题】逻辑回归篇](https://zhuanlan.zhihu.com/p/62653034)\n* [极大似然概率和最小损失函数，以及正则化简介](https://www.jianshu.com/p/9d2686cd407e)\n* [GLM(广义线性模型) 与 LR(逻辑回归) 详解](https://blog.csdn.net/Cdd2xd/article/details/75635688)\n\n#### 支持向量机(SVM) \n* [【机器学习面试总结】—— SVM](https://zhuanlan.zhihu.com/p/93715996)\n* [SVM系列-从基础到掌握](https://zhuanlan.zhihu.com/p/61123737)\n* [SVM通俗导论 July](https://github.com/Mikoto10032/DeepLearning/blob/master/books/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E9%80%9A%E4%BF%97%E5%AF%BC%E8%AE%BA%EF%BC%88%E7%90%86%E8%A7%A3SVM%E7%9A%84%E4%B8%89%E5%B1%82%E5%A2%83%E7%95%8C%EF%BC%89LaTeX%E6%9C%80%E6%96%B0%E7%89%88_2015.1.9.pdf) \n* [核函数 ](): [机器学习有很多关于核函数的说法，核函数的定义和作用是什么？](https://www.zhihu.com/question/24627666) && [SVM中，高斯核为什么会把原始维度映射到无穷多维？](https://www.zhihu.com/question/35602879) && [svm核函数的理解和选择](https://blog.csdn.net/leonis_v/article/details/50688766) && [核函数和径向基核函数 (Radial Basis Function)--RBF](https://blog.csdn.net/huang1024rui/article/details/51510611) && [SVM核函数](https://blog.csdn.net/xiaowei_cqu/article/details/35993729)\n\n#### 提升方法(Adaboost)\n\n* [当我们在谈论GBDT：从 AdaBoost 到 Gradient Boosting](https://zhuanlan.zhihu.com/p/25096501)\n\n#### 梯度提升决策树(GBDT)\n* [LightGBM大战XGBoost](https://zhuanlan.zhihu.com/p/35645973)\n* [概述XGBoost、Light GBM和CatBoost的同与不同](https://zhuanlan.zhihu.com/p/34698733)    && [XGBoost、LightGBM、Catboost总结](https://www.cnblogs.com/lvdongjie/p/11391245.html) && [XGBoost、Light GBM和CatBoost的参数及性能比较](https://zhuanlan.zhihu.com/p/34698733)\n* [梯度提升决策树](https://zhuanlan.zhihu.com/p/36339161)\n* [GBDT原理及应用](https://zhuanlan.zhihu.com/p/30339807)\n* [XGBOOST原理篇](https://zhuanlan.zhihu.com/p/31654000)\n* [xgboost入门与实战（原理篇）](https://blog.csdn.net/sb19931201/article/details/52557382) && [xgboost入门与实战（实战调参篇）](https://blog.csdn.net/sb19931201/article/details/52577592)\n* [【干货合集】通俗理解kaggle比赛大杀器xgboost](https://zhuanlan.zhihu.com/p/41417638)\n* [GBDT分类的原理及Python实现](https://blog.csdn.net/bf02jgtrs00xktcx/article/details/82719765)\n* [GBDT原理及利用GBDT构造新的特征-Python实现](https://blog.csdn.net/shine19930820/article/details/71713680)\n* [Python+GBDT算法实战——预测实现100%准确率](https://www.jianshu.com/p/47e73a985ba1)\n* [xgboost之近似分位数算法（直方图算法）详解](https://blog.csdn.net/m0_37870649/article/details/104561431)\n\n#### EM(期望最大化)			\n* [人人都懂的EM算法 ](https://zhuanlan.zhihu.com/p/36331115)\n* [EM算法入门文章](https://zhuanlan.zhihu.com/p/61768577)                      \n\n#### 高斯混合模型(GMM)\n* [高斯混合模型与EM算法的数学原理及应用实例](https://zhuanlan.zhihu.com/p/67107370)\n* [高斯混合模型（GMM）](https://zhuanlan.zhihu.com/p/30483076)\n\n#### 马尔科夫决策过程(MDP)		\n* [马尔科夫决策过程之Markov Processes（马尔科夫过程）](https://zhuanlan.zhihu.com/p/35124726)\n* [马尔科夫决策过程之Markov Reward Process（马尔科夫奖励过程）](https://zhuanlan.zhihu.com/p/35231424)\n* [马尔科夫决策过程之Bellman Equation（贝尔曼方程）](https://zhuanlan.zhihu.com/p/35261164)\n* [马尔科夫决策过程之Markov Decision Process(马尔科夫决策过程)](https://zhuanlan.zhihu.com/p/35354956)\n* [马尔科夫决策过程之最优价值函数与最优策略](https://zhuanlan.zhihu.com/p/35373905)\n\n#### 条件随机场(CRF, 判别式模型)\n* [如何轻松愉快地理解条件随机场](https://zhuanlan.zhihu.com/p/104562658)\n* [如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？](https://www.zhihu.com/question/35866596)\n* [HMM ,MHMM,CRF 优缺点与区别](https://blog.csdn.net/u013378306/article/details/55213029)\n\n#### 降维算法\n* [数据降维算法-从PCA到LargeVis](https://zhuanlan.zhihu.com/p/62470700)\n* [12种降维方法终极指南（含Python代码）](https://zhuanlan.zhihu.com/p/43225794)\n\n#### 主成分分析(PCA)\n* [主成分分析（PCA）原理详解](https://blog.csdn.net/program_developer/article/details/80632779)\n* [图文并茂的PCA教程](https://blog.csdn.net/hustqb/article/details/78394058)\n* [PCA数学原理](http://www.360doc.com/content/13/1124/02/9482_331688889.shtml)\n\n#### 奇异值分解(SVD)\n* [强大的矩阵奇异值分解(SVD)及其应用](https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html)\n* [奇异值分解（SVD）](https://zhuanlan.zhihu.com/p/29846048)\n* [奇异值分解(SVD)原理详解及推导](https://blog.csdn.net/zhongkejingwang/article/details/43053513)    \n* [SVD在推荐系统中的应用详解以及算法推导](https://blog.csdn.net/zhongkejingwang/article/details/43083603)\n\n#### 线性判别分析(LDA)\n* [教科书上的LDA为什么长这个样子？](https://zhuanlan.zhihu.com/p/42238953)\n\n#### 标签传播算法(Label Propagation Algorithm)    \n* [标签传播算法（Label Propagation）及Python实现](https://blog.csdn.net/zouxy09/article/details/49105265)\n    * [参考资料](https://github.com/Mikoto10032/DeepLearning/blob/master/books/Semi-Supervised%20Learning%20with%20Graphs.pdf)\n\n#### 蒙塔卡罗树搜索(MCTS)\n* [蒙特卡洛树搜索入门指南](https://zhuanlan.zhihu.com/p/34950988)\n\n#### 集成(Ensemble)\n\n* [集成学习之bagging,stacking,boosting概念理解](https://zhuanlan.zhihu.com/p/41809927) && [Bagging和Boosting的总结](https://www.zhihu.com/follow)\n\n* [集成学习法之bagging方法和boosting方法](https://blog.csdn.net/qq_30189255/article/details/51532442)\n* [Bagging,Boosting,Stacking](https://blog.csdn.net/Mr_tyting/article/details/72957853) && [常用的模型集成方法介绍：bagging、boosting 、stacking](https://zhuanlan.zhihu.com/p/65888174)\n\n#### t分布随机邻居嵌入(TSNE)\n* [流形学习-高维数据的降维与可视化](https://blog.csdn.net/u012162613/article/details/45920827)\n* [tSNE](https://blog.csdn.net/flyingzhan/article/details/79521765)\n* [使用t-SNE可视化图像embedding](https://zhuanlan.zhihu.com/p/81400277)\n\n#### 谱聚类(Spectral Clustering)\n* [谱聚类（Spectral Clustering）算法介绍](https://blog.csdn.net/qq_24519677/article/details/82291867)\n* [聚类5--谱和谱聚类](https://blog.csdn.net/xueyingxue001/article/details/51966980)\n\n#### 异常点检测\n* [数据挖掘中常见的「异常检测」算法有哪些？](https://www.zhihu.com/question/280696035/answer/417091151)\n* [异常点检测算法综述](https://zhuanlan.zhihu.com/p/30169110)\n* [异常检测的N种方法，其中有一个你一定想不到](https://mp.weixin.qq.com/s/RYLlUJiYbWqGIhzflbRGEg)\n* [异常检测资源汇总：anomaly-detection-resources](https://zhuanlan.zhihu.com/p/158349346)\n\n### 机器学习实战篇\n* [机器学习中，有哪些特征选择的工程方法？](https://www.zhihu.com/question/28641663) && [机器学习（四）：数据预处理--特征工程概述](https://zhuanlan.zhihu.com/p/103070096) && [特征工程完全手册 - 从预处理、构造、选择、降维、不平衡处理，到放弃](https://zhuanlan.zhihu.com/p/94994902) && [特征工程中的「归一化」有什么作用](https://www.zhihu.com/question/20455227)\n* [15分钟带你入门sklearn与机器学习——分类算法篇](https://mp.weixin.qq.com/s?__biz=Mzg5NzAxMDgwNg==&mid=2247484110&idx=1&sn=b016e270d7b7707e6ad41a81ca45fc28&chksm=c0791fd7f70e96c103a8a2aebee166ce14f5648b3b889dd85dd9786f48b6b8269f11e5e27e1c&scene=21#wechat_redirect) && [如何为你的回归问题选择最合适的机器学习方法？](https://zhuanlan.zhihu.com/p/62034592)\n* [十分钟上手sklearn：安装，获取数据，数据预处理](https://zhuanlan.zhihu.com/p/105039597) && [十分钟上手sklearn：特征提取，常用模型，交叉验证](https://zhuanlan.zhihu.com/p/105041301)\n* [MachineLearning_Python](https://github.com/lawlite19/MachineLearning_Python)\n* [Machine Learning Course with Python](https://github.com/machinelearningmindset/machine-learning-course)\n* [Statistical-Learning-Method_Code](https://github.com/Dod-o/Statistical-Learning-Method_Code)\n* [Python3机器学习](https://blog.csdn.net/c406495762/column/info/16415)\n* [含大牛总结的分类模型一般需要调节的参数](https://www.jianshu.com/p/9d2452fc93c2)\n\n\n## 机器学习、深度学习的一些研究方向\n\n### 多任务学习(Multi-Task Learning)\n* [模型汇总-14 多任务学习-Multitask Learning概述](https://zhuanlan.zhihu.com/p/27421983)\n* [(译)深度神经网络的多任务学习概览(An Overview of Multi-task Learning in Deep Neural Networks)](http://www.cnblogs.com/shuzirank/p/7141017.html)\n* [Multi-task Learning and Beyond: 过去，现在与未来](https://zhuanlan.zhihu.com/p/138597214)；\n\n### 零次学习(Zero Shot Learning)\n* [零次学习（Zero-Shot Learning）入门](https://zhuanlan.zhihu.com/p/34656727)\n\n### 小样本学习(Few-Shot Learning)\n\n* [few-shot learning是什么](https://blog.csdn.net/xhw205/article/details/79491649)\n* [零次学习（Zero-Shot Learning）入门](https://zhuanlan.zhihu.com/p/34656727)\n* [小样本学习（Few-shot Learning）综述](https://zhuanlan.zhihu.com/p/61215293)\n* [Few-Shot Learning in CVPR 2019](https://towardsdatascience.com/few-shot-learning-in-cvpr19-6c6892fc8c5)\n* [当小样本遇上机器学习 fewshot learning](https://blog.csdn.net/mao_feng/article/details/78939864)\n\n### 多视觉学习(Multi-View Learning)\n* [Multi-view Learning 多视角学习入门](https://blog.csdn.net/danliwoo/article/details/79278574)\n* [多视角学习 (Multi-View Learning)](https://blog.csdn.net/shine19930820/article/details/77426599)\n\n### 嵌入(Embedding)\n\n* [万物皆Embedding，从经典的word2vec到深度学习基本操作item2vec](https://zhuanlan.zhihu.com/p/53194407)\n* [YJango的Word Embedding--介绍](https://zhuanlan.zhihu.com/p/27830489)\n\n### 迁移学习(Transfer Learning)		\n* [1. 迁移学习：经典算法解析](https://blog.csdn.net/linolzhang/article/details/73358219)\n* [2. 什么是迁移学习 (Transfer Learning)？这个领域历史发展前景如何？](https://www.zhihu.com/question/41979241)\n* [3. 迁移学习个人笔记](https://github.com/Mikoto10032/DeepLearning/blob/master/notes/日常阅读笔记/2018_4_12_迁移学习.pdf)  \n* [迁移学习总结(One Shot Learning, Zero Shot Learning)](https://blog.csdn.net/XJTU_NOC_Wei/article/details/77850221)\n\n### 域自适应(Domain Adaptation)\n\n* [Domain Adaptation视频教程（附PPT）及经典论文分享](https://zhuanlan.zhihu.com/p/27519182)\n* [模型汇总15 领域适应性Domain Adaptation、One-shot/zero-shot Learning概述](https://zhuanlan.zhihu.com/p/27449079)\n* [【深度学习】论文导读：无监督域适应（Deep Transfer Network: Unsupervised Domain Adaptation）](https://blog.csdn.net/mao_xiao_feng/article/details/54426101)\n* [【论文阅读笔记】基于反向传播的无监督域自适应研究](https://zhuanlan.zhihu.com/p/37298073)\n* [【Valse大会首发】领域自适应及其在人脸识别中的应用](https://zhuanlan.zhihu.com/p/21441807)\n* [CVPR 2018：基于域适应弱监督学习的目标检测](https://zhuanlan.zhihu.com/p/41126114)\n\n### 元学习(Meta Learning)		\n\n* [OpenAI提出新型元学习方法EPG，调整损失函数实现新任务上的快速训练](https://zhuanlan.zhihu.com/p/35869158?group_id=970310501209645056)      \n\n### 强化学习(Reinforcement Learning)	\n\n* [强化学习（Reinforcement Learning）知识整理](https://zhuanlan.zhihu.com/p/25498081)\n* [强化学习从入门到放弃的资料](https://zhuanlan.zhihu.com/p/34918639)\n* [强化学习入门](https://zhuanlan.zhihu.com/p/25498081)\n    * [强化学习入门 第一讲 MDP](https://zhuanlan.zhihu.com/p/25498081)\n* [强化学习——从Q-Learning到DQN到底发生了什么？](https://zhuanlan.zhihu.com/p/35882937)\n* [从强化学习到深度强化学习（上）](https://zhuanlan.zhihu.com/p/35688924)                  \n* [从强化学习到深度强化学习（下）](https://zhuanlan.zhihu.com/p/35965070)\n* [一文带你理解Q-Learning的搜索策略](https://zhuanlan.zhihu.com/p/37048004)\n\n### 对比学习(Contrastive Learning)\n\n* [论文列表](https://github.com/asheeshcric/awesome-contrastive-self-supervised-learning)\n* [对比学习（Contrastive Learning）:研究进展精要](https://zhuanlan.zhihu.com/p/367290573)\n* [对比学习（Contrastive Learning）综述](https://zhuanlan.zhihu.com/p/346686467)\n* [理解对比损失的性质以及温度系数的作用](https://zhuanlan.zhihu.com/p/357071960)\n\n### 推荐系统(Recommendation System)\n\n#### 论文列表\n\n* [Embedding从入门到专家必读的十篇论文](https://zhuanlan.zhihu.com/p/58805184)\n* [Reco-papers](https://github.com/wzhe06/Reco-papers)\n* [Ad-papers](https://github.com/wzhe06/Ad-papers)\n* [deep-recommender-system](https://github.com/chocoluffy/deep-recommender-system)\n* [CTR预估系列入门手册](https://zhuanlan.zhihu.com/p/243243145)\n\n#### 教程\n\n* [推荐系统从入门到接着入门](https://zhuanlan.zhihu.com/p/27502172)\n* [深度学习推荐系统笔记](https://zhuanlan.zhihu.com/p/133528693)\n* [推荐系统干货总结](https://zhuanlan.zhihu.com/p/34004488)\n* [入门推荐系统，你不应该错过的知识清单](https://zhuanlan.zhihu.com/p/54819505)\n* [从零开始了解推荐系统全貌](https://zhuanlan.zhihu.com/p/259985388)\n* [推荐系统玩家 之 推荐系统入门——推荐系统的发展历程（上）](https://zhuanlan.zhihu.com/p/148207613)\n* [推荐系统技术演进趋势：从召回到排序再到重排](https://zhuanlan.zhihu.com/p/100019681)\n* [深入理解推荐系统：召回](https://zhuanlan.zhihu.com/p/115690499) && [深入理解推荐系统：排序](https://zhuanlan.zhihu.com/p/138235048)   \n* [召回算法有哪些](https://www.zhihu.com/question/423384620/answer/1687201890)\n* [《深度学习推荐系统》总结系列一](https://zhuanlan.zhihu.com/p/138446984) && [《深度学习推荐系统》总结系列二](https://zhuanlan.zhihu.com/p/140894123)\n* [推荐系统--完整的架构设计和算法(协同过滤、隐语义)](https://zhuanlan.zhihu.com/p/81752025) && [从0到1打造推荐系统-架构篇](https://zhuanlan.zhihu.com/p/123951784)\n* [协同过滤和基于内容推荐有什么区别？](https://www.zhihu.com/question/19971859)\n* [CTR深度交叉特征入门总结](https://zhuanlan.zhihu.com/p/257895631)\n* [推荐系统学习笔记](https://blog.csdn.net/wuzhongqiang/category_10128687.html)\n\n#### 实战\n\n* [ AI-RecommenderSystem](https://github.com/zhongqiangwu960812/AI-RecommenderSystem)\n* [team-learning-rs](https://github.com/datawhalechina/team-learning-rs)\n* [RecommendSystemPractice](https://github.com/Magic-Bubble/RecommendSystemPractice)\n* [Surprise](http://surpriselib.com/)', '{"language":"Jupyter Notebook","stars":16940,"forks":3837,"watchers":16940,"open_issues":15,"topics":["cnn","deep-learning","deeplearning","gan","gcn","kaggle","machine-learning","machinelearning","mxnet","pytorch","rnn","tensorflow"],"default_branch":"master","size_kb":958284,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:apachecn:AiLearning","source_url":"https://github.com/apachecn/AiLearning"},{"type":"has_code","target_id":"github:shunliz:Machine-Learning","source_url":"https://github.com/shunliz/Machine-Learning"},{"type":"has_code","target_id":"github:fengdu78:Data-Science-Notes","source_url":"https://github.com/fengdu78/Data-Science-Notes"},{"type":"has_code","target_id":"github:fengdu78:Data-Science-Notes","source_url":"https://github.com/fengdu78/Data-Science-Notes"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:datawhalechina:statistical-learning-method-solutions-manual","source_url":"https://github.com/datawhalechina/statistical-learning-method-solutions-manual"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:datawhalechina:pumpkin-book","source_url":"https://github.com/datawhalechina/pumpkin-book"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:dformoso:deeplearning-mindmap","source_url":"https://github.com/dformoso/deeplearning-mindmap"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:datawhalechina:leeml-notes","source_url":"https://github.com/datawhalechina/leeml-notes"},{"type":"has_code","target_id":"github:iamtrask:Grokking-Deep-Learning","source_url":"https://github.com/iamtrask/Grokking-Deep-Learning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:FudanNLP:nlp-beginner","source_url":"https://github.com/FudanNLP/nlp-beginner"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:azl397985856:leetcode","source_url":"https://github.com/azl397985856/leetcode"},{"type":"has_code","target_id":"github:huaxz1986:cplusplus-_Implementation_Of_Introduction_to_Algorithms","source_url":"https://github.com/huaxz1986/cplusplus-_Implementation_Of_Introduction_to_Algorithms"},{"type":"has_code","target_id":"github:CyC2018:CS-Notes","source_url":"https://github.com/CyC2018/CS-Notes"},{"type":"has_code","target_id":"github:wolverinn:Waking-Up","source_url":"https://github.com/wolverinn/Waking-Up"},{"type":"has_code","target_id":"github:keithnull:TeachYourselfCS-CN","source_url":"https://github.com/keithnull/TeachYourselfCS-CN"},{"type":"has_code","target_id":"github:imhuay:Algorithm_for_Interview-Chinese","source_url":"https://github.com/imhuay/Algorithm_for_Interview-Chinese"},{"type":"has_code","target_id":"github:DarLiner:Algorithm_Interview_Notes-Chinese","source_url":"https://github.com/DarLiner/Algorithm_Interview_Notes-Chinese"},{"type":"has_code","target_id":"github:ShanghaiTechAIClub:DLInterview","source_url":"https://github.com/ShanghaiTechAIClub/DLInterview"},{"type":"has_code","target_id":"github:scutan90:DeepLearning-500-questions","source_url":"https://github.com/scutan90/DeepLearning-500-questions"},{"type":"has_code","target_id":"github:amusi:AI-Job-Notes","source_url":"https://github.com/amusi/AI-Job-Notes#Strategy"},{"type":"has_code","target_id":"github:apachecn:kaggle","source_url":"https://github.com/apachecn/kaggle"},{"type":"has_code","target_id":"github:weslynn:AlphaTree-graphic-deep-neural-network","source_url":"https://github.com/weslynn/AlphaTree-graphic-deep-neural-network"},{"type":"has_code","target_id":"github:weiaicunzai:awesome-image-classification","source_url":"https://github.com/weiaicunzai/awesome-image-classification"},{"type":"has_code","target_id":"github:rwightman:pytorch-image-models","source_url":"https://github.com/rwightman/pytorch-image-models"},{"type":"has_code","target_id":"github:VCBE123:AnchorFreeDetection","source_url":"https://github.com/VCBE123/AnchorFreeDetection"},{"type":"has_code","target_id":"github:amusi:awesome-object-detection","source_url":"https://github.com/amusi/awesome-object-detection"},{"type":"has_code","target_id":"github:hoya012:deep_learning_object_detection","source_url":"https://github.com/hoya012/deep_learning_object_detection"},{"type":"has_code","target_id":"github:kemaloksuz:ObjectDetectionImbalance","source_url":"https://github.com/kemaloksuz/ObjectDetectionImbalance"},{"type":"has_code","target_id":"github:mrgloom:awesome-semantic-segmentation","source_url":"https://github.com/mrgloom/awesome-semantic-segmentation"},{"type":"has_code","target_id":"github:ChanChiChoi:awesome-Face_Recognition","source_url":"https://github.com/ChanChiChoi/awesome-Face_Recognition"},{"type":"has_code","target_id":"github:MarkMoHR:Awesome-Image-Colorization","source_url":"https://github.com/MarkMoHR/Awesome-Image-Colorization"},{"type":"has_code","target_id":"github:MarkMoHR:Awesome-Edge-Detection-Papers","source_url":"https://github.com/MarkMoHR/Awesome-Edge-Detection-Papers"},{"type":"has_code","target_id":"github:pytorch:vision","source_url":"https://github.com/pytorch/vision"},{"type":"has_code","target_id":"github:akamaster:pytorch_resnet_cifar10","source_url":"https://github.com/akamaster/pytorch_resnet_cifar10"},{"type":"has_code","target_id":"github:likelyzhao:CalFLOPS-Mxnet","source_url":"https://github.com/likelyzhao/CalFLOPS-Mxnet"},{"type":"has_code","target_id":"github:milesial:Pytorch-UNet","source_url":"https://github.com/milesial/Pytorch-UNet"},{"type":"has_code","target_id":"github:qubvel:segmentation_models.pytorch","source_url":"https://github.com/qubvel/segmentation_models.pytorch"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:crownpku:Awesome-Chinese-NLP","source_url":"https://github.com/crownpku/Awesome-Chinese-NLP"},{"type":"has_code","target_id":"github:apachecn:nlp-pytorch-zh","source_url":"https://github.com/apachecn/nlp-pytorch-zh"},{"type":"has_code","target_id":"github:graykode:nlp-tutorial","source_url":"https://github.com/graykode/nlp-tutorial"},{"type":"has_code","target_id":"github:lyeoni:nlp-tutorial","source_url":"https://github.com/lyeoni/nlp-tutorial"},{"type":"has_code","target_id":"github:DeepGraphLearning:LiteratureDL4Graph","source_url":"https://github.com/DeepGraphLearning/LiteratureDL4Graph"},{"type":"has_code","target_id":"github:nnzhan:Awesome-Graph-Neural-Networks","source_url":"https://github.com/nnzhan/Awesome-Graph-Neural-Networks"},{"type":"has_code","target_id":"github:L1aoXingyu:torchlib","source_url":"https://github.com/L1aoXingyu/torchlib"},{"type":"has_code","target_id":"github:ZhiningLiu1998:awesome-imbalanced-learning","source_url":"https://github.com/ZhiningLiu1998/awesome-imbalanced-learning"},{"type":"has_code","target_id":"github:vandit15:Class-balanced-loss-pytorch","source_url":"https://github.com/vandit15/Class-balanced-loss-pytorch"},{"type":"has_code","target_id":"github:jacobgil:pytorch-grad-cam","source_url":"https://github.com/jacobgil/pytorch-grad-cam"},{"type":"has_code","target_id":"github:insikk:Grad-CAM-tensorflow","source_url":"https://github.com/insikk/Grad-CAM-tensorflow"},{"type":"has_code","target_id":"github:Ankush96:grad-cam.tensorflow","source_url":"https://github.com/Ankush96/grad-cam.tensorflow"},{"type":"has_code","target_id":"github:js-fan:mxnet","source_url":"https://github.com/js-fan/mxnet"},{"type":"has_code","target_id":"github:jackfrued:Python-100-Days","source_url":"https://github.com/jackfrued/Python-100-Days"},{"type":"has_code","target_id":"github:hzy46:Deep-Learning-21-Examples","source_url":"https://github.com/hzy46/Deep-Learning-21-Examples"},{"type":"has_code","target_id":"github:ShusenTang:Dive-into-DL-PyTorch","source_url":"https://github.com/ShusenTang/Dive-into-DL-PyTorch"},{"type":"has_code","target_id":"github:INTERMT:Awesome-PyTorch-Chinese","source_url":"https://github.com/INTERMT/Awesome-PyTorch-Chinese"},{"type":"has_code","target_id":"github:Stephenfang51:command_for_deeplearning","source_url":"https://github.com/Stephenfang51/command_for_deeplearning"},{"type":"has_code","target_id":"github:tzutalin:labelImg","source_url":"https://github.com/tzutalin/labelImg"},{"type":"has_code","target_id":"github:wkentaro:labelme","source_url":"https://github.com/wkentaro/labelme"},{"type":"has_code","target_id":"github:Embedding:Chinese-Word-Vectors","source_url":"https://github.com/Embedding/Chinese-Word-Vectors"},{"type":"has_code","target_id":"github:msra-nlc:MSParS","source_url":"https://github.com/msra-nlc/MSParS"},{"type":"has_code","target_id":"github:beamandrew:medical-data","source_url":"https://github.com/beamandrew/medical-data"},{"type":"has_code","target_id":"github:sfikas:medical-imaging-datasets","source_url":"https://github.com/sfikas/medical-imaging-datasets"},{"type":"has_code","target_id":"github:JackieTseng:conference_call_for_paper","source_url":"https://github.com/JackieTseng/conference_call_for_paper"},{"type":"has_code","target_id":"github:abhshkdz:ai-deadlines","source_url":"https://github.com/abhshkdz/ai-deadlines"},{"type":"has_code","target_id":"github:shunliz:Machine-Learning","source_url":"https://github.com/shunliz/Machine-Learning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:lawlite19:MachineLearning_Python","source_url":"https://github.com/lawlite19/MachineLearning_Python"},{"type":"has_code","target_id":"github:machinelearningmindset:machine-learning-course","source_url":"https://github.com/machinelearningmindset/machine-learning-course"},{"type":"has_code","target_id":"github:Dod-o:Statistical-Learning-Method_Code","source_url":"https://github.com/Dod-o/Statistical-Learning-Method_Code"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:asheeshcric:awesome-contrastive-self-supervised-learning","source_url":"https://github.com/asheeshcric/awesome-contrastive-self-supervised-learning"},{"type":"has_code","target_id":"github:wzhe06:Reco-papers","source_url":"https://github.com/wzhe06/Reco-papers"},{"type":"has_code","target_id":"github:wzhe06:Ad-papers","source_url":"https://github.com/wzhe06/Ad-papers"},{"type":"has_code","target_id":"github:chocoluffy:deep-recommender-system","source_url":"https://github.com/chocoluffy/deep-recommender-system"},{"type":"has_code","target_id":"github:zhongqiangwu960812:AI-RecommenderSystem","source_url":"https://github.com/zhongqiangwu960812/AI-RecommenderSystem"},{"type":"has_code","target_id":"github:datawhalechina:team-learning-rs","source_url":"https://github.com/datawhalechina/team-learning-rs"},{"type":"has_code","target_id":"github:Magic-Bubble:RecommendSystemPractice","source_url":"https://github.com/Magic-Bubble/RecommendSystemPractice"}]', NULL, 'Apache-2.0', 'approved', 80, 'd25acbfc62960e72d13cad79dfa1fb4c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-Mikoto10032-DeepLearning from https://github.com/Mikoto10032.png
Image converted to WebP: data/images/github-Mikoto10032-DeepLearning.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-mlc-ai-web-llm', 'github--mlc-ai--web-llm', 'web-llm', 'mlc-ai', '<div align="center" id="top"> **High-Performance In-Browser LLM Inference Engine.** Documentation | Blogpost | Paper | Examples </div> WebLLM is a high-performance in-browser LLM inference engine that brings language model inference directly onto web browsers with hardware acceleration. Everything runs inside the browser with no server support and is accelerated with WebGPU. WebLLM is **fully compatible with OpenAI API.** That is, you can use the same OpenAI API on **any open source models** ...', '["chatgpt","deep-learning","language-model","llm","tvm","webgpu","webml","typescript"]', 'other', 16913, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/mlc-ai/web-llm","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<div align="center" id="top">\n\n# WebLLM\n[![NPM Package](https://img.shields.io/badge/NPM_Package-Published-cc3534)](https://www.npmjs.com/package/@mlc-ai/web-llm)\n[!["WebLLM Chat Deployed"](https://img.shields.io/badge/WebLLM_Chat-Deployed-%2332a852)](https://chat.webllm.ai/)\n[![Join Discord](https://img.shields.io/badge/Join-Discord-7289DA?logo=discord&logoColor=white)](https://discord.gg/9Xpy2HGBuD)\n[![Related Repository: WebLLM Chat](https://img.shields.io/badge/Related_Repo-WebLLM_Chat-fafbfc?logo=github)](https://github.com/mlc-ai/web-llm-chat/)\n[![Related Repository: MLC LLM](https://img.shields.io/badge/Related_Repo-MLC_LLM-fafbfc?logo=github)](https://github.com/mlc-ai/mlc-llm/)\n\n**High-Performance In-Browser LLM Inference Engine.**\n\n\n[Documentation](https://webllm.mlc.ai/docs/) | [Blogpost](https://blog.mlc.ai/2024/06/13/webllm-a-high-performance-in-browser-llm-inference-engine) | [Paper](https://arxiv.org/abs/2412.15803) | [Examples](examples)\n\n</div>\n\n## Overview\nWebLLM is a high-performance in-browser LLM inference engine that brings language model inference directly onto web browsers with hardware acceleration.\nEverything runs inside the browser with no server support and is accelerated with WebGPU.\n\nWebLLM is **fully compatible with [OpenAI API](https://platform.openai.com/docs/api-reference/chat).**\nThat is, you can use the same OpenAI API on **any open source models** locally, with functionalities\nincluding streaming, JSON-mode, function-calling (WIP), etc.\n\nWe can bring a lot of fun opportunities to build AI assistants for everyone and enable privacy while enjoying GPU acceleration.\n\nYou can use WebLLM as a base [npm package](https://www.npmjs.com/package/@mlc-ai/web-llm) and build your own web application on top of it by following the examples below. This project is a companion project of [MLC LLM](https://github.com/mlc-ai/mlc-llm), which enables universal deployment of LLM across hardware environments.\n\n<div align="center">\n\n**[Check out WebLLM Chat to try it out!](https://chat.webllm.ai/)**\n\n</div>\n\n## Key Features\n- **In-Browser Inference**: WebLLM is a high-performance, in-browser language model inference engine that leverages WebGPU for hardware acceleration, enabling powerful LLM operations directly within web browsers without server-side processing.\n\n- [**Full OpenAI API Compatibility**](#full-openai-compatibility): Seamlessly integrate your app with WebLLM using OpenAI API with functionalities such as streaming, JSON-mode, logit-level control, seeding, and more.\n\n- **Structured JSON Generation**: WebLLM supports state-of-the-art JSON mode structured generation, implemented in the WebAssembly portion of the model library for optimal performance. Check [WebLLM JSON Playground](https://huggingface.co/spaces/mlc-ai/WebLLM-JSON-Playground) on HuggingFace to try generating JSON output with custom JSON schema.\n\n- [**Extensive Model Support**](#built-in-models): WebLLM natively supports a range of models including Llama 3, Phi 3, Gemma, Mistral, Qwen(通义千问), and many others, making it versatile for various AI tasks. For the complete supported model list, check [MLC Models](https://mlc.ai/models).\n\n- [**Custom Model Integration**](#custom-models): Easily integrate and deploy custom models in MLC format, allowing you to adapt WebLLM to specific needs and scenarios, enhancing flexibility in model deployment.\n\n- **Plug-and-Play Integration**: Easily integrate WebLLM into your projects using package managers like NPM and Yarn, or directly via CDN, complete with comprehensive [examples](./examples/) and a modular design for connecting with UI components.\n\n- **Streaming & Real-Time Interactions**: Supports streaming chat completions, allowing real-time output generation which enhances interactive applications like chatbots and virtual assistants.\n\n- **Web Worker & Service Worker Support**: Optimize UI performance and manage the lifecycle of models efficiently by offloading computations to separate worker threads or service workers.\n\n- **Chrome Extension Support**: Extend the functionality of web browsers through custom Chrome extensions using WebLLM, with examples available for building both basic and advanced extensions.\n\n## Built-in Models\n\nCheck the complete list of available models on [MLC Models](https://mlc.ai/models). WebLLM supports a subset of these available models and the list can be accessed at [`prebuiltAppConfig.model_list`](https://github.com/mlc-ai/web-llm/blob/main/src/config.ts#L293).\n\nHere are the primary families of models currently supported:\n\n- **Llama**: Llama 3, Llama 2, Hermes-2-Pro-Llama-3\n- **Phi**: Phi 3, Phi 2, Phi 1.5\n- **Gemma**: Gemma-2B\n- **Mistral**: Mistral-7B-v0.3, Hermes-2-Pro-Mistral-7B, NeuralHermes-2.5-Mistral-7B, OpenHermes-2.5-Mistral-7B\n- **Qwen (通义千问)**: Qwen2 0.5B, 1.5B, 7B\n\nIf you need more models, [request a new model via opening an issue](https://github.com/mlc-ai/web-llm/issues/new/choose) or check [Custom Models](#custom-models) for how to compile and use your own models with WebLLM.\n\n## Jumpstart with Examples\n\nLearn how to use WebLLM to integrate large language models into your application and generate chat completions through this simple Chatbot example: \n\n[![Example Chatbot on JSFiddle](https://img.shields.io/badge/Example-JSFiddle-blue?logo=jsfiddle&logoColor=white)](https://jsfiddle.net/neetnestor/4nmgvsa2/)\n[![Example Chatbot on Codepen](https://img.shields.io/badge/Example-Codepen-gainsboro?logo=codepen)](https://codepen.io/neetnestor/pen/vYwgZaG)\n\nFor an advanced example of a larger, more complicated project, check [WebLLM Chat](https://github.com/mlc-ai/web-llm-chat/blob/main/app/client/webllm.ts).\n\nMore examples for different use cases are available in the [examples](./examples/) folder.\n\n## Get Started\n\nWebLLM offers a minimalist and modular interface to access the chatbot in the browser.\nThe package is designed in a modular way to hook to any of the UI components.\n\n### Installation\n\n#### Package Manager\n\n```sh\n# npm\nnpm install @mlc-ai/web-llm\n# yarn\nyarn add @mlc-ai/web-llm\n# or pnpm\npnpm install @mlc-ai/web-llm\n```\n\nThen import the module in your code.\n\n```typescript\n// Import everything\nimport * as webllm from "@mlc-ai/web-llm";\n// Or only import what you need\nimport { CreateMLCEngine } from "@mlc-ai/web-llm";\n```\n\n#### CDN Delivery\n\nThanks to [jsdelivr.com](https://www.jsdelivr.com/package/npm/@mlc-ai/web-llm), WebLLM can be imported directly through URL and work out-of-the-box on cloud development platforms like [jsfiddle.net](https://jsfiddle.net/), [Codepen.io](https://codepen.io/), and [Scribbler](https://scribbler.live):\n\n```javascript\nimport * as webllm from "https://esm.run/@mlc-ai/web-llm";\n```\nIt can also be dynamically imported as:\n```javascript\nconst webllm = await import ("https://esm.run/@mlc-ai/web-llm");\n```\n\n### Create MLCEngine\n\nMost operations in WebLLM are invoked through the `MLCEngine` interface. You can create an `MLCEngine` instance and loading the model by calling the `CreateMLCEngine()` factory function.\n\n(Note that loading models requires downloading and it can take a significant amount of time for the very first run without caching previously. You should properly handle this asynchronous call.)\n\n```typescript\nimport { CreateMLCEngine } from "@mlc-ai/web-llm";\n\n// Callback function to update model loading progress\nconst initProgressCallback = (initProgress) => {\n  console.log(initProgress);\n}\nconst selectedModel = "Llama-3.1-8B-Instruct-q4f32_1-MLC";\n\nconst engine = await CreateMLCEngine(\n  selectedModel,\n  { initProgressCallback: initProgressCallback }, // engineConfig\n);\n```\n\nUnder the hood, this factory function does the following steps for first creating an engine instance (synchronous) and then loading the model (asynchronous). You can also do them separately in your application.\n\n```typescript\nimport { MLCEngine } from "@mlc-ai/web-llm";\n\n// This is a synchronous call that returns immediately\nconst engine = new MLCEngine({\n  initProgressCallback: initProgressCallback\n});\n\n// This is an asynchronous call and can take a long time to finish\nawait engine.reload(selectedModel);\n```\n\n### Chat Completion\nAfter successfully initializing the engine, you can now invoke chat completions using OpenAI style chat APIs through the `engine.chat.completions` interface. For the full list of parameters and their descriptions, check [section below](#full-openai-compatibility) and [OpenAI API reference](https://platform.openai.com/docs/api-reference/chat/create).\n\n(Note: The `model` parameter is not supported and will be ignored here. Instead, call `CreateMLCEngine(model)` or `engine.reload(model)` instead as shown in the [Create MLCEngine](#create-mlcengine) above.)\n\n\n```typescript\nconst messages = [\n  { role: "system", content: "You are a helpful AI assistant." },\n  { role: "user", content: "Hello!" },\n]\n\nconst reply = await engine.chat.completions.create({\n  messages,\n});\nconsole.log(reply.choices[0].message);\nconsole.log(reply.usage);\n```\n\n### Streaming\n\nWebLLM also supports streaming chat completion generating. To use it, simply pass `stream: true` to the `engine.chat.completions.create` call.\n\n```typescript\nconst messages = [\n  { role: "system", content: "You are a helpful AI assistant." },\n  { role: "user", content: "Hello!" },\n]\n\n// Chunks is an AsyncGenerator object\nconst chunks = await engine.chat.completions.create({\n  messages,\n  temperature: 1,\n  stream: true, // <-- Enable streaming\n  stream_options: { include_usage: true },\n});\n\nlet reply = "";\nfor await (const chunk of chunks) {\n  reply += chunk.choices[0]?.delta.content || "";\n  console.log(reply);\n  if (chunk.usage) {\n    console.log(chunk.usage); // only last chunk has usage\n  }\n}\n\nconst fullReply = await engine.getMessage();\nconsole.log(fullReply);\n```\n\n## Advanced Usage\n\n### Using Workers\n\nYou can put the heavy computation in a worker script to optimize your application performance. To do so, you need to:\n\n1. Create a handler in the worker thread that communicates with the frontend while handling the requests.\n2. Create a Worker Engine in your main application, which under the hood sends messages to the handler in the worker thread.\n\nFor detailed implementations of different kinds of Workers, check the following sections.\n\n#### Dedicated Web Worker\n\nWebLLM comes with API support for WebWorker so you can hook\nthe generation process into a separate worker thread so that\nthe computing in the worker thread won''t disrupt the UI.\n\nWe create a handler in the worker thread that communicates with the frontend while handling the requests.\n\n```typescript\n// worker.ts\nimport { WebWorkerMLCEngineHandler } from "@mlc-ai/web-llm";\n\n// A handler that resides in the worker thread\nconst handler = new WebWorkerMLCEngineHandler();\nself.onmessage = (msg: MessageEvent) => {\n  handler.onmessage(msg);\n};\n```\n\nIn the main logic, we create a `WebWorkerMLCEngine` that\nimplements the same `MLCEngineInterface`. The rest of the logic remains the same.\n\n```typescript\n// main.ts\nimport { CreateWebWorkerMLCEngine } from "@mlc-ai/web-llm";\n\nasync function main() {\n  // Use a WebWorkerMLCEngine instead of MLCEngine here\n  const engine = await CreateWebWorkerMLCEngine(\n    new Worker(\n      new URL("./worker.ts", import.meta.url), \n      {\n        type: "module",\n      }\n    ),\n    selectedModel,\n    { initProgressCallback }, // engineConfig\n  );\n\n  // everything else remains the same\n}\n```\n\n### Use Service Worker\n\nWebLLM comes with API support for ServiceWorker so you can hook the generation process\ninto a service worker to avoid reloading the model in every page visit and optimize\nyour application''s offline experience.\n\n(Note, Service Worker''s life cycle is managed by the browser and can be killed any time without notifying the webapp. `ServiceWorkerMLCEngine` will try to keep the service worker thread alive by periodically sending heartbeat events, but your application should also include proper error handling. Check `keepAliveMs` and `missedHeatbeat` in [`ServiceWorkerMLCEngine`](https://github.com/mlc-ai/web-llm/blob/main/src/service_worker.ts#L234) for more details.)\n\nWe create a handler in the worker thread that communicates with the frontend while handling the requests.\n\n\n```typescript\n// sw.ts\nimport { ServiceWorkerMLCEngineHandler } from "@mlc-ai/web-llm";\n\nlet handler: ServiceWorkerMLCEngineHandler;\n\nself.addEventListener("activate", function (event) {\n  handler = new ServiceWorkerMLCEngineHandler();\n  console.log("Service Worker is ready");\n});\n```\n\nThen in the main logic, we register the service worker and create the engine using\n`CreateServiceWorkerMLCEngine` function. The rest of the logic remains the same.\n\n```typescript\n// main.ts\nimport { MLCEngineInterface, CreateServiceWorkerMLCEngine } from "@mlc-ai/web-llm";\n\nif ("serviceWorker" in navigator) {\n  navigator.serviceWorker.register(\n    new URL("sw.ts", import.meta.url),  // worker script\n    { type: "module" },\n  );\n}\n\nconst engine: MLCEngineInterface =\n  await CreateServiceWorkerMLCEngine(\n    selectedModel,\n    { initProgressCallback }, // engineConfig\n  );\n```\n\nYou can find a complete example on how to run WebLLM in service worker in [examples/service-worker](examples/service-worker/).\n\n### Chrome Extension\nYou can also find examples of building Chrome extension with WebLLM in [examples/chrome-extension](examples/chrome-extension/) and [examples/chrome-extension-webgpu-service-worker](examples/chrome-extension-webgpu-service-worker/). The latter one leverages service worker, so the extension is persistent in the background. Additionally, you can explore another full project of a Chrome extension, WebLLM Assistant, which leverages WebLLM [here](https://github.com/mlc-ai/web-llm-assistant).\n\n## Full OpenAI Compatibility\nWebLLM is designed to be fully compatible with [OpenAI API](https://platform.openai.com/docs/api-reference/chat). Thus, besides building a simple chatbot, you can also have the following functionalities with WebLLM:\n\n- [streaming](examples/streaming): return output as chunks in real-time in the form of an AsyncGenerator\n- [json-mode](examples/json-mode): efficiently ensure output is in JSON format, see [OpenAI Reference](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) for more.\n- [seed-to-reproduce](examples/seed-to-reproduce): use seeding to ensure a reproducible output with fields `seed`.\n- [function-calling](examples/function-calling) (WIP): function calling with fields `tools` and `tool_choice` (with preliminary support); or manual function calling without `tools` or `tool_choice` (keeps the most flexibility).\n\n## Custom Models\n\nWebLLM works as a companion project of [MLC LLM](https://github.com/mlc-ai/mlc-llm) and it supports custom models in MLC format. \nIt reuses the model artifact and builds the flow of MLC LLM. To compile and use your own models with WebLLM, please check out\n[MLC LLM document](https://llm.mlc.ai/docs/deploy/webllm.html)\non how to compile and deploy new model weights and libraries to WebLLM. \n\nHere, we go over the high-level idea. There are two elements of the WebLLM package that enable new models and weight variants.\n\n- `model`: Contains a URL to model artifacts, such as weights and meta-data.\n- `model_lib`: A URL to the web assembly library (i.e. wasm file) that contains the executables to accelerate the model computations.\n\nBoth are customizable in the WebLLM.\n\n```typescript\nimport { CreateMLCEngine } from "@mlc-ai/web-llm";\n\nasync main() {\n  const appConfig = {\n    "model_list": [\n      {\n        "model": "/url/to/my/llama",\n        "model_id": "MyLlama-3b-v1-q4f32_0",\n        "model_lib": "/url/to/myllama3b.wasm",\n      }\n    ],\n  };\n  // override default\n  const chatOpts = {\n    "repetition_penalty": 1.01\n  };\n\n  // load a prebuilt model\n  // with a chat option override and app config\n  // under the hood, it will load the model from myLlamaUrl\n  // and cache it in the browser cache\n  // The chat will also load the model library from "/url/to/myllama3b.wasm",\n  // assuming that it is compatible to the model in myLlamaUrl.\n  const engine = await CreateMLCEngine(\n    "MyLlama-3b-v1-q4f32_0",\n    { appConfig }, // engineConfig\n    chatOpts,\n  );\n}\n```\n\nIn many cases, we only want to supply the model weight variant, but\nnot necessarily a new model (e.g. `NeuralHermes-Mistral` can reuse `Mistral`''s\nmodel library). For examples of how a model library can be shared by different model variants,\nsee `webllm.prebuiltAppConfig`.\n\n## Build WebLLM Package From Source\n\nNOTE: you don''t need to build from source unless you would like to modify the WebLLM package.\nTo use the npm, simply follow [Get Started](#get-started) or any of the [examples](examples) instead.\n\nTo build from source, simply run:\n\n```bash\nnpm install\nnpm run build\n```\n\nThen, to test the effects of your code change in an example, inside `examples/get-started/package.json`, change from `"@mlc-ai/web-llm": "^0.2.80"` to `"@mlc-ai/web-llm": ../..`.\n\nThen run:\n\n```bash\ncd examples/get-started\nnpm install\nnpm start\n```\n\nNote that sometimes you would need to switch between `file:../..` and `../..` to trigger npm to recognize new changes. In the worst case, you can run:\n\n```bash\ncd examples/get-started\nrm -rf node_modules dist package-lock.json .parcel-cache\nnpm install\nnpm start\n```\n\n### In case you need to build TVMjs from source\n\nWebLLM''s runtime largely depends on TVMjs: https://github.com/apache/tvm/tree/main/web\n\nWhile it is also available as an npm package: https://www.npmjs.com/package/@mlc-ai/web-runtime, you can build it from source if needed by following the steps below.\n\n1. Install [emscripten](https://emscripten.org). It is an LLVM-based compiler that compiles C/C++ source code to WebAssembly.\n    - Follow the [installation instruction](https://emscripten.org/docs/getting_started/downloads.html#installation-instructions-using-the-emsdk-recommended) to install the latest emsdk.\n    - Source `emsdk_env.sh` by `source path/to/emsdk_env.sh`, so that `emcc` is reachable from PATH and the command `emcc` works.\n\n    We can verify the successful installation by trying out `emcc` terminal.\n\n    Note: We recently found that using the latest `emcc` version may run into issues during runtime. Use `./emsdk install 3.1.56` instead of `./emsdk install latest` for now as a workaround. The error may look like\n    ```\n    Init error, LinkError: WebAssembly.instantiate(): Import #6 module="wasi_snapshot_preview1"\n    function="proc_exit": function import requires a callable\n    ```\n\n2. In `./package.json`, change from `"@mlc-ai/web-runtime": "0.18.0-dev2",` to `"@mlc-ai/web-runtime": "file:./tvm_home/web",`.\n\n3. Setup necessary environment\n\n   Prepare all the necessary dependencies for web build:\n\n   ```shell\n   ./scripts/prep_deps.sh\n   ```\n\n   In this step, if `$TVM_SOURCE_DIR` is not defined in the environment, we will execute the following line to build `tvmjs` dependency:\n   ```shell\n   git clone https://github.com/mlc-ai/relax 3rdparty/tvm-unity --recursive\n   ```\n\n   This clones the current HEAD of `mlc-ai/relax`. However, it may not always be the correct branch or commit to clone. To build a specific npm version from source, refer to the version bump PR, which states which branch (i.e. `mlc-ai/relax` or `apache/tvm`) and which commit the current WebLLM version depends on. For instance, version 0.2.52, according to its version bump PR https://github.com/mlc-ai/web-llm/pull/521, is built by checking out the following commit https://github.com/apache/tvm/commit/e6476847753c80e054719ac47bc2091c888418b6 in `apache/tvm`, rather than the HEAD of `mlc-ai/relax`.\n\n   Besides, `--recursive` is necessary and important. Otherwise, you may encounter errors like `fatal error: ''dlpack/dlpack.h'' file not found`.\n\n4. Build WebLLM Package\n\n   ```shell\n   npm run build\n   ```\n\n5. Validate some of the sub-packages\n\n   You can then go to the subfolders in [examples](examples) to validate some of the sub-packages.\n   We use Parcelv2 for bundling. Although Parcel is not very good at tracking parent directory\n   changes sometimes. When you make a change in the WebLLM package, try to edit the `package.json`\n   of the subfolder and save it, which will trigger Parcel to rebuild.\n\n## Links\n\n- [Demo App: WebLLM Chat](https://chat.webllm.ai/)\n- If you want to run LLM on native runtime, check out [MLC-LLM](https://github.com/mlc-ai/mlc-llm)\n- You might also be interested in [Web Stable Diffusion](https://github.com/mlc-ai/web-stable-diffusion/).\n\n## Acknowledgement\n\nThis project is initiated by members from CMU Catalyst, UW SAMPL, SJTU, OctoML, and the MLC community. We would love to continue developing and supporting the open-source ML community.\n\nThis project is only possible thanks to the shoulders open-source ecosystems that we stand on. We want to thank the Apache TVM community and developers of the TVM Unity effort. The open-source ML community members made these models publicly available. PyTorch and Hugging Face communities make these models accessible. We would like to thank the teams behind Vicuna, SentencePiece, LLaMA, and Alpaca. We also would like to thank the WebAssembly, Emscripten, and WebGPU communities. Finally, thanks to Dawn and WebGPU developers.\n\n## Citation\nIf you find this project to be useful, please cite:\n\n```\n@misc{ruan2024webllmhighperformanceinbrowserllm,\n      title={WebLLM: A High-Performance In-Browser LLM Inference Engine}, \n      author={Charlie F. Ruan and Yucheng Qin and Xun Zhou and Ruihang Lai and Hongyi Jin and Yixin Dong and Bohan Hou and Meng-Shiun Yu and Yiyan Zhai and Sudeep Agarwal and Hangrui Cao and Siyuan Feng and Tianqi Chen},\n      year={2024},\n      eprint={2412.15803},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2412.15803}, \n}\n```\n\n## Contributors\n\n<a href="https://github.com/mlc-ai/web-llm/graphs/contributors">\n  <img alt="contributors" src="https://contrib.rocks/image?repo=mlc-ai/web-llm"/>\n</a>\n\n<p align="right">\n  <a href="#top">⬆ Back to Top ⬆</a>\n</p>\n', '{"language":"TypeScript","stars":16913,"forks":1151,"watchers":16913,"open_issues":147,"topics":["chatgpt","deep-learning","language-model","llm","tvm","webgpu","webml"],"default_branch":"main","size_kb":67640,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:mlc-ai:web-llm-chat","source_url":"https://github.com/mlc-ai/web-llm-chat"},{"type":"has_code","target_id":"github:mlc-ai:mlc-llm","source_url":"https://github.com/mlc-ai/mlc-llm"},{"type":"has_code","target_id":"github:mlc-ai:mlc-llm","source_url":"https://github.com/mlc-ai/mlc-llm"},{"type":"has_code","target_id":"github:mlc-ai:web-llm","source_url":"https://github.com/mlc-ai/web-llm"},{"type":"has_code","target_id":"github:mlc-ai:web-llm","source_url":"https://github.com/mlc-ai/web-llm"},{"type":"has_code","target_id":"github:mlc-ai:web-llm-chat","source_url":"https://github.com/mlc-ai/web-llm-chat"},{"type":"has_code","target_id":"github:mlc-ai:web-llm","source_url":"https://github.com/mlc-ai/web-llm"},{"type":"has_code","target_id":"github:mlc-ai:web-llm-assistant","source_url":"https://github.com/mlc-ai/web-llm-assistant"},{"type":"has_code","target_id":"github:mlc-ai:mlc-llm","source_url":"https://github.com/mlc-ai/mlc-llm"},{"type":"has_code","target_id":"github:apache:tvm","source_url":"https://github.com/apache/tvm"},{"type":"has_code","target_id":"github:mlc-ai:relax","source_url":"https://github.com/mlc-ai/relax"},{"type":"has_code","target_id":"github:mlc-ai:web-llm","source_url":"https://github.com/mlc-ai/web-llm"},{"type":"has_code","target_id":"github:apache:tvm","source_url":"https://github.com/apache/tvm"},{"type":"has_code","target_id":"github:mlc-ai:mlc-llm","source_url":"https://github.com/mlc-ai/mlc-llm"},{"type":"has_code","target_id":"github:mlc-ai:web-stable-diffusion","source_url":"https://github.com/mlc-ai/web-stable-diffusion"},{"type":"has_code","target_id":"github:mlc-ai:web-llm","source_url":"https://github.com/mlc-ai/web-llm"}]', NULL, 'Apache-2.0', 'approved', 80, 'e54c4f928fa01016bf231f0e89304211', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-mlc-ai-web-llm from https://github.com/mlc-ai.png
Image converted to WebP: data/images/github-mlc-ai-web-llm.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-tensorflow-tensor2tensor', 'github--tensorflow--tensor2tensor', 'tensor2tensor', 'tensorflow', 'Tensor2Tensor, or T2T for short, is a library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research. T2T was developed by researchers and engineers in the Google Brain team and a community of users. It is now deprecated &mdash; we keep it running and welcome bug-fixes, but encourage users to use the successor library Trax. This iPython notebook explains T2T and runs in your browser using a free VM from Google, no installation needed. Al...', '["deep-learning","machine-learning","machine-translation","reinforcement-learning","tpu","python"]', 'other', 16799, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/tensorflow/tensor2tensor","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '# Tensor2Tensor\n\n[![PyPI\nversion](https://badge.fury.io/py/tensor2tensor.svg)](https://badge.fury.io/py/tensor2tensor)\n[![GitHub\nIssues](https://img.shields.io/github/issues/tensorflow/tensor2tensor.svg)](https://github.com/tensorflow/tensor2tensor/issues)\n[![Contributions\nwelcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![Gitter](https://img.shields.io/gitter/room/nwjs/nw.js.svg)](https://gitter.im/tensor2tensor/Lobby)\n[![License](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Travis](https://img.shields.io/travis/tensorflow/tensor2tensor.svg)](https://travis-ci.org/tensorflow/tensor2tensor)\n[![Run on FH](https://static.floydhub.com/button/button-small.svg)](https://floydhub.com/run)\n\n[Tensor2Tensor](https://github.com/tensorflow/tensor2tensor), or\n[T2T](https://github.com/tensorflow/tensor2tensor) for short, is a library\nof deep learning models and datasets designed to make deep learning more\naccessible and [accelerate ML\nresearch](https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html).\n\n\nT2T was developed by researchers and engineers in the\n[Google Brain team](https://research.google.com/teams/brain/) and a community\nof users. It is now deprecated &mdash; we keep it running and welcome\nbug-fixes, but encourage users to use the successor library [Trax](https://github.com/google/trax).\n\n### Quick Start\n\n[This iPython notebook](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)\nexplains T2T and runs in your browser using a free VM from Google,\nno installation needed. Alternatively, here is a one-command version that\ninstalls T2T, downloads MNIST, trains a model and evaluates it:\n\n```\npip install tensor2tensor && t2t-trainer \\n  --generate_data \\n  --data_dir=~/t2t_data \\n  --output_dir=~/t2t_train/mnist \\n  --problem=image_mnist \\n  --model=shake_shake \\n  --hparams_set=shake_shake_quick \\n  --train_steps=1000 \\n  --eval_steps=100\n```\n\n### Contents\n\n* [Suggested Datasets and Models](#suggested-datasets-and-models)\n  * [Mathematical Language Understanding](#mathematical-language-understanding)\n  * [Story, Question and Answer](#story-question-and-answer)\n  * [Image Classification](#image-classification)\n  * [Image Generation](#image-generation)\n  * [Language Modeling](#language-modeling)\n  * [Sentiment Analysis](#sentiment-analysis)\n  * [Speech Recognition](#speech-recognition)\n  * [Summarization](#summarization)\n  * [Translation](#translation)\n* [Basics](#basics)\n  * [Walkthrough](#walkthrough)\n  * [Installation](#installation)\n  * [Features](#features)\n* [T2T Overview](#t2t-overview)\n  * [Datasets](#datasets)\n  * [Problems and Modalities](#problems-and-modalities)\n  * [Models](#models)\n  * [Hyperparameter Sets](#hyperparameter-sets)\n  * [Trainer](#trainer)\n* [Adding your own components](#adding-your-own-components)\n* [Adding a dataset](#adding-a-dataset)\n* [Papers](#papers)\n* [Run on FloydHub](#run-on-floydhub)\n\n## Suggested Datasets and Models\n\nBelow we list a number of tasks that can be solved with T2T when\nyou train the appropriate model on the appropriate problem.\nWe give the problem and model below and we suggest a setting of\nhyperparameters that we know works well in our setup. We usually\nrun either on Cloud TPUs or on 8-GPU machines; you might need\nto modify the hyperparameters if you run on a different setup.\n\n### Mathematical Language Understanding\n\nFor evaluating mathematical expressions at the character level involving addition, subtraction and multiplication of both positive and negative decimal numbers with variable digits assigned to symbolic variables, use\n\n* the [MLU](https://art.wangperawong.com/mathematical_language_understanding_train.tar.gz) data-set:\n `--problem=algorithmic_math_two_variables`\n\nYou can try solving the problem with different transformer models and hyperparameters as described in the [paper](https://arxiv.org/abs/1812.02825):\n* Standard transformer:\n`--model=transformer`\n`--hparams_set=transformer_tiny`\n* Universal transformer:\n`--model=universal_transformer`\n`--hparams_set=universal_transformer_tiny`\n* Adaptive universal transformer:\n`--model=universal_transformer`\n`--hparams_set=adaptive_universal_transformer_tiny`\n\n### Story, Question and Answer\n\nFor answering questions based on a story, use\n\n* the [bAbi](https://research.fb.com/downloads/babi/) data-set:\n `--problem=babi_qa_concat_task1_1k`\n\nYou can choose the bAbi task from the range [1,20] and the subset from 1k or\n10k. To combine test data from all tasks into a single test set, use\n`--problem=babi_qa_concat_all_tasks_10k`\n\n### Image Classification\n\nFor image classification, we have a number of standard data-sets:\n\n* ImageNet (a large data-set): `--problem=image_imagenet`, or one\n   of the re-scaled versions (`image_imagenet224`, `image_imagenet64`,\n   `image_imagenet32`)\n* CIFAR-10: `--problem=image_cifar10` (or\n    `--problem=image_cifar10_plain` to turn off data augmentation)\n* CIFAR-100: `--problem=image_cifar100`\n* MNIST: `--problem=image_mnist`\n\nFor ImageNet, we suggest to use the ResNet or Xception, i.e.,\nuse `--model=resnet --hparams_set=resnet_50` or\n`--model=xception --hparams_set=xception_base`.\nResnet should get to above 76% top-1 accuracy on ImageNet.\n\nFor CIFAR and MNIST, we suggest to try the shake-shake model:\n`--model=shake_shake --hparams_set=shakeshake_big`.\nThis setting trained for `--train_steps=700000` should yield\nclose to 97% accuracy on CIFAR-10.\n\n### Image Generation\n\nFor (un)conditional image generation, we have a number of standard data-sets:\n\n* CelebA: `--problem=img2img_celeba` for image-to-image translation, namely,\n    superresolution from 8x8 to 32x32.\n* CelebA-HQ: `--problem=image_celeba256_rev` for a downsampled 256x256.\n* CIFAR-10: `--problem=image_cifar10_plain_gen_rev` for class-conditional\n    32x32 generation.\n* LSUN Bedrooms: `--problem=image_lsun_bedrooms_rev`\n* MS-COCO: `--problem=image_text_ms_coco_rev` for text-to-image generation.\n* Small ImageNet (a large data-set): `--problem=image_imagenet32_gen_rev` for\n    32x32 or `--problem=image_imagenet64_gen_rev` for 64x64.\n\nWe suggest to use the Image Transformer, i.e., `--model=imagetransformer`, or\nthe Image Transformer Plus, i.e., `--model=imagetransformerpp` that uses\ndiscretized mixture of logistics, or variational auto-encoder, i.e.,\n`--model=transformer_ae`.\nFor CIFAR-10, using `--hparams_set=imagetransformer_cifar10_base` or\n`--hparams_set=imagetransformer_cifar10_base_dmol` yields 2.90 bits per\ndimension. For Imagenet-32, using\n`--hparams_set=imagetransformer_imagenet32_base` yields 3.77 bits per dimension.\n\n### Language Modeling\n\nFor language modeling, we have these data-sets in T2T:\n\n* PTB (a small data-set): `--problem=languagemodel_ptb10k` for\n    word-level modeling and `--problem=languagemodel_ptb_characters`\n    for character-level modeling.\n* LM1B (a billion-word corpus): `--problem=languagemodel_lm1b32k` for\n    subword-level modeling and `--problem=languagemodel_lm1b_characters`\n    for character-level modeling.\n\nWe suggest to start with `--model=transformer` on this task and use\n`--hparams_set=transformer_small` for PTB and\n`--hparams_set=transformer_base` for LM1B.\n\n### Sentiment Analysis\n\nFor the task of recognizing the sentiment of a sentence, use\n\n* the IMDB data-set: `--problem=sentiment_imdb`\n\nWe suggest to use `--model=transformer_encoder` here and since it is\na small data-set, try `--hparams_set=transformer_tiny` and train for\nfew steps (e.g., `--train_steps=2000`).\n\n### Speech Recognition\n\nFor speech-to-text, we have these data-sets in T2T:\n\n* Librispeech (US English): `--problem=librispeech` for\n    the whole set and `--problem=librispeech_clean` for a smaller\n    but nicely filtered part.\n\n* Mozilla Common Voice (US English): `--problem=common_voice` for the whole set\n    `--problem=common_voice_clean` for a quality-checked subset.\n\n### Summarization\n\nFor summarizing longer text into shorter one we have these data-sets:\n\n* CNN/DailyMail articles summarized into a few sentences:\n  `--problem=summarize_cnn_dailymail32k`\n\nWe suggest to use `--model=transformer` and\n`--hparams_set=transformer_prepend` for this task.\nThis yields good ROUGE scores.\n\n### Translation\n\nThere are a number of translation data-sets in T2T:\n\n* English-German: `--problem=translate_ende_wmt32k`\n* English-French: `--problem=translate_enfr_wmt32k`\n* English-Czech: `--problem=translate_encs_wmt32k`\n* English-Chinese: `--problem=translate_enzh_wmt32k`\n* English-Vietnamese: `--problem=translate_envi_iwslt32k`\n* English-Spanish: `--problem=translate_enes_wmt32k`\n\nYou can get translations in the other direction by appending `_rev` to\nthe problem name, e.g., for German-English use\n`--problem=translate_ende_wmt32k_rev`\n(note that you still need to download the original data with t2t-datagen\n`--problem=translate_ende_wmt32k`).\n\nFor all translation problems, we suggest to try the Transformer model:\n`--model=transformer`. At first it is best to try the base setting,\n`--hparams_set=transformer_base`. When trained on 8 GPUs for 300K steps\nthis should reach a BLEU score of about 28 on the English-German data-set,\nwhich is close to state-of-the art. If training on a single GPU, try the\n`--hparams_set=transformer_base_single_gpu` setting. For very good results\nor larger data-sets (e.g., for English-French), try the big model\nwith `--hparams_set=transformer_big`.\n\nSee this [example](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/Transformer_translate.ipynb) to know how the translation works.\n\n## Basics\n\n### Walkthrough\n\nHere''s a walkthrough training a good English-to-German translation\nmodel using the Transformer model from [*Attention Is All You\nNeed*](https://arxiv.org/abs/1706.03762) on WMT data.\n\n```\npip install tensor2tensor\n\n# See what problems, models, and hyperparameter sets are available.\n# You can easily swap between them (and add new ones).\nt2t-trainer --registry_help\n\nPROBLEM=translate_ende_wmt32k\nMODEL=transformer\nHPARAMS=transformer_base_single_gpu\n\nDATA_DIR=$HOME/t2t_data\nTMP_DIR=/tmp/t2t_datagen\nTRAIN_DIR=$HOME/t2t_train/$PROBLEM/$MODEL-$HPARAMS\n\nmkdir -p $DATA_DIR $TMP_DIR $TRAIN_DIR\n\n# Generate data\nt2t-datagen \\n  --data_dir=$DATA_DIR \\n  --tmp_dir=$TMP_DIR \\n  --problem=$PROBLEM\n\n# Train\n# *  If you run out of memory, add --hparams=''batch_size=1024''.\nt2t-trainer \\n  --data_dir=$DATA_DIR \\n  --problem=$PROBLEM \\n  --model=$MODEL \\n  --hparams_set=$HPARAMS \\n  --output_dir=$TRAIN_DIR\n\n# Decode\n\nDECODE_FILE=$DATA_DIR/decode_this.txt\necho "Hello world" >> $DECODE_FILE\necho "Goodbye world" >> $DECODE_FILE\necho -e ''Hallo Welt\nAuf Wiedersehen Welt'' > ref-translation.de\n\nBEAM_SIZE=4\nALPHA=0.6\n\nt2t-decoder \\n  --data_dir=$DATA_DIR \\n  --problem=$PROBLEM \\n  --model=$MODEL \\n  --hparams_set=$HPARAMS \\n  --output_dir=$TRAIN_DIR \\n  --decode_hparams="beam_size=$BEAM_SIZE,alpha=$ALPHA" \\n  --decode_from_file=$DECODE_FILE \\n  --decode_to_file=translation.en\n\n# See the translations\ncat translation.en\n\n# Evaluate the BLEU score\n# Note: Report this BLEU score in papers, not the internal approx_bleu metric.\nt2t-bleu --translation=translation.en --reference=ref-translation.de\n```\n\n### Installation\n\n\n```\n# Assumes tensorflow or tensorflow-gpu installed\npip install tensor2tensor\n\n# Installs with tensorflow-gpu requirement\npip install tensor2tensor[tensorflow_gpu]\n\n# Installs with tensorflow (cpu) requirement\npip install tensor2tensor[tensorflow]\n```\n\nBinaries:\n\n```\n# Data generator\nt2t-datagen\n\n# Trainer\nt2t-trainer --registry_help\n```\n\nLibrary usage:\n\n```\npython -c "from tensor2tensor.models.transformer import Transformer"\n```\n\n### Features\n\n* Many state of the art and baseline models are built-in and new models can be\n  added easily (open an issue or pull request!).\n* Many datasets across modalities - text, audio, image - available for\n  generation and use, and new ones can be added easily (open an issue or pull\n  request for public datasets!).\n* Models can be used with any dataset and input mode (or even multiple); all\n  modality-specific processing (e.g. embedding lookups for text tokens) is done\n  with `bottom` and `top` transformations, which are specified per-feature in the\n  model.\n* Support for multi-GPU machines and synchronous (1 master, many workers) and\n  asynchronous (independent workers synchronizing through a parameter server)\n  [distributed training](https://tensorflow.github.io/tensor2tensor/distributed_training.html).\n* Easily swap amongst datasets and models by command-line flag with the data\n  generation script `t2t-datagen` and the training script `t2t-trainer`.\n* Train on [Google Cloud ML](https://tensorflow.github.io/tensor2tensor/cloud_mlengine.html) and [Cloud TPUs](https://tensorflow.github.io/tensor2tensor/cloud_tpu.html).\n\n## T2T overview\n\n### Problems\n\n**Problems** consist of features such as inputs and targets, and metadata such\nas each feature''s modality (e.g. symbol, image, audio) and vocabularies. Problem\nfeatures are given by a dataset, which is stored as a `TFRecord` file with\n`tensorflow.Example` protocol buffers. All\nproblems are imported in\n[`all_problems.py`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/all_problems.py)\nor are registered with `@registry.register_problem`. Run\n[`t2t-datagen`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/bin/t2t-datagen)\nto see the list of available problems and download them.\n\n### Models\n\n**`T2TModel`s** define the core tensor-to-tensor computation. They apply a\ndefault transformation to each input and output so that models may deal with\nmodality-independent tensors (e.g. embeddings at the input; and a linear\ntransform at the output to produce logits for a softmax over classes). All\nmodels are imported in the\n[`models` subpackage](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/models/__init__.py),\ninherit from [`T2TModel`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/utils/t2t_model.py),\nand are registered with\n[`@registry.register_model`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/utils/registry.py).\n\n### Hyperparameter Sets\n\n**Hyperparameter sets** are encoded in\n[`HParams`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/utils/hparam.py)\nobjects, and are registered with\n[`@registry.register_hparams`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/utils/registry.py).\nEvery model and problem has a `HParams`. A basic set of hyperparameters are\ndefined in\n[`common_hparams.py`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/layers/common_hparams.py)\nand hyperparameter set functions can compose other hyperparameter set functions.\n\n### Trainer\n\nThe **trainer** binary is the entrypoint for training, evaluation, and\ninference. Users can easily switch between problems, models, and hyperparameter\nsets by using the `--model`, `--problem`, and `--hparams_set` flags. Specific\nhyperparameters can be overridden with the `--hparams` flag. `--schedule` and\nrelated flags control local and distributed training/evaluation\n([distributed training documentation](https://github.com/tensorflow/tensor2tensor/tree/master/docs/distributed_training.md)).\n\n## Adding your own components\n\nT2T''s components are registered using a central registration mechanism that\nenables easily adding new ones and easily swapping amongst them by command-line\nflag. You can add your own components without editing the T2T codebase by\nspecifying the `--t2t_usr_dir` flag in `t2t-trainer`.\n\nYou can do so for models, hyperparameter sets, modalities, and problems. Please\ndo submit a pull request if your component might be useful to others.\n\nSee the [`example_usr_dir`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/test_data/example_usr_dir)\nfor an example user directory.\n\n## Adding a dataset\n\nTo add a new dataset, subclass\n[`Problem`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py)\nand register it with `@registry.register_problem`. See\n[`TranslateEndeWmt8k`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/translate_ende.py)\nfor an example. Also see the [data generators\nREADME](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/README.md).\n\n## Run on FloydHub\n\n[![Run on FloydHub](https://static.floydhub.com/button/button.svg)](https://floydhub.com/run)\n\nClick this button to open a [Workspace](https://blog.floydhub.com/workspaces/) on [FloydHub](https://www.floydhub.com/?utm_medium=readme&utm_source=tensor2tensor&utm_campaign=jul_2018). You can use the workspace to develop and test your code on a fully configured cloud GPU machine.\n\nTensor2Tensor comes preinstalled in the environment, you can simply open a [Terminal](https://docs.floydhub.com/guides/workspace/#using-terminal) and run your code.\n\n```bash\n# Test the quick-start on a Workspace''s Terminal with this command\nt2t-trainer \\n  --generate_data \\n  --data_dir=./t2t_data \\n  --output_dir=./t2t_train/mnist \\n  --problem=image_mnist \\n  --model=shake_shake \\n  --hparams_set=shake_shake_quick \\n  --train_steps=1000 \\n  --eval_steps=100\n```\n\nNote: Ensure compliance with the FloydHub [Terms of Service](https://www.floydhub.com/about/terms).\n\n## Papers\n\nWhen referencing Tensor2Tensor, please cite [this\npaper](https://arxiv.org/abs/1803.07416).\n\n```\n@article{tensor2tensor,\n  author    = {Ashish Vaswani and Samy Bengio and Eugene Brevdo and\n    Francois Chollet and Aidan N. Gomez and Stephan Gouws and Llion Jones and\n    \L{}ukasz Kaiser and Nal Kalchbrenner and Niki Parmar and Ryan Sepassi and\n    Noam Shazeer and Jakob Uszkoreit},\n  title     = {Tensor2Tensor for Neural Machine Translation},\n  journal   = {CoRR},\n  volume    = {abs/1803.07416},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1803.07416},\n}\n```\n\nTensor2Tensor was used to develop a number of state-of-the-art models\nand deep learning methods. Here we list some papers that were based on T2T\nfrom the start and benefited from its features and architecture in ways\ndescribed in the [Google Research Blog post introducing\nT2T](https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html).\n\n* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n* [Depthwise Separable Convolutions for Neural Machine\n   Translation](https://arxiv.org/abs/1706.03059)\n* [One Model To Learn Them All](https://arxiv.org/abs/1706.05137)\n* [Discrete Autoencoders for Sequence Models](https://arxiv.org/abs/1801.09797)\n* [Generating Wikipedia by Summarizing Long\n   Sequences](https://arxiv.org/abs/1801.10198)\n* [Image Transformer](https://arxiv.org/abs/1802.05751)\n* [Training Tips for the Transformer Model](https://arxiv.org/abs/1804.00247)\n* [Self-Attention with Relative Position Representations](https://arxiv.org/abs/1803.02155)\n* [Fast Decoding in Sequence Models using Discrete Latent Variables](https://arxiv.org/abs/1803.03382)\n* [Adafactor: Adaptive Learning Rates with Sublinear Memory Cost](https://arxiv.org/abs/1804.04235)\n* [Universal Transformers](https://arxiv.org/abs/1807.03819)\n* [Attending to Mathematical Language with Transformers](https://arxiv.org/abs/1812.02825)\n* [The Evolved Transformer](https://arxiv.org/abs/1901.11117)\n* [Model-Based Reinforcement Learning for Atari](https://arxiv.org/abs/1903.00374)\n* [VideoFlow: A Flow-Based Generative Model for Video](https://arxiv.org/abs/1903.01434)\n\n*NOTE: This is not an official Google product.*\n', '{"language":"Python","stars":16799,"forks":3698,"watchers":16799,"open_issues":590,"topics":["deep-learning","machine-learning","machine-translation","reinforcement-learning","tpu"],"default_branch":"master","size_kb":17508,"archived":true,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:google:trax","source_url":"https://github.com/google/trax"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"}]', NULL, 'Apache-2.0', 'approved', 80, '8ed5a7c64c5807665ce518fa1c8e18ed', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-tensorflow-tensor2tensor from https://github.com/tensorflow.png
Image converted to WebP: data/images/github-tensorflow-tensor2tensor.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-emilwallner-Screenshot-to-code', 'github--emilwallner--screenshot-to-code', 'Screenshot-to-code', 'emilwallner', '<img src="/README_images/screenshot-to-code.svg?raw=true" width="800px"> --- **A detailed tutorial covering the code in this repository:** Turning design mockups into code with deep learning. **Plug:** 👉 Check out my 60-page guide, No ML Degree, on how to land a machine learning job without a degree. The neural network is built in three iterations. Starting with a Hello World version, followed by the main neural network layers, and ending by training it to generalize. The models are based on...', '["cnn","cnn-keras","deep-learning","encoder-decoder","floydhub","jupyter","jupyter-notebook","keras","lstm","machine-learning","seq2seq","html"]', 'other', 16523, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/emilwallner/Screenshot-to-code","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<img src="/README_images/screenshot-to-code.svg?raw=true" width="800px">\n\n---\n\n**A detailed tutorial covering the code in this repository:** [Turning design mockups into code with deep learning](https://emilwallner.medium.com/how-you-can-train-an-ai-to-convert-your-design-mockups-into-html-and-css-cc7afd82fed4).\n\n**Plug:** 👉 Check out my 60-page guide, [No ML Degree](https://www.emilwallner.com/p/no-ml-degree), on how to land a machine learning job without a degree.\n\nThe neural network is built in three iterations. Starting with a Hello World version, followed by the main neural network layers, and ending by training it to generalize. \n\nThe models are based on Tony Beltramelli''s [pix2code](https://github.com/tonybeltramelli/pix2code), and inspired by Airbnb''s [sketching interfaces](https://airbnb.design/sketching-interfaces/), and Harvard''s [im2markup](https://github.com/harvardnlp/im2markup).\n\n**Note:** only the Bootstrap version can generalize on new design mock-ups. It uses 16 domain-specific tokens which are translated into HTML/CSS. It has a 97% accuracy. The best model uses a GRU instead of an LSTM. This version can be trained on a few GPUs. The raw HTML version has potential to generalize, but is still unproven and requires a significant amount of GPUs to train. The current model is also trained on a homogeneous and small dataset, thus it''s hard to tell how well it behaves on more complex layouts.\n\nDataset: https://github.com/tonybeltramelli/pix2code/tree/master/datasets\n\nA quick overview of the process: \n\n### 1) Give a design image to the trained neural network\n\n![Insert image](https://i.imgur.com/LDmoLLV.png)\n\n### 2) The neural network converts the image into HTML markup \n\n<img src="/README_images/html_display.gif?raw=true" width="800px">\n\n### 3) Rendered output\n\n![Screenshot](https://i.imgur.com/tEAfyZ8.png)\n\n\n## Installation\n\n### FloydHub\n\n[![Run on FloydHub](https://static.floydhub.com/button/button.svg)](https://floydhub.com/run?template=https://github.com/floydhub/pix2code-template)\n\nClick this button to open a [Workspace](https://blog.floydhub.com/workspaces/) on [FloydHub](https://www.floydhub.com/?utm_medium=readme&utm_source=pix2code&utm_campaign=aug_2018) where you will find the same environment and dataset used for the *Bootstrap version*. You can also find the trained models for testing.\n\n### Local\n``` bash\npip install keras tensorflow pillow h5py jupyter\n```\n```\ngit clone https://github.com/emilwallner/Screenshot-to-code.git\ncd Screenshot-to-code/\njupyter notebook\n```\nGo do the desired notebook, files that end with ''.ipynb''. To run the model, go to the menu then click on Cell > Run all\n\nThe final version, the Bootstrap version, is prepared with a small set to test run the model. If you want to try it with all the data, you need to download the data here: https://www.floydhub.com/emilwallner/datasets/imagetocode, and specify the correct ```dir_name```.\n\n## Folder structure\n\n``` bash\n  |  |-Bootstrap                           #The Bootstrap version\n  |  |  |-compiler                         #A compiler to turn the tokens to HTML/CSS (by pix2code)\n  |  |  |-resources											\n  |  |  |  |-eval_light                    #10 test images and markup\n  |  |-Hello_world                         #The Hello World version\n  |  |-HTML                                #The HTML version\n  |  |  |-Resources_for_index_file         #CSS,images and scripts to test index.html file\n  |  |  |-html                             #HTML files to train it on\n  |  |  |-images                           #Screenshots for training\n  |-readme_images                          #Images for the readme page\n```\n\n\n## Hello World\n<p align="center"><img src="/README_images/Hello_world_model.png?raw=true" width="400px"></p>\n\n\n## HTML\n<p align="center"><img src="/README_images/HTML_model.png?raw=true" width="400px"></p>\n\n\n## Bootstrap\n<p align="center"><img src="/README_images/Bootstrap_model.png?raw=true" width="400px"></p>\n\n## Model weights\n- [Bootstrap](https://www.floydhub.com/emilwallner/datasets/imagetocode) (The pre-trained model uses GRUs instead of LSTMs)\n- [HTML](https://www.floydhub.com/emilwallner/datasets/html_models)\n\n## Acknowledgments\n- Thanks to IBM for donating computing power through their PowerAI platform\n- The code is largely influenced by Tony Beltramelli''s pix2code paper. [Code](https://github.com/tonybeltramelli/pix2code) [Paper](https://arxiv.org/abs/1705.07962)\n- The structure and some of the functions are from Jason Brownlee''s [excellent tutorial](https://machinelearningmastery.com/develop-a-caption-generation-model-in-keras/)\n', '{"language":"HTML","stars":16523,"forks":1556,"watchers":16523,"open_issues":22,"topics":["cnn","cnn-keras","deep-learning","encoder-decoder","floydhub","jupyter","jupyter-notebook","keras","lstm","machine-learning","seq2seq"],"default_branch":"master","size_kb":51643,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:tonybeltramelli:pix2code","source_url":"https://github.com/tonybeltramelli/pix2code"},{"type":"has_code","target_id":"github:harvardnlp:im2markup","source_url":"https://github.com/harvardnlp/im2markup"},{"type":"has_code","target_id":"github:tonybeltramelli:pix2code","source_url":"https://github.com/tonybeltramelli/pix2code"},{"type":"has_code","target_id":"github:floydhub:pix2code-template","source_url":"https://github.com/floydhub/pix2code-template"},{"type":"has_code","target_id":"github:emilwallner:Screenshot-to-code.git","source_url":"https://github.com/emilwallner/Screenshot-to-code.git"},{"type":"has_code","target_id":"github:tonybeltramelli:pix2code","source_url":"https://github.com/tonybeltramelli/pix2code"}]', NULL, 'NOASSERTION', 'approved', 65, '945213fedb35f93605bde15ce17fd92e', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-emilwallner-Screenshot-to-code from https://github.com/emilwallner.png
Image converted to WebP: data/images/github-emilwallner-Screenshot-to-code.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-mrdbourke-pytorch-deep-learning', 'github--mrdbourke--pytorch-deep-learning', 'pytorch-deep-learning', 'mrdbourke', 'Welcome to the Zero to Mastery Learn PyTorch for Deep Learning course, the second best place to learn PyTorch on the internet (the first being the PyTorch documentation). * **Update April 2023:** New tutorial for PyTorch 2.0 is live! And because PyTorch 2.0 is an additive (new features) and backward-compatible release, all previous course materials will *still* work with PyTorch 2.0. <div align="center"> <a href="https://learnpytorch.io"> <img src="https://raw.githubusercontent.com/mrdbourke/...', '["deep-learning","machine-learning","pytorch","jupyter notebook"]', 'other', 16454, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/mrdbourke/pytorch-deep-learning","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Learn PyTorch for Deep Learning\n\nWelcome to the [Zero to Mastery Learn PyTorch for Deep Learning course](https://dbourke.link/ZTMPyTorch), the second best place to learn PyTorch on the internet (the first being the [PyTorch documentation](https://pytorch.org/docs/stable/index.html)).\n\n* **Update April 2023:** New [tutorial for PyTorch 2.0](https://www.learnpytorch.io/pytorch_2_intro/) is live! And because PyTorch 2.0 is an additive (new features) and backward-compatible release, all previous course materials will *still* work with PyTorch 2.0.\n\n<div align="center">\n    <a href="https://learnpytorch.io">\n        <img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/misc-pytorch-course-launch-cover-white-text-black-background.jpg" width=750 alt="pytorch deep learning by zero to mastery cover photo with different sections of the course">\n    </a>\n</div>\n\n## Contents of this page\n\n* [Course materials/outline](https://github.com/mrdbourke/pytorch-deep-learning#course-materialsoutline)\n* [About this course](https://github.com/mrdbourke/pytorch-deep-learning#about-this-course)\n* [Status](https://github.com/mrdbourke/pytorch-deep-learning#status) (the progress of the course creation)\n* [Log](https://github.com/mrdbourke/pytorch-deep-learning#log) (a log of the course material creation process)\n\n## Course materials/outline\n\n* 📖 **Online book version:** All of course materials are available in a readable online book at [learnpytorch.io](https://learnpytorch.io).\n* 🎥 **First five sections on YouTube:** Learn Pytorch in a day by watching the [first 25-hours of material](https://youtu.be/Z_ikDlimN6A).\n* 🔬 **Course focus:** code, code, code, experiment, experiment, experiment.\n* 🏃‍♂️ **Teaching style:** [https://sive.rs/kimo](https://sive.rs/kimo).\n* 🤔 **Ask a question:** See the [GitHub Discussions page](https://github.com/mrdbourke/pytorch-deep-learning/discussions) for existing questions/ask your own.\n\n| **Section** | **What does it cover?** | **Exercises & Extra-curriculum** | **Slides** |\n| ----- | ----- | ----- | ----- |\n| [00 - PyTorch Fundamentals](https://www.learnpytorch.io/00_pytorch_fundamentals/) | Many fundamental PyTorch operations used for deep learning and neural networks. | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/00_pytorch_fundamentals/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/00_pytorch_and_deep_learning_fundamentals.pdf) |\n| [01 - PyTorch Workflow](https://www.learnpytorch.io/01_pytorch_workflow/) | Provides an outline for approaching deep learning problems and building neural networks with PyTorch. | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/01_pytorch_workflow/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/01_pytorch_workflow.pdf) |\n| [02 - PyTorch Neural Network Classification](https://www.learnpytorch.io/02_pytorch_classification/) | Uses the PyTorch workflow from 01 to go through a neural network classification problem. | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/02_pytorch_classification/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/02_pytorch_classification.pdf) |\n| [03 - PyTorch Computer Vision](https://www.learnpytorch.io/03_pytorch_computer_vision/) | Let''s see how PyTorch can be used for computer vision problems using the same workflow from 01 & 02. | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/03_pytorch_computer_vision/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/03_pytorch_computer_vision.pdf) |\n| [04 - PyTorch Custom Datasets](https://www.learnpytorch.io/04_pytorch_custom_datasets/) | How do you load a custom dataset into PyTorch? Also we''ll be laying the foundations in this notebook for our modular code (covered in 05). | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/04_pytorch_custom_datasets/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/04_pytorch_custom_datasets.pdf) |\n| [05 - PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/) | PyTorch is designed to be modular, let''s turn what we''ve created into a series of Python scripts (this is how you''ll often find PyTorch code in the wild). | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/05_pytorch_going_modular/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/05_pytorch_going_modular.pdf) |\n| [06 - PyTorch Transfer Learning](https://www.learnpytorch.io/06_pytorch_transfer_learning/) | Let''s take a well performing pre-trained model and adjust it to one of our own problems. | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/06_pytorch_transfer_learning/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/06_pytorch_transfer_learning.pdf) |\n| [07 - Milestone Project 1: PyTorch Experiment Tracking](https://www.learnpytorch.io/07_pytorch_experiment_tracking/) | We''ve built a bunch of models... wouldn''t it be good to track how they''re all going? | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/07_pytorch_experiment_tracking.pdf) |\n| [08 - Milestone Project 2: PyTorch Paper Replicating](https://www.learnpytorch.io/08_pytorch_paper_replicating/) | PyTorch is the most popular deep learning framework for machine learning research, let''s see why by replicating a machine learning paper. | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/08_pytorch_paper_replicating/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/08_pytorch_paper_replicating.pdf) |\n| [09 - Milestone Project 3: Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/) | So we''ve built a working PyTorch model... how do we get it in the hands of others? Hint: deploy it to the internet. | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/09_pytorch_model_deployment/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/09_pytorch_model_deployment.pdf) |\n| [PyTorch Extra Resources](https://www.learnpytorch.io/pytorch_extra_resources/) | This course covers a large amount of PyTorch and deep learning but the field of machine learning is vast, inside here you''ll find recommended books and resources for: PyTorch and deep learning, ML engineering, NLP (natural language processing), time series data, where to find datasets and more. | - | - |\n| [PyTorch Cheatsheet](https://www.learnpytorch.io/pytorch_cheatsheet/) | A very quick overview of some of the main features of PyTorch plus links to various resources where more can be found in the course and in the PyTorch documentation. | - | - |\n| [A Quick PyTorch 2.0 Tutorial](https://www.learnpytorch.io/pytorch_2_intro/) | A fasssssst introduction to PyTorch 2.0, what''s new and how to get started along with resources to learn more. | - | - |\n\n## Status\n\nAll materials completed and videos published on Zero to Mastery!\n\nSee the project page for work-in-progress board - https://github.com/users/mrdbourke/projects/1 \n\n* **Total video count:** 321\n* **Done skeleton code for:** 00, 01, 02, 03, 04, 05, 06, 07, 08, 09\n* **Done annotations (text) for:** 00, 01, 02, 03, 04, 05, 06, 07, 08, 09\n* **Done images for:** 00, 01, 02, 03, 04, 05, 06, 07, 08, 09\n* **Done keynotes for:** 00, 01, 02, 03, 04, 05, 06, 07, 08, 09\n* **Done exercises and solutions for:** 00, 01, 02, 03, 04, 05, 06, 07, 08, 09\n\nSee the [log](https://github.com/mrdbourke/pytorch-deep-learning#log) for almost daily updates.\n\n## About this course\n\n### Who is this course for?\n\n**You:** Are a beginner in the field of machine learning or deep learning and would like to learn PyTorch.\n\n**This course:** Teaches you PyTorch and many machine learning concepts in a hands-on, code-first way.\n\nIf you already have 1-year+ experience in machine learning, this course may help but it is specifically designed to be beginner-friendly.\n\n### What are the prerequisites?\n\n1. 3-6 months coding Python.\n2. At least one beginner machine learning course (however this might be able to be skipped, resources are linked for many different topics).\n3. Experience using Jupyter Notebooks or Google Colab (though you can pick this up as we go along).\n4. A willingness to learn (most important).\n\nFor 1 & 2, I''d recommend the [Zero to Mastery Data Science and Machine Learning Bootcamp](https://dbourke.link/ZTMMLcourse), it''ll teach you the fundamentals of machine learning and Python (I''m biased though, I also teach that course).\n\n### How is the course taught?\n\nAll of the course materials are available for free in an online book at [learnpytorch.io](https://learnpytorch.io). If you like to read, I''d recommend going through the resources there.\n\nIf you prefer to learn via video, the course is also taught in apprenticeship-style format, meaning I write PyTorch code, you write PyTorch code.\n\nThere''s a reason the course motto''s include *if in doubt, run the code* and *experiment, experiment, experiment!*.\n\nMy whole goal is to help you to do one thing: learn machine learning by writing PyTorch code.\n\nThe code is all written via [Google Colab Notebooks](https://colab.research.google.com) (you could also use Jupyter Notebooks), an incredible free resource to experiment with machine learning.\n\n### What will I get if I finish the course?\n\nThere''s certificates and all that jazz if you go through the videos.\n\nBut certificates are meh.\n\nYou can consider this course a machine learning momentum builder.\n\nBy the end, you''ll have written hundreds of lines of PyTorch code.\n\nAnd will have been exposed to many of the most important concepts in machine learning.\n\nSo when you go to build your own machine learning projects or inspect a public machine learning project made with PyTorch, it''ll feel familiar and if it doesn''t, at least you''ll know where to look.\n\n### What will I build in the course?\n\nWe start with the barebone fundamentals of PyTorch and machine learning, so even if you''re new to machine learning you''ll be caught up to speed.\n\nThen we’ll explore more advanced areas including PyTorch neural network classification, PyTorch workflows, computer vision, custom datasets, experiment tracking, model deployment, and my personal favourite: transfer learning, a powerful technique for taking what one machine learning model has learned on another problem and applying it to your own!\n\nAlong the way, you’ll build three milestone projects surrounding an overarching project called FoodVision, a neural network computer vision model to classify images of food. \n\nThese milestone projects will help you practice using PyTorch to cover important machine learning concepts and create a portfolio you can show employers and say "here''s what I''ve done".\n\n### How do I get started?\n\nYou can read the materials on any device but this course is best viewed and coded along within a desktop browser.\n\nThe course uses a free tool called Google Colab. If you''ve got no experience with it, I''d go through the free [Introduction to Google Colab tutorial](https://colab.research.google.com/notebooks/basic_features_overview.ipynb) and then come back here.\n\nTo start:\n\n1. Click on one of the notebook or section links above like "[00. PyTorch Fundamentals](https://www.learnpytorch.io/00_pytorch_fundamentals/)". \n2. Click the "Open in Colab" button up the top.\n3. Press SHIFT+Enter a few times and see what happens.\n\n### My question isn''t answered \n\nPlease leave a [discussion](https://github.com/mrdbourke/pytorch-deep-learning/discussions) or send me an email directly: daniel (at) mrdbourke (dot) com.\n\n## Log\n\nAlmost daily updates of what''s happening.\n\n* 15 May 2023 - PyTorch 2.0 tutorial finished + videos added to ZTM/Udemy, see code: https://www.learnpytorch.io/pytorch_2_intro/\n* 13 Apr 2023 - update PyTorch 2.0 notebook\n* 30 Mar 2023 - update PyTorch 2.0 notebook with more info/clean code\n* 23 Mar 2023 - upgrade PyTorch 2.0 tutorial with annotations and images\n* 13 Mar 2023 - add starter code for PyTorch 2.0 tutorial \n* 18 Nov 2022 - add a reference for 3 most common errors in PyTorch + links to course sections for more: https://www.learnpytorch.io/pytorch_most_common_errors/ \n* 9 Nov 2022 - add PyTorch cheatsheet for a very quick overview of the main features of PyTorch + links to course sections: https://www.learnpytorch.io/pytorch_cheatsheet/ \n* 9 Nov 2022 - full course materials (300+ videos) are now live on Udemy! You can sign up here: https://www.udemy.com/course/pytorch-for-deep-learning/?couponCode=ZTMGOODIES7 (launch deal code valid for 3-4 days from this line)\n* 4 Nov 2022 - add a notebook for PyTorch Cheatsheet in `extras/` (a simple overview of many of the most important functionality of PyTorch)\n* 2 Oct 2022 - all videos for section 08 and 09 published (100+ videos for the last two sections)!\n* 30 Aug 2022 - recorded 15 videos for 09, total videos: 321, finished section 09 videos!!!! ... even bigger than 08!!\n* 29 Aug 2022 - recorded 16 videos for 09, total videos: 306\n* 28 Aug 2022 - recorded 11 videos for 09, total videos: 290\n* 27 Aug 2022 - recorded 16 videos for 09, total videos: 279\n* 26 Aug 2022 - add finishing touchs to notebook 09, add slides for 09, create solutions and exercises for 09\n* 25 Aug 2022 - add annotations and cleanup 09, remove TK''s, cleanup images, make slides for 09\n* 24 Aug 2022 - add annotations to 09, main takeaways, exercises and extra-curriculum done\n* 23 Aug 2022 - add annotations to 09, add plenty of images/slides\n* 22 Aug 2022 - add annotations to 09, start working on slides/images\n* 20 Aug 2022 - add annotations to 09 \n* 19 Aug 2022 - add annotations to 09, check out the awesome demos!\n* 18 Aug 2022 - add annotations to 09 \n* 17 Aug 2022 - add annotations to 09\n* 16 Aug 2022 - add annotations to 09\n* 15 Aug 2022 - add annotations to 09\n* 13 Aug 2022 - add annotations to 09\n* 12 Aug 2022 - add demo files for notebook 09 to `demos/`, start annotating notebook 09 with explainer text\n* 11 Aug 2022 - finish skeleton code for notebook 09, course finishes deploying 2x models, one for FoodVision Mini & one for (secret)\n* 10 Aug 2022 - add section for PyTorch Extra Resources (places to learn more about PyTorch/deep learning): https://www.learnpytorch.io/pytorch_extra_resources/ \n* 09 Aug 2022 - add more skeleton code to notebook 09\n* 08 Aug 2022 - create draft notebook for 09, end goal to deploy FoodVision Mini model and make it publically accessible\n* 05 Aug 2022 - recorded 11 videos for 08, total videos: 263, section 08 videos finished!... the biggest section so far\n* 04 Aug 2022 - recorded 13 videos for 08, total videos: 252\n* 03 Aug 2022 - recorded 3 videos for 08, total videos: 239\n* 02 Aug 2022 - recorded 12 videos for 08, total videos: 236\n* 30 July 2022 - recorded 11 videos for 08, total videos: 224\n* 29 July 2022 - add exercises + solutions for 08, see live walkthrough on YouTube: https://youtu.be/tjpW_BY8y3g\n* 28 July 2022 - add slides for 08\n* 27 July 2022 - cleanup much of 08, start on slides for 08, exercises and extra-curriculum next\n* 26 July 2022 - add annotations and images for 08\n* 25 July 2022 - add annotations for 08 \n* 24 July 2022 - launched first half of course (notebooks 00-04) in a single video (25+ hours!!!) on YouTube: https://youtu.be/Z_ikDlimN6A \n* 21 July 2022 - add annotations and images for 08\n* 20 July 2022 - add annotations and images for 08, getting so close! this is an epic section \n* 19 July 2022 - add annotations and images for 08\n* 15 July 2022 - add annotations and images for 08 \n* 14 July 2022 - add annotations for 08\n* 12 July 2022 - add annotations for 08, woo woo this is bigggg section! \n* 11 July 2022 - add annotations for 08 \n* 9 July 2022 - add annotations for 08\n* 8 July 2022 - add a bunch of annotations to 08\n* 6 July 2022 - course launched on ZTM Academy with videos for sections 00-07! 🚀 - https://dbourke.link/ZTMPyTorch \n* 1 July 2022 - add annotations and images for 08 \n* 30 June 2022 - add annotations for 08\n* 28 June 2022 - recorded 11 videos for section 07, total video count 213, all videos for section 07 complete!\n* 27 June 2022 - recorded 11 videos for section 07, total video count 202\n* 25 June 2022 - recreated 7 videos for section 06 to include updated APIs, total video count 191\n* 24 June 2022 - recreated 12 videos for section 06 to include updated APIs\n* 23 June 2022 - finish annotations for 07, add exercise template and solutions for 07 + video walkthrough on YouTube: https://youtu.be/cO_r2FYcAjU\n* 21 June 2022 - make 08 runnable end-to-end, add images and annotations for 07\n* 17 June 2022 - fix up 06, 07 v2 for upcoming torchvision version upgrade, add plenty of annotations to 08\n* 13 June 2022 - add notebook 08 first version, starting to replicate the Vision Transformer paper\n* 10 June 2022 - add annotations for 07 v2\n* 09 June 2022 - create 07 v2 for `torchvision` v0.13 (this will replace 07 v1 when `torchvision=0.13` is released)\n* 08 June 2022 - adapt 06 v2 for `torchvision` v0.13 (this will replace 06 v1 when `torchvision=0.13` is released)\n* 07 June 2022 - create notebook 06 v2 for upcoming `torchvision` v0.13 update (new transfer learning methods)\n* 04 June 2022 - add annotations for 07\n* 03 June 2022 - huuuuuuge amount of annotations added to 07 \n* 31 May 2022 - add a bunch of annotations for 07, make code runnable end-to-end\n* 30 May 2022 - record 4 videos for 06, finished section 06, onto section 07, total videos 186\n* 28 May 2022 - record 10 videos for 06, total videos 182\n* 24 May 2022 - add solutions and exercises for 06\n* 23 May 2022 - finished annotations and images for 06, time to do exercises and solutions \n* 22 May 2202 - add plenty of images to 06\n* 18 May 2022 - add plenty of annotations to 06\n* 17 May 2022 - added a bunch of annotations for section 06\n* 16 May 2022 - recorded 10 videos for section 05, finish videos for section 05 ✅\n* 12 May 2022 - added exercises and solutions for 05\n* 11 May 2022 - clean up part 1 and part 2 notebooks for 05, make slides for 05, start on exercises and solutions for 05\n* 10 May 2022 - huuuuge updates to the 05 section, see the website, it looks pretty: https://www.learnpytorch.io/05_pytorch_going_modular/ \n* 09 May 2022 - add a bunch of materials for 05, cleanup docs\n* 08 May 2022 - add a bunch of materials for 05\n* 06 May 2022 - continue making materials for 05\n* 05 May 2022 - update section 05 with headings/outline\n* 28 Apr 2022 - recorded 13 videos for 04, finished videos for 04, now to make materials for 05\n* 27 Apr 2022 - recorded 3 videos for 04\n* 26 Apr 2022 - recorded 10 videos for 04\n* 25 Apr 2022 - recorded 11 videos for 04\n* 24 Apr 2022 - prepared slides for 04\n* 23 Apr 2022 - recorded 6 videos for 03, finished videos for 03, now to 04 \n* 22 Apr 2022 - recorded 5 videos for 03\n* 21 Apr 2022 - recorded 9 videos for 03\n* 20 Apr 2022 - recorded 3 videos for 03\n* 19 Apr 2022 - recorded 11 videos for 03\n* 18 Apr 2022 - finish exercises/solutions for 04, added live-coding walkthrough of 04 exercises/solutions on YouTube: https://youtu.be/vsFMF9wqWx0\n* 16 Apr 2022 - finish exercises/solutions for 03, added live-coding walkthrough of 03 exercises/solutions on YouTube: https://youtu.be/_PibmqpEyhA\n* 14 Apr 2022 - add final images/annotations for 04, begin on exercises/solutions for 03 & 04\n* 13 Apr 2022 - add more images/annotations for 04\n* 3 Apr 2022 - add more annotations for 04\n* 2 Apr 2022 - add more annotations for 04\n* 1 Apr 2022 - add more annotations for 04\n* 31 Mar 2022 - add more annotations for 04\n* 29 Mar 2022 - add more annotations for 04\n* 27 Mar 2022 - starting to add annotations for 04\n* 26 Mar 2022 - making dataset for 04\n* 25 Mar 2022 - make slides for 03\n* 24 Mar 2022 - fix error for 03 not working in docs (finally)\n* 23 Mar 2022 - add more images for 03\n* 22 Mar 2022 - add images for 03\n* 20 Mar 2022 - add more annotations for 03\n* 18 Mar 2022 - add more annotations for 03\n* 17 Mar 2022 - add more annotations for 03 \n* 16 Mar 2022 - add more annotations for 03\n* 15 Mar 2022 - add more annotations for 03\n* 14 Mar 2022 - start adding annotations for notebook 03, see the work in progress here: https://www.learnpytorch.io/03_pytorch_computer_vision/\n* 12 Mar 2022 - recorded 12 videos for 02, finished section 02, now onto making materials for 03, 04, 05\n* 11 Mar 2022 - recorded 9 videos for 02\n* 10 Mar 2022 - recorded 10 videos for 02\n* 9 Mar 2022 - cleaning up slides/code for 02, getting ready for recording\n* 8 Mar 2022 - recorded 9 videos for section 01, finished section 01, now onto 02\n* 7 Mar 2022 - recorded 4 videos for section 01\n* 6 Mar 2022 - recorded 4 videos for section 01\n* 4 Mar 2022 - recorded 10 videos for section 01\n* 20 Feb 2022 - recorded 8 videos for section 00, finished section, now onto 01\n* 18 Feb 2022 - recorded 13 videos for section 00\n* 17 Feb 2022 - recorded 11 videos for section 00 \n* 16 Feb 2022 - added setup guide \n* 12 Feb 2022 - tidy up README with table of course materials, finish images and slides for 01\n* 10 Feb 2022 - finished slides and images for 00, notebook is ready for publishing: https://www.learnpytorch.io/00_pytorch_fundamentals/\n* 01-07 Feb 2022 - add annotations for 02, finished, still need images, going to work on exercises/solutions today \n* 31 Jan 2022 - start adding annotations for 02\n* 28 Jan 2022 - add exercies and solutions for 01\n* 26 Jan 2022 - lots more annotations to 01, should be finished tomorrow, will do exercises + solutions then too\n* 24 Jan 2022 - add a bunch of annotations to 01\n* 21 Jan 2022 - start adding annotations for 01 \n* 20 Jan 2022 - finish annotations for 00 (still need to add images), add exercises and solutions for 00\n* 19 Jan 2022 - add more annotations for 00\n* 18 Jan 2022 - add more annotations for 00\n* 17 Jan 2022 - back from holidays, adding more annotations to 00 \n* 10 Dec 2021 - start adding annotations for 00\n* 9 Dec 2021 - Created a website for the course ([learnpytorch.io](https://learnpytorch.io)) you''ll see updates posted there as development continues \n* 8 Dec 2021 - Clean up notebook 07, starting to go back through code and add annotations\n* 26 Nov 2021 - Finish skeleton code for 07, added four different experiments, need to clean up and make more straightforward\n* 25 Nov 2021 - clean code for 06, add skeleton code for 07 (experiment tracking)\n* 24 Nov 2021 - Update 04, 05, 06 notebooks for easier digestion and learning, each section should cover a max of 3 big ideas, 05 is now dedicated to turning notebook code into modular code \n* 22 Nov 2021 - Update 04 train and test functions to make more straightforward\n* 19 Nov 2021 - Added 05 (transfer learning) notebook, update custom data loading code in 04\n* 18 Nov 2021 - Updated vision code for 03 and added custom dataset loading code in 04\n* 12 Nov 2021 - Added a bunch of skeleton code to notebook 04 for custom dataset loading, next is modelling with custom data\n* 10 Nov 2021 - researching best practice for custom datasets for 04\n* 9 Nov 2021 - Update 03 skeleton code to finish off building CNN model, onto 04 for loading custom datasets\n* 4 Nov 2021 - Add GPU code to 03 + train/test loops + `helper_functions.py`\n* 3 Nov 2021 - Add basic start for 03, going to finish by end of week\n* 29 Oct 2021 - Tidied up skeleton code for 02, still a few more things to clean/tidy, created 03\n* 28 Oct 2021 - Finished skeleton code for 02, going to clean/tidy tomorrow, 03 next week\n* 27 Oct 2021 - add a bunch of code for 02, going to finish tomorrow/by end of week\n* 26 Oct 2021 - update 00, 01, 02 with outline/code, skeleton code for 00 & 01 done, 02 next\n* 23, 24 Oct 2021 - update 00 and 01 notebooks with more outline/code\n* 20 Oct 2021 - add v0 outlines for 01 and 02, add rough outline of course to README, this course will focus on less but better \n* 19 Oct 2021 - Start repo 🔥, add fundamentals notebook draft v0\n', '{"language":"Jupyter Notebook","stars":16454,"forks":4529,"watchers":16454,"open_issues":140,"topics":["deep-learning","machine-learning","pytorch"],"default_branch":"main","size_kb":626988,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning#course-materialsoutline"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning#about-this-course"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning#status"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning#log"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:users:mrdbourke","source_url":"https://github.com/users/mrdbourke"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning#log"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"}]', NULL, 'MIT', 'approved', 80, '9e0372c1dfe3172f374e820b44b070f4', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-mrdbourke-pytorch-deep-learning from https://github.com/mrdbourke.png
Image converted to WebP: data/images/github-mrdbourke-pytorch-deep-learning.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-instillai-TensorFlow-Course', 'github--instillai--tensorflow-course', 'TensorFlow-Course', 'instillai', '******************** _ ******************** .. image:: https://travis-ci.org/instillai/TensorFlow-Course.svg?branch=master :target: https://travis-ci.org/instillai/TensorFlow-Course .. image:: https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat :target: https://github.com/open-source-for-science/TensorFlow-Course/pulls .. image:: https://img.shields.io/twitter/follow/machinemindset.svg?label=Follow&style=social :target: https://twitter.com/machinemindset .. image:: h...', '["deep-learning","deep-learning-tutorial","python","tensorflow","jupyter notebook"]', 'other', 16345, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/instillai/TensorFlow-Course","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '\n\n********************\n`TensorFlow Course`_\n********************\n.. image:: https://travis-ci.org/instillai/TensorFlow-Course.svg?branch=master\n    :target: https://travis-ci.org/instillai/TensorFlow-Course\n.. image:: https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat\n    :target: https://github.com/open-source-for-science/TensorFlow-Course/pulls\n.. image:: https://img.shields.io/twitter/follow/machinemindset.svg?label=Follow&style=social\n    :target: https://twitter.com/machinemindset\n.. image:: https://zenodo.org/badge/151300862.svg\n   :target: https://zenodo.org/badge/latestdoi/151300862\n\n\nThis repository aims to provide simple and ready-to-use tutorials for TensorFlow.\nEach tutorial includes ``source code`` and most of them are associated with a ``documentation``.\n\n.. .. image:: _img/mainpage/TensorFlow_World.gif\n\n.. The links.\n.. _TensorFlow: https://www.tensorflow.org/install/\n.. _Wikipedia: https://en.wikipedia.org/wiki/TensorFlow/\n\n\n##########################################################################\nSponsorship\n##########################################################################\n\nTo support maintaining and upgrading this project, please kindly consider `Sponsoring the project developer <https://github.com/sponsors/astorfi/dashboard>`_.\n\nAny level of support is a great contribution here :heart:\n\n**Status:** *This project has been updated to **TensorFlow 2.3**.*\n\n\n#################\nTable of Contents\n#################\n.. contents::\n  :local:\n  :depth: 3\n\n\n==========================================\nDownload Free TensorFlow Roadmap EBook\n==========================================\n\n.. raw:: html\n\n   <div align="center">\n\n.. raw:: html\n\n <a href="http://www.machinelearningmindset.com/tensorflow-roadmap-ebook/" target="_blank">\n  <img width="710" height="500" align="center" src="https://github.com/machinelearningmindset/TensorFlow-Course/blob/master/_img/mainpage/booksubscribe.png"/>\n </a>\n\n.. raw:: html\n\n   </div>\n\n==========================================\nSlack Group\n==========================================\n\n.. raw:: html\n\n   <div align="center">\n\n.. raw:: html\n\n <a href="https://www.machinelearningmindset.com/slack-group/" target="_blank">\n  <img width="1033" height="350" align="center" src="https://github.com/machinelearningmindset/TensorFlow-Course/blob/master/_img/0-welcome/joinslack.png"/>\n </a>\n\n.. raw:: html\n\n   </div>\n\n\n\n~~~~~~~~~~~~~~~~~~~~~\nWhat is TensorFlow?\n~~~~~~~~~~~~~~~~~~~~~\nTensorFlow is an open-source software library for dataflow programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks. It is used for both research and production at Google often replacing its closed-source predecessor, DistBelief.\n\nTensorFlow was developed by the Google Brain team for internal Google use. It was released under the Apache 2.0 open source license on November 9, 2015.\n\n\n============\nMotivation\n============\n\nThere are different motivations for this open source project. TensorFlow (as we write this document) is one of / the best deep learning frameworks available. The question that should be asked is why has this repository been created when there are so many other tutorials about TensorFlow available on the web?\n\n~~~~~~~~~~~~~~~~~~~~~\nWhy use TensorFlow?\n~~~~~~~~~~~~~~~~~~~~~\n\nDeep Learning is in very high interest these days - there''s a crucial need for rapid and optimized implementations of the algorithms and architectures. TensorFlow is designed to facilitate this goal.\n\nThe strong advantage of TensorFlow is it flexibility in designing highly modular models which can also be a disadvantage for beginners since a lot of the pieces must be considered together when creating the model.\n\nThis issue has been facilitated as well by developing high-level APIs such as `Keras <https://keras.io/>`_ and `Slim <https://github.com/tensorflow/models/blob/031a5a4ab41170d555bc3e8f8545cf9c8e3f1b28/research/inception/inception/slim/README.md>`_ which abstract a lot of the pieces used in designing machine learning algorithms.\n\nThe interesting thing about TensorFlow is that **it can be found anywhere these days**. Lots of the researchers and developers are using it and *its community is growing at the speed of light*! So many issues can be dealt with easily since they''re usually the same issues that a lot of other people run into considering the large number of people involved in the TensorFlow community.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nWhat''s the point of this repository?\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n**Developing open source projects for the sake of just developing something is not the reason behind this effort**.\nConsidering the large number of tutorials that are being added to this large community, this repository has been created to break the jump-in and jump-out process that usually happens to most of the open source projects, **but why and how**?\n\nFirst of all, what''s the point of putting effort into something that most of the people won''t stop by and take a look? What''s the point of creating something that does not help anyone in the developers and researchers community? Why spend time for something that can easily be forgotten? But **how we try to do it?** Even up to this\nvery moment there are countless tutorials on TensorFlow whether on the model design or TensorFlow\nworkflow.\n\nMost of them are too complicated or suffer from a lack of documentation. There are only a few available tutorials which are concise and well-structured and provide enough insight for their specific implemented models.\n\nThe goal of this project is to help the community with structured tutorials and simple and optimized code implementations to provide better insight about how to use TensorFlow *quick and effectively*.\n\nIt is worth noting that, **the main goal of this project is to provide well-documented tutorials and less-complicated code**!\n\n=================================================\nTensorFlow Installation and Setup the Environment\n=================================================\n\n\n.. image:: _img/mainpage/installation-logo.gif\n   :height: 100px\n   :width: 200 px\n   :scale: 50 %\n   :alt: alternate text\n   :align: right\n   :target: docs/tutorials/installation\n\n.. _TensorFlow Installation: https://www.tensorflow.org/install\n\nIn order to install TensorFlow please refer to the following link:\n\n  * `TensorFlow Installation`_\n\n\n.. image:: _img/mainpage/installation.gif\n    :target: https://www.tensorflow.org/install\n\nThe virtual environment installation is recommended in order to prevent package conflict and having the capacity to customize the working environment.\n\n====================\nTensorFlow Tutorials\n====================\n\nThe tutorials in this repository are partitioned into relevant categories.\n\n==========================\n\n~~~~~~~~\nWarm-up\n~~~~~~~~\n\n.. image:: _img/mainpage/welcome.gif\n   :height: 100px\n   :width: 200 px\n   :scale: 50 %\n   :alt: alternate text\n   :align: right\n\n\n.. _colab: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/0-welcome/welcome.ipynb\n.. _Documentationcnnwelcome: docs/tutorials/0-welcome\n.. _ipythonwelcome: codes/ipython/0-welcome/welcome.ipynb\n.. _pythonwelcome: https://github.com/instillai/TensorFlow-Course/blob/master/codes/python/0-welcome/welcome.py\n.. _videowelcome: https://youtu.be/xd0DVygHlNE\n\n\n.. |Welcome| image:: https://colab.research.google.com/assets/colab-badge.svg\n   :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/0-welcome/welcome.ipynb\n\n.. |youtubeim| image:: _img/mainpage/YouTube.png\n  :target: https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/YouTube.png\n\n\n+----+---------------------+--------------------------+------------------------------------------------------------------------+-------------------------------------------+\n| #  |       topic         |          Run             |  Source Code                                                           |  Media                                    |\n+====+=====================+==========================+========================================================================+===========================================+\n| 1  | Start-up            |       |Welcome|          | `Notebook <ipythonwelcome_>`_  / `Python <pythonwelcome_>`_            | `Video Tutorial <videowelcome_>`_         |\n+----+---------------------+--------------------------+------------------------------------------------------------------------+-------------------------------------------+\n\n==========================\n\n~~~~~~\nBasics\n~~~~~~\n\n.. raw:: html\n\n   <div align="left">\n\n.. raw:: html\n\n <a href="https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/basics.gif" target="_blank">\n  <img width="250" height="250" align="center" src="https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/basics.gif"/>\n </a>\n\n.. raw:: html\n\n   </div>\n\n.. raw:: html\n\n   <br>\n\n\n\n.. _ipythontensors: codes/ipython/1-basics/tensors.ipynb\n.. _pythontensors: codes/python/1-basics/tensors.py\n.. _videotensors: https://youtu.be/Od-VvnYUbFw\n.. |Tensors| image:: https://colab.research.google.com/assets/colab-badge.svg\n  :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/1-basics/tensors.ipynb\n\n.. _ipythonad: codes/ipython/1-basics/automatic_differentiation.ipynb\n.. _pythonad: codes/python/1-basics/automatic_differentiation.py\n.. _videoad: https://youtu.be/l-MGydWW-UE\n.. |AD| image:: https://colab.research.google.com/assets/colab-badge.svg\n  :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/1-basics/automatic_differentiation.ipynb\n\n.. _ipythongraphs: codes/ipython/1-basics/graph.ipynb\n.. _pythongraphs: codes/python/1-basics/graph.py\n.. _videographs: https://youtu.be/P9xA1s6AUNk\n.. |graphs| image:: https://colab.research.google.com/assets/colab-badge.svg\n  :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/1-basics/graph.ipynb\n\n\n.. _ipythonmodels: codes/ipython/1-basics/models.ipynb\n.. _pythonmodels: codes/python/1-basics/models.py\n.. _videomodels: https://youtu.be/WnlUE04REOY\n.. |models| image:: https://colab.research.google.com/assets/colab-badge.svg\n  :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/1-basics/models.ipynb\n\n\n\n+----+-----------------------------------+--------------------------+------------------------------------------------------------------------+-----------------------------------------+\n| #  |       topic                       |          Run             |  Source Code                                                           |        Media                            |\n+====+===================================+==========================+========================================================================+=========================================+\n| 1  | Tensors                           |       |Tensors|          | `Notebook <ipythontensors_>`_  / `Python <pythontensors_>`_            | `Video Tutorial <videotensors_>`_       |\n+----+-----------------------------------+--------------------------+------------------------------------------------------------------------+-----------------------------------------+\n| 2  | Automatic Differentiation         |       |AD|               | `Notebook <ipythonad_>`_  / `Python <pythonad_>`_                      | `Video Tutorial <videoad_>`_            |\n+----+-----------------------------------+--------------------------+------------------------------------------------------------------------+-----------------------------------------+\n| 3  | Introduction to Graphs            |       |graphs|           | `Notebook <ipythongraphs_>`_ / `Python <pythongraphs_>`_               | `Video Tutorial <videographs_>`_        |\n+----+-----------------------------------+--------------------------+------------------------------------------------------------------------+-----------------------------------------+\n| 4  | TensorFlow Models                 |       |models|           | `Notebook <ipythonmodels_>`_  / `Python <pythonmodels_>`_              | `Video Tutorial <videomodels_>`_        |\n+----+-----------------------------------+--------------------------+------------------------------------------------------------------------+-----------------------------------------+\n\n==========================\n\n~~~~~~~~~~~~~~~~~~~~~~\nBasic Machine Learning\n~~~~~~~~~~~~~~~~~~~~~~\n\n.. raw:: html\n\n   <div align="left">\n\n.. raw:: html\n\n <a href="https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/basicmodels.gif" target="_blank">\n  <img width="250" height="250" align="center" src="https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/basicmodels.gif"/>\n </a>\n\n.. raw:: html\n\n   </div>\n\n.. raw:: html\n\n   <br>\n\n.. .. image:: _img/mainpage/basicmodels.gif\n..    :height: 100px\n..    :width: 200 px\n..    :scale: 50 %\n..    :alt: alternate text\n..    :align: right\n\n\n.. _ipythonlinearreg: codes/ipython/basics_in_machine_learning/linearregression.ipynb\n.. _pythonlinearreg: codes/python/basics_in_machine_learning/linearregression.py\n.. _tutoriallinearreg: https://www.machinelearningmindset.com/linear-regression-with-tensorflow/\n.. _videoinearreg: https://youtu.be/2RTBBiKKuLI\n\n.. _tutorialdataaugmentation: https://www.machinelearningmindset.com/data-augmentation-with-tensorflow/\n.. _ipythondataaugmentation: https://github.com/instillai/TensorFlow-Course/blob/master/codes/ipython/basics_in_machine_learning/dataaugmentation.ipynb\n.. _pythondataaugmentation: https://github.com/instillai/TensorFlow-Course/blob/master/codes/python/basics_in_machine_learning/dataaugmentation.py\n.. _videodataaugmentation: https://youtu.be/HbzR2snHJF0\n\n.. |lr| image:: https://colab.research.google.com/assets/colab-badge.svg\n  :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/basics_in_machine_learning/linearregression.ipynb\n.. |da| image:: https://colab.research.google.com/assets/colab-badge.svg\n  :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/basics_in_machine_learning/dataaugmentation.ipynb\n\n\n+----+-----------------------------------+--------------------------+------------------------------------------------------------------------------------+----------------------------------------------+----------------------------------------------+\n| #  |       topic                       |          Run             |  Source Code                                                                       |  More                                        |           Media                              |\n+====+===================================+==========================+====================================================================================+==============================================+==============================================+\n| 1  | Linear Regression                 |       |lr|               | `Notebook <ipythonlinearreg_>`_  / `Python <pythonlinearreg_>`_                    | `Tutorial <tutoriallinearreg_>`_             | `Video Tutorial <videoinearreg_>`_           |\n+----+-----------------------------------+--------------------------+------------------------------------------------------------------------------------+----------------------------------------------+----------------------------------------------+\n| 2  | Data Augmentation                 |       |da|               | `Notebook <ipythondataaugmentation_>`_ / `Python <pythondataaugmentation_>`_       | `Tutorial <tutorialdataaugmentation_>`_      | `Video Tutorial <videodataaugmentation_>`_   |\n+----+-----------------------------------+--------------------------+------------------------------------------------------------------------------------+----------------------------------------------+----------------------------------------------+\n\n\n\n.. +----+----------------------------+----------------------------------------------------------------------------------------+----------------------------------------------+\n\n==========================\n\n~~~~~~~~~~~~~~~~\nNeural Networks\n~~~~~~~~~~~~~~~~\n\n.. raw:: html\n\n   <div align="left">\n\n.. raw:: html\n\n <a href="https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/CNNs.png" target="_blank">\n  <img width="600" height="180" align="center" src="https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/CNNs.png"/>\n </a>\n\n.. raw:: html\n\n   </div>\n\n.. raw:: html\n\n    <br>\n\n\n.. _ipythonmlp: https://github.com/instillai/TensorFlow-Course/blob/master/codes/ipython/neural_networks/mlp.ipynb\n.. _pythonmlp: https://github.com/instillai/TensorFlow-Course/blob/master/codes/python/neural_networks/mlp.py\n.. _videomlp: https://youtu.be/w20efZqSK2Y\n\n.. _ipythoncnn: https://github.com/instillai/TensorFlow-Course/blob/master/codes/ipython/neural_networks/CNNs.ipynb\n.. _pythoncnn: https://github.com/instillai/TensorFlow-Course/blob/master/codes/python/neural_networks/cnns.py\n.. _videocnn: https://youtu.be/WVifZBCRz8g\n\n\n.. |mlp| image:: https://colab.research.google.com/assets/colab-badge.svg\n :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/neural_networks/mlp.ipynb\n.. |cnn| image:: https://colab.research.google.com/assets/colab-badge.svg\n :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/neural_networks/CNNs.ipynb\n\n\n+----+------------------------------------------+--------------------------+------------------------------------------------------+------------------------------------+\n| #  |       topic                              |          Run             |  Source Code                                         |            Media                   |\n+====+==========================================+==========================+======================================================+====================================+\n| 1  |  *Multi Layer Perceptron*                |       |mlp|              | `Notebook <ipythonmlp_>`_ / `Python <pythonmlp_>`_   | `Video Tutorial <videomlp_>`_      |\n+----+------------------------------------------+--------------------------+------------------------------------------------------+------------------------------------+\n| 2  |  *Convolutional Neural Networks*         |       |cnn|              | `Notebook <ipythoncnn_>`_ / `Python <pythoncnn_>`_   | `Video Tutorial <videocnn_>`_      |\n+----+------------------------------------------+--------------------------+------------------------------------------------------+------------------------------------+\n\n==========================\n\n~~~~~~~~~~~~~~~~\nAdvanced\n~~~~~~~~~~~~~~~~\n\n\n.. raw:: html\n\n   <div align="left">\n\n.. raw:: html\n\n <a href="https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/Build.png" target="_blank">\n  <img width="180" height="180" align="center" src="https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/Build.png"/>\n </a>\n\n.. raw:: html\n\n   </div>\n\n.. raw:: html\n\n    <br>\n\n\n\n\n.. _ipythoncustomtr: https://github.com/instillai/TensorFlow-Course/blob/master/codes/ipython/advanced/custom_training.ipynb\n.. _pythoncustomtr: https://github.com/instillai/TensorFlow-Course/blob/master/codes/python/advanced/custom_training.py\n.. _videocustomtr: https://youtu.be/z5gcabfyPfA\n\n.. _ipythondgenerator: https://github.com/instillai/TensorFlow-Course/blob/master/codes/ipython/advanced/dataset_generator.ipynb\n.. _pythondgenerator: https://github.com/instillai/TensorFlow-Course/blob/master/codes/python/advanced/dataset_generator.py\n.. _videodgenerator: https://youtu.be/-YsgMdDPu3g\n\n.. _ipythontfrecords: https://github.com/instillai/TensorFlow-Course/blob/master/codes/ipython/advanced/tfrecords.ipynb\n.. _pythontfrecords: https://github.com/instillai/TensorFlow-Course/blob/master/codes/python/advanced/tfrecords.py\n.. _videotfrecords: https://youtu.be/zqavy_5QMk8\n\n\n.. |ctraining| image:: https://colab.research.google.com/assets/colab-badge.svg\n :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/advanced/custom_training.ipynb\n\n.. |dgenerator| image:: https://colab.research.google.com/assets/colab-badge.svg\n  :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/advanced/dataset_generator.ipynb\n\n.. |tfrecords| image:: https://colab.research.google.com/assets/colab-badge.svg\n  :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/advanced/tfrecords.ipynb\n\n\n+----+------------------------------------------+--------------------------+--------------------------------------------------------------------+----------------------------------------+\n| #  |       topic                              |          Run             |  Source Code                                                       |           Media                        |\n+====+==========================================+==========================+====================================================================+========================================+\n| 1  |  *Custom Training*                       |       |ctraining|        | `Notebook <ipythoncustomtr_>`_ / `Python <pythoncustomtr_>`_       | `Video Tutorial <videocustomtr_>`_     |\n+----+------------------------------------------+--------------------------+--------------------------------------------------------------------+----------------------------------------+\n| 2  |  *Dataset Generator*                     |       |dgenerator|       | `Notebook <ipythondgenerator_>`_ / `Python <pythondgenerator_>`_   | `Video Tutorial <videodgenerator_>`_   |\n+----+------------------------------------------+--------------------------+--------------------------------------------------------------------+----------------------------------------+\n| 3  |  *Create TFRecords*                      |       |tfrecords|        | `Notebook <ipythontfrecords_>`_ / `Python <pythontfrecords_>`_     | `Video Tutorial <videotfrecords_>`_    |\n+----+------------------------------------------+--------------------------+--------------------------------------------------------------------+----------------------------------------+\n\n\n\n=====================\nSome Useful Tutorials\n=====================\n\n  * `TensorFlow Examples <https://github.com/aymericdamien/TensorFlow-Examples>`_ - TensorFlow tutorials and code examples for beginners\n  * `Sungjoon''s TensorFlow-101 <https://github.com/sjchoi86/Tensorflow-101>`_ - TensorFlow tutorials written in Python with Jupyter Notebook\n  * `Terry Um’s TensorFlow Exercises <https://github.com/terryum/TensorFlow_Exercises>`_ - Re-create the codes from other TensorFlow examples\n  * `Classification on time series <https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition>`_ - Recurrent Neural Network classification in TensorFlow with LSTM on cellphone sensor data\n\n\n=============\nContributing\n=============\n\nWhen contributing to this repository, please first discuss the change you wish to make via issue,\nemail, or any other method with the owners of this repository before making a change. *For typos, please\ndo not create a pull request. Instead, declare them in issues or email the repository owner*.\n\nPlease note we have a code of conduct, please follow it in all your interactions with the project.\n\n~~~~~~~~~~~~~~~~~~~~\nPull Request Process\n~~~~~~~~~~~~~~~~~~~~\n\nPlease consider the following criterions in order to help us in a better way:\n\n  * The pull request is mainly expected to be a code script suggestion or improvement.\n  * Please do NOT change the ipython files. Instead, change the corresponsing PYTHON files.\n  * A pull request related to non-code-script sections is expected to make a significant difference in the documentation. Otherwise, it is expected to be announced in the issues section.\n  * Ensure any install or build dependencies are removed before the end of the layer when doing a build and creating a pull request.\n  * Add comments with details of changes to the interface, this includes new environment variables, exposed ports, useful file locations and container parameters.\n  * You may merge the Pull Request in once you have the sign-off of at least one other developer, or if you do not have permission to do that, you may request the owner to merge it for you if you believe all checks are passed.\n\n\n~~~~~~~~~~~\nFinal Note\n~~~~~~~~~~~\n\nWe are looking forward to your kind feedback. Please help us to improve this open source project and make our work better.\nFor contribution, please create a pull request and we will investigate it promptly. Once again, we appreciate\nyour kind feedback and elaborate code inspections.\n\n========================\nDevelopers\n========================\n\n\n**Company**: Instill AI [`Website\n<https://instillai.com/>`_]\n\n**Creator**: Machine Learning Mindset [`Blog\n<https://machinelearningmindset.com/blog/>`_, `GitHub\n<https://github.com/machinelearningmindset>`_, `Twitter\n<https://twitter.com/machinemindset>`_]\n\n**Developer**: Amirsina Torfi [`GitHub\n<https://github.com/astorfi>`_, `Personal Website\n<https://astorfi.github.io/>`_, `Linkedin\n<https://www.linkedin.com/in/amirsinatorfi/>`_ ]\n', '{"language":"Jupyter Notebook","stars":16345,"forks":3173,"watchers":16345,"open_issues":2,"topics":["deep-learning","deep-learning-tutorial","python","tensorflow"],"default_branch":"master","size_kb":21714,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:open-source-for-science:TensorFlow-Course","source_url":"https://github.com/open-source-for-science/TensorFlow-Course"},{"type":"has_code","target_id":"github:sponsors:astorfi","source_url":"https://github.com/sponsors/astorfi"},{"type":"has_code","target_id":"github:machinelearningmindset:TensorFlow-Course","source_url":"https://github.com/machinelearningmindset/TensorFlow-Course"},{"type":"has_code","target_id":"github:machinelearningmindset:TensorFlow-Course","source_url":"https://github.com/machinelearningmindset/TensorFlow-Course"},{"type":"has_code","target_id":"github:tensorflow:models","source_url":"https://github.com/tensorflow/models"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples>`_","source_url":"https://github.com/aymericdamien/TensorFlow-Examples>`_"},{"type":"has_code","target_id":"github:sjchoi86:Tensorflow-101>`_","source_url":"https://github.com/sjchoi86/Tensorflow-101>`_"},{"type":"has_code","target_id":"github:terryum:TensorFlow_Exercises>`_","source_url":"https://github.com/terryum/TensorFlow_Exercises>`_"},{"type":"has_code","target_id":"github:guillaume-chevalier:LSTM-Human-Activity-Recognition>`_","source_url":"https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition>`_"}]', NULL, 'MIT', 'approved', 80, '5a06479f67d9e266ed030ab9458c39de', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-instillai-TensorFlow-Course from https://github.com/instillai.png
Image converted to WebP: data/images/github-instillai-TensorFlow-Course.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-bharathgs-Awesome-pytorch-list', 'github--bharathgs--awesome-pytorch-list', 'Awesome-pytorch-list', 'bharathgs', 'Awesome-Pytorch-list ======================== !pytorch-logo-dark <p align="center"> <img src="https://img.shields.io/badge/stars-12400+-brightgreen.svg?style=flat"/> <img src="https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat"> </p> - Pytorch & related libraries - NLP & Speech Processing - Computer Vision - Probabilistic/Generative Libraries - Other libraries - Tutorials, books & examples - Paper implementations - Talks & Conferences - Pytorch elsewhere 1. pytorch:...', '["awesome","awesome-list","computer-vision","cv","data-science","deep-learning","facebook","machine-learning","natural-language-processing","neural-network","nlp","nlp-library","papers","probabilistic-programming","python","pytorch","pytorch-model","pytorch-tutorials","tutorials","utility-library"]', 'other', 16284, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/bharathgs/Awesome-pytorch-list","fetched_at":"2025-12-08T10:39:52.045Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', 'Awesome-Pytorch-list\n========================\n\n![pytorch-logo-dark](https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png)\n\n<p align="center">\n	<img src="https://img.shields.io/badge/stars-12400+-brightgreen.svg?style=flat"/>\n	<img src="https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat">\n</p>\n\n## Contents\n- [Pytorch & related libraries](#pytorch--related-libraries)\n  - [NLP & Speech Processing](#nlp--Speech-Processing)\n  - [Computer Vision](#cv)\n  - [Probabilistic/Generative Libraries](#probabilisticgenerative-libraries)\n  - [Other libraries](#other-libraries)\n- [Tutorials, books & examples](#tutorials-books--examples)\n- [Paper implementations](#paper-implementations)\n- [Talks & Conferences](#talks--conferences)\n- [Pytorch elsewhere](#pytorch-elsewhere)\n\n## Pytorch & related libraries\n\n1. [pytorch](http://pytorch.org): Tensors and Dynamic neural networks in Python with strong GPU acceleration.\n2. [Captum](https://github.com/pytorch/captum): Model interpretability and understanding for PyTorch.\n\n### NLP & Speech Processing:\n\n1. [pytorch text](https://github.com/pytorch/text): Torch text related contents.  \n2. [pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq): A framework for sequence-to-sequence (seq2seq) models implemented in PyTorch.  \n3. [anuvada](https://github.com/Sandeep42/anuvada): Interpretable Models for NLP using PyTorch.\n4. [audio](https://github.com/pytorch/audio): simple audio I/O for pytorch.\n5. [loop](https://github.com/facebookresearch/loop): A method to generate speech across multiple speakers\n6. [fairseq-py](https://github.com/facebookresearch/fairseq-py): Facebook AI Research Sequence-to-Sequence Toolkit written in Python.\n7. [speech](https://github.com/awni/speech): PyTorch ASR Implementation.\n8. [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py): Open-Source Neural Machine Translation in PyTorch http://opennmt.net \n9. [neuralcoref](https://github.com/huggingface/neuralcoref): State-of-the-art coreference resolution based on neural nets and spaCy huggingface.co/coref\n10. [sentiment-discovery](https://github.com/NVIDIA/sentiment-discovery): Unsupervised Language Modeling at scale for robust sentiment classification.\n11. [MUSE](https://github.com/facebookresearch/MUSE): A library for Multilingual Unsupervised or Supervised word Embeddings\n12. [nmtpytorch](https://github.com/lium-lst/nmtpytorch): Neural Machine Translation Framework in PyTorch.\n13. [pytorch-wavenet](https://github.com/vincentherrmann/pytorch-wavenet): An implementation of WaveNet with fast generation\n14. [Tacotron-pytorch](https://github.com/soobinseo/Tacotron-pytorch): Tacotron: Towards End-to-End Speech Synthesis.\n15. [AllenNLP](https://github.com/allenai/allennlp): An open-source NLP research library, built on PyTorch.\n16. [PyTorch-NLP](https://github.com/PetrochukM/PyTorch-NLP): Text utilities and datasets for PyTorch pytorchnlp.readthedocs.io\n17. [quick-nlp](https://github.com/outcastofmusic/quick-nlp): Pytorch NLP library based on FastAI. \n18. [TTS](https://github.com/mozilla/TTS): Deep learning for Text2Speech\n19. [LASER](https://github.com/facebookresearch/LASER): Language-Agnostic SEntence Representations\n20. [pyannote-audio](https://github.com/pyannote/pyannote-audio): Neural building blocks for speaker diarization: speech activity detection, speaker change detection, speaker embedding\n21. [gensen](https://github.com/Maluuba/gensen): Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning.\n22. [translate](https://github.com/pytorch/translate): Translate - a PyTorch Language Library.\n23. [espnet](https://github.com/espnet/espnet): End-to-End Speech Processing Toolkit espnet.github.io/espnet\n24. [pythia](https://github.com/facebookresearch/pythia): A software suite for Visual Question Answering\n25. [UnsupervisedMT](https://github.com/facebookresearch/UnsupervisedMT): Phrase-Based & Neural Unsupervised Machine Translation.\n26. [jiant](https://github.com/jsalt18-sentence-repl/jiant): The jiant sentence representation learning toolkit. \n27. [BERT-PyTorch](https://github.com/codertimo/BERT-pytorch): Pytorch implementation of Google AI''s 2018 BERT, with simple annotation\n28. [InferSent](https://github.com/facebookresearch/InferSent): Sentence embeddings (InferSent) and training code for NLI.\n29. [uis-rnn](https://github.com/google/uis-rnn):This is the library for the Unbounded Interleaved-State Recurrent Neural Network (UIS-RNN) algorithm, corresponding to the paper Fully Supervised Speaker Diarization. arxiv.org/abs/1810.04719 \n30. [flair](https://github.com/zalandoresearch/flair): A very simple framework for state-of-the-art Natural Language Processing (NLP)\n31. [pytext](https://github.com/facebookresearch/pytext): A natural language modeling framework based on PyTorch fb.me/pytextdocs\n32. [voicefilter](https://github.com/mindslab-ai/voicefilter): Unofficial PyTorch implementation of Google AI''s VoiceFilter system http://swpark.me/voicefilter\n33. [BERT-NER](https://github.com/kamalkraj/BERT-NER): Pytorch-Named-Entity-Recognition-with-BERT. \n34. [transfer-nlp](https://github.com/feedly/transfer-nlp): NLP library designed for flexible research and development\n35. [texar-pytorch](https://github.com/asyml/texar-pytorch): Toolkit for Machine Learning and Text Generation, in PyTorch texar.io\n36. [pytorch-kaldi](https://github.com/mravanelli/pytorch-kaldi): pytorch-kaldi is a project for developing state-of-the-art DNN/RNN hybrid speech recognition systems. The DNN part is managed by pytorch, while feature extraction, label computation, and decoding are performed with the kaldi toolkit.\n37. [NeMo](https://github.com/NVIDIA/NeMo): Neural Modules: a toolkit for conversational AI nvidia.github.io/NeMo\n38. [pytorch-struct](https://github.com/harvardnlp/pytorch-struct): A library of vectorized implementations of core structured prediction algorithms (HMM, Dep Trees, CKY, ..,)\n39. [espresso](https://github.com/freewym/espresso): Espresso: A Fast End-to-End Neural Speech Recognition Toolkit\n40. [transformers](https://github.com/huggingface/transformers): huggingface Transformers: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch. huggingface.co/transformers\n41. [reformer-pytorch](https://github.com/lucidrains/reformer-pytorch): Reformer, the efficient Transformer, in Pytorch\n42. [torch-metrics](https://github.com/enochkan/torch-metrics): Metrics for model evaluation in pytorch\n43. [speechbrain](https://github.com/speechbrain/speechbrain): SpeechBrain is an open-source and all-in-one speech toolkit based on PyTorch.\n44. [Backprop](https://github.com/backprop-ai/backprop): Backprop makes it simple to use, finetune, and deploy state-of-the-art ML models.\n\n### CV:\n\n1. [pytorch vision](https://github.com/pytorch/vision): Datasets, Transforms and Models specific to Computer Vision.\n2. [pt-styletransfer](https://github.com/tymokvo/pt-styletransfer): Neural style transfer as a class in PyTorch.\n3. [OpenFacePytorch](https://github.com/thnkim/OpenFacePytorch):  PyTorch module to use OpenFace''s nn4.small2.v1.t7 model\n4. [img_classification_pk_pytorch](https://github.com/felixgwu/img_classification_pk_pytorch): Quickly comparing your image classification models with the state-of-the-art models (such as DenseNet, ResNet, ...)\n5. [SparseConvNet](https://github.com/facebookresearch/SparseConvNet): Submanifold sparse convolutional networks.\n6. [Convolution_LSTM_pytorch](https://github.com/automan000/Convolution_LSTM_pytorch): A multi-layer convolution LSTM module\n7. [face-alignment](https://github.com/1adrianb/face-alignment): :fire: 2D and 3D Face alignment library build using pytorch adrianbulat.com\n8. [pytorch-semantic-segmentation](https://github.com/ZijunDeng/pytorch-semantic-segmentation): PyTorch for Semantic Segmentation.\n9. [RoIAlign.pytorch](https://github.com/longcw/RoIAlign.pytorch): This is a PyTorch version of RoIAlign. This implementation is based on crop_and_resize and supports both forward and backward on CPU and GPU.\n10. [pytorch-cnn-finetune](https://github.com/creafz/pytorch-cnn-finetune): Fine-tune pretrained Convolutional Neural Networks with PyTorch.\n11. [detectorch](https://github.com/ignacio-rocco/detectorch): Detectorch - detectron for PyTorch\n12. [Augmentor](https://github.com/mdbloice/Augmentor): Image augmentation library in Python for machine learning. http://augmentor.readthedocs.io\n13. [s2cnn](https://github.com/jonas-koehler/s2cnn): \nThis library contains a PyTorch implementation of the SO(3) equivariant CNNs for spherical signals (e.g. omnidirectional cameras, signals on the globe)\n14. [TorchCV](https://github.com/donnyyou/torchcv): A PyTorch-Based Framework for Deep Learning in Computer Vision. \n15. [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark): Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch.\n16. [image-classification-mobile](https://github.com/osmr/imgclsmob): Collection of classification models pretrained on the ImageNet-1K.\n17. [medicaltorch](https://github.com/perone/medicaltorch): A medical imaging framework for Pytorch http://medicaltorch.readthedocs.io\n18. [albumentations](https://github.com/albu/albumentations): Fast image augmentation library.\n19. [kornia](https://github.com/arraiyopensource/kornia): Differentiable computer vision library.\n20. [pytorch-text-recognition](https://github.com/s3nh/pytorch-text-recognition): Text recognition combo - CRAFT + CRNN.\n21. [facenet-pytorch](https://github.com/timesler/facenet-pytorch): Pretrained Pytorch face detection and recognition models ported from davidsandberg/facenet.\n22. [detectron2](https://github.com/facebookresearch/detectron2): Detectron2 is FAIR''s next-generation research platform for object detection and segmentation.\n23. [vedaseg](https://github.com/Media-Smart/vedaseg): A semantic segmentation framework by pyotrch\n24. [ClassyVision](https://github.com/facebookresearch/ClassyVision): An end-to-end PyTorch framework for image and video classification.\n25. [detecto](https://github.com/alankbi/detecto):Computer vision in Python with less than 10 lines of code\n26. [pytorch3d](https://github.com/facebookresearch/pytorch3d): PyTorch3D is FAIR''s library of reusable components for deep learning with 3D data pytorch3d.org\n27. [MMDetection](https://github.com/open-mmlab/mmdetection): MMDetection is an open source object detection toolbox, a part of the [OpenMMLab project](https://open-mmlab.github.io/).\n28. [neural-dream](https://github.com/ProGamerGov/neural-dream): A PyTorch implementation of the DeepDream algorithm. Creates dream-like hallucinogenic visuals.\n29. [FlashTorch](https://github.com/MisaOgura/flashtorch): Visualization toolkit for neural networks in PyTorch!\n30. [Lucent](https://github.com/greentfrapp/lucent): Tensorflow and OpenAI Clarity''s Lucid adapted for PyTorch.\n31. [MMDetection3D](https://github.com/open-mmlab/mmdetection3d): MMDetection3D is OpenMMLab''s next-generation platform for general 3D object detection, a part of the [OpenMMLab project](https://open-mmlab.github.io/).\n32. [MMSegmentation](https://github.com/open-mmlab/mmsegmentation): MMSegmentation is a semantic segmentation toolbox and benchmark, a part of the [OpenMMLab project](https://open-mmlab.github.io/).\n33. [MMEditing](https://github.com/open-mmlab/mmediting): MMEditing is a image and video editing toolbox, a part of the [OpenMMLab project](https://open-mmlab.github.io/).\n34. [MMAction2](https://github.com/open-mmlab/mmaction2): MMAction2 is OpenMMLab''s next generation action understanding toolbox and benchmark, a part of the [OpenMMLab project](https://open-mmlab.github.io/).\n35. [MMPose](https://github.com/open-mmlab/mmpose): MMPose is a pose estimation toolbox and benchmark, a part of the [OpenMMLab project](https://open-mmlab.github.io/).\n36. [lightly](https://github.com/lightly-ai/lightly) - Lightly is a computer vision framework for self-supervised learning.\n37. [RoMa](https://naver.github.io/roma/): a lightweight and efficient library to deal with 3D rotations.\n\n\n### Probabilistic/Generative Libraries:\n\n1. [ptstat](https://github.com/stepelu/ptstat): Probabilistic Programming and Statistical Inference in PyTorch\n2. [pyro](https://github.com/uber/pyro): Deep universal probabilistic programming with Python and PyTorch http://pyro.ai\n3. [probtorch](https://github.com/probtorch/probtorch): Probabilistic Torch is library for deep generative models that extends PyTorch.\n4. [paysage](https://github.com/drckf/paysage): Unsupervised learning and generative models in python/pytorch.\n5. [pyvarinf](https://github.com/ctallec/pyvarinf): Python package facilitating the use of Bayesian Deep Learning methods with Variational Inference for PyTorch. \n6. [pyprob](https://github.com/probprog/pyprob): A PyTorch-based library for probabilistic programming and inference compilation.\n7. [mia](https://github.com/spring-epfl/mia): A library for running membership inference attacks against ML models. \n8. [pro_gan_pytorch](https://github.com/akanimax/pro_gan_pytorch): ProGAN package implemented as an extension of PyTorch nn.Module.\n9. [botorch](https://github.com/pytorch/botorch): Bayesian optimization in PyTorch\n\n### Other libraries:\n\n1. [pytorch extras](https://github.com/mrdrozdov/pytorch-extras): Some extra features for pytorch.    \n2. [functional zoo](https://github.com/szagoruyko/functional-zoo): PyTorch, unlike lua torch, has autograd in it''s core, so using modular structure of torch.nn modules is not necessary, one can easily allocate needed Variables and write a function that utilizes them, which is sometimes more convenient. This repo contains model definitions in this functional way, with pretrained weights for some models. \n3. [torch-sampling](https://github.com/ncullen93/torchsample): This package provides a set of transforms and data structures for sampling from in-memory or out-of-memory data. \n4. [torchcraft-py](https://github.com/deepcraft/torchcraft-py): Python wrapper for TorchCraft, a bridge between Torch and StarCraft for AI research.\n5. [aorun](https://github.com/ramon-oliveira/aorun): Aorun intend to be a Keras with PyTorch as backend. \n6. [logger](https://github.com/oval-group/logger): A simple logger for experiments.\n7. [PyTorch-docset](https://github.com/iamaziz/PyTorch-docset): PyTorch docset! use with Dash, Zeal, Velocity, or LovelyDocs.  \n8. [convert_torch_to_pytorch](https://github.com/clcarwin/convert_torch_to_pytorch): Convert torch t7 model to pytorch model and source.\n9. [pretrained-models.pytorch](https://github.com/Cadene/pretrained-models.pytorch): The goal of this repo is to help to reproduce research papers results.  \n10. [pytorch_fft](https://github.com/locuslab/pytorch_fft): PyTorch wrapper for FFTs\n11. [caffe_to_torch_to_pytorch](https://github.com/fanq15/caffe_to_torch_to_pytorch)\n12. [pytorch-extension](https://github.com/sniklaus/pytorch-extension): This is a CUDA extension for PyTorch which computes the Hadamard product of two tensors.\n13. [tensorboard-pytorch](https://github.com/lanpa/tensorboard-pytorch): This module saves PyTorch tensors in tensorboard format for inspection. Currently supports scalar, image, audio, histogram features in tensorboard.\n14. [gpytorch](https://github.com/jrg365/gpytorch): GPyTorch is a Gaussian Process library, implemented using PyTorch. It is designed for creating flexible and modular Gaussian Process models with ease, so that you don''t have to be an expert to use GPs.\n15. [spotlight](https://github.com/maciejkula/spotlight): Deep recommender models using PyTorch.\n16. [pytorch-cns](https://github.com/awentzonline/pytorch-cns): Compressed Network Search with PyTorch\n17. [pyinn](https://github.com/szagoruyko/pyinn): CuPy fused PyTorch neural networks ops\n18. [inferno](https://github.com/nasimrahaman/inferno): A utility library around PyTorch\n19. [pytorch-fitmodule](https://github.com/henryre/pytorch-fitmodule): Super simple fit method for PyTorch modules\n20. [inferno-sklearn](https://github.com/dnouri/inferno): A scikit-learn compatible neural network library that wraps pytorch.\n21. [pytorch-caffe-darknet-convert](https://github.com/marvis/pytorch-caffe-darknet-convert): convert between pytorch, caffe prototxt/weights and darknet cfg/weights\n22. [pytorch2caffe](https://github.com/longcw/pytorch2caffe): Convert PyTorch model to Caffemodel\n23. [pytorch-tools](https://github.com/nearai/pytorch-tools): Tools for PyTorch\n24. [sru](https://github.com/taolei87/sru): Training RNNs as Fast as CNNs (arxiv.org/abs/1709.02755)\n25. [torch2coreml](https://github.com/prisma-ai/torch2coreml): Torch7 -> CoreML\n26. [PyTorch-Encoding](https://github.com/zhanghang1989/PyTorch-Encoding): PyTorch Deep Texture Encoding Network http://hangzh.com/PyTorch-Encoding\n27. [pytorch-ctc](https://github.com/ryanleary/pytorch-ctc): PyTorch-CTC is an implementation of CTC (Connectionist Temporal Classification) beam search decoding for PyTorch. C++ code borrowed liberally from TensorFlow with some improvements to increase flexibility.\n28. [candlegp](https://github.com/t-vi/candlegp): Gaussian Processes in Pytorch. \n29. [dpwa](https://github.com/loudinthecloud/dpwa): Distributed Learning by Pair-Wise Averaging. \n30. [dni-pytorch](https://github.com/koz4k/dni-pytorch): Decoupled Neural Interfaces using Synthetic Gradients for PyTorch.\n31. [skorch](https://github.com/dnouri/skorch): A scikit-learn compatible neural network library that wraps pytorch\n32. [ignite](https://github.com/pytorch/ignite): Ignite is a high-level library to help with training neural networks in PyTorch.\n33. [Arnold](https://github.com/glample/Arnold): Arnold - DOOM Agent\n34. [pytorch-mcn](https://github.com/albanie/pytorch-mcn): Convert models from MatConvNet to PyTorch\n35. [simple-faster-rcnn-pytorch](https://github.com/chenyuntc/simple-faster-rcnn-pytorch): A simplified implemention of Faster R-CNN with competitive performance.\n36. [generative_zoo](https://github.com/DL-IT/generative_zoo): generative_zoo is a repository that provides working implementations of some generative models in PyTorch.\n37. [pytorchviz](https://github.com/szagoruyko/pytorchviz): A small package to create visualizations of PyTorch execution graphs. \n38. [cogitare](https://github.com/cogitare-ai/cogitare): Cogitare - A Modern, Fast, and Modular Deep Learning and Machine Learning framework in Python. \n39. [pydlt](https://github.com/dmarnerides/pydlt): PyTorch based Deep Learning Toolbox\n40. [semi-supervised-pytorch](https://github.com/wohlert/semi-supervised-pytorch): Implementations of different VAE-based semi-supervised and generative models in PyTorch. \n41. [pytorch_cluster](https://github.com/rusty1s/pytorch_cluster): PyTorch Extension Library of Optimised Graph Cluster Algorithms.\n42. [neural-assembly-compiler](https://github.com/aditya-khant/neural-assembly-compiler): A neural assembly compiler for pyTorch based on adaptive-neural-compilation. \n43. [caffemodel2pytorch](https://github.com/vadimkantorov/caffemodel2pytorch): Convert Caffe models to PyTorch.\n44. [extension-cpp](https://github.com/pytorch/extension-cpp): C++ extensions in PyTorch\n45. [pytoune](https://github.com/GRAAL-Research/pytoune): A Keras-like framework and utilities for PyTorch\n46. [jetson-reinforcement](https://github.com/dusty-nv/jetson-reinforcement): Deep reinforcement learning libraries for NVIDIA Jetson TX1/TX2 with PyTorch, OpenAI Gym, and Gazebo robotics simulator.\n47. [matchbox](https://github.com/salesforce/matchbox): Write PyTorch code at the level of individual examples, then run it efficiently on minibatches.\n48. [torch-two-sample](https://github.com/josipd/torch-two-sample): A PyTorch library for two-sample tests\n49. [pytorch-summary](https://github.com/sksq96/pytorch-summary): Model summary in PyTorch similar to `model.summary()` in Keras\n50. [mpl.pytorch](https://github.com/BelBES/mpl.pytorch): Pytorch implementation of MaxPoolingLoss.\n51. [scVI-dev](https://github.com/YosefLab/scVI-dev): Development branch of the scVI project in PyTorch\n52. [apex](https://github.com/NVIDIA/apex): An Experimental PyTorch Extension(will be deprecated at a later point)\n53. [ELF](https://github.com/pytorch/ELF): ELF: a platform for game research.\n54. [Torchlite](https://github.com/EKami/Torchlite): A high level library on top of(not only) Pytorch\n55. [joint-vae](https://github.com/Schlumberger/joint-vae): Pytorch implementation of JointVAE, a framework for disentangling continuous and discrete factors of variation star2\n56. [SLM-Lab](https://github.com/kengz/SLM-Lab): Modular Deep Reinforcement Learning framework in PyTorch.\n57. [bindsnet](https://github.com/Hananel-Hazan/bindsnet): A Python package used for simulating spiking neural networks (SNNs) on CPUs or GPUs using PyTorch\n58. [pro_gan_pytorch](https://github.com/akanimax/pro_gan_pytorch): ProGAN package implemented as an extension of PyTorch nn.Module\n59. [pytorch_geometric](https://github.com/rusty1s/pytorch_geometric): Geometric Deep Learning Extension Library for PyTorch\n60. [torchplus](https://github.com/knighton/torchplus): Implements the + operator on PyTorch modules, returning sequences.\n61. [lagom](https://github.com/zuoxingdong/lagom): lagom: A light PyTorch infrastructure to quickly prototype reinforcement learning algorithms.\n62. [torchbearer](https://github.com/ecs-vlc/torchbearer): torchbearer: A model training library for researchers using PyTorch.\n63. [pytorch-maml-rl](https://github.com/tristandeleu/pytorch-maml-rl): Reinforcement Learning with Model-Agnostic Meta-Learning in Pytorch. \n64. [NALU](https://github.com/bharathgs/NALU): Basic pytorch implementation of NAC/NALU from Neural Arithmetic Logic Units paper by trask et.al arxiv.org/pdf/1808.00508.pdf\n66. [QuCumber](https://github.com/PIQuIL/QuCumber): Neural Network Many-Body Wavefunction Reconstruction\n67. [magnet](https://github.com/MagNet-DL/magnet): Deep Learning Projects that Build Themselves http://magnet-dl.readthedocs.io/\n68. [opencv_transforms](https://github.com/jbohnslav/opencv_transforms): OpenCV implementation of Torchvision''s image augmentations\n69. [fastai](https://github.com/fastai/fastai): The fast.ai deep learning library, lessons, and tutorials\n70. [pytorch-dense-correspondence](https://github.com/RobotLocomotion/pytorch-dense-correspondence): Code for "Dense Object Nets: Learning Dense Visual Object Descriptors By and For Robotic Manipulation" arxiv.org/pdf/1806.08756.pdf\n71. [colorization-pytorch](https://github.com/richzhang/colorization-pytorch): PyTorch reimplementation of Interactive Deep Colorization richzhang.github.io/ideepcolor\n72. [beauty-net](https://github.com/cms-flash/beauty-net): A simple, flexible, and extensible template for PyTorch. It''s beautiful.\n73. [OpenChem](https://github.com/Mariewelt/OpenChem): OpenChem: Deep Learning toolkit for Computational Chemistry and Drug Design Research mariewelt.github.io/OpenChem \n74. [torchani](https://github.com/aiqm/torchani): Accurate Neural Network Potential on PyTorch aiqm.github.io/torchani\n75. [PyTorch-LBFGS](https://github.com/hjmshi/PyTorch-LBFGS): A PyTorch implementation of L-BFGS.\n76. [gpytorch](https://github.com/cornellius-gp/gpytorch): A highly efficient and modular implementation of Gaussian Processes in PyTorch.\n77. [hessian](https://github.com/mariogeiger/hessian): hessian in pytorch. \n78. [vel](https://github.com/MillionIntegrals/vel): Velocity in deep-learning research.\n79. [nonechucks](https://github.com/msamogh/nonechucks): Skip bad items in your PyTorch DataLoader, use Transforms as Filters, and more!\n80. [torchstat](https://github.com/Swall0w/torchstat): Model analyzer in PyTorch.\n81. [QNNPACK](https://github.com/pytorch/QNNPACK): Quantized Neural Network PACKage - mobile-optimized implementation of quantized neural network operators.\n82. [torchdiffeq](https://github.com/rtqichen/torchdiffeq): Differentiable ODE solvers with full GPU support and O(1)-memory backpropagation.\n83. [redner](https://github.com/BachiLi/redner): A differentiable Monte Carlo path tracer\n84. [pixyz](https://github.com/masa-su/pixyz): a library for developing deep generative models in a more concise, intuitive and extendable way. \n85. [euclidesdb](https://github.com/perone/euclidesdb): A multi-model machine learning feature embedding database http://euclidesdb.readthedocs.io \n86. [pytorch2keras](https://github.com/nerox8664/pytorch2keras): Convert PyTorch dynamic graph to Keras model.\n87. [salad](https://github.com/domainadaptation/salad): Semi-Supervised Learning and Domain Adaptation.\n88. [netharn](https://github.com/Erotemic/netharn): Parameterized fit and prediction harnesses for pytorch.\n89. [dgl](https://github.com/dmlc/dgl): Python package built to ease deep learning on graph, on top of existing DL frameworks. http://dgl.ai. \n90. [gandissect](https://github.com/CSAILVision/gandissect): Pytorch-based tools for visualizing and understanding the neurons of a GAN. gandissect.csail.mit.edu \n91. [delira](https://github.com/justusschock/delira): Lightweight framework for fast prototyping and training deep neural networks in medical imaging delira.rtfd.io\n92. [mushroom](https://github.com/AIRLab-POLIMI/mushroom): Python library for Reinforcement Learning experiments.\n93. [Xlearn](https://github.com/thuml/Xlearn): Transfer Learning Library\n94. [geoopt](https://github.com/ferrine/geoopt): Riemannian Adaptive Optimization Methods with pytorch optim\n95. [vegans](https://github.com/unit8co/vegans): A library providing various existing GANs in PyTorch.\n96. [torchgeometry](https://github.com/arraiyopensource/torchgeometry): TGM: PyTorch Geometry\n97. [AdverTorch](https://github.com/BorealisAI/advertorch): A Toolbox for Adversarial Robustness (attack/defense/training) Research\n98. [AdaBound](https://github.com/Luolc/AdaBound): An optimizer that trains as fast as Adam and as good as SGD.a\n99. [fenchel-young-losses](https://github.com/mblondel/fenchel-young-losses): Probabilistic classification in PyTorch/TensorFlow/scikit-learn with Fenchel-Young losses\n100. [pytorch-OpCounter](https://github.com/Lyken17/pytorch-OpCounter): Count the FLOPs of your PyTorch model.\n101. [Tor10](https://github.com/kaihsin/Tor10): A Generic Tensor-Network library that is designed for quantum simulation, base on the pytorch.\n102. [Catalyst](https://github.com/catalyst-team/catalyst): High-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code/ideas reusing. Being able to research/develop something new, rather than write another regular train loop.\n103. [Ax](https://github.com/facebook/Ax): Adaptive Experimentation Platform\n104. [pywick](https://github.com/achaiah/pywick): High-level batteries-included neural network training library for Pytorch\n105. [torchgpipe](https://github.com/kakaobrain/torchgpipe): A GPipe implementation in PyTorch torchgpipe.readthedocs.io\n106. [hub](https://github.com/pytorch/hub): Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility.\n107. [pytorch-lightning](https://github.com/williamFalcon/pytorch-lightning): Rapid research framework for Pytorch. The researcher''s version of keras.\n108. [Tor10](https://github.com/kaihsin/Tor10): A Generic Tensor-Network library that is designed for quantum simulation, base on the pytorch.\n109. [tensorwatch](https://github.com/microsoft/tensorwatch): Debugging, monitoring and visualization for Deep Learning and Reinforcement Learning from Microsoft Research.\n110. [wavetorch](https://github.com/fancompute/wavetorch): Numerically solving and backpropagating through the wave equation arxiv.org/abs/1904.12831\n111. [diffdist](https://github.com/ag14774/diffdist): diffdist is a python library for pytorch. It extends the default functionality of torch.autograd and adds support for differentiable communication between processes. \n112. [torchprof](https://github.com/awwong1/torchprof): A minimal dependency library for layer-by-layer profiling of Pytorch models.\n113. [osqpth](https://github.com/oxfordcontrol/osqpth): The differentiable OSQP solver layer for PyTorch. \n114. [mctorch](https://github.com/mctorch/mctorch): A manifold optimization library for deep learning. \n115. [pytorch-hessian-eigenthings](https://github.com/noahgolmant/pytorch-hessian-eigenthings): Efficient PyTorch Hessian eigendecomposition using the Hessian-vector product and stochastic power iteration. \n116. [MinkowskiEngine](https://github.com/StanfordVL/MinkowskiEngine): Minkowski Engine is an auto-diff library for generalized sparse convolutions and high-dimensional sparse tensors.\n117. [pytorch-cpp-rl](https://github.com/Omegastick/pytorch-cpp-rl): PyTorch C++ Reinforcement Learning\n118. [pytorch-toolbelt](https://github.com/BloodAxe/pytorch-toolbelt): PyTorch extensions for fast R&D prototyping and Kaggle farming\n119. [argus-tensor-stream](https://github.com/Fonbet/argus-tensor-stream): A library for real-time video stream decoding to CUDA memory tensorstream.argus-ai.com\n120. [macarico](https://github.com/hal3/macarico): learning to search in pytorch\n121. [rlpyt](https://github.com/astooke/rlpyt): Reinforcement Learning in PyTorch\n122. [pywarm](https://github.com/blue-season/pywarm): A cleaner way to build neural networks for PyTorch. blue-season.github.io/pywarm\n123. [learn2learn](https://github.com/learnables/learn2learn): PyTorch Meta-learning Framework for Researchers http://learn2learn.net\n124. [torchbeast](https://github.com/facebookresearch/torchbeast): A PyTorch Platform for Distributed RL\n125. [higher](https://github.com/facebookresearch/higher): higher is a pytorch library allowing users to obtain higher order gradients over losses spanning training loops rather than individual training steps.\n126. [Torchelie](https://github.com/Vermeille/Torchelie/): Torchélie is a set of utility functions, layers, losses, models, trainers and other things for PyTorch. torchelie.readthedocs.org \n127. [CrypTen](https://github.com/facebookresearch/CrypTen): CrypTen is a Privacy Preserving Machine Learning framework written using PyTorch that allows researchers and developers to train models using encrypted data. CrypTen currently supports Secure multi-party computation as its encryption mechanism.\n128. [cvxpylayers](https://github.com/cvxgrp/cvxpylayers): cvxpylayers is a Python library for constructing differentiable convex optimization layers in PyTorch\n129. [RepDistiller](https://github.com/HobbitLong/RepDistiller): Contrastive Representation Distillation (CRD), and benchmark of recent knowledge distillation methods\n130. [kaolin](https://github.com/NVIDIAGameWorks/kaolin): PyTorch library aimed at accelerating 3D deep learning research\n131. [PySNN](https://github.com/BasBuller/PySNN): Efficient Spiking Neural Network framework, built on top of PyTorch for GPU acceleration.\n132. [sparktorch](https://github.com/dmmiller612/sparktorch): Train and run Pytorch models on Apache Spark.\n133. [pytorch-metric-learning](https://github.com/KevinMusgrave/pytorch-metric-learning): The easiest way to use metric learning in your application. Modular, flexible, and extensible. Written in PyTorch.\n134. [autonomous-learning-library](https://github.com/cpnota/autonomous-learning-library): A PyTorch library for building deep reinforcement learning agents.\n135. [flambe](https://github.com/asappresearch/flambe): An ML framework to accelerate research and its path to production. flambe.ai\n136. [pytorch-optimizer](https://github.com/jettify/pytorch-optimizer): Collections of modern optimization algorithms for PyTorch, includes: AccSGD, AdaBound, AdaMod, DiffGrad, Lamb, RAdam, RAdam, Yogi.\n137. [PyTorch-VAE](https://github.com/AntixK/PyTorch-VAE): A Collection of Variational Autoencoders (VAE) in PyTorch.\n138. [ray](https://github.com/ray-project/ray): A fast and simple framework for building and running distributed applications. Ray is packaged with RLlib, a scalable reinforcement learning library, and Tune, a scalable hyperparameter tuning library. ray.io\n139. [Pytorch Geometric Temporal](https://github.com/benedekrozemberczki/pytorch_geometric_temporal): A temporal extension library for PyTorch Geometric \n140. [Poutyne](https://github.com/GRAAL-Research/poutyne): A Keras-like framework for PyTorch that handles much of the boilerplating code needed to train neural networks.\n141. [Pytorch-Toolbox](https://github.com/PistonY/torch-toolbox): This is toolbox project for Pytorch. Aiming to make you write Pytorch code more easier, readable and concise.\n142. [Pytorch-contrib](https://github.com/pytorch/contrib): It contains reviewed implementations of ideas from recent machine learning papers.\n143. [EfficientNet PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch): It contains an op-for-op PyTorch reimplementation of EfficientNet, along with pre-trained models and examples.\n144. [PyTorch/XLA](https://github.com/pytorch/xla): PyTorch/XLA is a Python package that uses the XLA deep learning compiler to connect the PyTorch deep learning framework and Cloud TPUs.\n145. [webdataset](https://github.com/tmbdev/webdataset): WebDataset is a PyTorch Dataset (IterableDataset) implementation providing efficient access to datasets stored in POSIX tar archives.\n146. [volksdep](https://github.com/Media-Smart/volksdep): volksdep is an open-source toolbox for deploying and accelerating PyTorch, Onnx and Tensorflow models with TensorRT.\n147. [PyTorch-StudioGAN](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN): StudioGAN is a Pytorch library providing implementations of representative Generative Adversarial Networks (GANs) for conditional/unconditional image generation. StudioGAN aims to offer an identical playground for modern GANs so that machine learning researchers can readily compare and analyze a new idea.\n148. [torchdrift](https://github.com/torchdrift/torchdrift/): drift detection library\n149. [accelerate](https://github.com/huggingface/accelerate) : A simple way to train and use PyTorch models with multi-GPU, TPU, mixed-precision \n150. [lightning-transformers](https://github.com/PyTorchLightning/lightning-transformers):  Flexible interface for high-performance research using SOTA Transformers leveraging Pytorch Lightning, Transformers, and Hydra. \n151. [Flower](https://flower.dev/) A unified approach to federated learning, analytics, and evaluation. It allows to federated any machine learning workload.\n152. [lightning-flash](https://github.com/PyTorchLightning/lightning-flash): Flash is a collection of tasks for fast prototyping, baselining and fine-tuning scalable Deep Learning models, built on PyTorch Lightning.\n153. [Pytorch Geometric Signed Directed](https://github.com/SherylHYX/pytorch_geometric_signed_directed): A signed and directed extension library for PyTorch Geometric. \n154. [Koila](https://github.com/rentruewang/koila): A simple wrapper around pytorch that prevents CUDA out of memory issues.\n155. [Renate](https://github.com/awslabs/renate): A library for real-world continual learning.\n\n## Tutorials, books, & examples\n\n1. **[Practical Pytorch](https://github.com/spro/practical-pytorch)**: Tutorials explaining different RNN models\n2. [DeepLearningForNLPInPytorch](https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html): An IPython Notebook tutorial on deep learning, with an emphasis on Natural Language Processing. \n3. [pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial): tutorial for researchers to learn deep learning with pytorch.\n4.  [pytorch-exercises](https://github.com/keon/pytorch-exercises): pytorch-exercises collection. \n5.  [pytorch tutorials](https://github.com/pytorch/tutorials): Various pytorch tutorials. \n6.  [pytorch examples](https://github.com/pytorch/examples):  A repository showcasing examples of using pytorch \n7. [pytorch practice](https://github.com/napsternxg/pytorch-practice): Some example scripts on pytorch.  \n8.  [pytorch mini tutorials](https://github.com/vinhkhuc/PyTorch-Mini-Tutorials):  Minimal tutorials for PyTorch adapted from Alec Radford''s Theano tutorials. \n9.  [pytorch text classification](https://github.com/xiayandi/Pytorch_text_classification): A simple implementation of CNN based text classification in Pytorch \n10. [cats vs dogs](https://github.com/desimone/pytorch-cat-vs-dogs): Example of network fine-tuning in pytorch for the kaggle competition Dogs vs. Cats Redux: Kernels Edition. Currently #27 (0.05074) on the leaderboard.  \n11. [convnet](https://github.com/eladhoffer/convNet.pytorch): This is a complete training example for Deep Convolutional Networks on various datasets (ImageNet, Cifar10, Cifar100, MNIST).\n12. [pytorch-generative-adversarial-networks](https://github.com/mailmahee/pytorch-generative-adversarial-networks): simple generative adversarial network (GAN) using PyTorch.   \n13. [pytorch containers](https://github.com/amdegroot/pytorch-containers): This repository aims to help former Torchies more seamlessly transition to the "Containerless" world of PyTorch by providing a list of PyTorch implementations of Torch Table Layers.  \n14. [T-SNE in pytorch](https://github.com/cemoody/topicsne): t-SNE experiments in pytorch \n15. [AAE_pytorch](https://github.com/fducau/AAE_pytorch): Adversarial Autoencoders (with Pytorch). \n16. [Kind_PyTorch_Tutorial](https://github.com/GunhoChoi/Kind_PyTorch_Tutorial): Kind PyTorch Tutorial for beginners.  \n17.  [pytorch-poetry-gen](https://github.com/justdark/pytorch-poetry-gen): a char-RNN based on pytorch.  \n18. [pytorch-REINFORCE](https://github.com/JamesChuanggg/pytorch-REINFORCE): PyTorch implementation of REINFORCE, This repo supports both continuous and discrete environments in OpenAI gym.\n19.  **[PyTorch-Tutorial](https://github.com/MorvanZhou/PyTorch-Tutorial)**: Build your neural network easy and fast  https://morvanzhou.github.io/tutorials/ \n20. [pytorch-intro](https://github.com/joansj/pytorch-intro): A couple of scripts to illustrate how to do CNNs and RNNs in PyTorch\n21. [pytorch-classification](https://github.com/bearpaw/pytorch-classification): A unified framework for the image classification task on CIFAR-10/100 and ImageNet.\n22. [pytorch_notebooks - hardmaru](https://github.com/hardmaru/pytorch_notebooks): Random tutorials created in NumPy and PyTorch.\n23. [pytorch_tutoria-quick](https://github.com/soravux/pytorch_tutorial): Quick PyTorch introduction and tutorial. Targets computer vision, graphics and machine learning researchers eager to try a new framework.  \n24. [Pytorch_fine_tuning_Tutorial](https://github.com/Spandan-Madan/Pytorch_fine_tuning_Tutorial): A short tutorial on performing fine tuning or transfer learning in PyTorch.\n25. [pytorch_exercises](https://github.com/Kyubyong/pytorch_exercises): pytorch-exercises \n26. [traffic-sign-detection](https://github.com/soumith/traffic-sign-detection-homework): nyu-cv-fall-2017 example\n27. [mss_pytorch](https://github.com/Js-Mim/mss_pytorch): Singing Voice Separation via Recurrent Inference and Skip-Filtering Connections - PyTorch Implementation. Demo: js-mim.github.io/mss_pytorch\n28. [DeepNLP-models-Pytorch](https://github.com/DSKSD/DeepNLP-models-Pytorch) Pytorch implementations of various Deep NLP models in cs-224n(Stanford Univ: NLP with Deep Learning)\n29. [Mila introductory tutorials](https://github.com/mila-udem/welcome_tutorials): Various tutorials given for welcoming new students at MILA.\n30. [pytorch.rl.learning](https://github.com/moskomule/pytorch.rl.learning): for learning reinforcement learning using PyTorch.\n31. [minimal-seq2seq](https://github.com/keon/seq2seq): Minimal Seq2Seq model with Attention for Neural Machine Translation in PyTorch\n32. [tensorly-notebooks](https://github.com/JeanKossaifi/tensorly-notebooks): Tensor methods in Python with TensorLy tensorly.github.io/dev\n33. [pytorch_bits](https://github.com/jpeg729/pytorch_bits): time-series prediction related examples.\n34. [skip-thoughts](https://github.com/sanyam5/skip-thoughts): An implementation of Skip-Thought Vectors in PyTorch.\n35. [video-caption-pytorch](https://github.com/xiadingZ/video-caption-pytorch): pytorch code for video captioning. \n36. [Capsule-Network-Tutorial](https://github.com/higgsfield/Capsule-Network-Tutorial): Pytorch easy-to-follow Capsule Network tutorial.\n37. [code-of-learn-deep-learning-with-pytorch](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch): This is code of book "Learn Deep Learning with PyTorch" item.jd.com/17915495606.html\n38. [RL-Adventure](https://github.com/higgsfield/RL-Adventure): Pytorch easy-to-follow step-by-step Deep Q Learning tutorial with clean readable code.\n39. [accelerated_dl_pytorch](https://github.com/hpcgarage/accelerated_dl_pytorch): Accelerated Deep Learning with PyTorch at Jupyter Day Atlanta II. \n40. [RL-Adventure-2](https://github.com/higgsfield/RL-Adventure-2): PyTorch4 tutorial of: actor critic / proximal policy optimization / acer / ddpg / twin dueling ddpg / soft actor critic / generative adversarial imitation learning / hindsight experience replay\n41. [Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch)](https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f)\n42. [adversarial-autoencoders-with-pytorch](https://blog.paperspace.com/adversarial-autoencoders-with-pytorch/)\n43. [transfer learning using pytorch](https://medium.com/@vishnuvig/transfer-learning-using-pytorch-4c3475f4495)\n44. [how-to-implement-a-yolo-object-detector-in-pytorch](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/)\n45. [pytorch-for-recommenders-101](http://blog.fastforwardlabs.com/2018/04/10/pytorch-for-recommenders-101.html)\n46. [pytorch-for-numpy-users](https://github.com/wkentaro/pytorch-for-numpy-users)\n47. [PyTorch Tutorial](http://www.pytorchtutorial.com/): PyTorch Tutorials in Chinese.\n48. [grokking-pytorch](https://github.com/Kaixhin/grokking-pytorch): The Hitchiker''s Guide to PyTorch\n49. [PyTorch-Deep-Learning-Minicourse](https://github.com/Atcold/PyTorch-Deep-Learning-Minicourse): Minicourse in Deep Learning with PyTorch.\n50. [pytorch-custom-dataset-examples](https://github.com/utkuozbulak/pytorch-custom-dataset-examples): Some custom dataset examples for PyTorch\n51. [Multiplicative LSTM for sequence-based Recommenders](https://florianwilhelm.info/2018/08/multiplicative_LSTM_for_sequence_based_recos/)\n52. [deeplearning.ai-pytorch](https://github.com/furkanu/deeplearning.ai-pytorch): PyTorch Implementations of Coursera''s Deep Learning(deeplearning.ai) Specialization. \n53. [MNIST_Pytorch_python_and_capi](https://github.com/tobiascz/MNIST_Pytorch_python_and_capi): This is an example of how to train a MNIST network in Python and run it in c++ with pytorch 1.0\n54. [torch_light](https://github.com/ne7ermore/torch_light): Tutorials and examples include Reinforcement Training, NLP, CV\n55. [portrain-gan](https://github.com/dribnet/portrain-gan): torch code to decode (and almost encode) latents from art-DCGAN''s Portrait GAN.\n56. [mri-analysis-pytorch](https://github.com/omarsar/mri-analysis-pytorch): MRI analysis using PyTorch and MedicalTorch\n57. [cifar10-fast](https://github.com/davidcpage/cifar10-fast): \nDemonstration of training a small ResNet on CIFAR10 to 94% test accuracy in 79 seconds as described in this [blog series](https://www.myrtle.ai/2018/09/24/how_to_train_your_resnet/).\n58. [Intro to Deep Learning with PyTorch](https://in.udacity.com/course/deep-learning-pytorch--ud188): A free course by Udacity and facebook, with a good intro to PyTorch, and an interview with Soumith Chintala, one of the original authors of PyTorch.\n59. [pytorch-sentiment-analysis](https://github.com/bentrevett/pytorch-sentiment-analysis): Tutorials on getting started with PyTorch and TorchText for sentiment analysis.\n60. [pytorch-image-models](https://github.com/rwightman/pytorch-image-models): PyTorch image models, scripts, pretrained weights -- (SE)ResNet/ResNeXT, DPN, EfficientNet, MobileNet-V3/V2/V1, MNASNet, Single-Path NAS, FBNet, and more.\n61. [CIFAR-ZOO](https://github.com/BIGBALLON/CIFAR-ZOO): Pytorch implementation for multiple CNN architectures and improve methods with state-of-the-art results. \n62. [d2l-pytorch](https://github.com/dsgiitr/d2l-pytorch): This is an attempt to modify Dive into Deep Learning, Berkeley STAT 157 (Spring 2019) textbook''s code into PyTorch.\n63. [thinking-in-tensors-writing-in-pytorch](https://github.com/stared/thinking-in-tensors-writing-in-pytorch): Thinking in tensors, writing in PyTorch (a hands-on deep learning intro).\n64. [NER-BERT-pytorch](https://github.com/lemonhu/NER-BERT-pytorch): PyTorch solution of named entity recognition task Using Google AI''s pre-trained BERT model.\n65. [pytorch-sync-batchnorm-example](https://github.com/dougsouza/pytorch-sync-batchnorm-example): How to use Cross Replica / Synchronized Batchnorm in Pytorch. \n66. [SentimentAnalysis](https://github.com/barissayil/SentimentAnalysis): Sentiment analysis neural network trained by fine tuning BERT on the Stanford Sentiment Treebank, thanks to [Hugging Face](https://huggingface.co/transformers/)''s Transformers library.\n67. [pytorch-cpp](https://github.com/prabhuomkar/pytorch-cpp): C++ implementations of PyTorch tutorials for deep learning researchers (based on the Python tutorials from [pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial)). \n68. [Deep Learning with PyTorch: Zero to GANs](https://jovian.ml/aakashns/collections/deep-learning-with-pytorch): Interactive and coding-focused tutorial series on introduction to Deep Learning with PyTorch ([video](https://www.youtube.com/watch?v=GIsg-ZUy0MY)).\n69. [Deep Learning with PyTorch](https://www.manning.com/books/deep-learning-with-pytorch): Deep Learning with PyTorch teaches you how to implement deep learning algorithms with Python and PyTorch, the book includes a case study: building an algorithm capable of detecting malignant lung tumors using CT scans.\n70. [Serverless Machine Learning in Action with PyTorch and AWS](https://www.manning.com/books/serverless-machine-learning-in-action): Serverless Machine Learning in Action is a guide to bringing your experimental PyTorch machine learning code to production using serverless capabilities from major cloud providers like AWS, Azure, or GCP.\n71. [LabML NN](https://github.com/lab-ml/nn): A collection of PyTorch implementations of neural networks architectures and algorithms with side-by-side notes.\n72. [Run your PyTorch Example Fedarated with Flower](https://github.com/adap/flower/tree/main/examples/pytorch_from_centralized_to_federated): This example demonstrates how an already existing centralized PyTorch machine learning project can be federated with Flower. A Cifar-10 dataset is used together with a convolutional neural network (CNN).\n\n## Paper implementations\n\n1. [google_evolution](https://github.com/neuralix/google_evolution): This implements one of result networks from Large-scale evolution of image classifiers by Esteban Real, et. al. \n2. [pyscatwave](https://github.com/edouardoyallon/pyscatwave): Fast Scattering Transform with CuPy/PyTorch,read the paper [here](https://arxiv.org/abs/1703.08961)\n3. [scalingscattering](https://github.com/edouardoyallon/scalingscattering): Scaling The Scattering Transform : Deep Hybrid Networks.  \n4. [deep-auto-punctuation](https://github.com/episodeyang/deep-auto-punctuation): a pytorch implementation of auto-punctuation learned character by character.  \n5. [Realtime_Multi-Person_Pose_Estimation](https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation): This is a pytorch version of Realtime_Multi-Person_Pose_Estimation, origin code is [here](https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation) .\n6. [PyTorch-value-iteration-networks](https://github.com/onlytailei/PyTorch-value-iteration-networks): PyTorch implementation of the Value Iteration Networks (NIPS ''16) paper  \n7. [pytorch_Highway](https://github.com/analvikingur/pytorch_Highway): Highway network implemented in pytorch.\n8. [pytorch_NEG_loss](https://github.com/analvikingur/pytorch_NEG_loss): NEG loss implemented in pytorch.  \n9. [pytorch_RVAE](https://github.com/analvikingur/pytorch_RVAE): Recurrent Variational Autoencoder that generates sequential data implemented in pytorch.   \n10. [pytorch_TDNN](https://github.com/analvikingur/pytorch_TDNN): Time Delayed NN implemented in pytorch.  \n11. [eve.pytorch](https://github.com/moskomule/eve.pytorch): An implementation of Eve Optimizer, proposed in Imploving Stochastic Gradient Descent with Feedback, Koushik and Hayashi, 2016.  \n12. [e2e-model-learning](https://github.com/locuslab/e2e-model-learning): Task-based end-to-end model learning.  \n13. [pix2pix-pytorch](https://github.com/mrzhu-cool/pix2pix-pytorch): PyTorch implementation of "Image-to-Image Translation Using Conditional Adversarial Networks".   \n14. [Single Shot MultiBox Detector](https://github.com/amdegroot/ssd.pytorch): A PyTorch Implementation of Single Shot MultiBox Detector.  \n15. [DiscoGAN](https://github.com/carpedm20/DiscoGAN-pytorch): PyTorch implementation of "Learning to Discover Cross-Domain Relations with Generative Adversarial Networks"  \n16. [official DiscoGAN implementation](https://github.com/SKTBrain/DiscoGAN): Official implementation of "Learning to Discover Cross-Domain Relations with Generative Adversarial Networks".  \n17. [pytorch-es](https://github.com/atgambardella/pytorch-es): This is a PyTorch implementation of [Evolution Strategies](https://arxiv.org/abs/1703.03864) .  \n18. [piwise](https://github.com/bodokaiser/piwise): Pixel-wise segmentation on VOC2012 dataset using pytorch.  \n19. [pytorch-dqn](https://github.com/transedward/pytorch-dqn): Deep Q-Learning Network in pytorch.  \n20. [neuraltalk2-pytorch](https://github.com/ruotianluo/neuraltalk2.pytorch): image captioning model in pytorch(finetunable cnn in branch with_finetune)\n21. [vnet.pytorch](https://github.com/mattmacy/vnet.pytorch): A Pytorch implementation for V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation.    \n22. [pytorch-fcn](https://github.com/wkentaro/pytorch-fcn): PyTorch implementation of Fully Convolutional Networks.  \n23. [WideResNets](https://github.com/xternalz/WideResNet-pytorch): WideResNets for CIFAR10/100 implemented in PyTorch. This implementation requires less GPU memory than what is required by the official Torch implementation: https://github.com/szagoruyko/wide-residual-networks .\n24. [pytorch_highway_networks](https://github.com/c0nn3r/pytorch_highway_networks): Highway networks implemented in PyTorch.  \n25. [pytorch-NeuCom](https://github.com/ypxie/pytorch-NeuCom): Pytorch implementation of DeepMind''s differentiable neural computer paper.  \n26. [captionGen](https://github.com/eladhoffer/captionGen): Generate captions for an image using PyTorch.  \n27. [AnimeGAN](https://github.com/jayleicn/animeGAN): A simple PyTorch Implementation of Generative Adversarial Networks, focusing on anime face drawing. \n28. [Cnn-text classification](https://github.com/Shawn1993/cnn-text-classification-pytorch): This is the implementation of Kim''s Convolutional Neural Networks for Sentence Classification paper in PyTorch.  \n29. [deepspeech2](https://github.com/SeanNaren/deepspeech.pytorch): Implementation of DeepSpeech2 using Baidu Warp-CTC. Creates a network based on the DeepSpeech2 architecture, trained with the CTC activation function.\n30. [seq2seq](https://github.com/MaximumEntropy/Seq2Seq-PyTorch): This repository contains implementations of Sequence to Sequence (Seq2Seq) models in PyTorch  \n31. [Asynchronous Advantage Actor-Critic in PyTorch](https://github.com/rarilurelo/pytorch_a3c): This is PyTorch implementation of A3C as described in Asynchronous Methods for Deep Reinforcement Learning. Since PyTorch has a easy method to control shared memory within multiprocess, we can easily implement asynchronous method like A3C.    \n32. [densenet](https://github.com/bamos/densenet.pytorch): This is a PyTorch implementation of the DenseNet-BC architecture as described in the paper Densely Connected Convolutional Networks by G. Huang, Z. Liu, K. Weinberger, and L. van der Maaten. This implementation gets a CIFAR-10+ error rate of 4.77 with a 100-layer DenseNet-BC with a growth rate of 12. Their official implementation and links to many other third-party implementations are available in the liuzhuang13/DenseNet repo on GitHub.  \n33. [nninit](https://github.com/alykhantejani/nninit): Weight initialization schemes for PyTorch nn.Modules. This is a port of the popular nninit for Torch7 by @kaixhin.  \n34. [faster rcnn](https://github.com/longcw/faster_rcnn_pytorch): This is a PyTorch implementation of Faster RCNN. This project is mainly based on py-faster-rcnn and TFFRCNN.For details about R-CNN please refer to the paper Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks by Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \n35. [doomnet](https://github.com/akolishchak/doom-net-pytorch): PyTorch''s version of Doom-net implementing some RL models in ViZDoom environment.  \n36. [flownet](https://github.com/ClementPinard/FlowNetPytorch): Pytorch implementation of FlowNet by Dosovitskiy et al.  \n37. [sqeezenet](https://github.com/gsp-27/pytorch_Squeezenet): Implementation of Squeezenet in pytorch, #### pretrained models on CIFAR10 data to come Plan to train the model on cifar 10 and add block connections too.  \n38. [WassersteinGAN](https://github.com/martinarjovsky/WassersteinGAN): wassersteinGAN in pytorch. \n39. [optnet](https://github.com/locuslab/optnet): This repository is by Brandon Amos and J. Zico Kolter and contains the PyTorch source code to reproduce the experiments in our paper OptNet: Differentiable Optimization as a Layer in Neural Networks.  \n40. [qp solver](https://github.com/locuslab/qpth): A fast and differentiable QP solver for PyTorch. Crafted by Brandon Amos and J. Zico Kolter.  \n41. [Continuous Deep Q-Learning with Model-based Acceleration ](https://github.com/ikostrikov/pytorch-naf): Reimplementation of Continuous Deep Q-Learning with Model-based Acceleration.  \n42. [Learning to learn by gradient descent by gradient descent](https://github.com/ikostrikov/pytorch-meta-optimizer): PyTorch implementation of Learning to learn by gradient descent by gradient descent.\n43. [fast-neural-style](https://github.com/darkstar112358/fast-neural-style): pytorch implementation of fast-neural-style, The model uses the method described in [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155) along with Instance Normalization.\n44. [PytorchNeuralStyleTransfer](https://github.com/leongatys/PytorchNeuralStyleTransfer): Implementation of Neural Style Transfer in Pytorch. \n45. [Fast Neural Style for Image Style Transform by Pytorch](https://github.com/bengxy/FastNeuralStyle): Fast Neural Style for Image Style Transform by Pytorch .\n46. [neural style transfer](https://github.com/alexis-jacq/Pytorch-Tutorials): An introduction to PyTorch through the Neural-Style algorithm (https://arxiv.org/abs/1508.06576) developed by Leon A. Gatys, Alexander S. Ecker and Matthias Bethge.   \n47. [VIN_PyTorch_Visdom](https://github.com/zuoxingdong/VIN_PyTorch_Visdom): PyTorch implementation of Value Iteration Networks (VIN): Clean, Simple and Modular. Visualization in Visdom.  \n48. [YOLO2](https://github.com/longcw/yolo2-pytorch): YOLOv2 in PyTorch.   \n49. [attention-transfer](https://github.com/szagoruyko/attention-transfer): Attention transfer in pytorch, read the paper [here](https://arxiv.org/abs/1612.03928).  \n50. [SVHNClassifier](https://github.com/potterhsu/SVHNClassifier-PyTorch): A PyTorch implementation of [Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks](https://arxiv.org/pdf/1312.6082.pdf).  \n51. [pytorch-deform-conv](https://github.com/oeway/pytorch-deform-conv): PyTorch implementation of Deformable Convolution.  \n52. [BEGAN-pytorch](https://github.com/carpedm20/BEGAN-pytorch): PyTorch implementation of [BEGAN](https://arxiv.org/abs/1703.10717): Boundary Equilibrium Generative Adversarial Networks.  \n53. [treelstm.pytorch](https://github.com/dasguptar/treelstm.pytorch): Tree LSTM implementation in PyTorch.\n54. [AGE](https://github.com/DmitryUlyanov/AGE): Code for paper "Adversarial Generator-Encoder Networks" by Dmitry Ulyanov, Andrea Vedaldi and Victor Lempitsky which can be found [here](http://sites.skoltech.ru/app/data/uploads/sites/25/2017/04/AGE.pdf) \n55. [ResNeXt.pytorch](https://github.com/prlz77/ResNeXt.pytorch): Reproduces ResNet-V3 (Aggregated Residual Transformations for Deep Neural Networks) with pytorch.\n56. [pytorch-rl](https://github.com/jingweiz/pytorch-rl): Deep Reinforcement Learning with pytorch & visdom  \n57. [Deep-Leafsnap](https://github.com/sujithv28/Deep-Leafsnap): LeafSnap replicated using deep neural networks to test accuracy compared to traditional computer vision methods.  \n58. [pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix): PyTorch implementation for both unpaired and paired image-to-image translation.\n59. [A3C-PyTorch](https://github.com/onlytailei/A3C-PyTorch):PyTorch implementation of Advantage async actor-critic Algorithms (A3C) in PyTorch\n60. [pytorch-value-iteration-networks](https://github.com/kentsommer/pytorch-value-iteration-networks): Pytorch implementation of Value Iteration Networks (NIPS 2016 best paper)  \n61. [PyTorch-Style-Transfer](https://github.com/zhanghang1989/PyTorch-Style-Transfer): PyTorch Implementation of Multi-style Generative Network for Real-time Transfer\n62. [pytorch-deeplab-resnet](https://github.com/isht7/pytorch-deeplab-resnet): pytorch-deeplab-resnet-model.\n63. [pointnet.pytorch](https://github.com/fxia22/pointnet.pytorch): pytorch implementation for "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation" https://arxiv.org/abs/1612.00593  \n64. **[pytorch-playground](https://github.com/aaron-xichen/pytorch-playground): Base pretrained models and datasets in pytorch (MNIST, SVHN, CIFAR10, CIFAR100, STL10, AlexNet, VGG16, VGG19, ResNet, Inception, SqueezeNet)**.\n65. [pytorch-dnc](https://github.com/jingweiz/pytorch-dnc): Neural Turing Machine (NTM) & Differentiable Neural Computer (DNC) with pytorch & visdom. \n66. [pytorch_image_classifier](https://github.com/jinfagang/pytorch_image_classifier): Minimal But Practical Image Classifier Pipline Using Pytorch, Finetune on ResNet18, Got 99% Accuracy on Own Small Datasets.  \n67. [mnist-svhn-transfer](https://github.com/yunjey/mnist-svhn-transfer): PyTorch Implementation of CycleGAN and SGAN for Domain Transfer (Minimal).\n68. [pytorch-yolo2](https://github.com/marvis/pytorch-yolo2): pytorch-yolo2\n69. [dni](https://github.com/andrewliao11/dni.pytorch): Implement Decoupled Neural Interfaces using Synthetic Gradients in Pytorch\n70. [wgan-gp](https://github.com/caogang/wgan-gp): A pytorch implementation of Paper "Improved Training of Wasserstein GANs".\n71. [pytorch-seq2seq-intent-parsing](https://github.com/spro/pytorch-seq2seq-intent-parsing): Intent parsing and slot filling in PyTorch with seq2seq + attention\n72. [pyTorch_NCE](https://github.com/demelin/pyTorch_NCE): An implementation of the Noise Contrastive Estimation algorithm for pyTorch. Working, yet not very efficient.\n73. [molencoder](https://github.com/cxhernandez/molencoder): Molecular AutoEncoder in PyTorch\n74. [GAN-weight-norm](https://github.com/stormraiser/GAN-weight-norm): Code for "On the Effects of Batch and Weight Normalization in Generative Adversarial Networks"\n75. [lgamma](https://github.com/rachtsingh/lgamma): Implementations of polygamma, lgamma, and beta functions for PyTorch\n76. [bigBatch](https://github.com/eladhoffer/bigBatch): Code used to generate the results appearing in "Train longer, generalize better: closing the generalization gap in large batch training of neural networks" \n77. [rl_a3c_pytorch](https://github.com/dgriff777/rl_a3c_pytorch): Reinforcement learning with implementation of A3C LSTM for Atari 2600. \n78. [pytorch-retraining](https://github.com/ahirner/pytorch-retraining): Transfer Learning Shootout for PyTorch''s model zoo (torchvision)\n79. [nmp_qc](https://github.com/priba/nmp_qc): Neural Message Passing for Computer Vision\n80. [grad-cam](https://github.com/jacobgil/pytorch-grad-cam): Pytorch implementation of Grad-CAM\n81. [pytorch-trpo](https://github.com/mjacar/pytorch-trpo): PyTorch Implementation of Trust Region Policy Optimization (TRPO)\n82. [pytorch-explain-black-box](https://github.com/jacobgil/pytorch-explain-black-box): PyTorch implementation of Interpretable Explanations of Black Boxes by Meaningful Perturbation\n83. [vae_vpflows](https://github.com/jmtomczak/vae_vpflows): Code in PyTorch for the convex combination linear IAF and the Householder Flow, J.M. Tomczak & M. Welling https://jmtomczak.github.io/deebmed.html \n84. [relational-networks](https://github.com/kimhc6028/relational-networks): Pytorch implementation of "A simple neural network module for relational reasoning" (Relational Networks) https://arxiv.org/pdf/1706.01427.pdf\n85. [vqa.pytorch](https://github.com/Cadene/vqa.pytorch): Visual Question Answering in Pytorch\n86. [end-to-end-negotiator](https://github.com/facebookresearch/end-to-end-negotiator): Deal or No Deal? End-to-End Learning for Negotiation Dialogues\n87. [odin-pytorch](https://github.com/ShiyuLiang/odin-pytorch): Principled Detection of Out-of-Distribution Examples in Neural Networks. \n88. [FreezeOut](https://github.com/ajbrock/FreezeOut): Accelerate Neural Net Training by Progressively Freezing Layers. \n89. [ARAE](https://github.com/jakezhaojb/ARAE): Code for the paper "Adversarially Regularized Autoencoders for Generating Discrete Structures" by Zhao, Kim, Zhang, Rush and LeCun.\n90. [forward-thinking-pytorch](https://github.com/kimhc6028/forward-thinking-pytorch): Pytorch implementation of "Forward Thinking: Building and Training Neural Networks One Layer at a Time" https://arxiv.org/pdf/1706.02480.pdf  \n91. [context_encoder_pytorch](https://github.com/BoyuanJiang/context_encoder_pytorch): PyTorch Implement of Context Encoders\n92. [attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch): A PyTorch implementation of the Transformer model in "Attention is All You Need".https://github.com/thnkim/OpenFacePytorch\n93. [OpenFacePytorch](https://github.com/thnkim/OpenFacePytorch): PyTorch module to use OpenFace''s nn4.small2.v1.t7 model \n94. [neural-combinatorial-rl-pytorch](https://github.com/pemami4911/neural-combinatorial-rl-pytorch):  PyTorch implementation of Neural Combinatorial Optimization with Reinforcement Learning.\n95. [pytorch-nec](https://github.com/mjacar/pytorch-nec): PyTorch Implementation of Neural Episodic Control (NEC)\n96. [seq2seq.pytorch](https://github.com/eladhoffer/seq2seq.pytorch): Sequence-to-Sequence learning using PyTorch\n97. [Pytorch-Sketch-RNN](https://github.com/alexis-jacq/Pytorch-Sketch-RNN): a pytorch implementation of arxiv.org/abs/1704.03477\n98. [pytorch-pruning](https://github.com/jacobgil/pytorch-pruning): PyTorch Implementation of [1611.06440] Pruning Convolutional Neural Networks for Resource Efficient Inference\n99. [DrQA](https://github.com/hitvoice/DrQA): A pytorch implementation of Reading Wikipedia to Answer Open-Domain Questions.\n100. [YellowFin_Pytorch](https://github.com/JianGoForIt/YellowFin_Pytorch): auto-tuning momentum SGD optimizer\n101. [samplernn-pytorch](https://github.com/deepsound-project/samplernn-pytorch): PyTorch implementation of SampleRNN: An Unconditional End-to-End Neural Audio Generation Model. \n102. [AEGeAN](https://github.com/tymokvo/AEGeAN): Deeper DCGAN with AE stabilization\n103. [/pytorch-SRResNet](https://github.com/twtygqyy/pytorch-SRResNet): pytorch implementation for Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network arXiv:1609.04802v2 \n104. [vsepp](https://github.com/fartashf/vsepp): Code for the paper "VSE++: Improved Visual Semantic Embeddings"\n105. [Pytorch-DPPO](https://github.com/alexis-jacq/Pytorch-DPPO): Pytorch implementation of Distributed Proximal Policy Optimization: arxiv.org/abs/1707.02286\n106. [UNIT](https://github.com/mingyuliutw/UNIT): PyTorch Implementation of our Coupled VAE-GAN algorithm for Unsupervised Image-to-Image Translation\n107. [efficient_densenet_pytorch](https://github.com/gpleiss/efficient_densenet_pytorch): A memory-efficient implementation of DenseNets\n108. [tsn-pytorch](https://github.com/yjxiong/tsn-pytorch): Temporal Segment Networks (TSN) in PyTorch.\n109. [SMASH](https://github.com/ajbrock/SMASH): An experimental technique for efficiently exploring neural architectures.\n110. [pytorch-retinanet](https://github.com/kuangliu/pytorch-retinanet): RetinaNet in PyTorch\n111. [biogans](https://github.com/aosokin/biogans):  Implementation supporting the ICCV 2017 paper "GANs for Biological Image Synthesis". \n112. [Semantic Image Synthesis via Adversarial Learning]( https://github.com/woozzu/dong_iccv_2017): A PyTorch implementation of the paper "Semantic Image Synthesis via Adversarial Learning" in ICCV 2017. \n113. [fmpytorch](https://github.com/jmhessel/fmpytorch): A PyTorch implementation of a Factorization Machine module in cython.\n114. [ORN](https://github.com/ZhouYanzhao/ORN): A PyTorch implementation of the paper "Oriented Response Networks" in CVPR 2017. \n115. [pytorch-maml](https://github.com/katerakelly/pytorch-maml): PyTorch implementation of MAML: arxiv.org/abs/1703.03400\n116. [pytorch-generative-model-collections](https://github.com/znxlwm/pytorch-generative-model-collections):  Collection of generative models in Pytorch version.\n117. [vqa-winner-cvprw-2017](https://github.com/markdtw/vqa-winner-cvprw-2017): Pytorch Implementation of winner from VQA Chllange Workshop in CVPR''17. \n118. [tacotron_pytorch](https://github.com/r9y9/tacotron_pytorch):  PyTorch implementation of Tacotron speech synthesis model. \n119. [pspnet-pytorch](https://github.com/Lextal/pspnet-pytorch): PyTorch implementation of PSPNet segmentation network\n120. [LM-LSTM-CRF](https://github.com/LiyuanLucasLiu/LM-LSTM-CRF): Empower Sequence Labeling with Task-Aware Language Model http://arxiv.org/abs/1709.04109\n121. [face-alignment](https://github.com/1adrianb/face-alignment): Pytorch implementation of the paper "How far are we from solving the 2D & 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)", ICCV 2017\n122. [DepthNet](https://github.com/ClementPinard/DepthNet): PyTorch DepthNet Training on Still Box dataset. \n123. [EDSR-PyTorch](https://github.com/thstkdgus35/EDSR-PyTorch): PyTorch version of the paper ''Enhanced Deep Residual Networks for Single Image Super-Resolution'' (CVPRW 2017)\n124. [e2c-pytorch](https://github.com/ethanluoyc/e2c-pytorch): Embed to Control implementation in PyTorch.\n125. [3D-ResNets-PyTorch](https://github.com/kenshohara/3D-ResNets-PyTorch): 3D ResNets for Action Recognition.\n126. [bandit-nmt](https://github.com/khanhptnk/bandit-nmt): This is code repo for our EMNLP 2017 paper "Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback", which implements the A2C algorithm on top of a neural encoder-decoder model and benchmarks the combination under simulated noisy rewards.\n127. [pytorch-a2c-ppo-acktr](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr): PyTorch implementation of Advantage Actor Critic (A2C), Proximal Policy Optimization (PPO) and Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (ACKTR).\n128. [zalando-pytorch](https://github.com/baldassarreFe/zalando-pytorch): Various experiments on the [Fashion-MNIST](zalandoresearch/fashion-mnist) dataset from Zalando.\n129. [sphereface_pytorch](https://github.com/clcarwin/sphereface_pytorch): A PyTorch Implementation of SphereFace.\n130. [Categorical DQN](https://github.com/floringogianu/categorical-dqn): A PyTorch Implementation of Categorical DQN from [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887).\n131. [pytorch-ntm](https://github.com/loudinthecloud/pytorch-ntm): pytorch ntm implementation. \n132. [mask_rcnn_pytorch](https://github.com/felixgwu/mask_rcnn_pytorch): Mask RCNN in PyTorch.\n133. [graph_convnets_pytorch](https://github.com/xbresson/graph_convnets_pytorch): PyTorch implementation of graph ConvNets, NIPS’16\n134. [pytorch-faster-rcnn](https://github.com/ruotianluo/pytorch-faster-rcnn): A pytorch implementation of faster RCNN detection framework based on Xinlei Chen''s tf-faster-rcnn.\n135. [torchMoji](https://github.com/huggingface/torchMoji): A pyTorch implementation of the DeepMoji model: state-of-the-art deep learning model for analyzing sentiment, emotion, sarcasm etc.\n136. [semantic-segmentation-pytorch](https://github.com/hangzhaomit/semantic-segmentation-pytorch): Pytorch implementation for Semantic Segmentation/Scene Parsing on [MIT ADE20K dataset](http://sceneparsing.csail.mit.edu)\n137. [pytorch-qrnn](https://github.com/salesforce/pytorch-qrnn): PyTorch implementation of the Quasi-Recurrent Neural Network - up to 16 times faster than NVIDIA''s cuDNN LSTM\n138. [pytorch-sgns](https://github.com/theeluwin/pytorch-sgns): Skipgram Negative Sampling in PyTorch.\n139. [SfmLearner-Pytorch ](https://github.com/ClementPinard/SfmLearner-Pytorch): Pytorch version of SfmLearner from Tinghui Zhou et al.\n140. [deformable-convolution-pytorch](https://github.com/1zb/deformable-convolution-pytorch): PyTorch implementation of Deformable Convolution. \n141. [skip-gram-pytorch](https://github.com/fanglanting/skip-gram-pytorch): A complete pytorch implementation of skipgram model (with subsampling and negative sampling). The embedding result is tested with Spearman''s rank correlation.\n142. [stackGAN-v2](https://github.com/hanzhanggit/StackGAN-v2): Pytorch implementation for reproducing StackGAN_v2 results in the paper StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks by Han Zhang*, Tao Xu*, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas.\n143. [self-critical.pytorch](https://github.com/ruotianluo/self-critical.pytorch): Unofficial pytorch implementation for Self-critical Sequence Training for Image Captioning. \n144. [pygcn](https://github.com/tkipf/pygcn): Graph Convolutional Networks in PyTorch.\n145. [dnc](https://github.com/ixaxaar/pytorch-dnc): Differentiable Neural Computers, for Pytorch\n146. [prog_gans_pytorch_inference](https://github.com/ptrblck/prog_gans_pytorch_inference): PyTorch inference for "Progressive Growing of GANs" with CelebA snapshot.\n147. [pytorch-capsule](https://github.com/timomernick/pytorch-capsule): Pytorch implementation of Hinton''s Dynamic Routing Between Capsules.\n148. [PyramidNet-PyTorch](https://github.com/dyhan0920/PyramidNet-PyTorch): A PyTorch implementation for PyramidNets (Deep Pyramidal Residual Networks, arxiv.org/abs/1610.02915)\n149. [radio-transformer-networks](https://github.com/gram-ai/radio-transformer-networks): A PyTorch implementation of Radio Transformer Networks from the paper "An Introduction to Deep Learning for the Physical Layer". arxiv.org/abs/1702.00832\n150. [honk](https://github.com/castorini/honk): PyTorch reimplementation of Google''s TensorFlow CNNs for keyword spotting.\n151. [DeepCORAL](https://github.com/SSARCandy/DeepCORAL): A PyTorch implementation of ''Deep CORAL: Correlation Alignment for Deep Domain Adaptation.'', ECCV 2016\n152. [pytorch-pose](https://github.com/bearpaw/pytorch-pose): A PyTorch toolkit for 2D Human Pose Estimation.\n153. [lang-emerge-parlai](https://github.com/karandesai-96/lang-emerge-parlai): Implementation of EMNLP 2017 Paper "Natural Language Does Not Emerge ''Naturally'' in Multi-Agent Dialog" using PyTorch and ParlAI\n154. [Rainbow](https://github.com/Kaixhin/Rainbow): Rainbow: Combining Improvements in Deep Reinforcement Learning \n155. [pytorch_compact_bilinear_pooling v1](https://github.com/gdlg/pytorch_compact_bilinear_pooling): This repository has a pure Python implementation of Compact Bilinear Pooling and Count Sketch for PyTorch.\n156. [CompactBilinearPooling-Pytorch v2](https://github.com/DeepInsight-PCALab/CompactBilinearPooling-Pytorch): (Yang Gao, et al.) A Pytorch Implementation for Compact Bilinear Pooling.\n157. [FewShotLearning](https://github.com/gitabcworld/FewShotLearning): Pytorch implementation of the paper "Optimization as a Model for Few-Shot Learning"\n158. [meProp](https://github.com/jklj077/meProp): Codes for "meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting".\n159. [SFD_pytorch](https://github.com/clcarwin/SFD_pytorch): A PyTorch Implementation of Single Shot Scale-invariant Face Detector.\n160. [GradientEpisodicMemory](https://github.com/facebookresearch/GradientEpisodicMemory): Continuum Learning with GEM: Gradient Episodic Memory. https://arxiv.org/abs/1706.08840\n161. [DeblurGAN](https://github.com/KupynOrest/DeblurGAN): Pytorch implementation of the paper DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks.\n162. [StarGAN](https://github.com/yunjey/StarGAN): StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Tranlsation.\n163. [CapsNet-pytorch](https://github.com/adambielski/CapsNet-pytorch): PyTorch implementation of NIPS 2017 paper Dynamic Routing Between Capsules.\n164. [CondenseNet](https://github.com/ShichenLiu/CondenseNet): CondenseNet: An Efficient DenseNet using Learned Group Convolutions.\n165. [deep-image-prior](https://github.com/DmitryUlyanov/deep-image-prior): Image restoration with neural networks but without learning.\n166. [deep-head-pose](https://github.com/natanielruiz/deep-head-pose): Deep Learning Head Pose Estimation using PyTorch.\n167. [Random-Erasing](https://github.com/zhunzhong07/Random-Erasing): This code has the source code for the paper "Random Erasing Data Augmentation".\n168. [FaderNetworks](https://github.com/facebookresearch/FaderNetworks): Fader Networks: Manipulating Images by Sliding Attributes - NIPS 2017\n169. [FlowNet 2.0](https://github.com/NVIDIA/flownet2-pytorch): FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks\n170. [pix2pixHD](https://github.com/NVIDIA/pix2pixHD): Synthesizing and manipulating 2048x1024 images with conditional GANs tcwang0509.github.io/pix2pixHD \n171. [pytorch-smoothgrad](https://github.com/pkdn/pytorch-smoothgrad): SmoothGrad implementation in PyTorch\n172. [RetinaNet](https://github.com/c0nn3r/RetinaNet): An implementation of RetinaNet in PyTorch.\n173. [faster-rcnn.pytorch](https://github.com/jwyang/faster-rcnn.pytorch): This project is a faster faster R-CNN implementation, aimed to accelerating the training of faster R-CNN object detection models. \n174. [mixup_pytorch](https://github.com/leehomyc/mixup_pytorch): A PyTorch implementation of the paper Mixup: Beyond Empirical Risk Minimization in PyTorch.\n175. [inplace_abn](https://github.com/mapillary/inplace_abn): In-Place Activated BatchNorm for Memory-Optimized Training of DNNs\n176. [pytorch-pose-hg-3d](https://github.com/xingyizhou/pytorch-pose-hg-3d): PyTorch implementation for 3D human pose estimation\n177. [nmn-pytorch](https://github.com/HarshTrivedi/nmn-pytorch): Neural Module Network for VQA in Pytorch.\n178. [bytenet](https://github.com/kefirski/bytenet): Pytorch implementation of bytenet from "Neural Machine Translation in Linear Time" paper\n179. [bottom-up-attention-vqa](https://github.com/hengyuan-hu/bottom-up-attention-vqa): vqa, bottom-up-attention, pytorch\n180. [yolo2-pytorch](https://github.com/ruiminshen/yolo2-pytorch): The YOLOv2 is one of the most popular one-stage object detector. This project adopts PyTorch as the developing framework to increase productivity, and utilize ONNX to convert models into Caffe 2 to benifit engineering deployment.\n181. [reseg-pytorch](https://github.com/Wizaron/reseg-pytorch): PyTorch Implementation of ReSeg (arxiv.org/pdf/1511.07053.pdf)\n182. [binary-stochastic-neurons](https://github.com/Wizaron/binary-stochastic-neurons): Binary Stochastic Neurons in PyTorch.\n183. [pytorch-pose-estimation](https://github.com/DavexPro/pytorch-pose-estimation): PyTorch Implementation of Realtime Multi-Person Pose Estimation project.\n184. [interaction_network_pytorch](https://github.com/higgsfield/interaction_network_pytorch): Pytorch Implementation of Interaction Networks for Learning about Objects, Relations and Physics.\n185. [NoisyNaturalGradient](https://github.com/wlwkgus/NoisyNaturalGradient): Pytorch Implementation of paper "Noisy Natural Gradient as Variational Inference". \n186. [ewc.pytorch](https://github.com/moskomule/ewc.pytorch): An implementation of Elastic Weight Consolidation (EWC), proposed in James Kirkpatrick et al. Overcoming catastrophic forgetting in neural networks 2016(10.1073/pnas.1611835114).\n187. [pytorch-zssr](https://github.com/jacobgil/pytorch-zssr): PyTorch implementation of 1712.06087 "Zero-Shot" Super-Resolution using Deep Internal Learning\n188. [deep_image_prior](https://github.com/atiyo/deep_image_prior): An implementation of image reconstruction methods from Deep Image Prior (Ulyanov et al., 2017) in PyTorch.\n189. [pytorch-transformer](https://github.com/leviswind/pytorch-transformer): pytorch implementation of Attention is all you need.\n190. [DeepRL-Grounding](https://github.com/devendrachaplot/DeepRL-Grounding): This is a PyTorch implementation of the AAAI-18 paper Gated-Attention Architectures for Task-Oriented Language Grounding\n191. [deep-forecast-pytorch](https://github.com/Wizaron/deep-forecast-pytorch): Wind Speed Prediction using LSTMs in PyTorch (arxiv.org/pdf/1707.08110.pdf)\n192. [cat-net](https://github.com/utiasSTARS/cat-net):  Canonical Appearance Transformations\n193. [minimal_glo](https://github.com/tneumann/minimal_glo): Minimal PyTorch implementation of Generative Latent Optimization from the paper "Optimizing the Latent Space of Generative Networks"\n194. [LearningToCompare-Pytorch](https://github.com/dragen1860/LearningToCompare-Pytorch): Pytorch Implementation for Paper: Learning to Compare: Relation Network for Few-Shot Learning. \n195. [poincare-embeddings](https://github.com/facebookresearch/poincare-embeddings): PyTorch implementation of the NIPS-17 paper "Poincaré Embeddings for Learning Hierarchical Representations". \n196. [pytorch-trpo(Hessian-vector product version)](https://github.com/ikostrikov/pytorch-trpo): This is a PyTorch implementation of "Trust Region Policy Optimization (TRPO)" with exact Hessian-vector product instead of finite differences approximation.\n197. [ggnn.pytorch](https://github.com/JamesChuanggg/ggnn.pytorch): A PyTorch Implementation of Gated Graph Sequence Neural Networks (GGNN). \n198. [visual-interaction-networks-pytorch](https://github.com/Mrgemy95/visual-interaction-networks-pytorch): This''s an implementation of deepmind Visual Interaction Networks paper using pytorch\n199. [adversarial-patch](https://github.com/jhayes14/adversarial-patch): PyTorch implementation of adversarial patch. \n200. [Prototypical-Networks-for-Few-shot-Learning-PyTorch](https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch): Implementation of Prototypical Networks for Few Shot Learning (arxiv.org/abs/1703.05175) in Pytorch\n201. [Visual-Feature-Attribution-Using-Wasserstein-GANs-Pytorch](https://github.com/orobix/Visual-Feature-Attribution-Using-Wasserstein-GANs-Pytorch): Implementation of Visual Feature Attribution using Wasserstein GANs (arxiv.org/abs/1711.08998) in PyTorch.\n202. [PhotographicImageSynthesiswithCascadedRefinementNetworks-Pytorch](https://github.com/Blade6570/PhotographicImageSynthesiswithCascadedRefinementNetworks-Pytorch): Photographic Image Synthesis with Cascaded Refinement Networks - Pytorch Implementation\n203. [ENAS-pytorch](https://github.com/carpedm20/ENAS-pytorch): PyTorch implementation of "Efficient Neural Architecture Search via Parameters Sharing". \n204. [Neural-IMage-Assessment](https://github.com/kentsyx/Neural-IMage-Assessment): A PyTorch Implementation of Neural IMage Assessment. \n205. [proxprop](https://github.com/tfrerix/proxprop): Proximal Backpropagation - a neural network training algorithm that takes implicit instead of explicit gradient steps.\n206. [FastPhotoStyle](https://github.com/NVIDIA/FastPhotoStyle): A Closed-form Solution to Photorealistic Image Stylization\n207. [Deep-Image-Analogy-PyTorch](https://github.com/Ben-Louis/Deep-Image-Analogy-PyTorch): A python implementation of Deep-Image-Analogy based on pytorch.\n208. [Person-reID_pytorch](https://github.com/layumi/Person_reID_baseline_pytorch): PyTorch for Person re-ID. \n209. [pt-dilate-rnn](https://github.com/zalandoresearch/pt-dilate-rnn): Dilated RNNs in pytorch. \n210. [pytorch-i-revnet](https://github.com/jhjacobsen/pytorch-i-revnet): Pytorch implementation of i-RevNets.\n211. [OrthNet](https://github.com/Orcuslc/OrthNet): TensorFlow and PyTorch layers for generating Orthogonal Polynomials.\n212. [DRRN-pytorch](https://github.com/jt827859032/DRRN-pytorch): An implementation of Deep Recursive Residual Network for Super Resolution (DRRN), CVPR 2017\n213. [shampoo.pytorch](https://github.com/moskomule/shampoo.pytorch): An implementation of shampoo.\n214. [Neural-IMage-Assessment 2](https://github.com/truskovskiyk/nima.pytorch): A PyTorch Implementation of Neural IMage Assessment.\n215. [TCN](https://github.com/locuslab/TCN): Sequence modeling benchmarks and temporal convolutional networks locuslab/TCN\n216. [DCC](https://github.com/shahsohil/DCC): This repository contains the source code and data for reproducing results of Deep Continuous Clustering paper.\n217. [packnet](https://github.com/arunmallya/packnet): Code for PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning arxiv.org/abs/1711.05769\n218. [PyTorch-progressive_growing_of_gans](https://github.com/github-pengge/PyTorch-progressive_growing_of_gans): PyTorch implementation of Progressive Growing of GANs for Improved Quality, Stability, and Variation.\n219. [nonauto-nmt](https://github.com/salesforce/nonauto-nmt): PyTorch Implementation of "Non-Autoregressive Neural Machine Translation"\n220. [PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN): PyTorch implementations of Generative Adversarial Networks.\n221. [PyTorchWavelets](https://github.com/tomrunia/PyTorchWavelets): PyTorch implementation of the wavelet analysis found in Torrence and Compo (1998)\n222. [pytorch-made](https://github.com/karpathy/pytorch-made): MADE (Masked Autoencoder Density Estimation) implementation in PyTorch\n223. [VRNN](https://github.com/emited/VariationalRecurrentNeuralNetwork): Pytorch implementation of the Variational RNN (VRNN), from A Recurrent Latent Variable Model for Sequential Data.\n224. [flow](https://github.com/emited/flow): Pytorch implementation of ICLR 2018 paper Deep Learning for Physical Processes: Integrating Prior Scientific Knowledge.\n225. [deepvoice3_pytorch](https://github.com/r9y9/deepvoice3_pytorch): PyTorch implementation of convolutional networks-based text-to-speech synthesis models\n226. [psmm](https://github.com/elanmart/psmm): imlementation of the the Pointer Sentinel Mixture Model, as described in the paper by Stephen Merity et al.\n227. [tacotron2](https://github.com/NVIDIA/tacotron2): Tacotron 2 - PyTorch implementation with faster-than-realtime inference.\n228. [AccSGD](https://github.com/rahulkidambi/AccSGD): Implements pytorch code for the Accelerated SGD algorithm.\n229. [QANet-pytorch](https://github.com/hengruo/QANet-pytorch): an implementation of QANet with PyTorch (EM/F1 = 70.5/77.2 after 20 epoches for about 20 hours on one 1080Ti card.)\n230. [ConvE](https://github.com/TimDettmers/ConvE): Convolutional 2D Knowledge Graph Embeddings\n231. [Structured-Self-Attention](https://github.com/kaushalshetty/Structured-Self-Attention): Implementation for the paper A Structured Self-Attentive Sentence Embedding, which is published in ICLR 2017: arxiv.org/abs/1703.03130 .\n232. [graphsage-simple](https://github.com/williamleif/graphsage-simple): Simple reference implementation of GraphSAGE.\n233. [Detectron.pytorch](https://github.com/roytseng-tw/Detectron.pytorch): A pytorch implementation of Detectron. Both training from scratch and inferring directly from pretrained Detectron weights are available.\n234. [R2Plus1D-PyTorch](https://github.com/irhumshafkat/R2Plus1D-PyTorch): PyTorch implementation of the R2Plus1D convolution based ResNet architecture described in the paper "A Closer Look at Spatiotemporal Convolutions for Action Recognition"\n235. [StackNN](https://github.com/viking-sudo-rm/StackNN): A PyTorch implementation of differentiable stacks for use in neural networks.\n236. [translagent](https://github.com/facebookresearch/translagent): Code for Emergent Translation in Multi-Agent Communication.\n237. [ban-vqa](https://github.com/jnhwkim/ban-vqa): Bilinear attention networks for visual question answering. \n238. [pytorch-openai-transformer-lm](https://github.com/huggingface/pytorch-openai-transformer-lm): This is a PyTorch implementation of the TensorFlow code provided with OpenAI''s paper "Improving Language Understanding by Generative Pre-Training" by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.\n239. [T2F](https://github.com/akanimax/T2F): Text-to-Face generation using Deep Learning. This project combines two of the recent architectures StackGAN and ProGAN for synthesizing faces from textual descriptions.\n240. [pytorch - fid](https://github.com/mseitzer/pytorch-fid): A Port of Fréchet Inception Distance (FID score) to PyTorch\n241. [vae_vpflows](https://github.com/jmtomczak/vae_vpflows):Code in PyTorch for the convex combination linear IAF and the Householder Flow, J.M. Tomczak & M. Welling jmtomczak.github.io/deebmed.html\n242. [CoordConv-pytorch](https://github.com/mkocabas/CoordConv-pytorch): Pytorch implementation of CoordConv introduced in ''An intriguing failing of convolutional neural networks and the CoordConv solution'' paper. (arxiv.org/pdf/1807.03247.pdf)\n243. [SDPoint](https://github.com/xternalz/SDPoint): Implementation of "Stochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks", published in CVPR 2018. \n244. [SRDenseNet-pytorch](https://github.com/wxywhu/SRDenseNet-pytorch): SRDenseNet-pytorch（ICCV_2017）\n245. [GAN_stability](https://github.com/LMescheder/GAN_stability): Code for paper "Which Training Methods for GANs do actually Converge? (ICML 2018)"\n246. [Mask-RCNN](https://github.com/wannabeOG/Mask-RCNN): A PyTorch implementation of the architecture of Mask RCNN, serves as an introduction to working with PyTorch\n247. [pytorch-coviar](https://github.com/chaoyuaw/pytorch-coviar): Compressed Video Action Recognition\n248. [PNASNet.pytorch](https://github.com/chenxi116/PNASNet.pytorch): PyTorch implementation of PNASNet-5 on ImageNet. \n249. [NALU-pytorch](https://github.com/kevinzakka/NALU-pytorch): Basic pytorch implementation of NAC/NALU from Neural Arithmetic Logic Units arxiv.org/pdf/1808.00508.pdf\n250. [LOLA_DiCE](https://github.com/alexis-jacq/LOLA_DiCE): Pytorch implementation of LOLA (arxiv.org/abs/1709.04326) using DiCE (arxiv.org/abs/1802.05098)\n251. [generative-query-network-pytorch](https://github.com/wohlert/generative-query-network-pytorch): Generative Query Network (GQN) in PyTorch as described in "Neural Scene Representation and Rendering"\n252. [pytorch_hmax](https://github.com/wmvanvliet/pytorch_hmax): Implementation of the HMAX model of vision in PyTorch.\n253. [FCN-pytorch-easiest](https://github.com/yunlongdong/FCN-pytorch-easiest): trying to be the most easiest and just get-to-use pytorch implementation of FCN (Fully Convolotional Networks)\n254. [transducer](https://github.com/awni/transducer): A Fast Sequence Transducer Implementation with PyTorch Bindings.\n255. [AVO-pytorch](https://github.com/artix41/AVO-pytorch): Implementation of Adversarial Variational Optimization in PyTorch.\n256. [HCN-pytorch](https://github.com/huguyuehuhu/HCN-pytorch): A pytorch reimplementation of { Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation }.\n257. [binary-wide-resnet](https://github.com/szagoruyko/binary-wide-resnet): PyTorch implementation of Wide Residual Networks with 1-bit weights by McDonnel (ICLR 2018)\n258. [piggyback](https://github.com/arunmallya/piggyback): Code for Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights arxiv.org/abs/1801.06519\n259. [vid2vid](https://github.com/NVIDIA/vid2vid): Pytorch implementation of our method for high-resolution (e.g. 2048x1024) photorealistic video-to-video translation.\n260. [poisson-convolution-sum](https://github.com/cranmer/poisson-convolution-sum): Implements an infinite sum of poisson-weighted convolutions\n261. [tbd-nets](https://github.com/davidmascharka/tbd-nets): PyTorch implementation of "Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning" arxiv.org/abs/1803.05268 \n262. [attn2d](https://github.com/elbayadm/attn2d): Pervasive Attention: 2D Convolutional Networks for Sequence-to-Sequence Prediction\n263. [yolov3](https://github.com/ultralytics/yolov3): YOLOv3: Training and inference in PyTorch pjreddie.com/darknet/yolo\n264. [deep-dream-in-pytorch](https://github.com/duc0/deep-dream-in-pytorch): Pytorch implementation of the DeepDream computer vision algorithm. \n265. [pytorch-flows](https://github.com/ikostrikov/pytorch-flows): PyTorch implementations of algorithms for density estimation\n266. [quantile-regression-dqn-pytorch](https://github.com/ars-ashuha/quantile-regression-dqn-pytorch): Quantile Regression DQN a Minimal Working Example\n267. [relational-rnn-pytorch](https://github.com/L0SG/relational-rnn-pytorch): An implementation of DeepMind''s Relational Recurrent Neural Networks in PyTorch.\n268. [DEXTR-PyTorch](https://github.com/scaelles/DEXTR-PyTorch): Deep Extreme Cut http://www.vision.ee.ethz.ch/~cvlsegmentation/dextr\n269. [PyTorch_GBW_LM](https://github.com/rdspring1/PyTorch_GBW_LM): PyTorch Language Model for Google Billion Word Dataset.\n270. [Pytorch-NCE](https://github.com/Stonesjtu/Pytorch-NCE): The Noise Contrastive Estimation for softmax output written in Pytorch\n271. [generative-models](https://github.com/shayneobrien/generative-models): Annotated, understandable, and visually interpretable PyTorch implementations of: VAE, BIRVAE, NSGAN, MMGAN, WGAN, WGANGP, LSGAN, DRAGAN, BEGAN, RaGAN, InfoGAN, fGAN, FisherGAN. \n272. [convnet-aig](https://github.com/andreasveit/convnet-aig): PyTorch implementation for Convolutional Networks with Adaptive Inference Graphs.\n273. [integrated-gradient-pytorch](https://github.com/TianhongDai/integrated-gradient-pytorch): This is the pytorch implementation of the paper - Axiomatic Attribution for Deep Networks.\n274. [MalConv-Pytorch](https://github.com/Alexander-H-Liu/MalConv-Pytorch): Pytorch implementation of MalConv. \n275. [trellisnet](https://github.com/locuslab/trellisnet): Trellis Networks for Sequence Modeling\n276. [Learning to Communicate with Deep Multi-Agent Reinforcement Learning](https://github.com/minqi/learning-to-communicate-pytorch): pytorch implementation of  Learning to Communicate with Deep Multi-Agent Reinforcement Learning paper.\n277. [pnn.pytorch](https://github.com/michaelklachko/pnn.pytorch): PyTorch implementation of CVPR''18 - Perturbative Neural Networks http://xujuefei.com/pnn.html.\n278. [Face_Attention_Network](https://github.com/rainofmine/Face_Attention_Network): Pytorch implementation of face attention network as described in Face Attention Network: An Effective Face Detector for the Occluded Faces.\n279. [waveglow](https://github.com/NVIDIA/waveglow): A Flow-based Generative Network for Speech Synthesis.\n280. [deepfloat](https://github.com/facebookresearch/deepfloat): This repository contains the SystemVerilog RTL, C++, HLS (Intel FPGA OpenCL to wrap RTL code) and Python needed to reproduce the numerical results in "Rethinking floating point for deep learning" \n281. [EPSR](https://github.com/subeeshvasu/2018_subeesh_epsr_eccvw): Pytorch implementation of [Analyzing Perception-Distortion Tradeoff using Enhanced Perceptual Super-resolution Network](https://arxiv.org/pdf/1811.00344.pdf). This work has won the first place in PIRM2018-SR competition (region 1) held as part of the ECCV 2018.\n282. [ClariNet](https://github.com/ksw0306/ClariNet): A Pytorch Implementation of ClariNet arxiv.org/abs/1807.07281\n283. [pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT): PyTorch version of Google AI''s BERT model with script to load Google''s pre-trained models\n284. [torch_waveglow](https://github.com/npuichigo/waveglow): A PyTorch implementation of the WaveGlow: A Flow-based Generative Network for Speech Synthesis. \n285. [3DDFA](https://github.com/cleardusk/3DDFA): The pytorch improved re-implementation of TPAMI 2017 paper: Face Alignment in Full Pose Range: A 3D Total Solution.\n286. [loss-landscape](https://github.com/tomgoldstein/loss-landscape): loss-landscape Code for visualizing the loss landscape of neural nets.\n287. [famos](https://github.com/zalandoresearch/famos): \nPytorch implementation of the paper "Copy the Old or Paint Anew? An Adversarial Framework for (non-) Parametric Image Stylization" available at http://arxiv.org/abs/1811.09236.\n288. [back2future.pytorch](https://github.com/anuragranj/back2future.pytorch): This is a Pytorch implementation of\nJanai, J., Güney, F., Ranjan, A., Black, M. and Geiger, A., Unsupervised Learning of Multi-Frame Optical Flow with Occlusions. ECCV 2018.\n289. [FFTNet](https://github.com/mozilla/FFTNet): Unofficial Implementation of FFTNet vocode paper.\n290. [FaceBoxes.PyTorch](https://github.com/zisianw/FaceBoxes.PyTorch): A PyTorch Implementation of FaceBoxes.\n291. [Transformer-XL](https://github.com/kimiyoung/transformer-xl): Transformer-XL: Attentive Language Models Beyond a Fixed-Length Contexthttps://github.com/kimiyoung/transformer-xl\n292. [associative_compression_networks](https://github.com/jalexvig/associative_compression_networks): Associative Compression Networks for Representation Learning. \n293. [fluidnet_cxx](https://github.com/jolibrain/fluidnet_cxx): FluidNet re-written with ATen tensor lib. \n294. [Deep-Reinforcement-Learning-Algorithms-with-PyTorch](https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch): This repository contains PyTorch implementations of deep reinforcement learning algorithms.\n295. [Shufflenet-v2-Pytorch](https://github.com/ericsun99/Shufflenet-v2-Pytorch): This is a Pytorch implementation of faceplusplus''s ShuffleNet-v2. \n296. [GraphWaveletNeuralNetwork](https://github.com/benedekrozemberczki/GraphWaveletNeuralNetwork): This is a Pytorch implementation of Graph Wavelet Neural Network. ICLR 2019. \n297. [AttentionWalk](https://github.com/benedekrozemberczki/AttentionWalk): This is a Pytorch implementation of Watch Your Step: Learning Node Embeddings via Graph Attention. NIPS 2018.\n298. [SGCN](https://github.com/benedekrozemberczki/SGCN): This is a Pytorch implementation of Signed Graph Convolutional Network. ICDM 2018.\n299. [SINE](https://github.com/benedekrozemberczki/SINE): This is a Pytorch implementation of SINE: Scalable Incomplete Network Embedding. ICDM 2018.\n300. [GAM](https://github.com/benedekrozemberczki/GAM): This is a Pytorch implementation of Graph Classification using Structural Attention. KDD 2018.\n301. [neural-style-pt](https://github.com/ProGamerGov/neural-style-pt): A PyTorch implementation of Justin Johnson''s Neural-style.\n302. [TuckER](https://github.com/ibalazevic/TuckER): TuckER: Tensor Factorization for Knowledge Graph Completion.\n303. [pytorch-prunes](https://github.com/BayesWatch/pytorch-prunes): Pruning neural networks: is it time to nip it in the bud?\n304. [SimGNN](https://github.com/benedekrozemberczki/SimGNN): SimGNN: A Neural Network Approach to Fast Graph Similarity Computation.\n305. [Character CNN](https://github.com/ahmedbesbes/character-based-cnn): PyTorch implementation of the Character-level Convolutional Networks for Text Classification paper. \n306. [XLM](https://github.com/facebookresearch/XLM): PyTorch original implementation of Cross-lingual Language Model Pretraining.\n307. [DiffAI](https://github.com/eth-sri/diffai): A provable defense against adversarial examples and library for building compatible PyTorch models.\n308. [APPNP](https://github.com/benedekrozemberczki/APPNP): Combining Neural Networks with Personalized PageRank for Classification on Graphs. ICLR 2019.\n309. [NGCN](https://github.com/benedekrozemberczki/MixHop-and-N-GCN): A Higher-Order Graph Convolutional Layer. NeurIPS 2018.\n310. [gpt-2-Pytorch](https://github.com/graykode/gpt-2-Pytorch): Simple Text-Generator with OpenAI gpt-2 Pytorch Implementation\n311. [Splitter](https://github.com/benedekrozemberczki/Splitter): Splitter: Learning Node Representations that Capture Multiple Social Contexts. (WWW 2019).\n312. [CapsGNN](https://github.com/benedekrozemberczki/CapsGNN): Capsule Graph Neural Network. (ICLR 2019).\n313. [BigGAN-PyTorch](https://github.com/ajbrock/BigGAN-PyTorch): The author''s officially unofficial PyTorch BigGAN implementation.\n314. [ppo_pytorch_cpp](https://github.com/mhubii/ppo_pytorch_cpp): This is an implementation of the proximal policy optimization algorithm for the C++ API of Pytorch.\n315. [RandWireNN](https://github.com/seungwonpark/RandWireNN): Implementation of: "Exploring Randomly Wired Neural Networks for Image Recognition".\n316. [Zero-shot Intent CapsNet](https://github.com/joel-huang/zeroshot-capsnet-pytorch): GPU-accelerated PyTorch implementation of "Zero-shot User Intent Detection via Capsule Neural Networks".\n317. [SEAL-CI](https://github.com/benedekrozemberczki/SEAL-CI) Semi-Supervised Graph Classification: A Hierarchical Graph Perspective. (WWW 2019).\n318. [MixHop](https://github.com/benedekrozemberczki/MixHop-and-N-GCN): MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing. ICML 2019.\n319. [densebody_pytorch](https://github.com/Lotayou/densebody_pytorch): PyTorch implementation of CloudWalk''s recent paper DenseBody.\n320. [voicefilter](https://github.com/mindslab-ai/voicefilter): Unofficial PyTorch implementation of Google AI''s VoiceFilter system http://swpark.me/voicefilter. \n321. [NVIDIA/semantic-segmentation](https://github.com/NVIDIA/semantic-segmentation): A PyTorch Implementation of [Improving Semantic Segmentation via Video Propagation and Label Relaxation](https://arxiv.org/abs/1812.01593), In CVPR2019. \n322. [ClusterGCN](https://github.com/benedekrozemberczki/ClusterGCN): A PyTorch implementation of "Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks" (KDD 2019).\n323. [NVlabs/DG-Net](https://github.com/NVlabs/DG-Net): A PyTorch implementation of "Joint Discriminative and Generative Learning for Person Re-identification" (CVPR19 Oral). \n324. [NCRF](https://github.com/baidu-research/NCRF): Cancer metastasis detection with neural conditional random field (NCRF)\n325. [pytorch-sift](https://github.com/ducha-aiki/pytorch-sift): PyTorch implementation of SIFT descriptor. \n326. [brain-segmentation-pytorch](https://github.com/mateuszbuda/brain-segmentation-pytorch): U-Net implementation in PyTorch for FLAIR abnormality segmentation in brain MRI. \n327. [glow-pytorch](https://github.com/rosinality/glow-pytorch): PyTorch implementation of Glow, Generative Flow with Invertible 1x1 Convolutions (arxiv.org/abs/1807.03039) \n328. [EfficientNets-PyTorch](https://github.com/zsef123/EfficientNets-PyTorch): A PyT', '{"language":null,"stars":16284,"forks":2828,"watchers":16284,"open_issues":10,"topics":["awesome","awesome-list","computer-vision","cv","data-science","deep-learning","facebook","machine-learning","natural-language-processing","neural-network","nlp","nlp-library","papers","probabilistic-programming","python","pytorch","pytorch-model","pytorch-tutorials","tutorials","utility-library"],"default_branch":"master","size_kb":888,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:pytorch:captum","source_url":"https://github.com/pytorch/captum"},{"type":"has_code","target_id":"github:pytorch:text","source_url":"https://github.com/pytorch/text"},{"type":"has_code","target_id":"github:IBM:pytorch-seq2seq","source_url":"https://github.com/IBM/pytorch-seq2seq"},{"type":"has_code","target_id":"github:Sandeep42:anuvada","source_url":"https://github.com/Sandeep42/anuvada"},{"type":"has_code","target_id":"github:pytorch:audio","source_url":"https://github.com/pytorch/audio"},{"type":"has_code","target_id":"github:facebookresearch:loop","source_url":"https://github.com/facebookresearch/loop"},{"type":"has_code","target_id":"github:facebookresearch:fairseq-py","source_url":"https://github.com/facebookresearch/fairseq-py"},{"type":"has_code","target_id":"github:awni:speech","source_url":"https://github.com/awni/speech"},{"type":"has_code","target_id":"github:OpenNMT:OpenNMT-py","source_url":"https://github.com/OpenNMT/OpenNMT-py"},{"type":"has_code","target_id":"github:huggingface:neuralcoref","source_url":"https://github.com/huggingface/neuralcoref"},{"type":"has_code","target_id":"github:NVIDIA:sentiment-discovery","source_url":"https://github.com/NVIDIA/sentiment-discovery"},{"type":"has_code","target_id":"github:facebookresearch:MUSE","source_url":"https://github.com/facebookresearch/MUSE"},{"type":"has_code","target_id":"github:lium-lst:nmtpytorch","source_url":"https://github.com/lium-lst/nmtpytorch"},{"type":"has_code","target_id":"github:vincentherrmann:pytorch-wavenet","source_url":"https://github.com/vincentherrmann/pytorch-wavenet"},{"type":"has_code","target_id":"github:soobinseo:Tacotron-pytorch","source_url":"https://github.com/soobinseo/Tacotron-pytorch"},{"type":"has_code","target_id":"github:allenai:allennlp","source_url":"https://github.com/allenai/allennlp"},{"type":"has_code","target_id":"github:PetrochukM:PyTorch-NLP","source_url":"https://github.com/PetrochukM/PyTorch-NLP"},{"type":"has_code","target_id":"github:outcastofmusic:quick-nlp","source_url":"https://github.com/outcastofmusic/quick-nlp"},{"type":"has_code","target_id":"github:mozilla:TTS","source_url":"https://github.com/mozilla/TTS"},{"type":"has_code","target_id":"github:facebookresearch:LASER","source_url":"https://github.com/facebookresearch/LASER"},{"type":"has_code","target_id":"github:pyannote:pyannote-audio","source_url":"https://github.com/pyannote/pyannote-audio"},{"type":"has_code","target_id":"github:Maluuba:gensen","source_url":"https://github.com/Maluuba/gensen"},{"type":"has_code","target_id":"github:pytorch:translate","source_url":"https://github.com/pytorch/translate"},{"type":"has_code","target_id":"github:espnet:espnet","source_url":"https://github.com/espnet/espnet"},{"type":"has_code","target_id":"github:facebookresearch:pythia","source_url":"https://github.com/facebookresearch/pythia"},{"type":"has_code","target_id":"github:facebookresearch:UnsupervisedMT","source_url":"https://github.com/facebookresearch/UnsupervisedMT"},{"type":"has_code","target_id":"github:jsalt18-sentence-repl:jiant","source_url":"https://github.com/jsalt18-sentence-repl/jiant"},{"type":"has_code","target_id":"github:codertimo:BERT-pytorch","source_url":"https://github.com/codertimo/BERT-pytorch"},{"type":"has_code","target_id":"github:facebookresearch:InferSent","source_url":"https://github.com/facebookresearch/InferSent"},{"type":"has_code","target_id":"github:google:uis-rnn","source_url":"https://github.com/google/uis-rnn"},{"type":"has_code","target_id":"github:zalandoresearch:flair","source_url":"https://github.com/zalandoresearch/flair"},{"type":"has_code","target_id":"github:facebookresearch:pytext","source_url":"https://github.com/facebookresearch/pytext"},{"type":"has_code","target_id":"github:mindslab-ai:voicefilter","source_url":"https://github.com/mindslab-ai/voicefilter"},{"type":"has_code","target_id":"github:kamalkraj:BERT-NER","source_url":"https://github.com/kamalkraj/BERT-NER"},{"type":"has_code","target_id":"github:feedly:transfer-nlp","source_url":"https://github.com/feedly/transfer-nlp"},{"type":"has_code","target_id":"github:asyml:texar-pytorch","source_url":"https://github.com/asyml/texar-pytorch"},{"type":"has_code","target_id":"github:mravanelli:pytorch-kaldi","source_url":"https://github.com/mravanelli/pytorch-kaldi"},{"type":"has_code","target_id":"github:NVIDIA:NeMo","source_url":"https://github.com/NVIDIA/NeMo"},{"type":"has_code","target_id":"github:harvardnlp:pytorch-struct","source_url":"https://github.com/harvardnlp/pytorch-struct"},{"type":"has_code","target_id":"github:freewym:espresso","source_url":"https://github.com/freewym/espresso"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:lucidrains:reformer-pytorch","source_url":"https://github.com/lucidrains/reformer-pytorch"},{"type":"has_code","target_id":"github:enochkan:torch-metrics","source_url":"https://github.com/enochkan/torch-metrics"},{"type":"has_code","target_id":"github:speechbrain:speechbrain","source_url":"https://github.com/speechbrain/speechbrain"},{"type":"has_code","target_id":"github:backprop-ai:backprop","source_url":"https://github.com/backprop-ai/backprop"},{"type":"has_code","target_id":"github:pytorch:vision","source_url":"https://github.com/pytorch/vision"},{"type":"has_code","target_id":"github:tymokvo:pt-styletransfer","source_url":"https://github.com/tymokvo/pt-styletransfer"},{"type":"has_code","target_id":"github:thnkim:OpenFacePytorch","source_url":"https://github.com/thnkim/OpenFacePytorch"},{"type":"has_code","target_id":"github:felixgwu:img_classification_pk_pytorch","source_url":"https://github.com/felixgwu/img_classification_pk_pytorch"},{"type":"has_code","target_id":"github:facebookresearch:SparseConvNet","source_url":"https://github.com/facebookresearch/SparseConvNet"},{"type":"has_code","target_id":"github:automan000:Convolution_LSTM_pytorch","source_url":"https://github.com/automan000/Convolution_LSTM_pytorch"},{"type":"has_code","target_id":"github:1adrianb:face-alignment","source_url":"https://github.com/1adrianb/face-alignment"},{"type":"has_code","target_id":"github:ZijunDeng:pytorch-semantic-segmentation","source_url":"https://github.com/ZijunDeng/pytorch-semantic-segmentation"},{"type":"has_code","target_id":"github:longcw:RoIAlign.pytorch","source_url":"https://github.com/longcw/RoIAlign.pytorch"},{"type":"has_code","target_id":"github:creafz:pytorch-cnn-finetune","source_url":"https://github.com/creafz/pytorch-cnn-finetune"},{"type":"has_code","target_id":"github:ignacio-rocco:detectorch","source_url":"https://github.com/ignacio-rocco/detectorch"},{"type":"has_code","target_id":"github:mdbloice:Augmentor","source_url":"https://github.com/mdbloice/Augmentor"},{"type":"has_code","target_id":"github:jonas-koehler:s2cnn","source_url":"https://github.com/jonas-koehler/s2cnn"},{"type":"has_code","target_id":"github:donnyyou:torchcv","source_url":"https://github.com/donnyyou/torchcv"},{"type":"has_code","target_id":"github:facebookresearch:maskrcnn-benchmark","source_url":"https://github.com/facebookresearch/maskrcnn-benchmark"},{"type":"has_code","target_id":"github:osmr:imgclsmob","source_url":"https://github.com/osmr/imgclsmob"},{"type":"has_code","target_id":"github:perone:medicaltorch","source_url":"https://github.com/perone/medicaltorch"},{"type":"has_code","target_id":"github:albu:albumentations","source_url":"https://github.com/albu/albumentations"},{"type":"has_code","target_id":"github:arraiyopensource:kornia","source_url":"https://github.com/arraiyopensource/kornia"},{"type":"has_code","target_id":"github:s3nh:pytorch-text-recognition","source_url":"https://github.com/s3nh/pytorch-text-recognition"},{"type":"has_code","target_id":"github:timesler:facenet-pytorch","source_url":"https://github.com/timesler/facenet-pytorch"},{"type":"has_code","target_id":"github:facebookresearch:detectron2","source_url":"https://github.com/facebookresearch/detectron2"},{"type":"has_code","target_id":"github:Media-Smart:vedaseg","source_url":"https://github.com/Media-Smart/vedaseg"},{"type":"has_code","target_id":"github:facebookresearch:ClassyVision","source_url":"https://github.com/facebookresearch/ClassyVision"},{"type":"has_code","target_id":"github:alankbi:detecto","source_url":"https://github.com/alankbi/detecto"},{"type":"has_code","target_id":"github:facebookresearch:pytorch3d","source_url":"https://github.com/facebookresearch/pytorch3d"},{"type":"has_code","target_id":"github:open-mmlab:mmdetection","source_url":"https://github.com/open-mmlab/mmdetection"},{"type":"has_code","target_id":"github:ProGamerGov:neural-dream","source_url":"https://github.com/ProGamerGov/neural-dream"},{"type":"has_code","target_id":"github:MisaOgura:flashtorch","source_url":"https://github.com/MisaOgura/flashtorch"},{"type":"has_code","target_id":"github:greentfrapp:lucent","source_url":"https://github.com/greentfrapp/lucent"},{"type":"has_code","target_id":"github:open-mmlab:mmdetection3d","source_url":"https://github.com/open-mmlab/mmdetection3d"},{"type":"has_code","target_id":"github:open-mmlab:mmsegmentation","source_url":"https://github.com/open-mmlab/mmsegmentation"},{"type":"has_code","target_id":"github:open-mmlab:mmediting","source_url":"https://github.com/open-mmlab/mmediting"},{"type":"has_code","target_id":"github:open-mmlab:mmaction2","source_url":"https://github.com/open-mmlab/mmaction2"},{"type":"has_code","target_id":"github:open-mmlab:mmpose","source_url":"https://github.com/open-mmlab/mmpose"},{"type":"has_code","target_id":"github:lightly-ai:lightly","source_url":"https://github.com/lightly-ai/lightly"},{"type":"has_code","target_id":"github:stepelu:ptstat","source_url":"https://github.com/stepelu/ptstat"},{"type":"has_code","target_id":"github:uber:pyro","source_url":"https://github.com/uber/pyro"},{"type":"has_code","target_id":"github:probtorch:probtorch","source_url":"https://github.com/probtorch/probtorch"},{"type":"has_code","target_id":"github:drckf:paysage","source_url":"https://github.com/drckf/paysage"},{"type":"has_code","target_id":"github:ctallec:pyvarinf","source_url":"https://github.com/ctallec/pyvarinf"},{"type":"has_code","target_id":"github:probprog:pyprob","source_url":"https://github.com/probprog/pyprob"},{"type":"has_code","target_id":"github:spring-epfl:mia","source_url":"https://github.com/spring-epfl/mia"},{"type":"has_code","target_id":"github:akanimax:pro_gan_pytorch","source_url":"https://github.com/akanimax/pro_gan_pytorch"},{"type":"has_code","target_id":"github:pytorch:botorch","source_url":"https://github.com/pytorch/botorch"},{"type":"has_code","target_id":"github:mrdrozdov:pytorch-extras","source_url":"https://github.com/mrdrozdov/pytorch-extras"},{"type":"has_code","target_id":"github:szagoruyko:functional-zoo","source_url":"https://github.com/szagoruyko/functional-zoo"},{"type":"has_code","target_id":"github:ncullen93:torchsample","source_url":"https://github.com/ncullen93/torchsample"},{"type":"has_code","target_id":"github:deepcraft:torchcraft-py","source_url":"https://github.com/deepcraft/torchcraft-py"},{"type":"has_code","target_id":"github:ramon-oliveira:aorun","source_url":"https://github.com/ramon-oliveira/aorun"},{"type":"has_code","target_id":"github:oval-group:logger","source_url":"https://github.com/oval-group/logger"},{"type":"has_code","target_id":"github:iamaziz:PyTorch-docset","source_url":"https://github.com/iamaziz/PyTorch-docset"},{"type":"has_code","target_id":"github:clcarwin:convert_torch_to_pytorch","source_url":"https://github.com/clcarwin/convert_torch_to_pytorch"},{"type":"has_code","target_id":"github:Cadene:pretrained-models.pytorch","source_url":"https://github.com/Cadene/pretrained-models.pytorch"},{"type":"has_code","target_id":"github:locuslab:pytorch_fft","source_url":"https://github.com/locuslab/pytorch_fft"},{"type":"has_code","target_id":"github:fanq15:caffe_to_torch_to_pytorch","source_url":"https://github.com/fanq15/caffe_to_torch_to_pytorch"},{"type":"has_code","target_id":"github:sniklaus:pytorch-extension","source_url":"https://github.com/sniklaus/pytorch-extension"},{"type":"has_code","target_id":"github:lanpa:tensorboard-pytorch","source_url":"https://github.com/lanpa/tensorboard-pytorch"},{"type":"has_code","target_id":"github:jrg365:gpytorch","source_url":"https://github.com/jrg365/gpytorch"},{"type":"has_code","target_id":"github:maciejkula:spotlight","source_url":"https://github.com/maciejkula/spotlight"},{"type":"has_code","target_id":"github:awentzonline:pytorch-cns","source_url":"https://github.com/awentzonline/pytorch-cns"},{"type":"has_code","target_id":"github:szagoruyko:pyinn","source_url":"https://github.com/szagoruyko/pyinn"},{"type":"has_code","target_id":"github:nasimrahaman:inferno","source_url":"https://github.com/nasimrahaman/inferno"},{"type":"has_code","target_id":"github:henryre:pytorch-fitmodule","source_url":"https://github.com/henryre/pytorch-fitmodule"},{"type":"has_code","target_id":"github:dnouri:inferno","source_url":"https://github.com/dnouri/inferno"},{"type":"has_code","target_id":"github:marvis:pytorch-caffe-darknet-convert","source_url":"https://github.com/marvis/pytorch-caffe-darknet-convert"},{"type":"has_code","target_id":"github:longcw:pytorch2caffe","source_url":"https://github.com/longcw/pytorch2caffe"},{"type":"has_code","target_id":"github:nearai:pytorch-tools","source_url":"https://github.com/nearai/pytorch-tools"},{"type":"has_code","target_id":"github:taolei87:sru","source_url":"https://github.com/taolei87/sru"},{"type":"has_code","target_id":"github:prisma-ai:torch2coreml","source_url":"https://github.com/prisma-ai/torch2coreml"},{"type":"has_code","target_id":"github:zhanghang1989:PyTorch-Encoding","source_url":"https://github.com/zhanghang1989/PyTorch-Encoding"},{"type":"has_code","target_id":"github:ryanleary:pytorch-ctc","source_url":"https://github.com/ryanleary/pytorch-ctc"},{"type":"has_code","target_id":"github:t-vi:candlegp","source_url":"https://github.com/t-vi/candlegp"},{"type":"has_code","target_id":"github:loudinthecloud:dpwa","source_url":"https://github.com/loudinthecloud/dpwa"},{"type":"has_code","target_id":"github:koz4k:dni-pytorch","source_url":"https://github.com/koz4k/dni-pytorch"},{"type":"has_code","target_id":"github:dnouri:skorch","source_url":"https://github.com/dnouri/skorch"},{"type":"has_code","target_id":"github:pytorch:ignite","source_url":"https://github.com/pytorch/ignite"},{"type":"has_code","target_id":"github:glample:Arnold","source_url":"https://github.com/glample/Arnold"},{"type":"has_code","target_id":"github:albanie:pytorch-mcn","source_url":"https://github.com/albanie/pytorch-mcn"},{"type":"has_code","target_id":"github:chenyuntc:simple-faster-rcnn-pytorch","source_url":"https://github.com/chenyuntc/simple-faster-rcnn-pytorch"},{"type":"has_code","target_id":"github:DL-IT:generative_zoo","source_url":"https://github.com/DL-IT/generative_zoo"},{"type":"has_code","target_id":"github:szagoruyko:pytorchviz","source_url":"https://github.com/szagoruyko/pytorchviz"},{"type":"has_code","target_id":"github:cogitare-ai:cogitare","source_url":"https://github.com/cogitare-ai/cogitare"},{"type":"has_code","target_id":"github:dmarnerides:pydlt","source_url":"https://github.com/dmarnerides/pydlt"},{"type":"has_code","target_id":"github:wohlert:semi-supervised-pytorch","source_url":"https://github.com/wohlert/semi-supervised-pytorch"},{"type":"has_code","target_id":"github:rusty1s:pytorch_cluster","source_url":"https://github.com/rusty1s/pytorch_cluster"},{"type":"has_code","target_id":"github:aditya-khant:neural-assembly-compiler","source_url":"https://github.com/aditya-khant/neural-assembly-compiler"},{"type":"has_code","target_id":"github:vadimkantorov:caffemodel2pytorch","source_url":"https://github.com/vadimkantorov/caffemodel2pytorch"},{"type":"has_code","target_id":"github:pytorch:extension-cpp","source_url":"https://github.com/pytorch/extension-cpp"},{"type":"has_code","target_id":"github:GRAAL-Research:pytoune","source_url":"https://github.com/GRAAL-Research/pytoune"},{"type":"has_code","target_id":"github:dusty-nv:jetson-reinforcement","source_url":"https://github.com/dusty-nv/jetson-reinforcement"},{"type":"has_code","target_id":"github:salesforce:matchbox","source_url":"https://github.com/salesforce/matchbox"},{"type":"has_code","target_id":"github:josipd:torch-two-sample","source_url":"https://github.com/josipd/torch-two-sample"},{"type":"has_code","target_id":"github:sksq96:pytorch-summary","source_url":"https://github.com/sksq96/pytorch-summary"},{"type":"has_code","target_id":"github:BelBES:mpl.pytorch","source_url":"https://github.com/BelBES/mpl.pytorch"},{"type":"has_code","target_id":"github:YosefLab:scVI-dev","source_url":"https://github.com/YosefLab/scVI-dev"},{"type":"has_code","target_id":"github:NVIDIA:apex","source_url":"https://github.com/NVIDIA/apex"},{"type":"has_code","target_id":"github:pytorch:ELF","source_url":"https://github.com/pytorch/ELF"},{"type":"has_code","target_id":"github:EKami:Torchlite","source_url":"https://github.com/EKami/Torchlite"},{"type":"has_code","target_id":"github:Schlumberger:joint-vae","source_url":"https://github.com/Schlumberger/joint-vae"},{"type":"has_code","target_id":"github:kengz:SLM-Lab","source_url":"https://github.com/kengz/SLM-Lab"},{"type":"has_code","target_id":"github:Hananel-Hazan:bindsnet","source_url":"https://github.com/Hananel-Hazan/bindsnet"},{"type":"has_code","target_id":"github:akanimax:pro_gan_pytorch","source_url":"https://github.com/akanimax/pro_gan_pytorch"},{"type":"has_code","target_id":"github:rusty1s:pytorch_geometric","source_url":"https://github.com/rusty1s/pytorch_geometric"},{"type":"has_code","target_id":"github:knighton:torchplus","source_url":"https://github.com/knighton/torchplus"},{"type":"has_code","target_id":"github:zuoxingdong:lagom","source_url":"https://github.com/zuoxingdong/lagom"},{"type":"has_code","target_id":"github:ecs-vlc:torchbearer","source_url":"https://github.com/ecs-vlc/torchbearer"},{"type":"has_code","target_id":"github:tristandeleu:pytorch-maml-rl","source_url":"https://github.com/tristandeleu/pytorch-maml-rl"},{"type":"has_code","target_id":"github:bharathgs:NALU","source_url":"https://github.com/bharathgs/NALU"},{"type":"has_code","target_id":"github:PIQuIL:QuCumber","source_url":"https://github.com/PIQuIL/QuCumber"},{"type":"has_code","target_id":"github:MagNet-DL:magnet","source_url":"https://github.com/MagNet-DL/magnet"},{"type":"has_code","target_id":"github:jbohnslav:opencv_transforms","source_url":"https://github.com/jbohnslav/opencv_transforms"},{"type":"has_code","target_id":"github:fastai:fastai","source_url":"https://github.com/fastai/fastai"},{"type":"has_code","target_id":"github:RobotLocomotion:pytorch-dense-correspondence","source_url":"https://github.com/RobotLocomotion/pytorch-dense-correspondence"},{"type":"has_code","target_id":"github:richzhang:colorization-pytorch","source_url":"https://github.com/richzhang/colorization-pytorch"},{"type":"has_code","target_id":"github:cms-flash:beauty-net","source_url":"https://github.com/cms-flash/beauty-net"},{"type":"has_code","target_id":"github:Mariewelt:OpenChem","source_url":"https://github.com/Mariewelt/OpenChem"},{"type":"has_code","target_id":"github:aiqm:torchani","source_url":"https://github.com/aiqm/torchani"},{"type":"has_code","target_id":"github:hjmshi:PyTorch-LBFGS","source_url":"https://github.com/hjmshi/PyTorch-LBFGS"},{"type":"has_code","target_id":"github:cornellius-gp:gpytorch","source_url":"https://github.com/cornellius-gp/gpytorch"},{"type":"has_code","target_id":"github:mariogeiger:hessian","source_url":"https://github.com/mariogeiger/hessian"},{"type":"has_code","target_id":"github:MillionIntegrals:vel","source_url":"https://github.com/MillionIntegrals/vel"},{"type":"has_code","target_id":"github:msamogh:nonechucks","source_url":"https://github.com/msamogh/nonechucks"},{"type":"has_code","target_id":"github:Swall0w:torchstat","source_url":"https://github.com/Swall0w/torchstat"},{"type":"has_code","target_id":"github:pytorch:QNNPACK","source_url":"https://github.com/pytorch/QNNPACK"},{"type":"has_code","target_id":"github:rtqichen:torchdiffeq","source_url":"https://github.com/rtqichen/torchdiffeq"},{"type":"has_code","target_id":"github:BachiLi:redner","source_url":"https://github.com/BachiLi/redner"},{"type":"has_code","target_id":"github:masa-su:pixyz","source_url":"https://github.com/masa-su/pixyz"},{"type":"has_code","target_id":"github:perone:euclidesdb","source_url":"https://github.com/perone/euclidesdb"},{"type":"has_code","target_id":"github:nerox8664:pytorch2keras","source_url":"https://github.com/nerox8664/pytorch2keras"},{"type":"has_code","target_id":"github:domainadaptation:salad","source_url":"https://github.com/domainadaptation/salad"},{"type":"has_code","target_id":"github:Erotemic:netharn","source_url":"https://github.com/Erotemic/netharn"},{"type":"has_code","target_id":"github:dmlc:dgl","source_url":"https://github.com/dmlc/dgl"},{"type":"has_code","target_id":"github:CSAILVision:gandissect","source_url":"https://github.com/CSAILVision/gandissect"},{"type":"has_code","target_id":"github:justusschock:delira","source_url":"https://github.com/justusschock/delira"},{"type":"has_code","target_id":"github:AIRLab-POLIMI:mushroom","source_url":"https://github.com/AIRLab-POLIMI/mushroom"},{"type":"has_code","target_id":"github:thuml:Xlearn","source_url":"https://github.com/thuml/Xlearn"},{"type":"has_code","target_id":"github:ferrine:geoopt","source_url":"https://github.com/ferrine/geoopt"},{"type":"has_code","target_id":"github:unit8co:vegans","source_url":"https://github.com/unit8co/vegans"},{"type":"has_code","target_id":"github:arraiyopensource:torchgeometry","source_url":"https://github.com/arraiyopensource/torchgeometry"},{"type":"has_code","target_id":"github:BorealisAI:advertorch","source_url":"https://github.com/BorealisAI/advertorch"},{"type":"has_code","target_id":"github:Luolc:AdaBound","source_url":"https://github.com/Luolc/AdaBound"},{"type":"has_code","target_id":"github:mblondel:fenchel-young-losses","source_url":"https://github.com/mblondel/fenchel-young-losses"},{"type":"has_code","target_id":"github:Lyken17:pytorch-OpCounter","source_url":"https://github.com/Lyken17/pytorch-OpCounter"},{"type":"has_code","target_id":"github:kaihsin:Tor10","source_url":"https://github.com/kaihsin/Tor10"},{"type":"has_code","target_id":"github:catalyst-team:catalyst","source_url":"https://github.com/catalyst-team/catalyst"},{"type":"has_code","target_id":"github:facebook:Ax","source_url":"https://github.com/facebook/Ax"},{"type":"has_code","target_id":"github:achaiah:pywick","source_url":"https://github.com/achaiah/pywick"},{"type":"has_code","target_id":"github:kakaobrain:torchgpipe","source_url":"https://github.com/kakaobrain/torchgpipe"},{"type":"has_code","target_id":"github:pytorch:hub","source_url":"https://github.com/pytorch/hub"},{"type":"has_code","target_id":"github:williamFalcon:pytorch-lightning","source_url":"https://github.com/williamFalcon/pytorch-lightning"},{"type":"has_code","target_id":"github:kaihsin:Tor10","source_url":"https://github.com/kaihsin/Tor10"},{"type":"has_code","target_id":"github:microsoft:tensorwatch","source_url":"https://github.com/microsoft/tensorwatch"},{"type":"has_code","target_id":"github:fancompute:wavetorch","source_url":"https://github.com/fancompute/wavetorch"},{"type":"has_code","target_id":"github:ag14774:diffdist","source_url":"https://github.com/ag14774/diffdist"},{"type":"has_code","target_id":"github:awwong1:torchprof","source_url":"https://github.com/awwong1/torchprof"},{"type":"has_code","target_id":"github:oxfordcontrol:osqpth","source_url":"https://github.com/oxfordcontrol/osqpth"},{"type":"has_code","target_id":"github:mctorch:mctorch","source_url":"https://github.com/mctorch/mctorch"},{"type":"has_code","target_id":"github:noahgolmant:pytorch-hessian-eigenthings","source_url":"https://github.com/noahgolmant/pytorch-hessian-eigenthings"},{"type":"has_code","target_id":"github:StanfordVL:MinkowskiEngine","source_url":"https://github.com/StanfordVL/MinkowskiEngine"},{"type":"has_code","target_id":"github:Omegastick:pytorch-cpp-rl","source_url":"https://github.com/Omegastick/pytorch-cpp-rl"},{"type":"has_code","target_id":"github:BloodAxe:pytorch-toolbelt","source_url":"https://github.com/BloodAxe/pytorch-toolbelt"},{"type":"has_code","target_id":"github:Fonbet:argus-tensor-stream","source_url":"https://github.com/Fonbet/argus-tensor-stream"},{"type":"has_code","target_id":"github:hal3:macarico","source_url":"https://github.com/hal3/macarico"},{"type":"has_code","target_id":"github:astooke:rlpyt","source_url":"https://github.com/astooke/rlpyt"},{"type":"has_code","target_id":"github:blue-season:pywarm","source_url":"https://github.com/blue-season/pywarm"},{"type":"has_code","target_id":"github:learnables:learn2learn","source_url":"https://github.com/learnables/learn2learn"},{"type":"has_code","target_id":"github:facebookresearch:torchbeast","source_url":"https://github.com/facebookresearch/torchbeast"},{"type":"has_code","target_id":"github:facebookresearch:higher","source_url":"https://github.com/facebookresearch/higher"},{"type":"has_code","target_id":"github:Vermeille:Torchelie","source_url":"https://github.com/Vermeille/Torchelie"},{"type":"has_code","target_id":"github:facebookresearch:CrypTen","source_url":"https://github.com/facebookresearch/CrypTen"},{"type":"has_code","target_id":"github:cvxgrp:cvxpylayers","source_url":"https://github.com/cvxgrp/cvxpylayers"},{"type":"has_code","target_id":"github:HobbitLong:RepDistiller","source_url":"https://github.com/HobbitLong/RepDistiller"},{"type":"has_code","target_id":"github:NVIDIAGameWorks:kaolin","source_url":"https://github.com/NVIDIAGameWorks/kaolin"},{"type":"has_code","target_id":"github:BasBuller:PySNN","source_url":"https://github.com/BasBuller/PySNN"},{"type":"has_code","target_id":"github:dmmiller612:sparktorch","source_url":"https://github.com/dmmiller612/sparktorch"},{"type":"has_code","target_id":"github:KevinMusgrave:pytorch-metric-learning","source_url":"https://github.com/KevinMusgrave/pytorch-metric-learning"},{"type":"has_code","target_id":"github:cpnota:autonomous-learning-library","source_url":"https://github.com/cpnota/autonomous-learning-library"},{"type":"has_code","target_id":"github:asappresearch:flambe","source_url":"https://github.com/asappresearch/flambe"},{"type":"has_code","target_id":"github:jettify:pytorch-optimizer","source_url":"https://github.com/jettify/pytorch-optimizer"},{"type":"has_code","target_id":"github:AntixK:PyTorch-VAE","source_url":"https://github.com/AntixK/PyTorch-VAE"},{"type":"has_code","target_id":"github:ray-project:ray","source_url":"https://github.com/ray-project/ray"},{"type":"has_code","target_id":"github:benedekrozemberczki:pytorch_geometric_temporal","source_url":"https://github.com/benedekrozemberczki/pytorch_geometric_temporal"},{"type":"has_code","target_id":"github:GRAAL-Research:poutyne","source_url":"https://github.com/GRAAL-Research/poutyne"},{"type":"has_code","target_id":"github:PistonY:torch-toolbox","source_url":"https://github.com/PistonY/torch-toolbox"},{"type":"has_code","target_id":"github:pytorch:contrib","source_url":"https://github.com/pytorch/contrib"},{"type":"has_code","target_id":"github:lukemelas:EfficientNet-PyTorch","source_url":"https://github.com/lukemelas/EfficientNet-PyTorch"},{"type":"has_code","target_id":"github:pytorch:xla","source_url":"https://github.com/pytorch/xla"},{"type":"has_code","target_id":"github:tmbdev:webdataset","source_url":"https://github.com/tmbdev/webdataset"},{"type":"has_code","target_id":"github:Media-Smart:volksdep","source_url":"https://github.com/Media-Smart/volksdep"},{"type":"has_code","target_id":"github:POSTECH-CVLab:PyTorch-StudioGAN","source_url":"https://github.com/POSTECH-CVLab/PyTorch-StudioGAN"},{"type":"has_code","target_id":"github:torchdrift:torchdrift","source_url":"https://github.com/torchdrift/torchdrift"},{"type":"has_code","target_id":"github:huggingface:accelerate","source_url":"https://github.com/huggingface/accelerate"},{"type":"has_code","target_id":"github:PyTorchLightning:lightning-transformers","source_url":"https://github.com/PyTorchLightning/lightning-transformers"},{"type":"has_code","target_id":"github:PyTorchLightning:lightning-flash","source_url":"https://github.com/PyTorchLightning/lightning-flash"},{"type":"has_code","target_id":"github:SherylHYX:pytorch_geometric_signed_directed","source_url":"https://github.com/SherylHYX/pytorch_geometric_signed_directed"},{"type":"has_code","target_id":"github:rentruewang:koila","source_url":"https://github.com/rentruewang/koila"},{"type":"has_code","target_id":"github:awslabs:renate","source_url":"https://github.com/awslabs/renate"},{"type":"has_code","target_id":"github:spro:practical-pytorch","source_url":"https://github.com/spro/practical-pytorch"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:keon:pytorch-exercises","source_url":"https://github.com/keon/pytorch-exercises"},{"type":"has_code","target_id":"github:pytorch:tutorials","source_url":"https://github.com/pytorch/tutorials"},{"type":"has_code","target_id":"github:pytorch:examples","source_url":"https://github.com/pytorch/examples"},{"type":"has_code","target_id":"github:napsternxg:pytorch-practice","source_url":"https://github.com/napsternxg/pytorch-practice"},{"type":"has_code","target_id":"github:vinhkhuc:PyTorch-Mini-Tutorials","source_url":"https://github.com/vinhkhuc/PyTorch-Mini-Tutorials"},{"type":"has_code","target_id":"github:xiayandi:Pytorch_text_classification","source_url":"https://github.com/xiayandi/Pytorch_text_classification"},{"type":"has_code","target_id":"github:desimone:pytorch-cat-vs-dogs","source_url":"https://github.com/desimone/pytorch-cat-vs-dogs"},{"type":"has_code","target_id":"github:eladhoffer:convNet.pytorch","source_url":"https://github.com/eladhoffer/convNet.pytorch"},{"type":"has_code","target_id":"github:mailmahee:pytorch-generative-adversarial-networks","source_url":"https://github.com/mailmahee/pytorch-generative-adversarial-networks"},{"type":"has_code","target_id":"github:amdegroot:pytorch-containers","source_url":"https://github.com/amdegroot/pytorch-containers"},{"type":"has_code","target_id":"github:cemoody:topicsne","source_url":"https://github.com/cemoody/topicsne"},{"type":"has_code","target_id":"github:fducau:AAE_pytorch","source_url":"https://github.com/fducau/AAE_pytorch"},{"type":"has_code","target_id":"github:GunhoChoi:Kind_PyTorch_Tutorial","source_url":"https://github.com/GunhoChoi/Kind_PyTorch_Tutorial"},{"type":"has_code","target_id":"github:justdark:pytorch-poetry-gen","source_url":"https://github.com/justdark/pytorch-poetry-gen"},{"type":"has_code","target_id":"github:JamesChuanggg:pytorch-REINFORCE","source_url":"https://github.com/JamesChuanggg/pytorch-REINFORCE"},{"type":"has_code","target_id":"github:MorvanZhou:PyTorch-Tutorial","source_url":"https://github.com/MorvanZhou/PyTorch-Tutorial"},{"type":"has_code","target_id":"github:joansj:pytorch-intro","source_url":"https://github.com/joansj/pytorch-intro"},{"type":"has_code","target_id":"github:bearpaw:pytorch-classification","source_url":"https://github.com/bearpaw/pytorch-classification"},{"type":"has_code","target_id":"github:hardmaru:pytorch_notebooks","source_url":"https://github.com/hardmaru/pytorch_notebooks"},{"type":"has_code","target_id":"github:soravux:pytorch_tutorial","source_url":"https://github.com/soravux/pytorch_tutorial"},{"type":"has_code","target_id":"github:Spandan-Madan:Pytorch_fine_tuning_Tutorial","source_url":"https://github.com/Spandan-Madan/Pytorch_fine_tuning_Tutorial"},{"type":"has_code","target_id":"github:Kyubyong:pytorch_exercises","source_url":"https://github.com/Kyubyong/pytorch_exercises"},{"type":"has_code","target_id":"github:soumith:traffic-sign-detection-homework","source_url":"https://github.com/soumith/traffic-sign-detection-homework"},{"type":"has_code","target_id":"github:Js-Mim:mss_pytorch","source_url":"https://github.com/Js-Mim/mss_pytorch"},{"type":"has_code","target_id":"github:DSKSD:DeepNLP-models-Pytorch","source_url":"https://github.com/DSKSD/DeepNLP-models-Pytorch"},{"type":"has_code","target_id":"github:mila-udem:welcome_tutorials","source_url":"https://github.com/mila-udem/welcome_tutorials"},{"type":"has_code","target_id":"github:moskomule:pytorch.rl.learning","source_url":"https://github.com/moskomule/pytorch.rl.learning"},{"type":"has_code","target_id":"github:keon:seq2seq","source_url":"https://github.com/keon/seq2seq"},{"type":"has_code","target_id":"github:JeanKossaifi:tensorly-notebooks","source_url":"https://github.com/JeanKossaifi/tensorly-notebooks"},{"type":"has_code","target_id":"github:jpeg729:pytorch_bits","source_url":"https://github.com/jpeg729/pytorch_bits"},{"type":"has_code","target_id":"github:sanyam5:skip-thoughts","source_url":"https://github.com/sanyam5/skip-thoughts"},{"type":"has_code","target_id":"github:xiadingZ:video-caption-pytorch","source_url":"https://github.com/xiadingZ/video-caption-pytorch"},{"type":"has_code","target_id":"github:higgsfield:Capsule-Network-Tutorial","source_url":"https://github.com/higgsfield/Capsule-Network-Tutorial"},{"type":"has_code","target_id":"github:SherlockLiao:code-of-learn-deep-learning-with-pytorch","source_url":"https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch"},{"type":"has_code","target_id":"github:higgsfield:RL-Adventure","source_url":"https://github.com/higgsfield/RL-Adventure"},{"type":"has_code","target_id":"github:hpcgarage:accelerated_dl_pytorch","source_url":"https://github.com/hpcgarage/accelerated_dl_pytorch"},{"type":"has_code","target_id":"github:higgsfield:RL-Adventure-2","source_url":"https://github.com/higgsfield/RL-Adventure-2"},{"type":"has_code","target_id":"github:wkentaro:pytorch-for-numpy-users","source_url":"https://github.com/wkentaro/pytorch-for-numpy-users"},{"type":"has_code","target_id":"github:Kaixhin:grokking-pytorch","source_url":"https://github.com/Kaixhin/grokking-pytorch"},{"type":"has_code","target_id":"github:Atcold:PyTorch-Deep-Learning-Minicourse","source_url":"https://github.com/Atcold/PyTorch-Deep-Learning-Minicourse"},{"type":"has_code","target_id":"github:utkuozbulak:pytorch-custom-dataset-examples","source_url":"https://github.com/utkuozbulak/pytorch-custom-dataset-examples"},{"type":"has_code","target_id":"github:furkanu:deeplearning.ai-pytorch","source_url":"https://github.com/furkanu/deeplearning.ai-pytorch"},{"type":"has_code","target_id":"github:tobiascz:MNIST_Pytorch_python_and_capi","source_url":"https://github.com/tobiascz/MNIST_Pytorch_python_and_capi"},{"type":"has_code","target_id":"github:ne7ermore:torch_light","source_url":"https://github.com/ne7ermore/torch_light"},{"type":"has_code","target_id":"github:dribnet:portrain-gan","source_url":"https://github.com/dribnet/portrain-gan"},{"type":"has_code","target_id":"github:omarsar:mri-analysis-pytorch","source_url":"https://github.com/omarsar/mri-analysis-pytorch"},{"type":"has_code","target_id":"github:davidcpage:cifar10-fast","source_url":"https://github.com/davidcpage/cifar10-fast"},{"type":"has_code","target_id":"github:bentrevett:pytorch-sentiment-analysis","source_url":"https://github.com/bentrevett/pytorch-sentiment-analysis"},{"type":"has_code","target_id":"github:rwightman:pytorch-image-models","source_url":"https://github.com/rwightman/pytorch-image-models"},{"type":"has_code","target_id":"github:BIGBALLON:CIFAR-ZOO","source_url":"https://github.com/BIGBALLON/CIFAR-ZOO"},{"type":"has_code","target_id":"github:dsgiitr:d2l-pytorch","source_url":"https://github.com/dsgiitr/d2l-pytorch"},{"type":"has_code","target_id":"github:stared:thinking-in-tensors-writing-in-pytorch","source_url":"https://github.com/stared/thinking-in-tensors-writing-in-pytorch"},{"type":"has_code","target_id":"github:lemonhu:NER-BERT-pytorch","source_url":"https://github.com/lemonhu/NER-BERT-pytorch"},{"type":"has_code","target_id":"github:dougsouza:pytorch-sync-batchnorm-example","source_url":"https://github.com/dougsouza/pytorch-sync-batchnorm-example"},{"type":"has_code","target_id":"github:barissayil:SentimentAnalysis","source_url":"https://github.com/barissayil/SentimentAnalysis"},{"type":"has_code","target_id":"github:prabhuomkar:pytorch-cpp","source_url":"https://github.com/prabhuomkar/pytorch-cpp"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:lab-ml:nn","source_url":"https://github.com/lab-ml/nn"},{"type":"has_code","target_id":"github:adap:flower","source_url":"https://github.com/adap/flower"},{"type":"has_code","target_id":"github:neuralix:google_evolution","source_url":"https://github.com/neuralix/google_evolution"},{"type":"has_code","target_id":"github:edouardoyallon:pyscatwave","source_url":"https://github.com/edouardoyallon/pyscatwave"},{"type":"has_code","target_id":"github:edouardoyallon:scalingscattering","source_url":"https://github.com/edouardoyallon/scalingscattering"},{"type":"has_code","target_id":"github:episodeyang:deep-auto-punctuation","source_url":"https://github.com/episodeyang/deep-auto-punctuation"},{"type":"has_code","target_id":"github:tensorboy:pytorch_Realtime_Multi-Person_Pose_Estimation","source_url":"https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation"},{"type":"has_code","target_id":"github:ZheC:Realtime_Multi-Person_Pose_Estimation","source_url":"https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation"},{"type":"has_code","target_id":"github:onlytailei:PyTorch-value-iteration-networks","source_url":"https://github.com/onlytailei/PyTorch-value-iteration-networks"},{"type":"has_code","target_id":"github:analvikingur:pytorch_Highway","source_url":"https://github.com/analvikingur/pytorch_Highway"},{"type":"has_code","target_id":"github:analvikingur:pytorch_NEG_loss","source_url":"https://github.com/analvikingur/pytorch_NEG_loss"},{"type":"has_code","target_id":"github:analvikingur:pytorch_RVAE","source_url":"https://github.com/analvikingur/pytorch_RVAE"},{"type":"has_code","target_id":"github:analvikingur:pytorch_TDNN","source_url":"https://github.com/analvikingur/pytorch_TDNN"},{"type":"has_code","target_id":"github:moskomule:eve.pytorch","source_url":"https://github.com/moskomule/eve.pytorch"},{"type":"has_code","target_id":"github:locuslab:e2e-model-learning","source_url":"https://github.com/locuslab/e2e-model-learning"},{"type":"has_code","target_id":"github:mrzhu-cool:pix2pix-pytorch","source_url":"https://github.com/mrzhu-cool/pix2pix-pytorch"},{"type":"has_code","target_id":"github:amdegroot:ssd.pytorch","source_url":"https://github.com/amdegroot/ssd.pytorch"},{"type":"has_code","target_id":"github:carpedm20:DiscoGAN-pytorch","source_url":"https://github.com/carpedm20/DiscoGAN-pytorch"},{"type":"has_code","target_id":"github:SKTBrain:DiscoGAN","source_url":"https://github.com/SKTBrain/DiscoGAN"},{"type":"has_code","target_id":"github:atgambardella:pytorch-es","source_url":"https://github.com/atgambardella/pytorch-es"},{"type":"has_code","target_id":"github:bodokaiser:piwise","source_url":"https://github.com/bodokaiser/piwise"},{"type":"has_code","target_id":"github:transedward:pytorch-dqn","source_url":"https://github.com/transedward/pytorch-dqn"},{"type":"has_code","target_id":"github:ruotianluo:neuraltalk2.pytorch","source_url":"https://github.com/ruotianluo/neuraltalk2.pytorch"},{"type":"has_code","target_id":"github:mattmacy:vnet.pytorch","source_url":"https://github.com/mattmacy/vnet.pytorch"},{"type":"has_code","target_id":"github:wkentaro:pytorch-fcn","source_url":"https://github.com/wkentaro/pytorch-fcn"},{"type":"has_code","target_id":"github:xternalz:WideResNet-pytorch","source_url":"https://github.com/xternalz/WideResNet-pytorch"},{"type":"has_code","target_id":"github:szagoruyko:wide-residual-networks","source_url":"https://github.com/szagoruyko/wide-residual-networks"},{"type":"has_code","target_id":"github:c0nn3r:pytorch_highway_networks","source_url":"https://github.com/c0nn3r/pytorch_highway_networks"},{"type":"has_code","target_id":"github:ypxie:pytorch-NeuCom","source_url":"https://github.com/ypxie/pytorch-NeuCom"},{"type":"has_code","target_id":"github:eladhoffer:captionGen","source_url":"https://github.com/eladhoffer/captionGen"},{"type":"has_code","target_id":"github:jayleicn:animeGAN","source_url":"https://github.com/jayleicn/animeGAN"},{"type":"has_code","target_id":"github:Shawn1993:cnn-text-classification-pytorch","source_url":"https://github.com/Shawn1993/cnn-text-classification-pytorch"},{"type":"has_code","target_id":"github:SeanNaren:deepspeech.pytorch","source_url":"https://github.com/SeanNaren/deepspeech.pytorch"},{"type":"has_code","target_id":"github:MaximumEntropy:Seq2Seq-PyTorch","source_url":"https://github.com/MaximumEntropy/Seq2Seq-PyTorch"},{"type":"has_code","target_id":"github:rarilurelo:pytorch_a3c","source_url":"https://github.com/rarilurelo/pytorch_a3c"},{"type":"has_code","target_id":"github:bamos:densenet.pytorch","source_url":"https://github.com/bamos/densenet.pytorch"},{"type":"has_code","target_id":"github:alykhantejani:nninit","source_url":"https://github.com/alykhantejani/nninit"},{"type":"has_code","target_id":"github:longcw:faster_rcnn_pytorch","source_url":"https://github.com/longcw/faster_rcnn_pytorch"},{"type":"has_code","target_id":"github:akolishchak:doom-net-pytorch","source_url":"https://github.com/akolishchak/doom-net-pytorch"},{"type":"has_code","target_id":"github:ClementPinard:FlowNetPytorch","source_url":"https://github.com/ClementPinard/FlowNetPytorch"},{"type":"has_code","target_id":"github:gsp-27:pytorch_Squeezenet","source_url":"https://github.com/gsp-27/pytorch_Squeezenet"},{"type":"has_code","target_id":"github:martinarjovsky:WassersteinGAN","source_url":"https://github.com/martinarjovsky/WassersteinGAN"},{"type":"has_code","target_id":"github:locuslab:optnet","source_url":"https://github.com/locuslab/optnet"},{"type":"has_code","target_id":"github:locuslab:qpth","source_url":"https://github.com/locuslab/qpth"},{"type":"has_code","target_id":"github:ikostrikov:pytorch-naf","source_url":"https://github.com/ikostrikov/pytorch-naf"},{"type":"has_code","target_id":"github:ikostrikov:pytorch-meta-optimizer","source_url":"https://github.com/ikostrikov/pytorch-meta-optimizer"},{"type":"has_code","target_id":"github:darkstar112358:fast-neural-style","source_url":"https://github.com/darkstar112358/fast-neural-style"},{"type":"has_code","target_id":"github:leongatys:PytorchNeuralStyleTransfer","source_url":"https://github.com/leongatys/PytorchNeuralStyleTransfer"},{"type":"has_code","target_id":"github:bengxy:FastNeuralStyle","source_url":"https://github.com/bengxy/FastNeuralStyle"},{"type":"has_code","target_id":"github:alexis-jacq:Pytorch-Tutorials","source_url":"https://github.com/alexis-jacq/Pytorch-Tutorials"},{"type":"has_code","target_id":"github:zuoxingdong:VIN_PyTorch_Visdom","source_url":"https://github.com/zuoxingdong/VIN_PyTorch_Visdom"},{"type":"has_code","target_id":"github:longcw:yolo2-pytorch","source_url":"https://github.com/longcw/yolo2-pytorch"},{"type":"has_code","target_id":"github:szagoruyko:attention-transfer","source_url":"https://github.com/szagoruyko/attention-transfer"},{"type":"has_code","target_id":"github:potterhsu:SVHNClassifier-PyTorch","source_url":"https://github.com/potterhsu/SVHNClassifier-PyTorch"},{"type":"has_code","target_id":"github:oeway:pytorch-deform-conv","source_url":"https://github.com/oeway/pytorch-deform-conv"},{"type":"has_code","target_id":"github:carpedm20:BEGAN-pytorch","source_url":"https://github.com/carpedm20/BEGAN-pytorch"},{"type":"has_code","target_id":"github:dasguptar:treelstm.pytorch","source_url":"https://github.com/dasguptar/treelstm.pytorch"},{"type":"has_code","target_id":"github:DmitryUlyanov:AGE","source_url":"https://github.com/DmitryUlyanov/AGE"},{"type":"has_code","target_id":"github:prlz77:ResNeXt.pytorch","source_url":"https://github.com/prlz77/ResNeXt.pytorch"},{"type":"has_code","target_id":"github:jingweiz:pytorch-rl","source_url":"https://github.com/jingweiz/pytorch-rl"},{"type":"has_code","target_id":"github:sujithv28:Deep-Leafsnap","source_url":"https://github.com/sujithv28/Deep-Leafsnap"},{"type":"has_code","target_id":"github:junyanz:pytorch-CycleGAN-and-pix2pix","source_url":"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"},{"type":"has_code","target_id":"github:onlytailei:A3C-PyTorch","source_url":"https://github.com/onlytailei/A3C-PyTorch"},{"type":"has_code","target_id":"github:kentsommer:pytorch-value-iteration-networks","source_url":"https://github.com/kentsommer/pytorch-value-iteration-networks"},{"type":"has_code","target_id":"github:zhanghang1989:PyTorch-Style-Transfer","source_url":"https://github.com/zhanghang1989/PyTorch-Style-Transfer"},{"type":"has_code","target_id":"github:isht7:pytorch-deeplab-resnet","source_url":"https://github.com/isht7/pytorch-deeplab-resnet"},{"type":"has_code","target_id":"github:fxia22:pointnet.pytorch","source_url":"https://github.com/fxia22/pointnet.pytorch"},{"type":"has_code","target_id":"github:aaron-xichen:pytorch-playground","source_url":"https://github.com/aaron-xichen/pytorch-playground"},{"type":"has_code","target_id":"github:jingweiz:pytorch-dnc","source_url":"https://github.com/jingweiz/pytorch-dnc"},{"type":"has_code","target_id":"github:jinfagang:pytorch_image_classifier","source_url":"https://github.com/jinfagang/pytorch_image_classifier"},{"type":"has_code","target_id":"github:yunjey:mnist-svhn-transfer","source_url":"https://github.com/yunjey/mnist-svhn-transfer"},{"type":"has_code","target_id":"github:marvis:pytorch-yolo2","source_url":"https://github.com/marvis/pytorch-yolo2"},{"type":"has_code","target_id":"github:andrewliao11:dni.pytorch","source_url":"https://github.com/andrewliao11/dni.pytorch"},{"type":"has_code","target_id":"github:caogang:wgan-gp","source_url":"https://github.com/caogang/wgan-gp"},{"type":"has_code","target_id":"github:spro:pytorch-seq2seq-intent-parsing","source_url":"https://github.com/spro/pytorch-seq2seq-intent-parsing"},{"type":"has_code","target_id":"github:demelin:pyTorch_NCE","source_url":"https://github.com/demelin/pyTorch_NCE"},{"type":"has_code","target_id":"github:cxhernandez:molencoder","source_url":"https://github.com/cxhernandez/molencoder"},{"type":"has_code","target_id":"github:stormraiser:GAN-weight-norm","source_url":"https://github.com/stormraiser/GAN-weight-norm"},{"type":"has_code","target_id":"github:rachtsingh:lgamma","source_url":"https://github.com/rachtsingh/lgamma"},{"type":"has_code","target_id":"github:eladhoffer:bigBatch","source_url":"https://github.com/eladhoffer/bigBatch"},{"type":"has_code","target_id":"github:dgriff777:rl_a3c_pytorch","source_url":"https://github.com/dgriff777/rl_a3c_pytorch"},{"type":"has_code","target_id":"github:ahirner:pytorch-retraining","source_url":"https://github.com/ahirner/pytorch-retraining"},{"type":"has_code","target_id":"github:priba:nmp_qc","source_url":"https://github.com/priba/nmp_qc"},{"type":"has_code","target_id":"github:jacobgil:pytorch-grad-cam","source_url":"https://github.com/jacobgil/pytorch-grad-cam"},{"type":"has_code","target_id":"github:mjacar:pytorch-trpo","source_url":"https://github.com/mjacar/pytorch-trpo"},{"type":"has_code","target_id":"github:jacobgil:pytorch-explain-black-box","source_url":"https://github.com/jacobgil/pytorch-explain-black-box"},{"type":"has_code","target_id":"github:jmtomczak:vae_vpflows","source_url":"https://github.com/jmtomczak/vae_vpflows"},{"type":"has_code","target_id":"github:kimhc6028:relational-networks","source_url":"https://github.com/kimhc6028/relational-networks"},{"type":"has_code","target_id":"github:Cadene:vqa.pytorch","source_url":"https://github.com/Cadene/vqa.pytorch"},{"type":"has_code","target_id":"github:facebookresearch:end-to-end-negotiator","source_url":"https://github.com/facebookresearch/end-to-end-negotiator"},{"type":"has_code","target_id":"github:ShiyuLiang:odin-pytorch","source_url":"https://github.com/ShiyuLiang/odin-pytorch"},{"type":"has_code","target_id":"github:ajbrock:FreezeOut","source_url":"https://github.com/ajbrock/FreezeOut"},{"type":"has_code","target_id":"github:jakezhaojb:ARAE","source_url":"https://github.com/jakezhaojb/ARAE"},{"type":"has_code","target_id":"github:kimhc6028:forward-thinking-pytorch","source_url":"https://github.com/kimhc6028/forward-thinking-pytorch"},{"type":"has_code","target_id":"github:BoyuanJiang:context_encoder_pytorch","source_url":"https://github.com/BoyuanJiang/context_encoder_pytorch"},{"type":"has_code","target_id":"github:jadore801120:attention-is-all-you-need-pytorch","source_url":"https://github.com/jadore801120/attention-is-all-you-need-pytorch"},{"type":"has_code","target_id":"github:thnkim:OpenFacePytorch","source_url":"https://github.com/thnkim/OpenFacePytorch"},{"type":"has_code","target_id":"github:thnkim:OpenFacePytorch","source_url":"https://github.com/thnkim/OpenFacePytorch"},{"type":"has_code","target_id":"github:pemami4911:neural-combinatorial-rl-pytorch","source_url":"https://github.com/pemami4911/neural-combinatorial-rl-pytorch"},{"type":"has_code","target_id":"github:mjacar:pytorch-nec","source_url":"https://github.com/mjacar/pytorch-nec"},{"type":"has_code","target_id":"github:eladhoffer:seq2seq.pytorch","source_url":"https://github.com/eladhoffer/seq2seq.pytorch"},{"type":"has_code","target_id":"github:alexis-jacq:Pytorch-Sketch-RNN","source_url":"https://github.com/alexis-jacq/Pytorch-Sketch-RNN"},{"type":"has_code","target_id":"github:jacobgil:pytorch-pruning","source_url":"https://github.com/jacobgil/pytorch-pruning"},{"type":"has_code","target_id":"github:hitvoice:DrQA","source_url":"https://github.com/hitvoice/DrQA"},{"type":"has_code","target_id":"github:JianGoForIt:YellowFin_Pytorch","source_url":"https://github.com/JianGoForIt/YellowFin_Pytorch"},{"type":"has_code","target_id":"github:deepsound-project:samplernn-pytorch","source_url":"https://github.com/deepsound-project/samplernn-pytorch"},{"type":"has_code","target_id":"github:tymokvo:AEGeAN","source_url":"https://github.com/tymokvo/AEGeAN"},{"type":"has_code","target_id":"github:twtygqyy:pytorch-SRResNet","source_url":"https://github.com/twtygqyy/pytorch-SRResNet"},{"type":"has_code","target_id":"github:fartashf:vsepp","source_url":"https://github.com/fartashf/vsepp"},{"type":"has_code","target_id":"github:alexis-jacq:Pytorch-DPPO","source_url":"https://github.com/alexis-jacq/Pytorch-DPPO"},{"type":"has_code","target_id":"github:mingyuliutw:UNIT","source_url":"https://github.com/mingyuliutw/UNIT"},{"type":"has_code","target_id":"github:gpleiss:efficient_densenet_pytorch","source_url":"https://github.com/gpleiss/efficient_densenet_pytorch"},{"type":"has_code","target_id":"github:yjxiong:tsn-pytorch","source_url":"https://github.com/yjxiong/tsn-pytorch"},{"type":"has_code","target_id":"github:ajbrock:SMASH","source_url":"https://github.com/ajbrock/SMASH"},{"type":"has_code","target_id":"github:kuangliu:pytorch-retinanet","source_url":"https://github.com/kuangliu/pytorch-retinanet"},{"type":"has_code","target_id":"github:aosokin:biogans","source_url":"https://github.com/aosokin/biogans"},{"type":"has_code","target_id":"github:woozzu:dong_iccv_2017","source_url":"https://github.com/woozzu/dong_iccv_2017"},{"type":"has_code","target_id":"github:jmhessel:fmpytorch","source_url":"https://github.com/jmhessel/fmpytorch"},{"type":"has_code","target_id":"github:ZhouYanzhao:ORN","source_url":"https://github.com/ZhouYanzhao/ORN"},{"type":"has_code","target_id":"github:katerakelly:pytorch-maml","source_url":"https://github.com/katerakelly/pytorch-maml"},{"type":"has_code","target_id":"github:znxlwm:pytorch-generative-model-collections","source_url":"https://github.com/znxlwm/pytorch-generative-model-collections"},{"type":"has_code","target_id":"github:markdtw:vqa-winner-cvprw-2017","source_url":"https://github.com/markdtw/vqa-winner-cvprw-2017"},{"type":"has_code","target_id":"github:r9y9:tacotron_pytorch","source_url":"https://github.com/r9y9/tacotron_pytorch"},{"type":"has_code","target_id":"github:Lextal:pspnet-pytorch","source_url":"https://github.com/Lextal/pspnet-pytorch"},{"type":"has_code","target_id":"github:LiyuanLucasLiu:LM-LSTM-CRF","source_url":"https://github.com/LiyuanLucasLiu/LM-LSTM-CRF"},{"type":"has_code","target_id":"github:1adrianb:face-alignment","source_url":"https://github.com/1adrianb/face-alignment"},{"type":"has_code","target_id":"github:ClementPinard:DepthNet","source_url":"https://github.com/ClementPinard/DepthNet"},{"type":"has_code","target_id":"github:thstkdgus35:EDSR-PyTorch","source_url":"https://github.com/thstkdgus35/EDSR-PyTorch"},{"type":"has_code","target_id":"github:ethanluoyc:e2c-pytorch","source_url":"https://github.com/ethanluoyc/e2c-pytorch"},{"type":"has_code","target_id":"github:kenshohara:3D-ResNets-PyTorch","source_url":"https://github.com/kenshohara/3D-ResNets-PyTorch"},{"type":"has_code","target_id":"github:khanhptnk:bandit-nmt","source_url":"https://github.com/khanhptnk/bandit-nmt"},{"type":"has_code","target_id":"github:ikostrikov:pytorch-a2c-ppo-acktr","source_url":"https://github.com/ikostrikov/pytorch-a2c-ppo-acktr"},{"type":"has_code","target_id":"github:baldassarreFe:zalando-pytorch","source_url":"https://github.com/baldassarreFe/zalando-pytorch"},{"type":"has_code","target_id":"github:clcarwin:sphereface_pytorch","source_url":"https://github.com/clcarwin/sphereface_pytorch"},{"type":"has_code","target_id":"github:floringogianu:categorical-dqn","source_url":"https://github.com/floringogianu/categorical-dqn"},{"type":"has_code","target_id":"github:loudinthecloud:pytorch-ntm","source_url":"https://github.com/loudinthecloud/pytorch-ntm"},{"type":"has_code","target_id":"github:felixgwu:mask_rcnn_pytorch","source_url":"https://github.com/felixgwu/mask_rcnn_pytorch"},{"type":"has_code","target_id":"github:xbresson:graph_convnets_pytorch","source_url":"https://github.com/xbresson/graph_convnets_pytorch"},{"type":"has_code","target_id":"github:ruotianluo:pytorch-faster-rcnn","source_url":"https://github.com/ruotianluo/pytorch-faster-rcnn"},{"type":"has_code","target_id":"github:huggingface:torchMoji","source_url":"https://github.com/huggingface/torchMoji"},{"type":"has_code","target_id":"github:hangzhaomit:semantic-segmentation-pytorch","source_url":"https://github.com/hangzhaomit/semantic-segmentation-pytorch"},{"type":"has_code","target_id":"github:salesforce:pytorch-qrnn","source_url":"https://github.com/salesforce/pytorch-qrnn"},{"type":"has_code","target_id":"github:theeluwin:pytorch-sgns","source_url":"https://github.com/theeluwin/pytorch-sgns"},{"type":"has_code","target_id":"github:ClementPinard:SfmLearner-Pytorch","source_url":"https://github.com/ClementPinard/SfmLearner-Pytorch"},{"type":"has_code","target_id":"github:1zb:deformable-convolution-pytorch","source_url":"https://github.com/1zb/deformable-convolution-pytorch"},{"type":"has_code","target_id":"github:fanglanting:skip-gram-pytorch","source_url":"https://github.com/fanglanting/skip-gram-pytorch"},{"type":"has_code","target_id":"github:hanzhanggit:StackGAN-v2","source_url":"https://github.com/hanzhanggit/StackGAN-v2"},{"type":"has_code","target_id":"github:ruotianluo:self-critical.pytorch","source_url":"https://github.com/ruotianluo/self-critical.pytorch"},{"type":"has_code","target_id":"github:tkipf:pygcn","source_url":"https://github.com/tkipf/pygcn"},{"type":"has_code","target_id":"github:ixaxaar:pytorch-dnc","source_url":"https://github.com/ixaxaar/pytorch-dnc"},{"type":"has_code","target_id":"github:ptrblck:prog_gans_pytorch_inference","source_url":"https://github.com/ptrblck/prog_gans_pytorch_inference"},{"type":"has_code","target_id":"github:timomernick:pytorch-capsule","source_url":"https://github.com/timomernick/pytorch-capsule"},{"type":"has_code","target_id":"github:dyhan0920:PyramidNet-PyTorch","source_url":"https://github.com/dyhan0920/PyramidNet-PyTorch"},{"type":"has_code","target_id":"github:gram-ai:radio-transformer-networks","source_url":"https://github.com/gram-ai/radio-transformer-networks"},{"type":"has_code","target_id":"github:castorini:honk","source_url":"https://github.com/castorini/honk"},{"type":"has_code","target_id":"github:SSARCandy:DeepCORAL","source_url":"https://github.com/SSARCandy/DeepCORAL"},{"type":"has_code","target_id":"github:bearpaw:pytorch-pose","source_url":"https://github.com/bearpaw/pytorch-pose"},{"type":"has_code","target_id":"github:karandesai-96:lang-emerge-parlai","source_url":"https://github.com/karandesai-96/lang-emerge-parlai"},{"type":"has_code","target_id":"github:Kaixhin:Rainbow","source_url":"https://github.com/Kaixhin/Rainbow"},{"type":"has_code","target_id":"github:gdlg:pytorch_compact_bilinear_pooling","source_url":"https://github.com/gdlg/pytorch_compact_bilinear_pooling"},{"type":"has_code","target_id":"github:DeepInsight-PCALab:CompactBilinearPooling-Pytorch","source_url":"https://github.com/DeepInsight-PCALab/CompactBilinearPooling-Pytorch"},{"type":"has_code","target_id":"github:gitabcworld:FewShotLearning","source_url":"https://github.com/gitabcworld/FewShotLearning"},{"type":"has_code","target_id":"github:jklj077:meProp","source_url":"https://github.com/jklj077/meProp"},{"type":"has_code","target_id":"github:clcarwin:SFD_pytorch","source_url":"https://github.com/clcarwin/SFD_pytorch"},{"type":"has_code","target_id":"github:facebookresearch:GradientEpisodicMemory","source_url":"https://github.com/facebookresearch/GradientEpisodicMemory"},{"type":"has_code","target_id":"github:KupynOrest:DeblurGAN","source_url":"https://github.com/KupynOrest/DeblurGAN"},{"type":"has_code","target_id":"github:yunjey:StarGAN","source_url":"https://github.com/yunjey/StarGAN"},{"type":"has_code","target_id":"github:adambielski:CapsNet-pytorch","source_url":"https://github.com/adambielski/CapsNet-pytorch"},{"type":"has_code","target_id":"github:ShichenLiu:CondenseNet","source_url":"https://github.com/ShichenLiu/CondenseNet"},{"type":"has_code","target_id":"github:DmitryUlyanov:deep-image-prior","source_url":"https://github.com/DmitryUlyanov/deep-image-prior"},{"type":"has_code","target_id":"github:natanielruiz:deep-head-pose","source_url":"https://github.com/natanielruiz/deep-head-pose"},{"type":"has_code","target_id":"github:zhunzhong07:Random-Erasing","source_url":"https://github.com/zhunzhong07/Random-Erasing"},{"type":"has_code","target_id":"github:facebookresearch:FaderNetworks","source_url":"https://github.com/facebookresearch/FaderNetworks"},{"type":"has_code","target_id":"github:NVIDIA:flownet2-pytorch","source_url":"https://github.com/NVIDIA/flownet2-pytorch"},{"type":"has_code","target_id":"github:NVIDIA:pix2pixHD","source_url":"https://github.com/NVIDIA/pix2pixHD"},{"type":"has_code","target_id":"github:pkdn:pytorch-smoothgrad","source_url":"https://github.com/pkdn/pytorch-smoothgrad"},{"type":"has_code","target_id":"github:c0nn3r:RetinaNet","source_url":"https://github.com/c0nn3r/RetinaNet"},{"type":"has_code","target_id":"github:jwyang:faster-rcnn.pytorch","source_url":"https://github.com/jwyang/faster-rcnn.pytorch"},{"type":"has_code","target_id":"github:leehomyc:mixup_pytorch","source_url":"https://github.com/leehomyc/mixup_pytorch"},{"type":"has_code","target_id":"github:mapillary:inplace_abn","source_url":"https://github.com/mapillary/inplace_abn"},{"type":"has_code","target_id":"github:xingyizhou:pytorch-pose-hg-3d","source_url":"https://github.com/xingyizhou/pytorch-pose-hg-3d"},{"type":"has_code","target_id":"github:HarshTrivedi:nmn-pytorch","source_url":"https://github.com/HarshTrivedi/nmn-pytorch"},{"type":"has_code","target_id":"github:kefirski:bytenet","source_url":"https://github.com/kefirski/bytenet"},{"type":"has_code","target_id":"github:hengyuan-hu:bottom-up-attention-vqa","source_url":"https://github.com/hengyuan-hu/bottom-up-attention-vqa"},{"type":"has_code","target_id":"github:ruiminshen:yolo2-pytorch","source_url":"https://github.com/ruiminshen/yolo2-pytorch"},{"type":"has_code","target_id":"github:Wizaron:reseg-pytorch","source_url":"https://github.com/Wizaron/reseg-pytorch"},{"type":"has_code","target_id":"github:Wizaron:binary-stochastic-neurons","source_url":"https://github.com/Wizaron/binary-stochastic-neurons"},{"type":"has_code","target_id":"github:DavexPro:pytorch-pose-estimation","source_url":"https://github.com/DavexPro/pytorch-pose-estimation"},{"type":"has_code","target_id":"github:higgsfield:interaction_network_pytorch","source_url":"https://github.com/higgsfield/interaction_network_pytorch"},{"type":"has_code","target_id":"github:wlwkgus:NoisyNaturalGradient","source_url":"https://github.com/wlwkgus/NoisyNaturalGradient"},{"type":"has_code","target_id":"github:moskomule:ewc.pytorch","source_url":"https://github.com/moskomule/ewc.pytorch"},{"type":"has_code","target_id":"github:jacobgil:pytorch-zssr","source_url":"https://github.com/jacobgil/pytorch-zssr"},{"type":"has_code","target_id":"github:atiyo:deep_image_prior","source_url":"https://github.com/atiyo/deep_image_prior"},{"type":"has_code","target_id":"github:leviswind:pytorch-transformer","source_url":"https://github.com/leviswind/pytorch-transformer"},{"type":"has_code","target_id":"github:devendrachaplot:DeepRL-Grounding","source_url":"https://github.com/devendrachaplot/DeepRL-Grounding"},{"type":"has_code","target_id":"github:Wizaron:deep-forecast-pytorch","source_url":"https://github.com/Wizaron/deep-forecast-pytorch"},{"type":"has_code","target_id":"github:utiasSTARS:cat-net","source_url":"https://github.com/utiasSTARS/cat-net"},{"type":"has_code","target_id":"github:tneumann:minimal_glo","source_url":"https://github.com/tneumann/minimal_glo"},{"type":"has_code","target_id":"github:dragen1860:LearningToCompare-Pytorch","source_url":"https://github.com/dragen1860/LearningToCompare-Pytorch"},{"type":"has_code","target_id":"github:facebookresearch:poincare-embeddings","source_url":"https://github.com/facebookresearch/poincare-embeddings"},{"type":"has_code","target_id":"github:ikostrikov:pytorch-trpo","source_url":"https://github.com/ikostrikov/pytorch-trpo"},{"type":"has_code","target_id":"github:JamesChuanggg:ggnn.pytorch","source_url":"https://github.com/JamesChuanggg/ggnn.pytorch"},{"type":"has_code","target_id":"github:Mrgemy95:visual-interaction-networks-pytorch","source_url":"https://github.com/Mrgemy95/visual-interaction-networks-pytorch"},{"type":"has_code","target_id":"github:jhayes14:adversarial-patch","source_url":"https://github.com/jhayes14/adversarial-patch"},{"type":"has_code","target_id":"github:orobix:Prototypical-Networks-for-Few-shot-Learning-PyTorch","source_url":"https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch"},{"type":"has_code","target_id":"github:orobix:Visual-Feature-Attribution-Using-Wasserstein-GANs-Pytorch","source_url":"https://github.com/orobix/Visual-Feature-Attribution-Using-Wasserstein-GANs-Pytorch"},{"type":"has_code","target_id":"github:Blade6570:PhotographicImageSynthesiswithCascadedRefinementNetworks-Pytorch","source_url":"https://github.com/Blade6570/PhotographicImageSynthesiswithCascadedRefinementNetworks-Pytorch"},{"type":"has_code","target_id":"github:carpedm20:ENAS-pytorch","source_url":"https://github.com/carpedm20/ENAS-pytorch"},{"type":"has_code","target_id":"github:kentsyx:Neural-IMage-Assessment","source_url":"https://github.com/kentsyx/Neural-IMage-Assessment"},{"type":"has_code","target_id":"github:tfrerix:proxprop","source_url":"https://github.com/tfrerix/proxprop"},{"type":"has_code","target_id":"github:NVIDIA:FastPhotoStyle","source_url":"https://github.com/NVIDIA/FastPhotoStyle"},{"type":"has_code","target_id":"github:Ben-Louis:Deep-Image-Analogy-PyTorch","source_url":"https://github.com/Ben-Louis/Deep-Image-Analogy-PyTorch"},{"type":"has_code","target_id":"github:layumi:Person_reID_baseline_pytorch","source_url":"https://github.com/layumi/Person_reID_baseline_pytorch"},{"type":"has_code","target_id":"github:zalandoresearch:pt-dilate-rnn","source_url":"https://github.com/zalandoresearch/pt-dilate-rnn"},{"type":"has_code","target_id":"github:jhjacobsen:pytorch-i-revnet","source_url":"https://github.com/jhjacobsen/pytorch-i-revnet"},{"type":"has_code","target_id":"github:Orcuslc:OrthNet","source_url":"https://github.com/Orcuslc/OrthNet"},{"type":"has_code","target_id":"github:jt827859032:DRRN-pytorch","source_url":"https://github.com/jt827859032/DRRN-pytorch"},{"type":"has_code","target_id":"github:moskomule:shampoo.pytorch","source_url":"https://github.com/moskomule/shampoo.pytorch"},{"type":"has_code","target_id":"github:truskovskiyk:nima.pytorch","source_url":"https://github.com/truskovskiyk/nima.pytorch"},{"type":"has_code","target_id":"github:locuslab:TCN","source_url":"https://github.com/locuslab/TCN"},{"type":"has_code","target_id":"github:shahsohil:DCC","source_url":"https://github.com/shahsohil/DCC"},{"type":"has_code","target_id":"github:arunmallya:packnet","source_url":"https://github.com/arunmallya/packnet"},{"type":"has_code","target_id":"github:github-pengge:PyTorch-progressive_growing_of_gans","source_url":"https://github.com/github-pengge/PyTorch-progressive_growing_of_gans"},{"type":"has_code","target_id":"github:salesforce:nonauto-nmt","source_url":"https://github.com/salesforce/nonauto-nmt"},{"type":"has_code","target_id":"github:eriklindernoren:PyTorch-GAN","source_url":"https://github.com/eriklindernoren/PyTorch-GAN"},{"type":"has_code","target_id":"github:tomrunia:PyTorchWavelets","source_url":"https://github.com/tomrunia/PyTorchWavelets"},{"type":"has_code","target_id":"github:karpathy:pytorch-made","source_url":"https://github.com/karpathy/pytorch-made"},{"type":"has_code","target_id":"github:emited:VariationalRecurrentNeuralNetwork","source_url":"https://github.com/emited/VariationalRecurrentNeuralNetwork"},{"type":"has_code","target_id":"github:emited:flow","source_url":"https://github.com/emited/flow"},{"type":"has_code","target_id":"github:r9y9:deepvoice3_pytorch","source_url":"https://github.com/r9y9/deepvoice3_pytorch"},{"type":"has_code","target_id":"github:elanmart:psmm","source_url":"https://github.com/elanmart/psmm"},{"type":"has_code","target_id":"github:NVIDIA:tacotron2","source_url":"https://github.com/NVIDIA/tacotron2"},{"type":"has_code","target_id":"github:rahulkidambi:AccSGD","source_url":"https://github.com/rahulkidambi/AccSGD"},{"type":"has_code","target_id":"github:hengruo:QANet-pytorch","source_url":"https://github.com/hengruo/QANet-pytorch"},{"type":"has_code","target_id":"github:TimDettmers:ConvE","source_url":"https://github.com/TimDettmers/ConvE"},{"type":"has_code","target_id":"github:kaushalshetty:Structured-Self-Attention","source_url":"https://github.com/kaushalshetty/Structured-Self-Attention"},{"type":"has_code","target_id":"github:williamleif:graphsage-simple","source_url":"https://github.com/williamleif/graphsage-simple"},{"type":"has_code","target_id":"github:roytseng-tw:Detectron.pytorch","source_url":"https://github.com/roytseng-tw/Detectron.pytorch"},{"type":"has_code","target_id":"github:irhumshafkat:R2Plus1D-PyTorch","source_url":"https://github.com/irhumshafkat/R2Plus1D-PyTorch"},{"type":"has_code","target_id":"github:viking-sudo-rm:StackNN","source_url":"https://github.com/viking-sudo-rm/StackNN"},{"type":"has_code","target_id":"github:facebookresearch:translagent","source_url":"https://github.com/facebookresearch/translagent"},{"type":"has_code","target_id":"github:jnhwkim:ban-vqa","source_url":"https://github.com/jnhwkim/ban-vqa"},{"type":"has_code","target_id":"github:huggingface:pytorch-openai-transformer-lm","source_url":"https://github.com/huggingface/pytorch-openai-transformer-lm"},{"type":"has_code","target_id":"github:akanimax:T2F","source_url":"https://github.com/akanimax/T2F"},{"type":"has_code","target_id":"github:mseitzer:pytorch-fid","source_url":"https://github.com/mseitzer/pytorch-fid"},{"type":"has_code","target_id":"github:jmtomczak:vae_vpflows","source_url":"https://github.com/jmtomczak/vae_vpflows"},{"type":"has_code","target_id":"github:mkocabas:CoordConv-pytorch","source_url":"https://github.com/mkocabas/CoordConv-pytorch"},{"type":"has_code","target_id":"github:xternalz:SDPoint","source_url":"https://github.com/xternalz/SDPoint"},{"type":"has_code","target_id":"github:wxywhu:SRDenseNet-pytorch","source_url":"https://github.com/wxywhu/SRDenseNet-pytorch"},{"type":"has_code","target_id":"github:LMescheder:GAN_stability","source_url":"https://github.com/LMescheder/GAN_stability"},{"type":"has_code","target_id":"github:wannabeOG:Mask-RCNN","source_url":"https://github.com/wannabeOG/Mask-RCNN"},{"type":"has_code","target_id":"github:chaoyuaw:pytorch-coviar","source_url":"https://github.com/chaoyuaw/pytorch-coviar"},{"type":"has_code","target_id":"github:chenxi116:PNASNet.pytorch","source_url":"https://github.com/chenxi116/PNASNet.pytorch"},{"type":"has_code","target_id":"github:kevinzakka:NALU-pytorch","source_url":"https://github.com/kevinzakka/NALU-pytorch"},{"type":"has_code","target_id":"github:alexis-jacq:LOLA_DiCE","source_url":"https://github.com/alexis-jacq/LOLA_DiCE"},{"type":"has_code","target_id":"github:wohlert:generative-query-network-pytorch","source_url":"https://github.com/wohlert/generative-query-network-pytorch"},{"type":"has_code","target_id":"github:wmvanvliet:pytorch_hmax","source_url":"https://github.com/wmvanvliet/pytorch_hmax"},{"type":"has_code","target_id":"github:yunlongdong:FCN-pytorch-easiest","source_url":"https://github.com/yunlongdong/FCN-pytorch-easiest"},{"type":"has_code","target_id":"github:awni:transducer","source_url":"https://github.com/awni/transducer"},{"type":"has_code","target_id":"github:artix41:AVO-pytorch","source_url":"https://github.com/artix41/AVO-pytorch"},{"type":"has_code","target_id":"github:huguyuehuhu:HCN-pytorch","source_url":"https://github.com/huguyuehuhu/HCN-pytorch"},{"type":"has_code","target_id":"github:szagoruyko:binary-wide-resnet","source_url":"https://github.com/szagoruyko/binary-wide-resnet"},{"type":"has_code","target_id":"github:arunmallya:piggyback","source_url":"https://github.com/arunmallya/piggyback"},{"type":"has_code","target_id":"github:NVIDIA:vid2vid","source_url":"https://github.com/NVIDIA/vid2vid"},{"type":"has_code","target_id":"github:cranmer:poisson-convolution-sum","source_url":"https://github.com/cranmer/poisson-convolution-sum"},{"type":"has_code","target_id":"github:davidmascharka:tbd-nets","source_url":"https://github.com/davidmascharka/tbd-nets"},{"type":"has_code","target_id":"github:elbayadm:attn2d","source_url":"https://github.com/elbayadm/attn2d"},{"type":"has_code","target_id":"github:ultralytics:yolov3","source_url":"https://github.com/ultralytics/yolov3"},{"type":"has_code","target_id":"github:duc0:deep-dream-in-pytorch","source_url":"https://github.com/duc0/deep-dream-in-pytorch"},{"type":"has_code","target_id":"github:ikostrikov:pytorch-flows","source_url":"https://github.com/ikostrikov/pytorch-flows"},{"type":"has_code","target_id":"github:ars-ashuha:quantile-regression-dqn-pytorch","source_url":"https://github.com/ars-ashuha/quantile-regression-dqn-pytorch"},{"type":"has_code","target_id":"github:L0SG:relational-rnn-pytorch","source_url":"https://github.com/L0SG/relational-rnn-pytorch"},{"type":"has_code","target_id":"github:scaelles:DEXTR-PyTorch","source_url":"https://github.com/scaelles/DEXTR-PyTorch"},{"type":"has_code","target_id":"github:rdspring1:PyTorch_GBW_LM","source_url":"https://github.com/rdspring1/PyTorch_GBW_LM"},{"type":"has_code","target_id":"github:Stonesjtu:Pytorch-NCE","source_url":"https://github.com/Stonesjtu/Pytorch-NCE"},{"type":"has_code","target_id":"github:shayneobrien:generative-models","source_url":"https://github.com/shayneobrien/generative-models"},{"type":"has_code","target_id":"github:andreasveit:convnet-aig","source_url":"https://github.com/andreasveit/convnet-aig"},{"type":"has_code","target_id":"github:TianhongDai:integrated-gradient-pytorch","source_url":"https://github.com/TianhongDai/integrated-gradient-pytorch"},{"type":"has_code","target_id":"github:Alexander-H-Liu:MalConv-Pytorch","source_url":"https://github.com/Alexander-H-Liu/MalConv-Pytorch"},{"type":"has_code","target_id":"github:locuslab:trellisnet","source_url":"https://github.com/locuslab/trellisnet"},{"type":"has_code","target_id":"github:minqi:learning-to-communicate-pytorch","source_url":"https://github.com/minqi/learning-to-communicate-pytorch"},{"type":"has_code","target_id":"github:michaelklachko:pnn.pytorch","source_url":"https://github.com/michaelklachko/pnn.pytorch"},{"type":"has_code","target_id":"github:rainofmine:Face_Attention_Network","source_url":"https://github.com/rainofmine/Face_Attention_Network"},{"type":"has_code","target_id":"github:NVIDIA:waveglow","source_url":"https://github.com/NVIDIA/waveglow"},{"type":"has_code","target_id":"github:facebookresearch:deepfloat","source_url":"https://github.com/facebookresearch/deepfloat"},{"type":"has_code","target_id":"github:subeeshvasu:2018_subeesh_epsr_eccvw","source_url":"https://github.com/subeeshvasu/2018_subeesh_epsr_eccvw"},{"type":"has_code","target_id":"github:ksw0306:ClariNet","source_url":"https://github.com/ksw0306/ClariNet"},{"type":"has_code","target_id":"github:huggingface:pytorch-pretrained-BERT","source_url":"https://github.com/huggingface/pytorch-pretrained-BERT"},{"type":"has_code","target_id":"github:npuichigo:waveglow","source_url":"https://github.com/npuichigo/waveglow"},{"type":"has_code","target_id":"github:cleardusk:3DDFA","source_url":"https://github.com/cleardusk/3DDFA"},{"type":"has_code","target_id":"github:tomgoldstein:loss-landscape","source_url":"https://github.com/tomgoldstein/loss-landscape"},{"type":"has_code","target_id":"github:zalandoresearch:famos","source_url":"https://github.com/zalandoresearch/famos"},{"type":"has_code","target_id":"github:anuragranj:back2future.pytorch","source_url":"https://github.com/anuragranj/back2future.pytorch"},{"type":"has_code","target_id":"github:mozilla:FFTNet","source_url":"https://github.com/mozilla/FFTNet"},{"type":"has_code","target_id":"github:zisianw:FaceBoxes.PyTorch","source_url":"https://github.com/zisianw/FaceBoxes.PyTorch"},{"type":"has_code","target_id":"github:kimiyoung:transformer-xl","source_url":"https://github.com/kimiyoung/transformer-xl"},{"type":"has_code","target_id":"github:kimiyoung:transformer-xl","source_url":"https://github.com/kimiyoung/transformer-xl"},{"type":"has_code","target_id":"github:jalexvig:associative_compression_networks","source_url":"https://github.com/jalexvig/associative_compression_networks"},{"type":"has_code","target_id":"github:jolibrain:fluidnet_cxx","source_url":"https://github.com/jolibrain/fluidnet_cxx"},{"type":"has_code","target_id":"github:p-christ:Deep-Reinforcement-Learning-Algorithms-with-PyTorch","source_url":"https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"},{"type":"has_code","target_id":"github:ericsun99:Shufflenet-v2-Pytorch","source_url":"https://github.com/ericsun99/Shufflenet-v2-Pytorch"},{"type":"has_code","target_id":"github:benedekrozemberczki:GraphWaveletNeuralNetwork","source_url":"https://github.com/benedekrozemberczki/GraphWaveletNeuralNetwork"},{"type":"has_code","target_id":"github:benedekrozemberczki:AttentionWalk","source_url":"https://github.com/benedekrozemberczki/AttentionWalk"},{"type":"has_code","target_id":"github:benedekrozemberczki:SGCN","source_url":"https://github.com/benedekrozemberczki/SGCN"},{"type":"has_code","target_id":"github:benedekrozemberczki:SINE","source_url":"https://github.com/benedekrozemberczki/SINE"},{"type":"has_code","target_id":"github:benedekrozemberczki:GAM","source_url":"https://github.com/benedekrozemberczki/GAM"},{"type":"has_code","target_id":"github:ProGamerGov:neural-style-pt","source_url":"https://github.com/ProGamerGov/neural-style-pt"},{"type":"has_code","target_id":"github:ibalazevic:TuckER","source_url":"https://github.com/ibalazevic/TuckER"},{"type":"has_code","target_id":"github:BayesWatch:pytorch-prunes","source_url":"https://github.com/BayesWatch/pytorch-prunes"},{"type":"has_code","target_id":"github:benedekrozemberczki:SimGNN","source_url":"https://github.com/benedekrozemberczki/SimGNN"},{"type":"has_code","target_id":"github:ahmedbesbes:character-based-cnn","source_url":"https://github.com/ahmedbesbes/character-based-cnn"},{"type":"has_code","target_id":"github:facebookresearch:XLM","source_url":"https://github.com/facebookresearch/XLM"},{"type":"has_code","target_id":"github:eth-sri:diffai","source_url":"https://github.com/eth-sri/diffai"},{"type":"has_code","target_id":"github:benedekrozemberczki:APPNP","source_url":"https://github.com/benedekrozemberczki/APPNP"},{"type":"has_code","target_id":"github:benedekrozemberczki:MixHop-and-N-GCN","source_url":"https://github.com/benedekrozemberczki/MixHop-and-N-GCN"},{"type":"has_code","target_id":"github:graykode:gpt-2-Pytorch","source_url":"https://github.com/graykode/gpt-2-Pytorch"},{"type":"has_code","target_id":"github:benedekrozemberczki:Splitter","source_url":"https://github.com/benedekrozemberczki/Splitter"},{"type":"has_code","target_id":"github:benedekrozemberczki:CapsGNN","source_url":"https://github.com/benedekrozemberczki/CapsGNN"},{"type":"has_code","target_id":"github:ajbrock:BigGAN-PyTorch","source_url":"https://github.com/ajbrock/BigGAN-PyTorch"},{"type":"has_code","target_id":"github:mhubii:ppo_pytorch_cpp","source_url":"https://github.com/mhubii/ppo_pytorch_cpp"},{"type":"has_code","target_id":"github:seungwonpark:RandWireNN","source_url":"https://github.com/seungwonpark/RandWireNN"},{"type":"has_code","target_id":"github:joel-huang:zeroshot-capsnet-pytorch","source_url":"https://github.com/joel-huang/zeroshot-capsnet-pytorch"},{"type":"has_code","target_id":"github:benedekrozemberczki:SEAL-CI","source_url":"https://github.com/benedekrozemberczki/SEAL-CI"},{"type":"has_code","target_id":"github:benedekrozemberczki:MixHop-and-N-GCN","source_url":"https://github.com/benedekrozemberczki/MixHop-and-N-GCN"},{"type":"has_code","target_id":"github:Lotayou:densebody_pytorch","source_url":"https://github.com/Lotayou/densebody_pytorch"},{"type":"has_code","target_id":"github:mindslab-ai:voicefilter","source_url":"https://github.com/mindslab-ai/voicefilter"},{"type":"has_code","target_id":"github:NVIDIA:semantic-segmentation","source_url":"https://github.com/NVIDIA/semantic-segmentation"},{"type":"has_code","target_id":"github:benedekrozemberczki:ClusterGCN","source_url":"https://github.com/benedekrozemberczki/ClusterGCN"},{"type":"has_code","target_id":"github:NVlabs:DG-Net","source_url":"https://github.com/NVlabs/DG-Net"},{"type":"has_code","target_id":"github:baidu-research:NCRF","source_url":"https://github.com/baidu-research/NCRF"},{"type":"has_code","target_id":"github:ducha-aiki:pytorch-sift","source_url":"https://github.com/ducha-aiki/pytorch-sift"},{"type":"has_code","target_id":"github:mateuszbuda:brain-segmentation-pytorch","source_url":"https://github.com/mateuszbuda/brain-segmentation-pytorch"},{"type":"has_code","target_id":"github:rosinality:glow-pytorch","source_url":"https://github.com/rosinality/glow-pytorch"},{"type":"has_code","target_id":"github:zsef123:EfficientNets-PyTorch","source_url":"https://github.com/zsef123/EfficientNets-PyTorch"}]', NULL, NULL, 'pending', 70, '43b5e78113302d8cdd55a4593728cdac', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-bharathgs-Awesome-pytorch-list from https://github.com/bharathgs.png
Image converted to WebP: data/images/github-bharathgs-Awesome-pytorch-list.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-dair-ai-ml-visuals', 'github--dair-ai--ml-visuals', 'ml-visuals', 'dair-ai', '📣 Stay tuned for significant updates to both the slides and repository.!!! 📣 In the meantime, Join our Discord ML Visuals is a new collaborative effort to help the machine learning community in improving science communication by providing free professional, compelling and adequate visuals and figures. Currently, we have over 100 figures (all open community contributions). You are free to use the visuals in your machine learning presentations or blog posts. You don’t need to ask permission t...', '["artificial-intelligence","deep-learning","design","machine-learning","natural-language-processing"]', 'other', 16185, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/dair-ai/ml-visuals","fetched_at":"2025-12-08T10:39:52.045Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# ML Visuals\n\n📣 Stay tuned for significant updates to both the slides and repository.!!!\n\n📣 In the meantime, [Join our Discord](https://discord.gg/SKgkVT8BGJ)\n\n[ML Visuals](https://docs.google.com/presentation/d/11mR1nkIR9fbHegFkcFq8z9oDQ5sjv8E3JJp1LfLGKuk/edit?usp=sharing) is a new collaborative effort to help the machine learning community in improving science communication by providing free professional, compelling and adequate visuals and figures. Currently, we have over 100 figures (all open community contributions). You are free to use the visuals in your machine learning presentations or blog posts. You don’t need to ask permission to use any of the visuals but it will be nice if you can provide credit to the designer/author (author information found in the slide notes). Check out the versions of the visuals below. \n\nThis is a project made by the [dair.ai](https://dair.ai/) community. The latest version of the Google slides can be found in this GitHub repository. Our community members will continue to add more common figures and basic elements in upcoming versions. Think of this as free and open artifacts and templates which you can freely and easily download, copy, distribute, reuse and customize to your own needs.\n\nML Visuals is now being used to power 100s of figures used by master/PhD students, papers (like this [one](https://arxiv.org/abs/2010.05113)), among other use cases. \n\n## How to Use?\n\nEssentially, we are using Google Slides to maintain all visuals and figures (check the versions below). To add your own custom figures, simply add a new slide and reuse any of the basic visual components (remember to request edit permissions). You can also create your own copy of the slides and customize whatever you like. We encourage authors/designers to add their visuals here and allow others to reuse them. Make sure to include your author information (in the notes section of the slide) so that others can provide credit if they use the visuals elsewhere (e.g. blog/presentations). Also, provide a short description of your visual to help the user understand what it is about and how they can use it. If you need "Edit" permission, just click on the "request edit access" option under the "view only" toolbar (in Google Slides) or send me an email at ellfae@gmail.com.\n\nDownloading a figure from any of the slides is easy. Just click on File→Download→(choose your format).\n\nIf you need help with customizing a figure or have an idea of something that could be valuable to others, we can help. Just open an issue [here](https://github.com/dair-ai/ml-visuals/issues/new) and we will do our best to come up with the visual. Thanks.\n\nFeel free to reach out to me on [Twitter](https://twitter.com/omarsar0) for an invite to our Slack group.\n\n## Versions:\n- [Version 1.0](https://docs.google.com/presentation/d/11mR1nkIR9fbHegFkcFq8z9oDQ5sjv8E3JJp1LfLGKuk/edit?usp=sharing)\n\n\n## How to Contribute?\n- You can check out our [Project page](https://github.com/orgs/dair-ai/projects/8) to see all the ongoing tasks or issues related to this research project. Lookout for the main `ml_visuals` tag. Issues with the `good first issue` tag are good tasks to get started with.\n- You can also just check the [issues tab](https://github.com/dair-ai/ml-visuals/issues).\n- You can ask anything related to this project in our Slack group\n- Slack channel: #ml_visuals\n\n**Some ideas for figures to add to the Slides** ([issue](https://github.com/dair-ai/ml-visuals/issues/14))\n\n- [ ] Linear regression, single-layer neural network\n- [ ] Multilayer Perceptron with hidden layer\n- [ ] Backpropagation\n- [ ] Batch Normalization and alternatives\n- [ ] Computational Graphs\n- [ ] Dropout\n- [ ] CNN - padding, stride, pooling,... \n- [ ] LeNet\n- [ ] AlexNet\n- [ ] VGG\n- [ ] GoogleNet\n- [ ] ResNet\n- [ ] DenseNet\n- [ ] Memory Networks\n- [ ] RNN\n- [ ] Deep RNN\n- [ ] Bidirectional RNN\n- [ ] GRU\n- [ ] LSTM\n- [ ] Language RNN models\n- [ ] Backpropagation through time\n- [ ] Encoder-Decoder Architecture\n- [ ] Seq2seq with RNN encoder-decoder\n- [ ] Bearm search and other decoding strategies\n- [ ] Attention\n- [ ] Multi-head attention\n- [ ] Self-attention\n- [ ] Transformer\n- [ ] Word2vec/GloVe/Skip-gram/CBOW/BERT/GPT....\n- [ ] Common/Popular CV/NLP Tasks\n\nList adopted from multiple resources including [nlpoverview](https://nlpoverview.com/index.html) and [d2l.ai](https://d2l.ai/) which both contain a very solid syllabus.  \n\n## Examples of Visuals\n![](1.png)\n![](2.png)\n![](3.png)\n', '{"language":null,"stars":16185,"forks":1506,"watchers":16185,"open_issues":44,"topics":["artificial-intelligence","deep-learning","design","machine-learning","natural-language-processing"],"default_branch":"master","size_kb":141,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:dair-ai:ml-visuals","source_url":"https://github.com/dair-ai/ml-visuals"},{"type":"has_code","target_id":"github:orgs:dair-ai","source_url":"https://github.com/orgs/dair-ai"},{"type":"has_code","target_id":"github:dair-ai:ml-visuals","source_url":"https://github.com/dair-ai/ml-visuals"},{"type":"has_code","target_id":"github:dair-ai:ml-visuals","source_url":"https://github.com/dair-ai/ml-visuals"}]', NULL, 'MIT', 'approved', 65, '2a19415931fe1a20eb283192ff83d26c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-dair-ai-ml-visuals from https://github.com/dair-ai.png
Image converted to WebP: data/images/github-dair-ai-ml-visuals.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-Significant-Gravitas-AutoGPT', 'github--significant-gravitas--autogpt', 'AutoGPT', 'Significant-Gravitas', '&ensp; &ensp; <!-- Keep these links. Translations will automatically update with the README. --> Deutsch | Español | français | 日本語 | 한국어 | Português | Русский | 中文 **AutoGPT** is a powerful platform that allows you to create, deploy, and manage continuous AI agents that automate complex workflows. - Download to self-host (Free!) - Join the Waitlist for the cloud-hosted beta (Closed Beta - Public release Coming Soon!) > [!NOTE] > Setting up and hosting the AutoGPT Platform yourself is a techn...', '["ai","artificial-intelligence","autonomous-agents","gpt-4","llama-api","openai","python","python"]', 'other', 180172, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/Significant-Gravitas/AutoGPT","fetched_at":"2025-12-08T10:39:52.045Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# AutoGPT: Build, Deploy, and Run AI Agents\n\n[![Discord Follow](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fautogpt%3Fwith_counts%3Dtrue&query=%24.approximate_member_count&label=total%20members&logo=discord&logoColor=white&color=7289da)](https://discord.gg/autogpt) &ensp;\n[![Twitter Follow](https://img.shields.io/twitter/follow/Auto_GPT?style=social)](https://twitter.com/Auto_GPT) &ensp;\n\n<!-- Keep these links. Translations will automatically update with the README. -->\n[Deutsch](https://zdoc.app/de/Significant-Gravitas/AutoGPT) | \n[Español](https://zdoc.app/es/Significant-Gravitas/AutoGPT) | \n[français](https://zdoc.app/fr/Significant-Gravitas/AutoGPT) | \n[日本語](https://zdoc.app/ja/Significant-Gravitas/AutoGPT) | \n[한국어](https://zdoc.app/ko/Significant-Gravitas/AutoGPT) | \n[Português](https://zdoc.app/pt/Significant-Gravitas/AutoGPT) | \n[Русский](https://zdoc.app/ru/Significant-Gravitas/AutoGPT) | \n[中文](https://zdoc.app/zh/Significant-Gravitas/AutoGPT)\n\n**AutoGPT** is a powerful platform that allows you to create, deploy, and manage continuous AI agents that automate complex workflows. \n\n## Hosting Options \n   - Download to self-host (Free!)\n   - [Join the Waitlist](https://bit.ly/3ZDijAI) for the cloud-hosted beta (Closed Beta - Public release Coming Soon!)\n\n## How to Self-Host the AutoGPT Platform\n> [!NOTE]\n> Setting up and hosting the AutoGPT Platform yourself is a technical process. \n> If you''d rather something that just works, we recommend [joining the waitlist](https://bit.ly/3ZDijAI) for the cloud-hosted beta.\n\n### System Requirements\n\nBefore proceeding with the installation, ensure your system meets the following requirements:\n\n#### Hardware Requirements\n- CPU: 4+ cores recommended\n- RAM: Minimum 8GB, 16GB recommended\n- Storage: At least 10GB of free space\n\n#### Software Requirements\n- Operating Systems:\n  - Linux (Ubuntu 20.04 or newer recommended)\n  - macOS (10.15 or newer)\n  - Windows 10/11 with WSL2\n- Required Software (with minimum versions):\n  - Docker Engine (20.10.0 or newer)\n  - Docker Compose (2.0.0 or newer)\n  - Git (2.30 or newer)\n  - Node.js (16.x or newer)\n  - npm (8.x or newer)\n  - VSCode (1.60 or newer) or any modern code editor\n\n#### Network Requirements\n- Stable internet connection\n- Access to required ports (will be configured in Docker)\n- Ability to make outbound HTTPS connections\n\n### Updated Setup Instructions:\nWe''ve moved to a fully maintained and regularly updated documentation site.\n\n👉 [Follow the official self-hosting guide here](https://docs.agpt.co/platform/getting-started/)\n\n\nThis tutorial assumes you have Docker, VSCode, git and npm installed.\n\n---\n\n#### ⚡ Quick Setup with One-Line Script (Recommended for Local Hosting)\n\nSkip the manual steps and get started in minutes using our automatic setup script.\n\nFor macOS/Linux:\n```\ncurl -fsSL https://setup.agpt.co/install.sh -o install.sh && bash install.sh\n```\n\nFor Windows (PowerShell):\n```\npowershell -c "iwr https://setup.agpt.co/install.bat -o install.bat; ./install.bat"\n```\n\nThis will install dependencies, configure Docker, and launch your local instance — all in one go.\n\n### 🧱 AutoGPT Frontend\n\nThe AutoGPT frontend is where users interact with our powerful AI automation platform. It offers multiple ways to engage with and leverage our AI agents. This is the interface where you''ll bring your AI automation ideas to life:\n\n   **Agent Builder:** For those who want to customize, our intuitive, low-code interface allows you to design and configure your own AI agents. \n   \n   **Workflow Management:** Build, modify, and optimize your automation workflows with ease. You build your agent by connecting blocks, where each block     performs a single action.\n   \n   **Deployment Controls:** Manage the lifecycle of your agents, from testing to production.\n   \n   **Ready-to-Use Agents:** Don''t want to build? Simply select from our library of pre-configured agents and put them to work immediately.\n   \n   **Agent Interaction:** Whether you''ve built your own or are using pre-configured agents, easily run and interact with them through our user-friendly      interface.\n\n   **Monitoring and Analytics:** Keep track of your agents'' performance and gain insights to continually improve your automation processes.\n\n[Read this guide](https://docs.agpt.co/platform/new_blocks/) to learn how to build your own custom blocks.\n\n### 💽 AutoGPT Server\n\nThe AutoGPT Server is the powerhouse of our platform This is where your agents run. Once deployed, agents can be triggered by external sources and can operate continuously. It contains all the essential components that make AutoGPT run smoothly.\n\n   **Source Code:** The core logic that drives our agents and automation processes.\n   \n   **Infrastructure:** Robust systems that ensure reliable and scalable performance.\n   \n   **Marketplace:** A comprehensive marketplace where you can find and deploy a wide range of pre-built agents.\n\n### 🐙 Example Agents\n\nHere are two examples of what you can do with AutoGPT:\n\n1. **Generate Viral Videos from Trending Topics**\n   - This agent reads topics on Reddit.\n   - It identifies trending topics.\n   - It then automatically creates a short-form video based on the content. \n\n2. **Identify Top Quotes from Videos for Social Media**\n   - This agent subscribes to your YouTube channel.\n   - When you post a new video, it transcribes it.\n   - It uses AI to identify the most impactful quotes to generate a summary.\n   - Then, it writes a post to automatically publish to your social media. \n\nThese examples show just a glimpse of what you can achieve with AutoGPT! You can create customized workflows to build agents for any use case.\n\n---\n\n### **License Overview:**\n\n🛡️ **Polyform Shield License:**\nAll code and content within the `autogpt_platform` folder is licensed under the Polyform Shield License. This new project is our in-developlemt platform for building, deploying and managing agents.</br>_[Read more about this effort](https://agpt.co/blog/introducing-the-autogpt-platform)_\n\n🦉 **MIT License:**\nAll other portions of the AutoGPT repository (i.e., everything outside the `autogpt_platform` folder) are licensed under the MIT License. This includes the original stand-alone AutoGPT Agent, along with projects such as [Forge](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/forge), [agbenchmark](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/benchmark) and the [AutoGPT Classic GUI](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/frontend).</br>We also publish additional work under the MIT Licence in other repositories, such as [GravitasML](https://github.com/Significant-Gravitas/gravitasml) which is developed for and used in the AutoGPT Platform. See also our MIT Licenced [Code Ability](https://github.com/Significant-Gravitas/AutoGPT-Code-Ability) project.\n\n---\n### Mission\nOur mission is to provide the tools, so that you can focus on what matters:\n\n- 🏗️ **Building** - Lay the foundation for something amazing.\n- 🧪 **Testing** - Fine-tune your agent to perfection.\n- 🤝 **Delegating** - Let AI work for you, and have your ideas come to life.\n\nBe part of the revolution! **AutoGPT** is here to stay, at the forefront of AI innovation.\n\n**📖 [Documentation](https://docs.agpt.co)**\n&ensp;|&ensp;\n**🚀 [Contributing](CONTRIBUTING.md)**\n\n---\n## 🤖 AutoGPT Classic\n> Below is information about the classic version of AutoGPT.\n\n**🛠️ [Build your own Agent - Quickstart](classic/FORGE-QUICKSTART.md)**\n\n### 🏗️ Forge\n\n**Forge your own agent!** &ndash; Forge is a ready-to-go toolkit to build your own agent application. It handles most of the boilerplate code, letting you channel all your creativity into the things that set *your* agent apart. All tutorials are located [here](https://medium.com/@aiedge/autogpt-forge-e3de53cc58ec). Components from [`forge`](/classic/forge/) can also be used individually to speed up development and reduce boilerplate in your agent project.\n\n🚀 [**Getting Started with Forge**](https://github.com/Significant-Gravitas/AutoGPT/blob/master/classic/forge/tutorials/001_getting_started.md) &ndash;\nThis guide will walk you through the process of creating your own agent and using the benchmark and user interface.\n\n📘 [Learn More](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/forge) about Forge\n\n### 🎯 Benchmark\n\n**Measure your agent''s performance!** The `agbenchmark` can be used with any agent that supports the agent protocol, and the integration with the project''s [CLI] makes it even easier to use with AutoGPT and forge-based agents. The benchmark offers a stringent testing environment. Our framework allows for autonomous, objective performance evaluations, ensuring your agents are primed for real-world action.\n\n<!-- TODO: insert visual demonstrating the benchmark -->\n\n📦 [`agbenchmark`](https://pypi.org/project/agbenchmark/) on Pypi\n&ensp;|&ensp;\n📘 [Learn More](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/benchmark) about the Benchmark\n\n### 💻 UI\n\n**Makes agents easy to use!** The `frontend` gives you a user-friendly interface to control and monitor your agents. It connects to agents through the [agent protocol](#-agent-protocol), ensuring compatibility with many agents from both inside and outside of our ecosystem.\n\n<!-- TODO: insert screenshot of front end -->\n\nThe frontend works out-of-the-box with all agents in the repo. Just use the [CLI] to run your agent of choice!\n\n📘 [Learn More](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/frontend) about the Frontend\n\n### ⌨️ CLI\n\n[CLI]: #-cli\n\nTo make it as easy as possible to use all of the tools offered by the repository, a CLI is included at the root of the repo:\n\n```shell\n$ ./run\nUsage: cli.py [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  agent      Commands to create, start and stop agents\n  benchmark  Commands to start the benchmark and list tests and categories\n  setup      Installs dependencies needed for your system.\n```\n\nJust clone the repo, install dependencies with `./run setup`, and you should be good to go!\n\n## 🤔 Questions? Problems? Suggestions?\n\n### Get help - [Discord 💬](https://discord.gg/autogpt)\n\n[![Join us on Discord](https://invidget.switchblade.xyz/autogpt)](https://discord.gg/autogpt)\n\nTo report a bug or request a feature, create a [GitHub Issue](https://github.com/Significant-Gravitas/AutoGPT/issues/new/choose). Please ensure someone else hasn''t created an issue for the same topic.\n\n## 🤝 Sister projects\n\n### 🔄 Agent Protocol\n\nTo maintain a uniform standard and ensure seamless compatibility with many current and future applications, AutoGPT employs the [agent protocol](https://agentprotocol.ai/) standard by the AI Engineer Foundation. This standardizes the communication pathways from your agent to the frontend and benchmark.\n\n---\n\n## Stars stats\n\n<p align="center">\n<a href="https://star-history.com/#Significant-Gravitas/AutoGPT">\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=Significant-Gravitas/AutoGPT&type=Date&theme=dark" />\n    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=Significant-Gravitas/AutoGPT&type=Date" />\n    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=Significant-Gravitas/AutoGPT&type=Date" />\n  </picture>\n</a>\n</p>\n\n\n## ⚡ Contributors\n\n<a href="https://github.com/Significant-Gravitas/AutoGPT/graphs/contributors" alt="View Contributors">\n  <img src="https://contrib.rocks/image?repo=Significant-Gravitas/AutoGPT&max=1000&columns=10" alt="Contributors" />\n</a>\n', '{"language":"Python","stars":180172,"forks":46185,"watchers":180172,"open_issues":297,"topics":["ai","artificial-intelligence","autonomous-agents","gpt-4","llama-api","openai","python"],"default_branch":"master","size_kb":309120,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT","source_url":"https://github.com/Significant-Gravitas/AutoGPT"},{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT","source_url":"https://github.com/Significant-Gravitas/AutoGPT"},{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT","source_url":"https://github.com/Significant-Gravitas/AutoGPT"},{"type":"has_code","target_id":"github:Significant-Gravitas:gravitasml","source_url":"https://github.com/Significant-Gravitas/gravitasml"},{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT-Code-Ability","source_url":"https://github.com/Significant-Gravitas/AutoGPT-Code-Ability"},{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT","source_url":"https://github.com/Significant-Gravitas/AutoGPT"},{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT","source_url":"https://github.com/Significant-Gravitas/AutoGPT"},{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT","source_url":"https://github.com/Significant-Gravitas/AutoGPT"},{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT","source_url":"https://github.com/Significant-Gravitas/AutoGPT"},{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT","source_url":"https://github.com/Significant-Gravitas/AutoGPT"},{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT","source_url":"https://github.com/Significant-Gravitas/AutoGPT"}]', NULL, 'NOASSERTION', 'approved', 80, '63a0d38ddc945dafe65d7edb9e691830', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-Significant-Gravitas-AutoGPT from https://github.com/Significant-Gravitas.png
Image converted to WebP: data/images/github-Significant-Gravitas-AutoGPT.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-n8n-io-n8n', 'github--n8n-io--n8n', 'n8n', 'n8n-io', '!Banner image n8n is a workflow automation platform that gives technical teams the flexibility of code with the speed of no-code. With 400+ integrations, native AI capabilities, and a fair-code license, n8n lets you build powerful automations while maintaining full control over your data and deployments. !n8n.io - Screenshot - **Code When You Need It**: Write JavaScript/Python, add npm packages, or use the visual interface - **AI-Native Platform**: Build AI agent workflows based on LangChain ...', '["ai","apis","automation","cli","data-flow","development","integration-framework","integrations","ipaas","low-code","low-code-platform","mcp","mcp-client","mcp-server","n8n","no-code","self-hosted","typescript","workflow","workflow-automation","typescript"]', 'other', 161254, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/n8n-io/n8n","fetched_at":"2025-12-08T10:39:52.045Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '![Banner image](https://user-images.githubusercontent.com/10284570/173569848-c624317f-42b1-45a6-ab09-f0ea3c247648.png)\n\n# n8n - Secure Workflow Automation for Technical Teams\n\nn8n is a workflow automation platform that gives technical teams the flexibility of code with the speed of no-code. With 400+ integrations, native AI capabilities, and a fair-code license, n8n lets you build powerful automations while maintaining full control over your data and deployments.\n\n![n8n.io - Screenshot](https://raw.githubusercontent.com/n8n-io/n8n/master/assets/n8n-screenshot-readme.png)\n\n## Key Capabilities\n\n- **Code When You Need It**: Write JavaScript/Python, add npm packages, or use the visual interface\n- **AI-Native Platform**: Build AI agent workflows based on LangChain with your own data and models\n- **Full Control**: Self-host with our fair-code license or use our [cloud offering](https://app.n8n.cloud/login)\n- **Enterprise-Ready**: Advanced permissions, SSO, and air-gapped deployments\n- **Active Community**: 400+ integrations and 900+ ready-to-use [templates](https://n8n.io/workflows)\n\n## Quick Start\n\nTry n8n instantly with [npx](https://docs.n8n.io/hosting/installation/npm/) (requires [Node.js](https://nodejs.org/en/)):\n\n```\nnpx n8n\n```\n\nOr deploy with [Docker](https://docs.n8n.io/hosting/installation/docker/):\n\n```\ndocker volume create n8n_data\ndocker run -it --rm --name n8n -p 5678:5678 -v n8n_data:/home/node/.n8n docker.n8n.io/n8nio/n8n\n```\n\nAccess the editor at http://localhost:5678\n\n## Resources\n\n- 📚 [Documentation](https://docs.n8n.io)\n- 🔧 [400+ Integrations](https://n8n.io/integrations)\n- 💡 [Example Workflows](https://n8n.io/workflows)\n- 🤖 [AI & LangChain Guide](https://docs.n8n.io/advanced-ai/)\n- 👥 [Community Forum](https://community.n8n.io)\n- 📖 [Community Tutorials](https://community.n8n.io/c/tutorials/28)\n\n## Support\n\nNeed help? Our community forum is the place to get support and connect with other users:\n[community.n8n.io](https://community.n8n.io)\n\n## License\n\nn8n is [fair-code](https://faircode.io) distributed under the [Sustainable Use License](https://github.com/n8n-io/n8n/blob/master/LICENSE.md) and [n8n Enterprise License](https://github.com/n8n-io/n8n/blob/master/LICENSE_EE.md).\n\n- **Source Available**: Always visible source code\n- **Self-Hostable**: Deploy anywhere\n- **Extensible**: Add your own nodes and functionality\n\n[Enterprise licenses](mailto:license@n8n.io) available for additional features and support.\n\nAdditional information about the license model can be found in the [docs](https://docs.n8n.io/sustainable-use-license/).\n\n## Contributing\n\nFound a bug 🐛 or have a feature idea ✨? Check our [Contributing Guide](https://github.com/n8n-io/n8n/blob/master/CONTRIBUTING.md) to get started.\n\n## Join the Team\n\nWant to shape the future of automation? Check out our [job posts](https://n8n.io/careers) and join our team!\n\n## What does n8n mean?\n\n**Short answer:** It means "nodemation" and is pronounced as n-eight-n.\n\n**Long answer:** "I get that question quite often (more often than I expected) so I decided it is probably best to answer it here. While looking for a good name for the project with a free domain I realized very quickly that all the good ones I could think of were already taken. So, in the end, I chose nodemation. ''node-'' in the sense that it uses a Node-View and that it uses Node.js and ''-mation'' for ''automation'' which is what the project is supposed to help with. However, I did not like how long the name was and I could not imagine writing something that long every time in the CLI. That is when I then ended up on ''n8n''." - **Jan Oberhauser, Founder and CEO, n8n.io**\n', '{"language":"TypeScript","stars":161254,"forks":51639,"watchers":161254,"open_issues":1312,"topics":["ai","apis","automation","cli","data-flow","development","integration-framework","integrations","ipaas","low-code","low-code-platform","mcp","mcp-client","mcp-server","n8n","no-code","self-hosted","typescript","workflow","workflow-automation"],"default_branch":"master","size_kb":303710,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:n8n-io:n8n","source_url":"https://github.com/n8n-io/n8n"},{"type":"has_code","target_id":"github:n8n-io:n8n","source_url":"https://github.com/n8n-io/n8n"},{"type":"has_code","target_id":"github:n8n-io:n8n","source_url":"https://github.com/n8n-io/n8n"}]', NULL, 'NOASSERTION', 'approved', 65, '3a35f7002dfb43f18dac3118cc475e64', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-n8n-io-n8n from https://github.com/n8n-io.png
Image converted to WebP: data/images/github-n8n-io-n8n.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-langchain-ai-langchain', 'github--langchain-ai--langchain', 'langchain', 'langchain-ai', '<div align="center"> <a href="https://www.langchain.com/"> <picture> <source media="(prefers-color-scheme: light)" srcset=".github/images/logo-dark.svg"> <source media="(prefers-color-scheme: dark)" srcset=".github/images/logo-light.svg"> <img alt="LangChain Logo" src=".github/images/logo-dark.svg" width="80%"> </picture> </a> </div> <div align="center"> <h3>The platform for reliable agents.</h3> </div> <div align="center"> <a href="https://opensource.org/licenses/MIT" target="_blank"><img sr...', '["agents","ai","ai-agents","ai-agents-framework","aiagentframework","anthropic","chatgpt","enterprise","framework","gemini","generative-ai","langchain","llm","multiagent","open-source","openai","pydantic","python","rag","python"]', 'other', 121388, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/langchain-ai/langchain","fetched_at":"2025-12-08T10:39:52.045Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n  <a href="https://www.langchain.com/">\n    <picture>\n      <source media="(prefers-color-scheme: light)" srcset=".github/images/logo-dark.svg">\n      <source media="(prefers-color-scheme: dark)" srcset=".github/images/logo-light.svg">\n      <img alt="LangChain Logo" src=".github/images/logo-dark.svg" width="80%">\n    </picture>\n  </a>\n</div>\n\n<div align="center">\n  <h3>The platform for reliable agents.</h3>\n</div>\n\n<div align="center">\n  <a href="https://opensource.org/licenses/MIT" target="_blank"><img src="https://img.shields.io/pypi/l/langchain" alt="PyPI - License"></a>\n  <a href="https://pypistats.org/packages/langchain" target="_blank"><img src="https://img.shields.io/pepy/dt/langchain" alt="PyPI - Downloads"></a>\n  <a href="https://pypi.org/project/langchain/#history" target="_blank"><img src="https://img.shields.io/pypi/v/langchain?label=%20" alt="Version"></a>\n  <a href="https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain" target="_blank"><img src="https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode" alt="Open in Dev Containers"></a>\n  <a href="https://codespaces.new/langchain-ai/langchain" target="_blank"><img src="https://github.com/codespaces/badge.svg" alt="Open in Github Codespace" title="Open in Github Codespace" width="150" height="20"></a>\n  <a href="https://codspeed.io/langchain-ai/langchain" target="_blank"><img src="https://img.shields.io/endpoint?url=https://codspeed.io/badge.json" alt="CodSpeed Badge"></a>\n  <a href="https://twitter.com/langchainai" target="_blank"><img src="https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI" alt="Twitter / X"></a>\n</div>\n\nLangChain is a framework for building agents and LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development – all while future-proofing decisions as the underlying technology evolves.\n\n```bash\npip install langchain\n```\n\nIf you''re looking for more advanced customization or agent orchestration, check out [LangGraph](https://docs.langchain.com/oss/python/langgraph/overview), our framework for building controllable agent workflows.\n\n---\n\n**Documentation**:\n\n- [docs.langchain.com](https://docs.langchain.com/oss/python/langchain/overview) – Comprehensive documentation, including conceptual overviews and guides\n- [reference.langchain.com/python](https://reference.langchain.com/python) – API reference docs for LangChain packages\n\n**Discussions**: Visit the [LangChain Forum](https://forum.langchain.com) to connect with the community and share all of your technical questions, ideas, and feedback.\n\n> [!NOTE]\n> Looking for the JS/TS library? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\n\n## Why use LangChain?\n\nLangChain helps developers build applications powered by LLMs through a standard interface for models, embeddings, vector stores, and more.\n\nUse LangChain for:\n\n- **Real-time data augmentation**. Easily connect LLMs to diverse data sources and external/internal systems, drawing from LangChain''s vast library of integrations with model providers, tools, vector stores, retrievers, and more.\n- **Model interoperability**. Swap models in and out as your engineering team experiments to find the best choice for your application''s needs. As the industry frontier evolves, adapt quickly – LangChain''s abstractions keep you moving without losing momentum.\n- **Rapid prototyping**. Quickly build and iterate on LLM applications with LangChain''s modular, component-based architecture. Test different approaches and workflows without rebuilding from scratch, accelerating your development cycle.\n- **Production-ready features**. Deploy reliable applications with built-in support for monitoring, evaluation, and debugging through integrations like LangSmith. Scale with confidence using battle-tested patterns and best practices.\n- **Vibrant community and ecosystem**. Leverage a rich ecosystem of integrations, templates, and community-contributed components. Benefit from continuous improvements and stay up-to-date with the latest AI developments through an active open-source community.\n- **Flexible abstraction layers**. Work at the level of abstraction that suits your needs - from high-level chains for quick starts to low-level components for fine-grained control. LangChain grows with your application''s complexity.\n\n## LangChain ecosystem\n\nWhile the LangChain framework can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools when building LLM applications.\n\nTo improve your LLM application development, pair LangChain with:\n\n- [LangGraph](https://docs.langchain.com/oss/python/langgraph/overview) – Build agents that can reliably handle complex tasks with LangGraph, our low-level agent orchestration framework. LangGraph offers customizable architecture, long-term memory, and human-in-the-loop workflows – and is trusted in production by companies like LinkedIn, Uber, Klarna, and GitLab.\n- [Integrations](https://docs.langchain.com/oss/python/integrations/providers/overview) – List of LangChain integrations, including chat & embedding models, tools & toolkits, and more\n- [LangSmith](https://www.langchain.com/langsmith) – Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\n- [LangSmith Deployment](https://docs.langchain.com/langsmith/deployments) – Deploy and scale agents effortlessly with a purpose-built deployment platform for long-running, stateful workflows. Discover, reuse, configure, and share agents across teams – and iterate quickly with visual prototyping in [LangSmith Studio](https://docs.langchain.com/langsmith/studio).\n- [Deep Agents](https://github.com/langchain-ai/deepagents) *(new!)* – Build agents that can plan, use subagents, and leverage file systems for complex tasks\n\n## Additional resources\n\n- [API Reference](https://reference.langchain.com/python) – Detailed reference on navigating base packages and integrations for LangChain.\n- [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview) – Learn how to contribute to LangChain projects and find good first issues.\n- [Code of Conduct](https://github.com/langchain-ai/langchain/blob/master/.github/CODE_OF_CONDUCT.md) – Our community guidelines and standards for participation.\n', '{"language":"Python","stars":121388,"forks":20013,"watchers":121388,"open_issues":339,"topics":["agents","ai","ai-agents","ai-agents-framework","aiagentframework","anthropic","chatgpt","enterprise","framework","gemini","generative-ai","langchain","llm","multiagent","open-source","openai","pydantic","python","rag"],"default_branch":"master","size_kb":499322,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:langchain-ai:langchain\"","source_url":"https://github.com/langchain-ai/langchain\""},{"type":"has_code","target_id":"github:codespaces:badge.svg\"","source_url":"https://github.com/codespaces/badge.svg\""},{"type":"has_code","target_id":"github:langchain-ai:langchainjs","source_url":"https://github.com/langchain-ai/langchainjs"},{"type":"has_code","target_id":"github:langchain-ai:deepagents","source_url":"https://github.com/langchain-ai/deepagents"},{"type":"has_code","target_id":"github:langchain-ai:langchain","source_url":"https://github.com/langchain-ai/langchain"}]', NULL, 'MIT', 'approved', 65, '22dc3b11f24ba6584642a92694f4d8e3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-langchain-ai-langchain from https://github.com/langchain-ai.png
Image converted to WebP: data/images/github-langchain-ai-langchain.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-langgenius-dify', 'github--langgenius--dify', 'dify', 'langgenius', '!cover-v5-optimized <p align="center"> 📌 <a href="https://dify.ai/blog/introducing-dify-workflow-file-upload-a-demo-on-ai-podcast">Introducing Dify Workflow File Upload: Recreate Google NotebookLM Podcast</a> </p> <p align="center"> <a href="https://cloud.dify.ai">Dify Cloud</a> · <a href="https://docs.dify.ai/getting-started/install-self-hosted">Self-hosting</a> · <a href="https://docs.dify.ai">Documentation</a> · <a href="https://dify.ai/pricing">Dify edition overview</a> </p> <p align="ce...', '["agent","agentic-ai","agentic-framework","agentic-workflow","ai","automation","gemini","genai","gpt","gpt-4","llm","low-code","mcp","nextjs","no-code","openai","orchestration","python","rag","workflow","typescript"]', 'other', 120850, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/langgenius/dify","fetched_at":"2025-12-08T10:39:52.045Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '![cover-v5-optimized](./images/GitHub_README_if.png)\n\n<p align="center">\n  📌 <a href="https://dify.ai/blog/introducing-dify-workflow-file-upload-a-demo-on-ai-podcast">Introducing Dify Workflow File Upload: Recreate Google NotebookLM Podcast</a>\n</p>\n\n<p align="center">\n  <a href="https://cloud.dify.ai">Dify Cloud</a> ·\n  <a href="https://docs.dify.ai/getting-started/install-self-hosted">Self-hosting</a> ·\n  <a href="https://docs.dify.ai">Documentation</a> ·\n  <a href="https://dify.ai/pricing">Dify edition overview</a>\n</p>\n\n<p align="center">\n    <a href="https://dify.ai" target="_blank">\n        <img alt="Static Badge" src="https://img.shields.io/badge/Product-F04438"></a>\n    <a href="https://dify.ai/pricing" target="_blank">\n        <img alt="Static Badge" src="https://img.shields.io/badge/free-pricing?logo=free&color=%20%23155EEF&label=pricing&labelColor=%20%23528bff"></a>\n    <a href="https://discord.gg/FngNHpbcY7" target="_blank">\n        <img src="https://img.shields.io/discord/1082486657678311454?logo=discord&labelColor=%20%235462eb&logoColor=%20%23f5f5f5&color=%20%235462eb"\n            alt="chat on Discord"></a>\n    <a href="https://reddit.com/r/difyai" target="_blank">  \n        <img src="https://img.shields.io/reddit/subreddit-subscribers/difyai?style=plastic&logo=reddit&label=r%2Fdifyai&labelColor=white"\n            alt="join Reddit"></a>\n    <a href="https://twitter.com/intent/follow?screen_name=dify_ai" target="_blank">\n        <img src="https://img.shields.io/twitter/follow/dify_ai?logo=X&color=%20%23f5f5f5"\n            alt="follow on X(Twitter)"></a>\n    <a href="https://www.linkedin.com/company/langgenius/" target="_blank">\n        <img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff"\n            alt="follow on LinkedIn"></a>\n    <a href="https://hub.docker.com/u/langgenius" target="_blank">\n        <img alt="Docker Pulls" src="https://img.shields.io/docker/pulls/langgenius/dify-web?labelColor=%20%23FDB062&color=%20%23f79009"></a>\n    <a href="https://github.com/langgenius/dify/graphs/commit-activity" target="_blank">\n        <img alt="Commits last month" src="https://img.shields.io/github/commit-activity/m/langgenius/dify?labelColor=%20%2332b583&color=%20%2312b76a"></a>\n    <a href="https://github.com/langgenius/dify/" target="_blank">\n        <img alt="Issues closed" src="https://img.shields.io/github/issues-search?query=repo%3Alanggenius%2Fdify%20is%3Aclosed&label=issues%20closed&labelColor=%20%237d89b0&color=%20%235d6b98"></a>\n    <a href="https://github.com/langgenius/dify/discussions/" target="_blank">\n        <img alt="Discussion posts" src="https://img.shields.io/github/discussions/langgenius/dify?labelColor=%20%239b8afb&color=%20%237a5af8"></a>\n    <a href="https://insights.linuxfoundation.org/project/langgenius-dify" target="_blank">\n        <img alt="LFX Health Score" src="https://insights.linuxfoundation.org/api/badge/health-score?project=langgenius-dify"></a>\n    <a href="https://insights.linuxfoundation.org/project/langgenius-dify" target="_blank">\n        <img alt="LFX Contributors" src="https://insights.linuxfoundation.org/api/badge/contributors?project=langgenius-dify"></a>\n    <a href="https://insights.linuxfoundation.org/project/langgenius-dify" target="_blank">\n        <img alt="LFX Active Contributors" src="https://insights.linuxfoundation.org/api/badge/active-contributors?project=langgenius-dify"></a>\n</p>\n\n<p align="center">\n  <a href="./README.md"><img alt="README in English" src="https://img.shields.io/badge/English-d9d9d9"></a>\n  <a href="./docs/zh-TW/README.md"><img alt="繁體中文文件" src="https://img.shields.io/badge/繁體中文-d9d9d9"></a>\n  <a href="./docs/zh-CN/README.md"><img alt="简体中文文件" src="https://img.shields.io/badge/简体中文-d9d9d9"></a>\n  <a href="./docs/ja-JP/README.md"><img alt="日本語のREADME" src="https://img.shields.io/badge/日本語-d9d9d9"></a>\n  <a href="./docs/es-ES/README.md"><img alt="README en Español" src="https://img.shields.io/badge/Español-d9d9d9"></a>\n  <a href="./docs/fr-FR/README.md"><img alt="README en Français" src="https://img.shields.io/badge/Français-d9d9d9"></a>\n  <a href="./docs/tlh/README.md"><img alt="README tlhIngan Hol" src="https://img.shields.io/badge/Klingon-d9d9d9"></a>\n  <a href="./docs/ko-KR/README.md"><img alt="README in Korean" src="https://img.shields.io/badge/한국어-d9d9d9"></a>\n  <a href="./docs/ar-SA/README.md"><img alt="README بالعربية" src="https://img.shields.io/badge/العربية-d9d9d9"></a>\n  <a href="./docs/tr-TR/README.md"><img alt="Türkçe README" src="https://img.shields.io/badge/Türkçe-d9d9d9"></a>\n  <a href="./docs/vi-VN/README.md"><img alt="README Tiếng Việt" src="https://img.shields.io/badge/Ti%E1%BA%BFng%20Vi%E1%BB%87t-d9d9d9"></a>\n  <a href="./docs/de-DE/README.md"><img alt="README in Deutsch" src="https://img.shields.io/badge/German-d9d9d9"></a>\n  <a href="./docs/bn-BD/README.md"><img alt="README in বাংলা" src="https://img.shields.io/badge/বাংলা-d9d9d9"></a>\n</p>\n\nDify is an open-source platform for developing LLM applications. Its intuitive interface combines agentic AI workflows, RAG pipelines, agent capabilities, model management, observability features, and more—allowing you to quickly move from prototype to production.\n\n## Quick start\n\n> Before installing Dify, make sure your machine meets the following minimum system requirements:\n>\n> - CPU >= 2 Core\n> - RAM >= 4 GiB\n\n<br/>\n\nThe easiest way to start the Dify server is through [Docker Compose](docker/docker-compose.yaml). Before running Dify with the following commands, make sure that [Docker](https://docs.docker.com/get-docker/) and [Docker Compose](https://docs.docker.com/compose/install/) are installed on your machine:\n\n```bash\ncd dify\ncd docker\ncp .env.example .env\ndocker compose up -d\n```\n\nAfter running, you can access the Dify dashboard in your browser at [http://localhost/install](http://localhost/install) and start the initialization process.\n\n#### Seeking help\n\nPlease refer to our [FAQ](https://docs.dify.ai/getting-started/install-self-hosted/faqs) if you encounter problems setting up Dify. Reach out to [the community and us](#community--contact) if you are still having issues.\n\n> If you''d like to contribute to Dify or do additional development, refer to our [guide to deploying from source code](https://docs.dify.ai/getting-started/install-self-hosted/local-source-code)\n\n## Key features\n\n**1. Workflow**:\nBuild and test powerful AI workflows on a visual canvas, leveraging all the following features and beyond.\n\n**2. Comprehensive model support**:\nSeamless integration with hundreds of proprietary / open-source LLMs from dozens of inference providers and self-hosted solutions, covering GPT, Mistral, Llama3, and any OpenAI API-compatible models. A full list of supported model providers can be found [here](https://docs.dify.ai/getting-started/readme/model-providers).\n\n![providers-v5](https://github.com/langgenius/dify/assets/13230914/5a17bdbe-097a-4100-8363-40255b70f6e3)\n\n**3. Prompt IDE**:\nIntuitive interface for crafting prompts, comparing model performance, and adding additional features such as text-to-speech to a chat-based app.\n\n**4. RAG Pipeline**:\nExtensive RAG capabilities that cover everything from document ingestion to retrieval, with out-of-box support for text extraction from PDFs, PPTs, and other common document formats.\n\n**5. Agent capabilities**:\nYou can define agents based on LLM Function Calling or ReAct, and add pre-built or custom tools for the agent. Dify provides 50+ built-in tools for AI agents, such as Google Search, DALL·E, Stable Diffusion and WolframAlpha.\n\n**6. LLMOps**:\nMonitor and analyze application logs and performance over time. You could continuously improve prompts, datasets, and models based on production data and annotations.\n\n**7. Backend-as-a-Service**:\nAll of Dify''s offerings come with corresponding APIs, so you could effortlessly integrate Dify into your own business logic.\n\n## Using Dify\n\n- **Cloud <br/>**\n  We host a [Dify Cloud](https://dify.ai) service for anyone to try with zero setup. It provides all the capabilities of the self-deployed version, and includes 200 free GPT-4 calls in the sandbox plan.\n\n- **Self-hosting Dify Community Edition<br/>**\n  Quickly get Dify running in your environment with this [starter guide](#quick-start).\n  Use our [documentation](https://docs.dify.ai) for further references and more in-depth instructions.\n\n- **Dify for enterprise / organizations<br/>**\n  We provide additional enterprise-centric features. [Send us an email](mailto:business@dify.ai?subject=%5BGitHub%5DBusiness%20License%20Inquiry) to discuss your enterprise needs. <br/>\n\n  > For startups and small businesses using AWS, check out [Dify Premium on AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-t22mebxzwjhu6) and deploy it to your own AWS VPC with one click. It''s an affordable AMI offering with the option to create apps with custom logo and branding.\n\n## Staying ahead\n\nStar Dify on GitHub and be instantly notified of new releases.\n\n![star-us](https://github.com/langgenius/dify/assets/13230914/b823edc1-6388-4e25-ad45-2f6b187adbb4)\n\n## Advanced Setup\n\n### Custom configurations\n\nIf you need to customize the configuration, please refer to the comments in our [.env.example](docker/.env.example) file and update the corresponding values in your `.env` file. Additionally, you might need to make adjustments to the `docker-compose.yaml` file itself, such as changing image versions, port mappings, or volume mounts, based on your specific deployment environment and requirements. After making any changes, please re-run `docker-compose up -d`. You can find the full list of available environment variables [here](https://docs.dify.ai/getting-started/install-self-hosted/environments).\n\n#### Customizing Suggested Questions\n\nYou can now customize the "Suggested Questions After Answer" feature to better fit your use case. For example, to generate longer, more technical questions:\n\n```bash\n# In your .env file\nSUGGESTED_QUESTIONS_PROMPT=''Please help me predict the five most likely technical follow-up questions a developer would ask. Focus on implementation details, best practices, and architecture considerations. Keep each question between 40-60 characters. Output must be JSON array: ["question1","question2","question3","question4","question5"]''\nSUGGESTED_QUESTIONS_MAX_TOKENS=512\nSUGGESTED_QUESTIONS_TEMPERATURE=0.3\n```\n\nSee the [Suggested Questions Configuration Guide](docs/suggested-questions-configuration.md) for detailed examples and usage instructions.\n\n### Metrics Monitoring with Grafana\n\nImport the dashboard to Grafana, using Dify''s PostgreSQL database as data source, to monitor metrics in granularity of apps, tenants, messages, and more.\n\n- [Grafana Dashboard by @bowenliang123](https://github.com/bowenliang123/dify-grafana-dashboard)\n\n### Deployment with Kubernetes\n\nIf you''d like to configure a highly-available setup, there are community-contributed [Helm Charts](https://helm.sh/) and YAML files which allow Dify to be deployed on Kubernetes.\n\n- [Helm Chart by @LeoQuote](https://github.com/douban/charts/tree/master/charts/dify)\n- [Helm Chart by @BorisPolonsky](https://github.com/BorisPolonsky/dify-helm)\n- [Helm Chart by @magicsong](https://github.com/magicsong/ai-charts)\n- [YAML file by @Winson-030](https://github.com/Winson-030/dify-kubernetes)\n- [YAML file by @wyy-holding](https://github.com/wyy-holding/dify-k8s)\n- [🚀 NEW! YAML files (Supports Dify v1.6.0) by @Zhoneym](https://github.com/Zhoneym/DifyAI-Kubernetes)\n\n#### Using Terraform for Deployment\n\nDeploy Dify to Cloud Platform with a single click using [terraform](https://www.terraform.io/)\n\n##### Azure Global\n\n- [Azure Terraform by @nikawang](https://github.com/nikawang/dify-azure-terraform)\n\n##### Google Cloud\n\n- [Google Cloud Terraform by @sotazum](https://github.com/DeNA/dify-google-cloud-terraform)\n\n#### Using AWS CDK for Deployment\n\nDeploy Dify to AWS with [CDK](https://aws.amazon.com/cdk/)\n\n##### AWS\n\n- [AWS CDK by @KevinZhao (EKS based)](https://github.com/aws-samples/solution-for-deploying-dify-on-aws)\n- [AWS CDK by @tmokmss (ECS based)](https://github.com/aws-samples/dify-self-hosted-on-aws)\n\n#### Using Alibaba Cloud Computing Nest\n\nQuickly deploy Dify to Alibaba cloud with [Alibaba Cloud Computing Nest](https://computenest.console.aliyun.com/service/instance/create/default?type=user&ServiceName=Dify%E7%A4%BE%E5%8C%BA%E7%89%88)\n\n#### Using Alibaba Cloud Data Management\n\nOne-Click deploy Dify to Alibaba Cloud with [Alibaba Cloud Data Management](https://www.alibabacloud.com/help/en/dms/dify-in-invitational-preview/)\n\n#### Deploy to AKS with Azure Devops Pipeline\n\nOne-Click deploy Dify to AKS with [Azure Devops Pipeline Helm Chart by @LeoZhang](https://github.com/Ruiruiz30/Dify-helm-chart-AKS)\n\n## Contributing\n\nFor those who''d like to contribute code, see our [Contribution Guide](https://github.com/langgenius/dify/blob/main/CONTRIBUTING.md).\nAt the same time, please consider supporting Dify by sharing it on social media and at events and conferences.\n\n> We are looking for contributors to help translate Dify into languages other than Mandarin or English. If you are interested in helping, please see the [i18n README](https://github.com/langgenius/dify/blob/main/web/i18n-config/README.md) for more information, and leave us a comment in the `global-users` channel of our [Discord Community Server](https://discord.gg/8Tpq4AcN9c).\n\n## Community & contact\n\n- [GitHub Discussion](https://github.com/langgenius/dify/discussions). Best for: sharing feedback and asking questions.\n- [GitHub Issues](https://github.com/langgenius/dify/issues). Best for: bugs you encounter using Dify.AI, and feature proposals. See our [Contribution Guide](https://github.com/langgenius/dify/blob/main/CONTRIBUTING.md).\n- [Discord](https://discord.gg/FngNHpbcY7). Best for: sharing your applications and hanging out with the community.\n- [X(Twitter)](https://twitter.com/dify_ai). Best for: sharing your applications and hanging out with the community.\n\n**Contributors**\n\n<a href="https://github.com/langgenius/dify/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=langgenius/dify" />\n</a>\n\n## Star history\n\n[![Star History Chart](https://api.star-history.com/svg?repos=langgenius/dify&type=Date)](https://star-history.com/#langgenius/dify&Date)\n\n## Security disclosure\n\nTo protect your privacy, please avoid posting security issues on GitHub. Instead, report issues to security@dify.ai, and our team will respond with detailed answer.\n\n## License\n\nThis repository is licensed under the [Dify Open Source License](LICENSE), based on Apache 2.0 with additional conditions.\n', '{"language":"TypeScript","stars":120850,"forks":18780,"watchers":120850,"open_issues":681,"topics":["agent","agentic-ai","agentic-framework","agentic-workflow","ai","automation","gemini","genai","gpt","gpt-4","llm","low-code","mcp","nextjs","no-code","openai","orchestration","python","rag","workflow"],"default_branch":"main","size_kb":180565,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:bowenliang123:dify-grafana-dashboard","source_url":"https://github.com/bowenliang123/dify-grafana-dashboard"},{"type":"has_code","target_id":"github:douban:charts","source_url":"https://github.com/douban/charts"},{"type":"has_code","target_id":"github:BorisPolonsky:dify-helm","source_url":"https://github.com/BorisPolonsky/dify-helm"},{"type":"has_code","target_id":"github:magicsong:ai-charts","source_url":"https://github.com/magicsong/ai-charts"},{"type":"has_code","target_id":"github:Winson-030:dify-kubernetes","source_url":"https://github.com/Winson-030/dify-kubernetes"},{"type":"has_code","target_id":"github:wyy-holding:dify-k8s","source_url":"https://github.com/wyy-holding/dify-k8s"},{"type":"has_code","target_id":"github:Zhoneym:DifyAI-Kubernetes","source_url":"https://github.com/Zhoneym/DifyAI-Kubernetes"},{"type":"has_code","target_id":"github:nikawang:dify-azure-terraform","source_url":"https://github.com/nikawang/dify-azure-terraform"},{"type":"has_code","target_id":"github:DeNA:dify-google-cloud-terraform","source_url":"https://github.com/DeNA/dify-google-cloud-terraform"},{"type":"has_code","target_id":"github:aws-samples:solution-for-deploying-dify-on-aws","source_url":"https://github.com/aws-samples/solution-for-deploying-dify-on-aws"},{"type":"has_code","target_id":"github:aws-samples:dify-self-hosted-on-aws","source_url":"https://github.com/aws-samples/dify-self-hosted-on-aws"},{"type":"has_code","target_id":"github:Ruiruiz30:Dify-helm-chart-AKS","source_url":"https://github.com/Ruiruiz30/Dify-helm-chart-AKS"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"}]', NULL, 'NOASSERTION', 'approved', 80, '7e25c28120be0a2a1279d33460894fb1', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-langgenius-dify from https://github.com/langgenius.png
Image converted to WebP: data/images/github-langgenius-dify.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-open-webui-open-webui', 'github--open-webui--open-webui', 'open-webui', 'open-webui', '!GitHub stars !GitHub forks !GitHub watchers !GitHub repo size !GitHub language count !GitHub top language !GitHub last commit **Open WebUI is an extensible, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline.** It supports various LLM runners like **Ollama** and **OpenAI-compatible APIs**, with **built-in inference engine** for RAG, making it a **powerful AI deployment solution**. Passionate about open-source AI? Join our team → !Open WebUI Demo > [!...', '["ai","llm","llm-ui","llm-webui","llms","mcp","ollama","ollama-webui","open-webui","openai","openapi","rag","self-hosted","ui","webui","svelte"]', 'other', 117239, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/open-webui/open-webui","fetched_at":"2025-12-08T10:39:52.045Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Open WebUI 👋\n\n![GitHub stars](https://img.shields.io/github/stars/open-webui/open-webui?style=social)\n![GitHub forks](https://img.shields.io/github/forks/open-webui/open-webui?style=social)\n![GitHub watchers](https://img.shields.io/github/watchers/open-webui/open-webui?style=social)\n![GitHub repo size](https://img.shields.io/github/repo-size/open-webui/open-webui)\n![GitHub language count](https://img.shields.io/github/languages/count/open-webui/open-webui)\n![GitHub top language](https://img.shields.io/github/languages/top/open-webui/open-webui)\n![GitHub last commit](https://img.shields.io/github/last-commit/open-webui/open-webui?color=red)\n[![Discord](https://img.shields.io/badge/Discord-Open_WebUI-blue?logo=discord&logoColor=white)](https://discord.gg/5rJgQTnV4s)\n[![](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/tjbck)\n\n**Open WebUI is an [extensible](https://docs.openwebui.com/features/plugin/), feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline.** It supports various LLM runners like **Ollama** and **OpenAI-compatible APIs**, with **built-in inference engine** for RAG, making it a **powerful AI deployment solution**.\n\nPassionate about open-source AI? [Join our team →](https://careers.openwebui.com/)\n\n![Open WebUI Demo](./demo.gif)\n\n> [!TIP]  \n> **Looking for an [Enterprise Plan](https://docs.openwebui.com/enterprise)?** – **[Speak with Our Sales Team Today!](https://docs.openwebui.com/enterprise)**\n>\n> Get **enhanced capabilities**, including **custom theming and branding**, **Service Level Agreement (SLA) support**, **Long-Term Support (LTS) versions**, and **more!**\n\nFor more information, be sure to check out our [Open WebUI Documentation](https://docs.openwebui.com/).\n\n## Key Features of Open WebUI ⭐\n\n- 🚀 **Effortless Setup**: Install seamlessly using Docker or Kubernetes (kubectl, kustomize or helm) for a hassle-free experience with support for both `:ollama` and `:cuda` tagged images.\n\n- 🤝 **Ollama/OpenAI API Integration**: Effortlessly integrate OpenAI-compatible APIs for versatile conversations alongside Ollama models. Customize the OpenAI API URL to link with **LMStudio, GroqCloud, Mistral, OpenRouter, and more**.\n\n- 🛡️ **Granular Permissions and User Groups**: By allowing administrators to create detailed user roles and permissions, we ensure a secure user environment. This granularity not only enhances security but also allows for customized user experiences, fostering a sense of ownership and responsibility amongst users.\n\n- 📱 **Responsive Design**: Enjoy a seamless experience across Desktop PC, Laptop, and Mobile devices.\n\n- 📱 **Progressive Web App (PWA) for Mobile**: Enjoy a native app-like experience on your mobile device with our PWA, providing offline access on localhost and a seamless user interface.\n\n- ✒️🔢 **Full Markdown and LaTeX Support**: Elevate your LLM experience with comprehensive Markdown and LaTeX capabilities for enriched interaction.\n\n- 🎤📹 **Hands-Free Voice/Video Call**: Experience seamless communication with integrated hands-free voice and video call features using multiple Speech-to-Text providers (Local Whisper, OpenAI, Deepgram, Azure) and Text-to-Speech engines (Azure, ElevenLabs, OpenAI, Transformers, WebAPI), allowing for dynamic and interactive chat environments.\n\n- 🛠️ **Model Builder**: Easily create Ollama models via the Web UI. Create and add custom characters/agents, customize chat elements, and import models effortlessly through [Open WebUI Community](https://openwebui.com/) integration.\n\n- 🐍 **Native Python Function Calling Tool**: Enhance your LLMs with built-in code editor support in the tools workspace. Bring Your Own Function (BYOF) by simply adding your pure Python functions, enabling seamless integration with LLMs.\n\n- 💾 **Persistent Artifact Storage**: Built-in key-value storage API for artifacts, enabling features like journals, trackers, leaderboards, and collaborative tools with both personal and shared data scopes across sessions.\n\n- 📚 **Local RAG Integration**: Dive into the future of chat interactions with groundbreaking Retrieval Augmented Generation (RAG) support using your choice of 9 vector databases and multiple content extraction engines (Tika, Docling, Document Intelligence, Mistral OCR, External loaders). Load documents directly into chat or add files to your document library, effortlessly accessing them using the `#` command before a query.\n\n- 🔍 **Web Search for RAG**: Perform web searches using 15+ providers including `SearXNG`, `Google PSE`, `Brave Search`, `Kagi`, `Mojeek`, `Tavily`, `Perplexity`, `serpstack`, `serper`, `Serply`, `DuckDuckGo`, `SearchApi`, `SerpApi`, `Bing`, `Jina`, `Exa`, `Sougou`, `Azure AI Search`, and `Ollama Cloud`, injecting results directly into your chat experience.\n\n- 🌐 **Web Browsing Capability**: Seamlessly integrate websites into your chat experience using the `#` command followed by a URL. This feature allows you to incorporate web content directly into your conversations, enhancing the richness and depth of your interactions.\n\n- 🎨 **Image Generation & Editing Integration**: Create and edit images using multiple engines including OpenAI''s DALL-E, Gemini, ComfyUI (local), and AUTOMATIC1111 (local), with support for both generation and prompt-based editing workflows.\n\n- ⚙️ **Many Models Conversations**: Effortlessly engage with various models simultaneously, harnessing their unique strengths for optimal responses. Enhance your experience by leveraging a diverse set of models in parallel.\n\n- 🔐 **Role-Based Access Control (RBAC)**: Ensure secure access with restricted permissions; only authorized individuals can access your Ollama, and exclusive model creation/pulling rights are reserved for administrators.\n\n- 🗄️ **Flexible Database & Storage Options**: Choose from SQLite (with optional encryption), PostgreSQL, or configure cloud storage backends (S3, Google Cloud Storage, Azure Blob Storage) for scalable deployments.\n\n- 🔍 **Advanced Vector Database Support**: Select from 9 vector database options including ChromaDB, PGVector, Qdrant, Milvus, Elasticsearch, OpenSearch, Pinecone, S3Vector, and Oracle 23ai for optimal RAG performance.\n\n- 🔐 **Enterprise Authentication**: Full support for LDAP/Active Directory integration, SCIM 2.0 automated provisioning, and SSO via trusted headers alongside OAuth providers. Enterprise-grade user and group provisioning through SCIM 2.0 protocol, enabling seamless integration with identity providers like Okta, Azure AD, and Google Workspace for automated user lifecycle management.\n\n- ☁️ **Cloud-Native Integration**: Native support for Google Drive and OneDrive/SharePoint file picking, enabling seamless document import from enterprise cloud storage.\n\n- 📊 **Production Observability**: Built-in OpenTelemetry support for traces, metrics, and logs, enabling comprehensive monitoring with your existing observability stack.\n\n- ⚖️ **Horizontal Scalability**: Redis-backed session management and WebSocket support for multi-worker and multi-node deployments behind load balancers.\n\n- 🌐🌍 **Multilingual Support**: Experience Open WebUI in your preferred language with our internationalization (i18n) support. Join us in expanding our supported languages! We''re actively seeking contributors!\n\n- 🧩 **Pipelines, Open WebUI Plugin Support**: Seamlessly integrate custom logic and Python libraries into Open WebUI using [Pipelines Plugin Framework](https://github.com/open-webui/pipelines). Launch your Pipelines instance, set the OpenAI URL to the Pipelines URL, and explore endless possibilities. [Examples](https://github.com/open-webui/pipelines/tree/main/examples) include **Function Calling**, User **Rate Limiting** to control access, **Usage Monitoring** with tools like Langfuse, **Live Translation with LibreTranslate** for multilingual support, **Toxic Message Filtering** and much more.\n\n- 🌟 **Continuous Updates**: We are committed to improving Open WebUI with regular updates, fixes, and new features.\n\nWant to learn more about Open WebUI''s features? Check out our [Open WebUI documentation](https://docs.openwebui.com/features) for a comprehensive overview!\n\n---\n\nWe are incredibly grateful for the generous support of our sponsors. Their contributions help us to maintain and improve our project, ensuring we can continue to deliver quality work to our community. Thank you!\n\n## How to Install 🚀\n\n### Installation via Python pip 🐍\n\nOpen WebUI can be installed using pip, the Python package installer. Before proceeding, ensure you''re using **Python 3.11** to avoid compatibility issues.\n\n1. **Install Open WebUI**:\n   Open your terminal and run the following command to install Open WebUI:\n\n   ```bash\n   pip install open-webui\n   ```\n\n2. **Running Open WebUI**:\n   After installation, you can start Open WebUI by executing:\n\n   ```bash\n   open-webui serve\n   ```\n\nThis will start the Open WebUI server, which you can access at [http://localhost:8080](http://localhost:8080)\n\n### Quick Start with Docker 🐳\n\n> [!NOTE]  \n> Please note that for certain Docker environments, additional configurations might be needed. If you encounter any connection issues, our detailed guide on [Open WebUI Documentation](https://docs.openwebui.com/) is ready to assist you.\n\n> [!WARNING]\n> When using Docker to install Open WebUI, make sure to include the `-v open-webui:/app/backend/data` in your Docker command. This step is crucial as it ensures your database is properly mounted and prevents any loss of data.\n\n> [!TIP]  \n> If you wish to utilize Open WebUI with Ollama included or CUDA acceleration, we recommend utilizing our official images tagged with either `:cuda` or `:ollama`. To enable CUDA, you must install the [Nvidia CUDA container toolkit](https://docs.nvidia.com/dgx/nvidia-container-runtime-upgrade/) on your Linux/WSL system.\n\n### Installation with Default Configuration\n\n- **If Ollama is on your computer**, use this command:\n\n  ```bash\n  docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n  ```\n\n- **If Ollama is on a Different Server**, use this command:\n\n  To connect to Ollama on another server, change the `OLLAMA_BASE_URL` to the server''s URL:\n\n  ```bash\n  docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n  ```\n\n- **To run Open WebUI with Nvidia GPU support**, use this command:\n\n  ```bash\n  docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda\n  ```\n\n### Installation for OpenAI API Usage Only\n\n- **If you''re only using OpenAI API**, use this command:\n\n  ```bash\n  docker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n  ```\n\n### Installing Open WebUI with Bundled Ollama Support\n\nThis installation method uses a single container image that bundles Open WebUI with Ollama, allowing for a streamlined setup via a single command. Choose the appropriate command based on your hardware setup:\n\n- **With GPU Support**:\n  Utilize GPU resources by running the following command:\n\n  ```bash\n  docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\n  ```\n\n- **For CPU Only**:\n  If you''re not using a GPU, use this command instead:\n\n  ```bash\n  docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\n  ```\n\nBoth commands facilitate a built-in, hassle-free installation of both Open WebUI and Ollama, ensuring that you can get everything up and running swiftly.\n\nAfter installation, you can access Open WebUI at [http://localhost:3000](http://localhost:3000). Enjoy! 😄\n\n### Other Installation Methods\n\nWe offer various installation alternatives, including non-Docker native installation methods, Docker Compose, Kustomize, and Helm. Visit our [Open WebUI Documentation](https://docs.openwebui.com/getting-started/) or join our [Discord community](https://discord.gg/5rJgQTnV4s) for comprehensive guidance.\n\nLook at the [Local Development Guide](https://docs.openwebui.com/getting-started/advanced-topics/development) for instructions on setting up a local development environment.\n\n### Troubleshooting\n\nEncountering connection issues? Our [Open WebUI Documentation](https://docs.openwebui.com/troubleshooting/) has got you covered. For further assistance and to join our vibrant community, visit the [Open WebUI Discord](https://discord.gg/5rJgQTnV4s).\n\n#### Open WebUI: Server Connection Error\n\nIf you''re experiencing connection issues, it’s often due to the WebUI docker container not being able to reach the Ollama server at 127.0.0.1:11434 (host.docker.internal:11434) inside the container . Use the `--network=host` flag in your docker command to resolve this. Note that the port changes from 3000 to 8080, resulting in the link: `http://localhost:8080`.\n\n**Example Docker Command**:\n\n```bash\ndocker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n```\n\n### Keeping Your Docker Installation Up-to-Date\n\nIn case you want to update your local Docker installation to the latest version, you can do it with [Watchtower](https://containrrr.dev/watchtower/):\n\n```bash\ndocker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui\n```\n\nIn the last part of the command, replace `open-webui` with your container name if it is different.\n\nCheck our Updating Guide available in our [Open WebUI Documentation](https://docs.openwebui.com/getting-started/updating).\n\n### Using the Dev Branch 🌙\n\n> [!WARNING]\n> The `:dev` branch contains the latest unstable features and changes. Use it at your own risk as it may have bugs or incomplete features.\n\nIf you want to try out the latest bleeding-edge features and are okay with occasional instability, you can use the `:dev` tag like this:\n\n```bash\ndocker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev\n```\n\n### Offline Mode\n\nIf you are running Open WebUI in an offline environment, you can set the `HF_HUB_OFFLINE` environment variable to `1` to prevent attempts to download models from the internet.\n\n```bash\nexport HF_HUB_OFFLINE=1\n```\n\n## What''s Next? 🌟\n\nDiscover upcoming features on our roadmap in the [Open WebUI Documentation](https://docs.openwebui.com/roadmap/).\n\n## License 📜\n\nThis project contains code under multiple licenses. The current codebase includes components licensed under the Open WebUI License with an additional requirement to preserve the "Open WebUI" branding, as well as prior contributions under their respective original licenses. For a detailed record of license changes and the applicable terms for each section of the code, please refer to [LICENSE_HISTORY](./LICENSE_HISTORY). For complete and updated licensing details, please see the [LICENSE](./LICENSE) and [LICENSE_HISTORY](./LICENSE_HISTORY) files.\n\n## Support 💬\n\nIf you have any questions, suggestions, or need assistance, please open an issue or join our\n[Open WebUI Discord community](https://discord.gg/5rJgQTnV4s) to connect with us! 🤝\n\n## Star History\n\n<a href="https://star-history.com/#open-webui/open-webui&Date">\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date&theme=dark" />\n    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date" />\n    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date" />\n  </picture>\n</a>\n\n---\n\nCreated by [Timothy Jaeryang Baek](https://github.com/tjbck) - Let''s make Open WebUI even more amazing together! 💪\n', '{"language":"Svelte","stars":117239,"forks":16476,"watchers":117239,"open_issues":249,"topics":["ai","llm","llm-ui","llm-webui","llms","mcp","ollama","ollama-webui","open-webui","openai","openapi","rag","self-hosted","ui","webui"],"default_branch":"main","size_kb":304982,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:sponsors:tjbck","source_url":"https://github.com/sponsors/tjbck"},{"type":"has_code","target_id":"github:open-webui:pipelines","source_url":"https://github.com/open-webui/pipelines"},{"type":"has_code","target_id":"github:open-webui:pipelines","source_url":"https://github.com/open-webui/pipelines"}]', NULL, 'NOASSERTION', 'approved', 80, '5e128e481ea323ede0b7e29b44135e48', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-open-webui-open-webui from https://github.com/open-webui.png
Image converted to WebP: data/images/github-open-webui-open-webui.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-microsoft-generative-ai-for-beginners', 'github--microsoft--generative-ai-for-beginners', 'generative-ai-for-beginners', 'microsoft', '!Generative AI For Beginners <!-- CO-OP TRANSLATOR LANGUAGES TABLE START --> Arabic | Bengali | Bulgarian | Burmese (Myanmar) | Chinese (Simplified) | Chinese (Traditional, Hong Kong) | Chinese (Traditional, Macau) | Chinese (Traditional, Taiwan) | Croatian | Czech | Danish | Dutch | Estonian | Finnish | French | German | Greek | Hebrew | Hindi | Hungarian | Indonesian | Italian | Japanese | Korean | Lithuanian | Malay | Marathi | Nepali | Norwegian | Persian (Farsi) | Polish | Portuguese (Br...', '["ai","azure","chatgpt","dall-e","generative-ai","generativeai","gpt","language-model","llms","microsoft-for-beginners","openai","prompt-engineering","semantic-search","transformers","jupyter notebook"]', 'other', 102792, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/microsoft/generative-ai-for-beginners","fetched_at":"2025-12-08T10:39:52.045Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '![Generative AI For Beginners](./images/repo-thumbnailv4-fixed.png?WT.mc_id=academic-105485-koreyst)\n\n### 21 Lessons teaching everything you need to know to start building Generative AI applications\n\n[![GitHub license](https://img.shields.io/github/license/microsoft/Generative-AI-For-Beginners.svg)](https://github.com/microsoft/Generative-AI-For-Beginners/blob/master/LICENSE?WT.mc_id=academic-105485-koreyst)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/Generative-AI-For-Beginners.svg)](https://GitHub.com/microsoft/Generative-AI-For-Beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/Generative-AI-For-Beginners.svg)](https://GitHub.com/microsoft/Generative-AI-For-Beginners/issues/?WT.mc_id=academic-105485-koreyst)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/Generative-AI-For-Beginners.svg)](https://GitHub.com/microsoft/Generative-AI-For-Beginners/pulls/?WT.mc_id=academic-105485-koreyst)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst)\n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/Generative-AI-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/Generative-AI-For-Beginners/watchers/?WT.mc_id=academic-105485-koreyst)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/Generative-AI-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/Generative-AI-For-Beginners/network/?WT.mc_id=academic-105485-koreyst)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/Generative-AI-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/Generative-AI-For-Beginners/stargazers/?WT.mc_id=academic-105485-koreyst)\n\n[![Microsoft Foundry Discord](https://dcbadge.limes.pink/api/server/nTYy5BXMWG)](https://discord.gg/nTYy5BXMWG)\n\n### 🌐 Multi-Language Support\n\n#### Supported via GitHub Action (Automated & Always Up-to-Date)\n\n<!-- CO-OP TRANSLATOR LANGUAGES TABLE START -->\n[Arabic](./translations/ar/README.md) | [Bengali](./translations/bn/README.md) | [Bulgarian](./translations/bg/README.md) | [Burmese (Myanmar)](./translations/my/README.md) | [Chinese (Simplified)](./translations/zh/README.md) | [Chinese (Traditional, Hong Kong)](./translations/hk/README.md) | [Chinese (Traditional, Macau)](./translations/mo/README.md) | [Chinese (Traditional, Taiwan)](./translations/tw/README.md) | [Croatian](./translations/hr/README.md) | [Czech](./translations/cs/README.md) | [Danish](./translations/da/README.md) | [Dutch](./translations/nl/README.md) | [Estonian](./translations/et/README.md) | [Finnish](./translations/fi/README.md) | [French](./translations/fr/README.md) | [German](./translations/de/README.md) | [Greek](./translations/el/README.md) | [Hebrew](./translations/he/README.md) | [Hindi](./translations/hi/README.md) | [Hungarian](./translations/hu/README.md) | [Indonesian](./translations/id/README.md) | [Italian](./translations/it/README.md) | [Japanese](./translations/ja/README.md) | [Korean](./translations/ko/README.md) | [Lithuanian](./translations/lt/README.md) | [Malay](./translations/ms/README.md) | [Marathi](./translations/mr/README.md) | [Nepali](./translations/ne/README.md) | [Norwegian](./translations/no/README.md) | [Persian (Farsi)](./translations/fa/README.md) | [Polish](./translations/pl/README.md) | [Portuguese (Brazil)](./translations/br/README.md) | [Portuguese (Portugal)](./translations/pt/README.md) | [Punjabi (Gurmukhi)](./translations/pa/README.md) | [Romanian](./translations/ro/README.md) | [Russian](./translations/ru/README.md) | [Serbian (Cyrillic)](./translations/sr/README.md) | [Slovak](./translations/sk/README.md) | [Slovenian](./translations/sl/README.md) | [Spanish](./translations/es/README.md) | [Swahili](./translations/sw/README.md) | [Swedish](./translations/sv/README.md) | [Tagalog (Filipino)](./translations/tl/README.md) | [Tamil](./translations/ta/README.md) | [Thai](./translations/th/README.md) | [Turkish](./translations/tr/README.md) | [Ukrainian](./translations/uk/README.md) | [Urdu](./translations/ur/README.md) | [Vietnamese](./translations/vi/README.md)\n<!-- CO-OP TRANSLATOR LANGUAGES TABLE END -->\n\n# Generative AI for Beginners (Version 3) - A Course\n\nLearn the fundamentals of building Generative AI applications with our 21-lesson comprehensive course by Microsoft Cloud Advocates.\n\n## 🌱 Getting Started\n\nThis course has 21 lessons. Each lesson covers its own topic so start wherever you like!\n\nLessons are labeled either "Learn" lessons explaining a Generative AI concept or "Build" lessons that explain a concept and code examples in both **Python** and **TypeScript** when possible.\n\nFor .NET Developers checkout [Generative AI for Beginners (.NET Edition)](https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst)!\n\nEach lesson also includes a "Keep Learning" section with additional learning tools.\n\n## What You Need\n### To run the code of this course, you can use either: \n - [Azure OpenAI Service](https://aka.ms/genai-beginners/azure-open-ai?WT.mc_id=academic-105485-koreyst) - **Lessons:** "aoai-assignment"\n - [GitHub Marketplace Model Catalog](https://aka.ms/genai-beginners/gh-models?WT.mc_id=academic-105485-koreyst) - **Lessons:** "githubmodels"\n - [OpenAI API](https://aka.ms/genai-beginners/open-ai?WT.mc_id=academic-105485-koreyst) - **Lessons:** "oai-assignment" \n   \n- Basic knowledge of Python or TypeScript is helpful - \*For absolute beginners check out these [Python](https://aka.ms/genai-beginners/python?WT.mc_id=academic-105485-koreyst) and [TypeScript](https://aka.ms/genai-beginners/typescript?WT.mc_id=academic-105485-koreyst) courses\n- A GitHub account to [fork this entire repo](https://aka.ms/genai-beginners/github?WT.mc_id=academic-105485-koreyst) to your own GitHub account\n\nWe have created a **[Course Setup](./00-course-setup/README.md?WT.mc_id=academic-105485-koreyst)** lesson to help you with setting up your development environment.\n\nDon''t forget to [star (🌟) this repo](https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst) to find it easier later.\n\n## 🧠 Ready to Deploy?\n\nIf you are looking for more advanced code samples, check out our [collection of Generative AI Code Samples](https://aka.ms/genai-beg-code?WT.mc_id=academic-105485-koreyst) in both **Python** and **TypeScript**.\n\n## 🗣️ Meet Other Learners, Get Support\n\nJoin our [official Azure AI Foundry Discord server](https://aka.ms/genai-discord?WT.mc_id=academic-105485-koreyst) to meet and network with other learners taking this course and get support.\n\nAsk questions or share product feedback in our [Azure AI Foundry Developer Forum](https://aka.ms/azureaifoundry/forum) on Github.\n\n## 🚀 Building a Startup?\n\nVisit [Microsoft for Startups](https://www.microsoft.com/startups) to find out how to get started building with Azure credits today.\n\n## 🙏 Want to help?\n\nDo you have suggestions or found spelling or code errors? [Raise an issue](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) or [Create a pull request](https://github.com/microsoft/generative-ai-for-beginners/pulls?WT.mc_id=academic-105485-koreyst)\n\n## 📂 Each lesson includes:\n\n- A short video introduction to the topic\n- A written lesson located in the README\n- Python and TypeScript code samples supporting Azure OpenAI and OpenAI API\n- Links to extra resources to continue your learning\n\n## 🗃️ Lessons\n\n| #   | **Lesson Link**                                                                                                                              | **Description**                                                                                 | **Video**                                                                   | **Extra Learning**                                                             |\n| --- | -------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- | ------------------------------------------------------------------------------ |\n| 00  | [Course Setup](./00-course-setup/README.md?WT.mc_id=academic-105485-koreyst)                                                                 | **Learn:** How to Setup Your Development Environment                                            | Video Coming Soon                                                                 | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 01  | [Introduction to Generative AI and LLMs](./01-introduction-to-genai/README.md?WT.mc_id=academic-105485-koreyst)                              | **Learn:** Understanding what Generative AI is and how Large Language Models (LLMs) work.       | [Video](https://aka.ms/gen-ai-lesson-1-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 02  | [Exploring and comparing different LLMs](./02-exploring-and-comparing-different-llms/README.md?WT.mc_id=academic-105485-koreyst)             | **Learn:** How to select the right model for your use case                                      | [Video](https://aka.ms/gen-ai-lesson2-gh?WT.mc_id=academic-105485-koreyst)  | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 03  | [Using Generative AI Responsibly](./03-using-generative-ai-responsibly/README.md?WT.mc_id=academic-105485-koreyst)                           | **Learn:** How to build Generative AI Applications responsibly                                  | [Video](https://aka.ms/gen-ai-lesson3-gh?WT.mc_id=academic-105485-koreyst)  | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 04  | [Understanding Prompt Engineering Fundamentals](./04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)             | **Learn:** Hands-on Prompt Engineering Best Practices                                           | [Video](https://aka.ms/gen-ai-lesson4-gh?WT.mc_id=academic-105485-koreyst)  | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 05  | [Creating Advanced Prompts](./05-advanced-prompts/README.md?WT.mc_id=academic-105485-koreyst)                                                | **Learn:** How to apply prompt engineering techniques that improve the outcome of your prompts. | [Video](https://aka.ms/gen-ai-lesson5-gh?WT.mc_id=academic-105485-koreyst)  | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 06  | [Building Text Generation Applications](./06-text-generation-apps/README.md?WT.mc_id=academic-105485-koreyst)                                | **Build:** A text generation app using Azure OpenAI / OpenAI API                                | [Video](https://aka.ms/gen-ai-lesson6-gh?WT.mc_id=academic-105485-koreyst)  | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 07  | [Building Chat Applications](./07-building-chat-applications/README.md?WT.mc_id=academic-105485-koreyst)                                     | **Build:** Techniques for efficiently building and integrating chat applications.               | [Video](https://aka.ms/gen-ai-lessons7-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 08  | [Building Search Apps Vector Databases](./08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)                        | **Build:** A search application that uses Embeddings to search for data.                        | [Video](https://aka.ms/gen-ai-lesson8-gh?WT.mc_id=academic-105485-koreyst)  | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 09  | [Building Image Generation Applications](./09-building-image-applications/README.md?WT.mc_id=academic-105485-koreyst)                        | **Build:** An image generation application                                                       | [Video](https://aka.ms/gen-ai-lesson9-gh?WT.mc_id=academic-105485-koreyst)  | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 10  | [Building Low Code AI Applications](./10-building-low-code-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)                       | **Build:** A Generative AI application using Low Code tools                                     | [Video](https://aka.ms/gen-ai-lesson10-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 11  | [Integrating External Applications with Function Calling](./11-integrating-with-function-calling/README.md?WT.mc_id=academic-105485-koreyst) | **Build:** What is function calling and its use cases for applications                          | [Video](https://aka.ms/gen-ai-lesson11-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 12  | [Designing UX for AI Applications](./12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)                         | **Learn:** How to apply UX design principles when developing Generative AI Applications         | [Video](https://aka.ms/gen-ai-lesson12-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 13  | [Securing Your Generative AI Applications](./13-securing-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)                         | **Learn:** The threats and risks to AI systems and methods to secure these systems.             | [Video](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 14  | [The Generative AI Application Lifecycle](./14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)           | **Learn:** The tools and metrics to manage the LLM Lifecycle and LLMOps                         | [Video](https://aka.ms/gen-ai-lesson14-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 15  | [Retrieval Augmented Generation (RAG) and Vector Databases](./15-rag-and-vector-databases/README.md?WT.mc_id=academic-105485-koreyst)        | **Build:** An application using a RAG Framework to retrieve embeddings from a Vector Databases  | [Video](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 16  | [Open Source Models and Hugging Face](./16-open-source-models/README.md?WT.mc_id=academic-105485-koreyst)                                    | **Build:** An application using open source models available on Hugging Face                    | [Video](https://aka.ms/gen-ai-lesson16-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 17  | [AI Agents](./17-ai-agents/README.md?WT.mc_id=academic-105485-koreyst)                                                                       | **Build:** An application using an AI Agent Framework                                           | [Video](https://aka.ms/gen-ai-lesson17-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 18  | [Fine-Tuning LLMs](./18-fine-tuning/README.md?WT.mc_id=academic-105485-koreyst)                                                              | **Learn:** The what, why and how of fine-tuning LLMs                                            | [Video](https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 19  | [Building with SLMs](./19-slm/README.md?WT.mc_id=academic-105485-koreyst)                                                              | **Learn:** The benefits of building with Small Language Models                                            | Video Coming Soon | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 20  | [Building with Mistral Models](./20-mistral/README.md?WT.mc_id=academic-105485-koreyst)                                                              | **Learn:** The features and differences of the Mistral Family Models                                           | Video Coming Soon | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 21  | [Building with Meta Models](./21-meta/README.md?WT.mc_id=academic-105485-koreyst)                                                              | **Learn:** The features and differences of the Meta Family Models                                           | Video Coming Soon | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n\n### 🌟 Special thanks\n\nSpecial thanks to [**John Aziz**](https://www.linkedin.com/in/john0isaac/) for creating all of the GitHub Actions and workflows\n\n[**Bernhard Merkle**](https://www.linkedin.com/in/bernhard-merkle-738b73/) for making key contributions to each lesson to improve the learner and code experience. \n\n## 🎒 Other Courses\n\nOur team produces other courses! Check out:\n\n<!-- CO-OP TRANSLATOR OTHER COURSES START -->\n### Azure / Edge / MCP / Agents\n[![AZD for Beginners](https://img.shields.io/badge/AZD%20for%20Beginners-0078D4?style=for-the-badge&labelColor=E5E7EB&color=0078D4)](https://github.com/microsoft/AZD-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![Edge AI for Beginners](https://img.shields.io/badge/Edge%20AI%20for%20Beginners-00B8E4?style=for-the-badge&labelColor=E5E7EB&color=00B8E4)](https://github.com/microsoft/edgeai-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![MCP for Beginners](https://img.shields.io/badge/MCP%20for%20Beginners-009688?style=for-the-badge&labelColor=E5E7EB&color=009688)](https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![AI Agents for Beginners](https://img.shields.io/badge/AI%20Agents%20for%20Beginners-00C49A?style=for-the-badge&labelColor=E5E7EB&color=00C49A)](https://github.com/microsoft/ai-agents-for-beginners?WT.mc_id=academic-105485-koreyst)\n\n---\n \n### Generative AI Series\n[![Generative AI for Beginners](https://img.shields.io/badge/Generative%20AI%20for%20Beginners-8B5CF6?style=for-the-badge&labelColor=E5E7EB&color=8B5CF6)](https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![Generative AI (.NET)](https://img.shields.io/badge/Generative%20AI%20(.NET)-9333EA?style=for-the-badge&labelColor=E5E7EB&color=9333EA)](https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst)\n[![Generative AI (Java)](https://img.shields.io/badge/Generative%20AI%20(Java)-C084FC?style=for-the-badge&labelColor=E5E7EB&color=C084FC)](https://github.com/microsoft/generative-ai-for-beginners-java?WT.mc_id=academic-105485-koreyst)\n[![Generative AI (JavaScript)](https://img.shields.io/badge/Generative%20AI%20(JavaScript)-E879F9?style=for-the-badge&labelColor=E5E7EB&color=E879F9)](https://github.com/microsoft/generative-ai-with-javascript?WT.mc_id=academic-105485-koreyst)\n\n---\n \n### Core Learning\n[![ML for Beginners](https://img.shields.io/badge/ML%20for%20Beginners-22C55E?style=for-the-badge&labelColor=E5E7EB&color=22C55E)](https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst)\n[![Data Science for Beginners](https://img.shields.io/badge/Data%20Science%20for%20Beginners-84CC16?style=for-the-badge&labelColor=E5E7EB&color=84CC16)](https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst)\n[![AI for Beginners](https://img.shields.io/badge/AI%20for%20Beginners-A3E635?style=for-the-badge&labelColor=E5E7EB&color=A3E635)](https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst)\n[![Cybersecurity for Beginners](https://img.shields.io/badge/Cybersecurity%20for%20Beginners-F97316?style=for-the-badge&labelColor=E5E7EB&color=F97316)](https://github.com/microsoft/Security-101?WT.mc_id=academic-96948-sayoung)\n[![Web Dev for Beginners](https://img.shields.io/badge/Web%20Dev%20for%20Beginners-EC4899?style=for-the-badge&labelColor=E5E7EB&color=EC4899)](https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst)\n[![IoT for Beginners](https://img.shields.io/badge/IoT%20for%20Beginners-14B8A6?style=for-the-badge&labelColor=E5E7EB&color=14B8A6)](https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst)\n[![XR Development for Beginners](https://img.shields.io/badge/XR%20Development%20for%20Beginners-38BDF8?style=for-the-badge&labelColor=E5E7EB&color=38BDF8)](https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst)\n\n---\n \n### Copilot Series\n[![Copilot for AI Paired Programming](https://img.shields.io/badge/Copilot%20for%20AI%20Paired%20Programming-FACC15?style=for-the-badge&labelColor=E5E7EB&color=FACC15)](https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst)\n[![Copilot for C#/.NET](https://img.shields.io/badge/Copilot%20for%20C%23/.NET-FBBF24?style=for-the-badge&labelColor=E5E7EB&color=FBBF24)](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst)\n[![Copilot Adventure](https://img.shields.io/badge/Copilot%20Adventure-FDE68A?style=for-the-badge&labelColor=E5E7EB&color=FDE68A)](https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst)\n<!-- CO-OP TRANSLATOR OTHER COURSES END -->\n\n## Getting Help\n\nIf you get stuck or have any questions about building AI apps. Join fellow learners and experienced developers in discussions about MCP. It''s a supportive community where questions are welcome and knowledge is shared freely.\n\n[![Microsoft Foundry Discord](https://dcbadge.limes.pink/api/server/nTYy5BXMWG)](https://discord.gg/nTYy5BXMWG)\n\nIf you have product feedback or errors while building visit:\n\n[![Microsoft Foundry Developer Forum](https://img.shields.io/badge/GitHub-Microsoft_Foundry_Developer_Forum-blue?style=for-the-badge&logo=github&color=000000&logoColor=fff)](https://aka.ms/foundry/forum)\n', '{"language":"Jupyter Notebook","stars":102792,"forks":54721,"watchers":102792,"open_issues":11,"topics":["ai","azure","chatgpt","dall-e","generative-ai","generativeai","gpt","language-model","llms","microsoft-for-beginners","openai","prompt-engineering","semantic-search","transformers"],"default_branch":"main","size_kb":5933211,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:microsoft:Generative-AI-For-Beginners","source_url":"https://github.com/microsoft/Generative-AI-For-Beginners"},{"type":"has_code","target_id":"github:microsoft:Generative-AI-for-beginners-dotnet","source_url":"https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:generative-ai-for-beginners","source_url":"https://github.com/microsoft/generative-ai-for-beginners"},{"type":"has_code","target_id":"github:microsoft:generative-ai-for-beginners","source_url":"https://github.com/microsoft/generative-ai-for-beginners"},{"type":"has_code","target_id":"github:microsoft:AZD-for-beginners","source_url":"https://github.com/microsoft/AZD-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:edgeai-for-beginners","source_url":"https://github.com/microsoft/edgeai-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:mcp-for-beginners","source_url":"https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:ai-agents-for-beginners","source_url":"https://github.com/microsoft/ai-agents-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:generative-ai-for-beginners","source_url":"https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:Generative-AI-for-beginners-dotnet","source_url":"https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:generative-ai-for-beginners-java","source_url":"https://github.com/microsoft/generative-ai-for-beginners-java?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:generative-ai-with-javascript","source_url":"https://github.com/microsoft/generative-ai-with-javascript?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:Security-101","source_url":"https://github.com/microsoft/Security-101?WT.mc_id=academic-96948-sayoung"},{"type":"has_code","target_id":"github:microsoft:xr-development-for-beginners","source_url":"https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:mastering-github-copilot-for-dotnet-csharp-developers","source_url":"https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:CopilotAdventures","source_url":"https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst"}]', NULL, 'MIT', 'approved', 80, '7489091b5b6b66c96a81fbd4503b0ac3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-microsoft-generative-ai-for-beginners from https://github.com/microsoft.png
Image converted to WebP: data/images/github-microsoft-generative-ai-for-beginners.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-x1xhlol-system-prompts-and-models-of-ai-tools', 'github--x1xhlol--system-prompts-and-models-of-ai-tools', 'system-prompts-and-models-of-ai-tools', 'x1xhlol', '--- <p align="center"> <sub>Special thanks to</sub> </p> <p align="center"> <a href="https://latitude.so/developers?utm_source=github&utm_medium=readme&utm_campaign=prompt_repo_sponsorship" target="_blank"> <img src="assets/Latitude_logo.png" alt="Latitude Logo" width="700"/> </a> </p> <div align="center" markdown="1"> <a href="https://latitude.so/developers?utm_source=github&utm_medium=readme&utm_campaign=prompt_repo_sponsorship" target="_blank">Open Source AI Engineering Platform</a><br> </...', '["ai","bolt","cluely","copilot","cursor","cursorai","devin","github-copilot","lovable","open-source","perplexity","replit","system-prompts","trae","trae-ai","trae-ide","v0","vscode","windsurf","windsurf-ai"]', 'other', 99875, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools","fetched_at":"2025-12-08T10:39:52.045Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '# **System Prompts and Models of AI Tools**  \n---\n<p align="center">\n  <sub>Special thanks to</sub>  \n</p> \n\n<p align="center">\n  <a href="https://latitude.so/developers?utm_source=github&utm_medium=readme&utm_campaign=prompt_repo_sponsorship" target="_blank">\n    <img src="assets/Latitude_logo.png" alt="Latitude Logo" width="700"/>\n  </a>\n</p>\n\n<div align="center" markdown="1">\n\n### <a href="https://latitude.so/developers?utm_source=github&utm_medium=readme&utm_campaign=prompt_repo_sponsorship" target="_blank">The tools you need for building reliable Agents and Prompts</a>  \n<a href="https://latitude.so/developers?utm_source=github&utm_medium=readme&utm_campaign=prompt_repo_sponsorship" target="_blank">Open Source AI Engineering Platform</a><br>\n\n</div>\n\n---\n\n<a href="https://discord.gg/NwzrWErdMU" target="_blank">\n  <img src="https://img.shields.io/discord/1402660735833604126?label=LeaksLab%20Discord&logo=discord&style=for-the-badge" alt="LeaksLab Discord" />\n</a>\n\n\n<a href="https://trendshift.io/repositories/14084" target="_blank"><img src="https://trendshift.io/api/badge/repositories/14084" alt="x1xhlol%2Fsystem-prompts-and-models-of-ai-tools | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n\n📜 Over **30,000+ lines** of insights into their structure and functionality.  \n\n\n[![Build Status](https://app.cloudback.it/badge/x1xhlol/system-prompts-and-models-of-ai-tools)](https://cloudback.it)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/x1xhlol/system-prompts-and-models-of-ai-tools)\n\n---\n\n## ❤️ Support the Project\n\nIf you find this collection valuable and appreciate the effort involved in obtaining and sharing these insights, please consider supporting the project.\n\nYou can show your support via:\n\n- **Cryptocurrency:**  \n  - **BTC:** `bc1q7zldmzjwspnaa48udvelwe6k3fef7xrrhg5625`  \n  - **LTC:** `LRWgqwEYDwqau1WeiTs6Mjg85NJ7m3fsdQ`  \n  - **ETH:** `0x3f844B2cc3c4b7242964373fB0A41C4fdffB192A`\n- **Patreon:** https://patreon.com/lucknite\n- **Ko-fi:** https://ko-fi.com/lucknite\n\n🙏 Thank you for your support!\n\n---\n\n# Sponsors\n\nSponsor the most comprehensive repository of AI system prompts and reach thousands of developers.\n\n[Get Started](mailto:lucknitelol@proton.me)\n\n---\n\n## 🛠 Roadmap & Feedback\n\n> Open an issue.\n\n> **Latest Update:** 02/12/2025\n\n---\n\n## 🔗 Connect With Me\n\n- **X:** [NotLucknite](https://x.com/NotLucknite)\n- **Discord**: `x1xhlol`\n- **Email**: `lucknitelol@pm.me`\n\n---\n\n## 🛡️ Security Notice for AI Startups\n\n> ⚠️ **Warning:** If you''re an AI startup, make sure your data is secure. Exposed prompts or AI models can easily become a target for hackers.\n\n> 🔐 **Important:** Interested in securing your AI systems?  \n> Check out **[ZeroLeaks](https://zeroleaks.io/)**, a service designed to help startups **identify and secure** leaks in system instructions, internal tools, and model configurations. **Get a free AI security audit** to ensure your AI is protected from vulnerabilities.\n\n---\n\n## 📊 Star History\n\n<a href="https://www.star-history.com/#x1xhlol/system-prompts-and-models-of-ai-tools&Date">\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=x1xhlol/system-prompts-and-models-of-ai-tools&type=Date&theme=dark" />\n    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=x1xhlol/system-prompts-and-models-of-ai-tools&type=Date" />\n    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=x1xhlol/system-prompts-and-models-of-ai-tools&type=Date" />\n  </picture>\n</a>\n\n⭐ **Drop a star if you find this useful!**\n', '{"language":null,"stars":99875,"forks":26730,"watchers":99875,"open_issues":101,"topics":["ai","bolt","cluely","copilot","cursor","cursorai","devin","github-copilot","lovable","open-source","perplexity","replit","system-prompts","trae","trae-ai","trae-ide","v0","vscode","windsurf","windsurf-ai"],"default_branch":"main","size_kb":1244,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[]', NULL, 'GPL-3.0', 'approved', 65, '2466dab53967703ae52eb6991df07de9', NULL, NULL, CURRENT_TIMESTAMP);
