/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-flan-t5-xl', 'huggingface--google--flan-t5-xl', 'flan-t5-xl', 'google', '--- language: - en - fr - ro - de - multilingual widget: - text: "Translate to German: My name is Arthur" example_title: "Translation" - text: "Please answer to the following question. Who is going to be the next Ballon d''or?" example_title: "Question Answering" - text: "Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering." example_title: "Logical reasoning" - text: "Please answer the following question. What is the boiling point of Nitrogen?...', '["transformers","pytorch","tf","jax","safetensors","t5","text2text-generation","en","fr","ro","de","multilingual","dataset:svakulenk0/qrecc","dataset:taskmaster2","dataset:djaym7/wiki_dialog","dataset:deepmind/code_contests","dataset:lambada","dataset:gsm8k","dataset:aqua_rat","dataset:esnli","dataset:quasc","dataset:qed","arxiv:2210.11416","arxiv:1910.09700","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'other', 525, 201959, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/flan-t5-xl","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: \n- en\n- fr\n- ro\n- de\n- multilingual\n\nwidget:\n- text: "Translate to German:  My name is Arthur"\n  example_title: "Translation"\n- text: "Please answer to the following question. Who is going to be the next Ballon d''or?"\n  example_title: "Question Answering"\n- text: "Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering."\n  example_title: "Logical reasoning"\n- text: "Please answer the following question. What is the boiling point of Nitrogen?"\n  example_title: "Scientific knowledge"\n- text: "Answer the following yes/no question. Can you write a whole Haiku in a single tweet?"\n  example_title: "Yes/no question"\n- text: "Answer the following yes/no question by reasoning step-by-step. Can you write a whole Haiku in a single tweet?"\n  example_title: "Reasoning task"\n- text: "Q: ( False or not False or False ) is? A: Let''s think step by step"\n  example_title: "Boolean Expressions"\n- text: "The square root of x is the cube root of y. What is y to the power of 2, if x = 4?"\n  example_title: "Math reasoning"\n- text: "Premise:  At my age you will probably have learnt one lesson. Hypothesis:  It''s not certain how many lessons you''ll learn by your thirties. Does the premise entail the hypothesis?"\n  example_title: "Premise and hypothesis"\n\ntags:\n- text2text-generation\n\ndatasets:\n- svakulenk0/qrecc\n- taskmaster2\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- lambada\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed\n\n\nlicense: apache-2.0\n---\n\n# Model Card for FLAN-T5 XL\n\n<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/flan2_architecture.jpg"\nalt="drawing" width="600"/>\n\n#  Table of Contents\n\n0. [TL;DR](#TL;DR)\n1. [Model Details](#model-details)\n2. [Usage](#usage)\n3. [Uses](#uses)\n4. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n5. [Training Details](#training-details)\n6. [Evaluation](#evaluation)\n7. [Environmental Impact](#environmental-impact)\n8. [Citation](#citation)\n\n# TL;DR\n\nIf you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. \nAs mentioned in the first few lines of the abstract : \n>  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.\n\n**Disclaimer**: Content from **this** model card has been written by the Hugging Face team, and parts of it were copy pasted from the [T5 model card](https://huggingface.co/t5-large).\n\n# Model Details\n\n## Model Description\n\n\n- **Model type:** Language model\n- **Language(s) (NLP):** English, Spanish, Japanese, Persian, Hindi, French, Chinese, Bengali, Gujarati, German, Telugu, Italian, Arabic, Polish, Tamil, Marathi, Malayalam, Oriya, Panjabi, Portuguese, Urdu, Galician, Hebrew, Korean, Catalan, Thai, Dutch, Indonesian, Vietnamese, Bulgarian, Filipino, Central Khmer, Lao, Turkish, Russian, Croatian, Swedish, Yoruba, Kurdish, Burmese, Malay, Czech, Finnish, Somali, Tagalog, Swahili, Sinhala, Kannada, Zhuang, Igbo, Xhosa, Romanian, Haitian, Estonian, Slovak, Lithuanian, Greek, Nepali, Assamese, Norwegian\n- **License:** Apache 2.0\n- **Related Models:** [All FLAN-T5 Checkpoints](https://huggingface.co/models?search=flan-t5)\n- **Original Checkpoints:** [All Original FLAN-T5 Checkpoints](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)\n- **Resources for more information:**\n  - [Research paper](https://arxiv.org/pdf/2210.11416.pdf)\n  - [GitHub Repo](https://github.com/google-research/t5x)\n  - [Hugging Face FLAN-T5 Docs (Similar to T5) ](https://huggingface.co/docs/transformers/model_doc/t5)\n\n# Usage\n\nFind below some example scripts on how to use the model in `transformers`:\n\n## Using the Pytorch model\n\n### Running the model on a CPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-xl")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-xl")\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-xl")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-xl", device_map="auto")\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU using different precisions\n\n#### FP16\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-xl")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-xl", device_map="auto", torch_dtype=torch.float16)\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n#### INT8\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-xl")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-xl", device_map="auto", load_in_8bit=True)\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n# Uses\n\n## Direct Use and Downstream Use\n\nThe authors write in [the original paper''s model card](https://arxiv.org/pdf/2210.11416.pdf) that: \n\n> The primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models\n\nSee the [research paper](https://arxiv.org/pdf/2210.11416.pdf) for further details.\n\n## Out-of-Scope Use\n\nMore information needed.\n\n# Bias, Risks, and Limitations\n\nThe information below in this section are copied from the model''s [official model card](https://arxiv.org/pdf/2210.11416.pdf):\n\n> Language models, including Flan-T5, can potentially be used for language generation in a harmful way, according to Rae et al. (2021). Flan-T5 should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.\n\n## Ethical considerations and risks\n\n> Flan-T5 is fine-tuned on a large corpus of text data that was not filtered for explicit content or assessed for existing biases. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\n\n## Known Limitations\n\n> Flan-T5 has not been tested in real world applications.\n\n## Sensitive Use:\n\n> Flan-T5 should not be applied for any unacceptable use cases, e.g., generation of abusive speech.\n\n# Training Details\n\n## Training Data\n\nThe model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2):\n\n![table.png](https://s3.amazonaws.com/moonup/production/uploads/1666363265279-62441d1d9fdefb55a0b7d12c.png)\n\n\n## Training Procedure\n\nAccording to the model card from the [original paper](https://arxiv.org/pdf/2210.11416.pdf):\n\n> These models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size.\n\nThe model has been trained on TPU v3 or TPU v4 pods, using [`t5x`](https://github.com/google-research/t5x) codebase together with [`jax`](https://github.com/google/jax).\n\n\n# Evaluation\n\n## Testing Data, Factors & Metrics\n\nThe authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation:\n![image.png](https://s3.amazonaws.com/moonup/production/uploads/1668072995230-62441d1d9fdefb55a0b7d12c.png)\nFor full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf).\n\n## Results \n\nFor full results for FLAN-T5-XL, see the [research paper](https://arxiv.org/pdf/2210.11416.pdf), Table 3.\n\n# Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** Google Cloud TPU Pods - TPU v3 or TPU v4  | Number of chips ≥ 4.\n- **Hours used:** More information needed\n- **Cloud Provider:** GCP\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@misc{https://doi.org/10.48550/arxiv.2210.11416,\n  doi = {10.48550/ARXIV.2210.11416},\n  \n  url = {https://arxiv.org/abs/2210.11416},\n  \n  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n  \n  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {Scaling Instruction-Finetuned Language Models},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```\n\n', '{"pipeline_tag":null,"library_name":"transformers","framework":"transformers","params":2849757184,"storage_bytes":45598305491,"files_count":20,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["T5ForConditionalGeneration"],"model_type":"t5","tokenizer_config":{"eos_token":"</s>","pad_token":"<pad>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google:jax","source_url":"https://github.com/google/jax"},{"type":"based_on_paper","target_id":"arxiv:2210.11416","source_url":"https://arxiv.org/abs/2210.11416"},{"type":"based_on_paper","target_id":"arxiv:1910.09700","source_url":"https://arxiv.org/abs/1910.09700"}]', NULL, 'Apache-2.0', 'approved', 77.2, '4b5e3f27d0d6619c5de236fc6d4f0624', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mistralai-Mistral-Small-3.2-24B-Instruct-2506', 'huggingface--mistralai--mistral-small-3.2-24b-instruct-2506', 'Mistral-Small-3.2-24B-Instruct-2506', 'mistralai', '--- library_name: vllm language: - en - fr - de - es - pt - it - ja - ko - ru - zh - ar - fa - id - ms - ne - pl - ro - sr - sv - tr - uk - vi - hi - bn license: apache-2.0 inference: false base_model: - mistralai/Mistral-Small-3.1-24B-Base-2503 extra_gated_description: >- If you want to learn more about how we process your personal data, please read our <a href="https://mistral.ai/terms/">Privacy Policy</a>. tags: - mistral-common --- Mistral-Small-3.2-24B-Instruct-2506 is a minor update of ...', '["vllm","safetensors","mistral3","mistral-common","en","fr","de","es","pt","it","ja","ko","ru","zh","ar","fa","id","ms","ne","pl","ro","sr","sv","tr","uk","vi","hi","bn","license:apache-2.0","region:us"]', 'other', 524, 140515, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: vllm\nlanguage:\n- en\n- fr\n- de\n- es\n- pt\n- it\n- ja\n- ko\n- ru\n- zh\n- ar\n- fa\n- id\n- ms\n- ne\n- pl\n- ro\n- sr\n- sv\n- tr\n- uk\n- vi\n- hi\n- bn\nlicense: apache-2.0\ninference: false\nbase_model:\n- mistralai/Mistral-Small-3.1-24B-Base-2503\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href="https://mistral.ai/terms/">Privacy Policy</a>.\ntags:\n- mistral-common\n---\n\n# Mistral-Small-3.2-24B-Instruct-2506\n\nMistral-Small-3.2-24B-Instruct-2506 is a minor update of [Mistral-Small-3.1-24B-Instruct-2503](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503).\n\nSmall-3.2 improves in the following categories:\n- **Instruction following**: Small-3.2 is better at following precise instructions\n- **Repetition errors**: Small-3.2 produces less infinite generations or repetitive answers\n- **Function calling**: Small-3.2''s function calling template is more robust (see [here](https://github.com/mistralai/mistral-common/blob/535b4d0a0fc94674ea17db6cf8dc2079b81cbcfa/src/mistral_common/tokens/tokenizers/instruct.py#L778) and [examples](#function-calling))\n\nIn all other categories Small-3.2 should match or slightly improve compared to [Mistral-Small-3.1-24B-Instruct-2503](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503).\n\n## Key Features\n- same as [Mistral-Small-3.1-24B-Instruct-2503](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503#key-features)\n\n## Benchmark Results\n\nWe compare Mistral-Small-3.2-24B to [Mistral-Small-3.1-24B-Instruct-2503](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503).\nFor more comparison against other models of similar size, please check [Mistral-Small-3.1''s Benchmarks''](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503#benchmark-results)\n\n### Text \n\n#### Instruction Following / Chat / Tone\n\n| Model | Wildbench v2 | Arena Hard v2 | IF (Internal; accuracy) |\n|-------|---------------|---------------|------------------------|\n| Small 3.1 24B Instruct | 55.6% | 19.56% | 82.75% |\n| **Small 3.2 24B Instruct** | **65.33%** | **43.1%** | **84.78%** |\n\n#### Infinite Generations\n\nSmall 3.2 reduces infinite generations by 2x on challenging, long and repetitive prompts.\n\n| Model | Infinite Generations (Internal; Lower is better) |\n|-------|-------|\n| Small 3.1 24B Instruct | 2.11% |\n| **Small 3.2 24B Instruct** | **1.29%** |\n\n#### STEM\n\n| Model                          | MMLU      | MMLU Pro (5-shot CoT) | MATH                   | GPQA Main (5-shot CoT) | GPQA Diamond (5-shot CoT )| MBPP Plus - Pass@5 | HumanEval Plus - Pass@5 | SimpleQA (TotalAcc)|\n|--------------------------------|-----------|-----------------------|------------------------|------------------------|---------------------------|--------------------|-------------------------|--------------------|\n| Small 3.1 24B Instruct         | 80.62%    | 66.76%                | 69.30%                 | 44.42%                 | 45.96%                    | 74.63%             | 88.99%                  | 10.43%             |\n| **Small 3.2 24B Instruct**     | 80.50%    | **69.06%**            | 69.42%                 | 44.22%                 | 46.13%                    | **78.33%**         | **92.90%**              | **12.10%**         |\n\n### Vision\n\n| Model                          | MMMU       | Mathvista | ChartQA   | DocVQA    | AI2D      |\n|--------------------------------|------------|-----------|-----------|-----------|-----------|\n| Small 3.1 24B Instruct         | **64.00%** | **68.91%**| 86.24%    | 94.08%    | 93.72%  | \n| **Small 3.2 24B Instruct**     | 62.50%     | 67.09%    | **87.4%** | 94.86%    | 92.91%  | \n\n\n## Usage\n\nThe model can be used with the following frameworks;\n- [`vllm (recommended)`](https://github.com/vllm-project/vllm): See [here](#vllm-recommended)\n- [`transformers`](https://github.com/huggingface/transformers): See [here](#transformers)\n\n**Note 1**: We recommend using a relatively low temperature, such as `temperature=0.15`.\n\n**Note 2**: Make sure to add a system prompt to the model to best tailor it to your needs. If you want to use the model as a general assistant, we recommend to use the one provided in the [SYSTEM_PROMPT.txt](https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506/blob/main/SYSTEM_PROMPT.txt) file.\n\n### vLLM (recommended)\n\nWe recommend using this model with [vLLM](https://github.com/vllm-project/vllm).\n\n#### Installation\n\nMake sure to install [`vLLM >= 0.9.1`](https://github.com/vllm-project/vllm/releases/tag/v0.9.1):\n\n```\npip install vllm --upgrade\n```\n\nDoing so should automatically install [`mistral_common >= 1.6.2`](https://github.com/mistralai/mistral-common/releases/tag/v1.6.2).\n\nTo check:\n```\npython -c "import mistral_common; print(mistral_common.__version__)"\n```\n\nYou can also make use of a ready-to-go [docker image](https://github.com/vllm-project/vllm/blob/main/Dockerfile) or on the [docker hub](https://hub.docker.com/layers/vllm/vllm-openai/latest/images/sha256-de9032a92ffea7b5c007dad80b38fd44aac11eddc31c435f8e52f3b7404bbf39).\n\n#### Serve\n\nWe recommend that you use Mistral-Small-3.2-24B-Instruct-2506 in a server/client setting. \n\n1. Spin up a server:\n\n```\nvllm serve mistralai/Mistral-Small-3.2-24B-Instruct-2506 \\n  --tokenizer_mode mistral --config_format mistral \\n  --load_format mistral --tool-call-parser mistral \\n  --enable-auto-tool-choice --limit-mm-per-prompt ''{"image":10}'' \\n  --tensor-parallel-size 2\n```\n\n**Note:** Running Mistral-Small-3.2-24B-Instruct-2506 on GPU requires ~55 GB of GPU RAM in bf16 or fp16. \n\n\n2. To ping the client you can use a simple Python snippet. See the following examples.\n\n\n#### Vision reasoning\n\nLeverage the vision capabilities of Mistral-Small-3.2-24B-Instruct-2506 to make the best choice given a scenario, go catch them all !\n\n<details>\n  <summary>Python snippet</summary>\n\n```py\nfrom datetime import datetime, timedelta\n\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n\n# Modify OpenAI''s API key and API base to use vLLM''s API server.\nopenai_api_key = "EMPTY"\nopenai_api_base = "http://localhost:8000/v1"\n\nTEMP = 0.15\nMAX_TOK = 131072\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, "r") as file:\n        system_prompt = file.read()\n    today = datetime.today().strftime("%Y-%m-%d")\n    yesterday = (datetime.today() - timedelta(days=1)).strftime("%Y-%m-%d")\n    model_name = repo_id.split("/")[-1]\n    return system_prompt.format(name=model_name, today=today, yesterday=yesterday)\n\n\nmodel_id = "mistralai/Mistral-Small-3.2-24B-Instruct-2506"\nSYSTEM_PROMPT = load_system_prompt(model_id, "SYSTEM_PROMPT.txt")\nimage_url = "https://static.wikia.nocookie.net/essentialsdocs/images/7/70/Battle.png/revision/latest?cb=20220523172438"\n\nmessages = [\n    {"role": "system", "content": SYSTEM_PROMPT},\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "text",\n                "text": "What action do you think I should take in this situation? List all the possible actions and explain why you think they are good or bad.",\n            },\n            {"type": "image_url", "image_url": {"url": image_url}},\n        ],\n    },\n]\n\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=TEMP,\n    max_tokens=MAX_TOK,\n)\n\nprint(response.choices[0].message.content)\n# In this situation, you are playing a Pokémon game where your Pikachu (Level 42) is facing a wild Pidgey (Level 17). Here are the possible actions you can take and an analysis of each:\n\n# 1. **FIGHT**:\n#    - **Pros**: Pikachu is significantly higher level than the wild Pidgey, which suggests that it should be able to defeat Pidgey easily. This could be a good opportunity to gain experience points and possibly items or money.\n#    - **Cons**: There is always a small risk of Pikachu fainting, especially if Pidgey has a powerful move or a status effect that could hinder Pikachu. However, given the large level difference, this risk is minimal.\n\n# 2. **BAG**:\n#    - **Pros**: You might have items in your bag that could help in this battle, such as Potions, Poké Balls, or Berries. Using an item could help you capture the Pidgey or heal your Pikachu if needed.\n#    - **Cons**: Using items might not be necessary given the level difference. It could be more efficient to just fight and defeat the Pidgey quickly.\n\n# 3. **POKÉMON**:\n#    - **Pros**: You might have another Pokémon in your party that is better suited for this battle or that you want to gain experience. Switching Pokémon could also be a strategic move if you want to train a lower-level Pokémon.\n#    - **Cons**: Switching Pokémon might not be necessary since Pikachu is at a significant advantage. It could also waste time and potentially give Pidgey a turn to attack.\n\n# 4. **RUN**:\n#    - **Pros**: Running away could save time and conserve your Pokémon''s health and resources. If you are in a hurry or do not need the experience or items, running away is a safe option.\n#    - **Cons**: Running away means you miss out on the experience points and potential items or money that you could gain from defeating the Pidgey. It also means you do not get the chance to capture the Pidgey if you wanted to.\n\n# ### Recommendation:\n# Given the significant level advantage, the best action is likely to **FIGHT**. This will allow you to quickly defeat the Pidgey, gain experience points, and potentially earn items or money. If you are concerned about Pikachu''s health, you could use an item from your **BAG** to heal it before or during the battle. Running away or switching Pokémon does not seem necessary in this situation.\n```\n</details>\n\n#### Function calling\n\nMistral-Small-3.2-24B-Instruct-2506 is excellent at function / tool calling tasks via vLLM. *E.g.:*\n\n<details>\n  <summary>Python snippet - easy</summary>\n\n```py\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n\n# Modify OpenAI''s API key and API base to use vLLM''s API server.\nopenai_api_key = "EMPTY"\nopenai_api_base = "http://localhost:8000/v1"\n\nTEMP = 0.15\nMAX_TOK = 131072\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, "r") as file:\n        system_prompt = file.read()\n    return system_prompt\n\nmodel_id = "mistralai/Mistral-Small-3.2-24B-Instruct-2506"\nSYSTEM_PROMPT = load_system_prompt(model_id, "SYSTEM_PROMPT.txt")\n\nimage_url = "https://huggingface.co/datasets/patrickvonplaten/random_img/resolve/main/europe.png"\n\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "get_current_population",\n            "description": "Get the up-to-date population of a given country.",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "country": {\n                        "type": "string",\n                        "description": "The country to find the population of.",\n                    },\n                    "unit": {\n                        "type": "string",\n                        "description": "The unit for the population.",\n                        "enum": ["millions", "thousands"],\n                    },\n                },\n                "required": ["country", "unit"],\n            },\n        },\n    },\n    {\n        "type": "function",\n        "function": {\n            "name": "rewrite",\n            "description": "Rewrite a given text for improved clarity",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "text": {\n                        "type": "string",\n                        "description": "The input text to rewrite",\n                    }\n                },\n            },\n        },\n    },\n]\n\nmessages = [\n    {"role": "system", "content": SYSTEM_PROMPT},\n    {\n        "role": "user",\n        "content": "Could you please make the below article more concise?\n\nOpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.",\n    },\n    {\n        "role": "assistant",\n        "content": "",\n        "tool_calls": [\n            {\n                "id": "bbc5b7ede",\n                "type": "function",\n                "function": {\n                    "name": "rewrite",\n                    "arguments": ''{"text": "OpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership."}'',\n                },\n            }\n        ],\n    },\n    {\n        "role": "tool",\n        "content": ''{"action":"rewrite","outcome":"OpenAI is a FOR-profit company."}'',\n        "tool_call_id": "bbc5b7ede",\n        "name": "rewrite",\n    },\n    {\n        "role": "assistant",\n        "content": "---\n\nOpenAI is a FOR-profit company.",\n    },\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "text",\n                "text": "Can you tell me what is the biggest country depicted on the map?",\n            },\n            {\n                "type": "image_url",\n                "image_url": {\n                    "url": image_url,\n                },\n            },\n        ],\n    }\n]\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=TEMP,\n    max_tokens=MAX_TOK,\n    tools=tools,\n    tool_choice="auto",\n)\n\nassistant_message = response.choices[0].message.content\nprint(assistant_message)\n# The biggest country depicted on the map is Russia.\n\nmessages.extend([\n    {"role": "assistant", "content": assistant_message},\n    {"role": "user", "content": "What is the population of that country in millions?"},\n])\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=TEMP,\n    max_tokens=MAX_TOK,\n    tools=tools,\n    tool_choice="auto",\n)\n\nprint(response.choices[0].message.tool_calls)\n# [ChatCompletionMessageToolCall(id=''3e92V6Vfo'', function=Function(arguments=''{"country": "Russia", "unit": "millions"}'', name=''get_current_population''), type=''function'')]\n```\n\n</details>\n\n<details>\n  <summary>Python snippet - complex</summary>\n\n```python\nimport json\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n\n# Modify OpenAI''s API key and API base to use vLLM''s API server.\nopenai_api_key = "EMPTY"\nopenai_api_base = "http://localhost:8000/v1"\n\nTEMP = 0.15\nMAX_TOK = 131072\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, "r") as file:\n        system_prompt = file.read()\n    return system_prompt\n\n\nmodel_id = "mistralai/Mistral-Small-3.2-24B-Instruct-2506"\nSYSTEM_PROMPT = load_system_prompt(model_id, "SYSTEM_PROMPT.txt")\n\nimage_url = "https://math-coaching.com/img/fiche/46/expressions-mathematiques.jpg"\n\n\ndef my_calculator(expression: str) -> str:\n    return str(eval(expression))\n\n\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "my_calculator",\n            "description": "A calculator that can evaluate a mathematical expression.",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "expression": {\n                        "type": "string",\n                        "description": "The mathematical expression to evaluate.",\n                    },\n                },\n                "required": ["expression"],\n            },\n        },\n    },\n    {\n        "type": "function",\n        "function": {\n            "name": "rewrite",\n            "description": "Rewrite a given text for improved clarity",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "text": {\n                        "type": "string",\n                        "description": "The input text to rewrite",\n                    }\n                },\n            },\n        },\n    },\n]\n\nmessages = [\n    {"role": "system", "content": SYSTEM_PROMPT},\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "text",\n                "text": "Can you calculate the results for all the equations displayed in the image? Only compute the ones that involve numbers.",\n            },\n            {\n                "type": "image_url",\n                "image_url": {\n                    "url": image_url,\n                },\n            },\n        ],\n    },\n]\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=TEMP,\n    max_tokens=MAX_TOK,\n    tools=tools,\n    tool_choice="auto",\n)\n\ntool_calls = response.choices[0].message.tool_calls\nprint(tool_calls)\n# [ChatCompletionMessageToolCall(id=''CyQBSAtGh'', function=Function(arguments=''{"expression": "6 + 2 * 3"}'', name=''my_calculator''), type=''function''), ChatCompletionMessageToolCall(id=''KQqRCqvzc'', function=Function(arguments=''{"expression": "19 - (8 + 2) + 1"}'', name=''my_calculator''), type=''function'')]\n\nresults = []\nfor tool_call in tool_calls:\n    function_name = tool_call.function.name\n    function_args = tool_call.function.arguments\n    if function_name == "my_calculator":\n        result = my_calculator(**json.loads(function_args))\n        results.append(result)\n\nmessages.append({"role": "assistant", "tool_calls": tool_calls})\nfor tool_call, result in zip(tool_calls, results):\n    messages.append(\n        {\n            "role": "tool",\n            "tool_call_id": tool_call.id,\n            "name": tool_call.function.name,\n            "content": result,\n        }\n    )\n\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=TEMP,\n    max_tokens=MAX_TOK,\n)\n\nprint(response.choices[0].message.content)\n# Here are the results for the equations that involve numbers:\n\n# 1. \( 6 + 2 \times 3 = 12 \)\n# 3. \( 19 - (8 + 2) + 1 = 10 \)\n\n# For the other equations, you need to substitute the variables with specific values to compute the results.\n```\n\n</details>\n\n#### Instruction following\n\nMistral-Small-3.2-24B-Instruct-2506 will follow your instructions down to the last letter ! \n\n<details>\n  <summary>Python snippet</summary>\n\n```python\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n\n# Modify OpenAI''s API key and API base to use vLLM''s API server.\nopenai_api_key = "EMPTY"\nopenai_api_base = "http://localhost:8000/v1"\n\nTEMP = 0.15\nMAX_TOK = 131072\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, "r") as file:\n        system_prompt = file.read()\n    return system_prompt\n\n\nmodel_id = "mistralai/Mistral-Small-3.2-24B-Instruct-2506"\nSYSTEM_PROMPT = load_system_prompt(model_id, "SYSTEM_PROMPT.txt")\n\nmessages = [\n    {"role": "system", "content": SYSTEM_PROMPT},\n    {\n        "role": "user",\n        "content": "Write me a sentence where every word starts with the next letter in the alphabet - start with ''a'' and end with ''z''.",\n    },\n]\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=TEMP,\n    max_tokens=MAX_TOK,\n)\n\nassistant_message = response.choices[0].message.content\nprint(assistant_message)\n\n# Here''s a sentence where each word starts with the next letter of the alphabet, starting from ''a'' and ending with ''z'':\n\n# "Always brave cats dance elegantly, fluffy giraffes happily ignore jungle kites, lovingly munching nuts, observing playful quails racing swiftly, tiny unicorns vaulting while xylophones yodel zealously."\n\n# This sentence follows the sequence from A to Z without skipping any letters.\n```\n</details>\n\n### Transformers\n\nYou can also use Mistral-Small-3.2-24B-Instruct-2506 with `Transformers` !\n\nTo make the best use of our model with `Transformers` make sure to have [installed](https://github.com/mistralai/mistral-common) `mistral-common >= 1.6.2` to use our tokenizer.\n\n```bash\npip install mistral-common --upgrade\n```\n\nThen load our tokenizer along with the model and generate:\n\n<details>\n  <summary>Python snippet</summary>\n\n```python\nfrom datetime import datetime, timedelta\nimport torch\n\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom huggingface_hub import hf_hub_download\nfrom transformers import Mistral3ForConditionalGeneration\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, "r") as file:\n        system_prompt = file.read()\n    today = datetime.today().strftime("%Y-%m-%d")\n    yesterday = (datetime.today() - timedelta(days=1)).strftime("%Y-%m-%d")\n    model_name = repo_id.split("/")[-1]\n    return system_prompt.format(name=model_name, today=today, yesterday=yesterday)\n\n\nmodel_id = "mistralai/Mistral-Small-3.2-24B-Instruct-2506"\nSYSTEM_PROMPT = load_system_prompt(model_id, "SYSTEM_PROMPT.txt")\n\ntokenizer = MistralTokenizer.from_hf_hub(model_id)\n\nmodel = Mistral3ForConditionalGeneration.from_pretrained(\n    model_id, torch_dtype=torch.bfloat16\n)\n\nimage_url = "https://static.wikia.nocookie.net/essentialsdocs/images/7/70/Battle.png/revision/latest?cb=20220523172438"\n\nmessages = [\n    {"role": "system", "content": SYSTEM_PROMPT},\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "text",\n                "text": "What action do you think I should take in this situation? List all the possible actions and explain why you think they are good or bad.",\n            },\n            {"type": "image_url", "image_url": {"url": image_url}},\n        ],\n    },\n]\n\ntokenized = tokenizer.encode_chat_completion(ChatCompletionRequest(messages=messages))\n\ninput_ids = torch.tensor([tokenized.tokens])\nattention_mask = torch.ones_like(input_ids)\npixel_values = torch.tensor(tokenized.images[0], dtype=torch.bfloat16).unsqueeze(0)\nimage_sizes = torch.tensor([pixel_values.shape[-2:]])\n\noutput = model.generate(\n    input_ids=input_ids,\n    attention_mask=attention_mask,\n    pixel_values=pixel_values,\n    image_sizes=image_sizes,\n    max_new_tokens=1000,\n)[0]\n\ndecoded_output = tokenizer.decode(output[len(tokenized.tokens) :])\nprint(decoded_output)\n# In this situation, you are playing a Pokémon game where your Pikachu (Level 42) is facing a wild Pidgey (Level 17). Here are the possible actions you can take and an analysis of each:\n\n# 1. **FIGHT**:\n#    - **Pros**: Pikachu is significantly higher level than the wild Pidgey, which suggests that it should be able to defeat Pidgey easily. This could be a good opportunity to gain experience points and possibly items or money.\n#    - **Cons**: There is always a small risk of Pikachu fainting, especially if Pidgey has a powerful move or a status effect that could hinder Pikachu. However, given the large level difference, this risk is minimal.\n\n# 2. **BAG**:\n#    - **Pros**: You might have items in your bag that could help in this battle, such as Potions, Poké Balls, or Berries. Using an item could help you capture Pidgey or heal Pikachu if needed.\n#    - **Cons**: Using items might not be necessary given the level difference. It could be more efficient to just fight and defeat Pidgey quickly.\n\n# 3. **POKÉMON**:\n#    - **Pros**: You might have another Pokémon in your party that is better suited for this battle or that you want to gain experience. Switching Pokémon could also be strategic if you want to train a lower-level Pokémon.\n#    - **Cons**: Switching Pokémon might not be necessary since Pikachu is at a significant advantage. It could also waste time and potentially give Pidgey a turn to attack.\n\n# 4. **RUN**:\n#    - **Pros**: Running away could be a quick way to avoid the battle altogether. This might be useful if you are trying to conserve resources or if you are in a hurry to get to another location.\n#    - **Cons**: Running away means you miss out on the experience points, items, or money that you could gain from defeating Pidgey. It also might not be the most efficient use of your time if you are trying to train your Pokémon.\n\n# ### Recommendation:\n# Given the significant level advantage, the best action to take is likely **FIGHT**. This will allow you to quickly defeat Pidgey and gain experience points for Pikachu. If you are concerned about Pikachu''s health, you could use the **BAG** to heal Pikachu before or during the battle. Running away or switching Pokémon does not seem necessary in this situation.\n```\n\n</details>', '{"pipeline_tag":null,"library_name":"vllm","framework":"vllm","params":24011361280,"storage_bytes":96082070772,"files_count":19,"spaces_count":12,"gated":false,"private":false,"config":{"architectures":["Mistral3ForConditionalGeneration"],"model_type":"mistral3"}}', '[]', '[{"type":"has_code","target_id":"github:mistralai:mistral-common","source_url":"https://github.com/mistralai/mistral-common"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:mistralai:mistral-common","source_url":"https://github.com/mistralai/mistral-common"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:mistralai:mistral-common","source_url":"https://github.com/mistralai/mistral-common"}]', NULL, 'Apache-2.0', 'approved', 77.2, '111cfdf752d2818efceacef03d4683a2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-hkunlp-instructor-large', 'huggingface--hkunlp--instructor-large', 'instructor-large', 'hkunlp', '--- pipeline_tag: sentence-similarity tags: - text-embedding - embeddings - information-retrieval - beir - text-classification - language-model - text-clustering - text-semantic-similarity - text-evaluation - prompt-retrieval - text-reranking - sentence-transformers - feature-extraction - sentence-similarity - transformers - t5 - English - Sentence Similarity - natural_questions - ms_marco - fever - hotpot_qa - mteb language: en inference: false license: apache-2.0 model-index: - name: INSTRU...', '["sentence-transformers","pytorch","t5","text-embedding","embeddings","information-retrieval","beir","text-classification","language-model","text-clustering","text-semantic-similarity","text-evaluation","prompt-retrieval","text-reranking","feature-extraction","sentence-similarity","transformers","english","sentence similarity","natural_questions","ms_marco","fever","hotpot_qa","mteb","en","arxiv:2212.09741","license:apache-2.0","model-index","text-generation-inference","deploy:azure","region:us"]', 'sentence-similarity', 523, 215290, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/hkunlp/instructor-large","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\npipeline_tag: sentence-similarity\ntags:\n- text-embedding\n- embeddings\n- information-retrieval\n- beir\n- text-classification\n- language-model\n- text-clustering\n- text-semantic-similarity\n- text-evaluation\n- prompt-retrieval\n- text-reranking\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\n- t5\n- English\n- Sentence Similarity\n- natural_questions\n- ms_marco\n- fever\n- hotpot_qa\n- mteb\nlanguage: en\ninference: false\nlicense: apache-2.0\nmodel-index:\n- name: INSTRUCTOR\n  results:\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en)\n      config: en\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 88.13432835820896\n    - type: ap\n      value: 59.298209334395665\n    - type: f1\n      value: 83.31769058643586\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_polarity\n      name: MTEB AmazonPolarityClassification\n      config: default\n      split: test\n      revision: e2d317d38cd51312af73b3d32a06d1a08b442046\n    metrics:\n    - type: accuracy\n      value: 91.526375\n    - type: ap\n      value: 88.16327709705504\n    - type: f1\n      value: 91.51095801287843\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (en)\n      config: en\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 47.856\n    - type: f1\n      value: 45.41490917650942\n  - task:\n      type: Retrieval\n    dataset:\n      type: arguana\n      name: MTEB ArguAna\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 31.223\n    - type: map_at_10\n      value: 47.947\n    - type: map_at_100\n      value: 48.742000000000004\n    - type: map_at_1000\n      value: 48.745\n    - type: map_at_3\n      value: 43.137\n    - type: map_at_5\n      value: 45.992\n    - type: mrr_at_1\n      value: 32.432\n    - type: mrr_at_10\n      value: 48.4\n    - type: mrr_at_100\n      value: 49.202\n    - type: mrr_at_1000\n      value: 49.205\n    - type: mrr_at_3\n      value: 43.551\n    - type: mrr_at_5\n      value: 46.467999999999996\n    - type: ndcg_at_1\n      value: 31.223\n    - type: ndcg_at_10\n      value: 57.045\n    - type: ndcg_at_100\n      value: 60.175\n    - type: ndcg_at_1000\n      value: 60.233000000000004\n    - type: ndcg_at_3\n      value: 47.171\n    - type: ndcg_at_5\n      value: 52.322\n    - type: precision_at_1\n      value: 31.223\n    - type: precision_at_10\n      value: 8.599\n    - type: precision_at_100\n      value: 0.991\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_3\n      value: 19.63\n    - type: precision_at_5\n      value: 14.282\n    - type: recall_at_1\n      value: 31.223\n    - type: recall_at_10\n      value: 85.989\n    - type: recall_at_100\n      value: 99.075\n    - type: recall_at_1000\n      value: 99.502\n    - type: recall_at_3\n      value: 58.89\n    - type: recall_at_5\n      value: 71.408\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-p2p\n      name: MTEB ArxivClusteringP2P\n      config: default\n      split: test\n      revision: a122ad7f3f0291bf49cc6f4d32aa80929df69d5d\n    metrics:\n    - type: v_measure\n      value: 43.1621946393635\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-s2s\n      name: MTEB ArxivClusteringS2S\n      config: default\n      split: test\n      revision: f910caf1a6075f7329cdf8c1a6135696f37dbd53\n    metrics:\n    - type: v_measure\n      value: 32.56417132407894\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/askubuntudupquestions-reranking\n      name: MTEB AskUbuntuDupQuestions\n      config: default\n      split: test\n      revision: 2000358ca161889fa9c082cb41daa8dcfb161a54\n    metrics:\n    - type: map\n      value: 64.29539304390207\n    - type: mrr\n      value: 76.44484017060196\n  - task:\n      type: STS\n    dataset:\n      type: mteb/biosses-sts\n      name: MTEB BIOSSES\n      config: default\n      split: test\n      revision: d3fb88f8f02e40887cd149695127462bbcf29b4a\n    metrics:\n    - type: cos_sim_spearman\n      value: 84.38746499431112\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/banking77\n      name: MTEB Banking77Classification\n      config: default\n      split: test\n      revision: 0fd18e25b25c072e09e0d92ab615fda904d66300\n    metrics:\n    - type: accuracy\n      value: 78.51298701298701\n    - type: f1\n      value: 77.49041754069235\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-p2p\n      name: MTEB BiorxivClusteringP2P\n      config: default\n      split: test\n      revision: 65b79d1d13f80053f67aca9498d9402c2d9f1f40\n    metrics:\n    - type: v_measure\n      value: 37.61848554098577\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-s2s\n      name: MTEB BiorxivClusteringS2S\n      config: default\n      split: test\n      revision: 258694dd0231531bc1fd9de6ceb52a0853c6d908\n    metrics:\n    - type: v_measure\n      value: 31.32623280148178\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackAndroidRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 35.803000000000004\n    - type: map_at_10\n      value: 48.848\n    - type: map_at_100\n      value: 50.5\n    - type: map_at_1000\n      value: 50.602999999999994\n    - type: map_at_3\n      value: 45.111000000000004\n    - type: map_at_5\n      value: 47.202\n    - type: mrr_at_1\n      value: 44.635000000000005\n    - type: mrr_at_10\n      value: 55.593\n    - type: mrr_at_100\n      value: 56.169999999999995\n    - type: mrr_at_1000\n      value: 56.19499999999999\n    - type: mrr_at_3\n      value: 53.361999999999995\n    - type: mrr_at_5\n      value: 54.806999999999995\n    - type: ndcg_at_1\n      value: 44.635000000000005\n    - type: ndcg_at_10\n      value: 55.899\n    - type: ndcg_at_100\n      value: 60.958\n    - type: ndcg_at_1000\n      value: 62.302\n    - type: ndcg_at_3\n      value: 51.051\n    - type: ndcg_at_5\n      value: 53.351000000000006\n    - type: precision_at_1\n      value: 44.635000000000005\n    - type: precision_at_10\n      value: 10.786999999999999\n    - type: precision_at_100\n      value: 1.6580000000000001\n    - type: precision_at_1000\n      value: 0.213\n    - type: precision_at_3\n      value: 24.893\n    - type: precision_at_5\n      value: 17.740000000000002\n    - type: recall_at_1\n      value: 35.803000000000004\n    - type: recall_at_10\n      value: 68.657\n    - type: recall_at_100\n      value: 89.77199999999999\n    - type: recall_at_1000\n      value: 97.67\n    - type: recall_at_3\n      value: 54.066\n    - type: recall_at_5\n      value: 60.788\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackEnglishRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 33.706\n    - type: map_at_10\n      value: 44.896\n    - type: map_at_100\n      value: 46.299\n    - type: map_at_1000\n      value: 46.44\n    - type: map_at_3\n      value: 41.721000000000004\n    - type: map_at_5\n      value: 43.486000000000004\n    - type: mrr_at_1\n      value: 41.592\n    - type: mrr_at_10\n      value: 50.529\n    - type: mrr_at_100\n      value: 51.22\n    - type: mrr_at_1000\n      value: 51.258\n    - type: mrr_at_3\n      value: 48.205999999999996\n    - type: mrr_at_5\n      value: 49.528\n    - type: ndcg_at_1\n      value: 41.592\n    - type: ndcg_at_10\n      value: 50.77199999999999\n    - type: ndcg_at_100\n      value: 55.383\n    - type: ndcg_at_1000\n      value: 57.288\n    - type: ndcg_at_3\n      value: 46.324\n    - type: ndcg_at_5\n      value: 48.346000000000004\n    - type: precision_at_1\n      value: 41.592\n    - type: precision_at_10\n      value: 9.516\n    - type: precision_at_100\n      value: 1.541\n    - type: precision_at_1000\n      value: 0.2\n    - type: precision_at_3\n      value: 22.399\n    - type: precision_at_5\n      value: 15.770999999999999\n    - type: recall_at_1\n      value: 33.706\n    - type: recall_at_10\n      value: 61.353\n    - type: recall_at_100\n      value: 80.182\n    - type: recall_at_1000\n      value: 91.896\n    - type: recall_at_3\n      value: 48.204\n    - type: recall_at_5\n      value: 53.89699999999999\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackGamingRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 44.424\n    - type: map_at_10\n      value: 57.169000000000004\n    - type: map_at_100\n      value: 58.202\n    - type: map_at_1000\n      value: 58.242000000000004\n    - type: map_at_3\n      value: 53.825\n    - type: map_at_5\n      value: 55.714\n    - type: mrr_at_1\n      value: 50.470000000000006\n    - type: mrr_at_10\n      value: 60.489000000000004\n    - type: mrr_at_100\n      value: 61.096\n    - type: mrr_at_1000\n      value: 61.112\n    - type: mrr_at_3\n      value: 58.192\n    - type: mrr_at_5\n      value: 59.611999999999995\n    - type: ndcg_at_1\n      value: 50.470000000000006\n    - type: ndcg_at_10\n      value: 63.071999999999996\n    - type: ndcg_at_100\n      value: 66.964\n    - type: ndcg_at_1000\n      value: 67.659\n    - type: ndcg_at_3\n      value: 57.74399999999999\n    - type: ndcg_at_5\n      value: 60.367000000000004\n    - type: precision_at_1\n      value: 50.470000000000006\n    - type: precision_at_10\n      value: 10.019\n    - type: precision_at_100\n      value: 1.29\n    - type: precision_at_1000\n      value: 0.13899999999999998\n    - type: precision_at_3\n      value: 25.558999999999997\n    - type: precision_at_5\n      value: 17.467\n    - type: recall_at_1\n      value: 44.424\n    - type: recall_at_10\n      value: 77.02\n    - type: recall_at_100\n      value: 93.738\n    - type: recall_at_1000\n      value: 98.451\n    - type: recall_at_3\n      value: 62.888\n    - type: recall_at_5\n      value: 69.138\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackGisRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 26.294\n    - type: map_at_10\n      value: 34.503\n    - type: map_at_100\n      value: 35.641\n    - type: map_at_1000\n      value: 35.724000000000004\n    - type: map_at_3\n      value: 31.753999999999998\n    - type: map_at_5\n      value: 33.190999999999995\n    - type: mrr_at_1\n      value: 28.362\n    - type: mrr_at_10\n      value: 36.53\n    - type: mrr_at_100\n      value: 37.541000000000004\n    - type: mrr_at_1000\n      value: 37.602000000000004\n    - type: mrr_at_3\n      value: 33.917\n    - type: mrr_at_5\n      value: 35.358000000000004\n    - type: ndcg_at_1\n      value: 28.362\n    - type: ndcg_at_10\n      value: 39.513999999999996\n    - type: ndcg_at_100\n      value: 44.815\n    - type: ndcg_at_1000\n      value: 46.839\n    - type: ndcg_at_3\n      value: 34.02\n    - type: ndcg_at_5\n      value: 36.522\n    - type: precision_at_1\n      value: 28.362\n    - type: precision_at_10\n      value: 6.101999999999999\n    - type: precision_at_100\n      value: 0.9129999999999999\n    - type: precision_at_1000\n      value: 0.11399999999999999\n    - type: precision_at_3\n      value: 14.161999999999999\n    - type: precision_at_5\n      value: 9.966\n    - type: recall_at_1\n      value: 26.294\n    - type: recall_at_10\n      value: 53.098\n    - type: recall_at_100\n      value: 76.877\n    - type: recall_at_1000\n      value: 91.834\n    - type: recall_at_3\n      value: 38.266\n    - type: recall_at_5\n      value: 44.287\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackMathematicaRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 16.407\n    - type: map_at_10\n      value: 25.185999999999996\n    - type: map_at_100\n      value: 26.533\n    - type: map_at_1000\n      value: 26.657999999999998\n    - type: map_at_3\n      value: 22.201999999999998\n    - type: map_at_5\n      value: 23.923\n    - type: mrr_at_1\n      value: 20.522000000000002\n    - type: mrr_at_10\n      value: 29.522\n    - type: mrr_at_100\n      value: 30.644\n    - type: mrr_at_1000\n      value: 30.713\n    - type: mrr_at_3\n      value: 26.679000000000002\n    - type: mrr_at_5\n      value: 28.483000000000004\n    - type: ndcg_at_1\n      value: 20.522000000000002\n    - type: ndcg_at_10\n      value: 30.656\n    - type: ndcg_at_100\n      value: 36.864999999999995\n    - type: ndcg_at_1000\n      value: 39.675\n    - type: ndcg_at_3\n      value: 25.319000000000003\n    - type: ndcg_at_5\n      value: 27.992\n    - type: precision_at_1\n      value: 20.522000000000002\n    - type: precision_at_10\n      value: 5.795999999999999\n    - type: precision_at_100\n      value: 1.027\n    - type: precision_at_1000\n      value: 0.13999999999999999\n    - type: precision_at_3\n      value: 12.396\n    - type: precision_at_5\n      value: 9.328\n    - type: recall_at_1\n      value: 16.407\n    - type: recall_at_10\n      value: 43.164\n    - type: recall_at_100\n      value: 69.695\n    - type: recall_at_1000\n      value: 89.41900000000001\n    - type: recall_at_3\n      value: 28.634999999999998\n    - type: recall_at_5\n      value: 35.308\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackPhysicsRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 30.473\n    - type: map_at_10\n      value: 41.676\n    - type: map_at_100\n      value: 43.120999999999995\n    - type: map_at_1000\n      value: 43.230000000000004\n    - type: map_at_3\n      value: 38.306000000000004\n    - type: map_at_5\n      value: 40.355999999999995\n    - type: mrr_at_1\n      value: 37.536\n    - type: mrr_at_10\n      value: 47.643\n    - type: mrr_at_100\n      value: 48.508\n    - type: mrr_at_1000\n      value: 48.551\n    - type: mrr_at_3\n      value: 45.348\n    - type: mrr_at_5\n      value: 46.744\n    - type: ndcg_at_1\n      value: 37.536\n    - type: ndcg_at_10\n      value: 47.823\n    - type: ndcg_at_100\n      value: 53.395\n    - type: ndcg_at_1000\n      value: 55.271\n    - type: ndcg_at_3\n      value: 42.768\n    - type: ndcg_at_5\n      value: 45.373000000000005\n    - type: precision_at_1\n      value: 37.536\n    - type: precision_at_10\n      value: 8.681\n    - type: precision_at_100\n      value: 1.34\n    - type: precision_at_1000\n      value: 0.165\n    - type: precision_at_3\n      value: 20.468\n    - type: precision_at_5\n      value: 14.495\n    - type: recall_at_1\n      value: 30.473\n    - type: recall_at_10\n      value: 60.092999999999996\n    - type: recall_at_100\n      value: 82.733\n    - type: recall_at_1000\n      value: 94.875\n    - type: recall_at_3\n      value: 45.734\n    - type: recall_at_5\n      value: 52.691\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackProgrammersRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 29.976000000000003\n    - type: map_at_10\n      value: 41.097\n    - type: map_at_100\n      value: 42.547000000000004\n    - type: map_at_1000\n      value: 42.659000000000006\n    - type: map_at_3\n      value: 37.251\n    - type: map_at_5\n      value: 39.493\n    - type: mrr_at_1\n      value: 37.557\n    - type: mrr_at_10\n      value: 46.605000000000004\n    - type: mrr_at_100\n      value: 47.487\n    - type: mrr_at_1000\n      value: 47.54\n    - type: mrr_at_3\n      value: 43.721\n    - type: mrr_at_5\n      value: 45.411\n    - type: ndcg_at_1\n      value: 37.557\n    - type: ndcg_at_10\n      value: 47.449000000000005\n    - type: ndcg_at_100\n      value: 53.052\n    - type: ndcg_at_1000\n      value: 55.010999999999996\n    - type: ndcg_at_3\n      value: 41.439\n    - type: ndcg_at_5\n      value: 44.292\n    - type: precision_at_1\n      value: 37.557\n    - type: precision_at_10\n      value: 8.847\n    - type: precision_at_100\n      value: 1.357\n    - type: precision_at_1000\n      value: 0.16999999999999998\n    - type: precision_at_3\n      value: 20.091\n    - type: precision_at_5\n      value: 14.384\n    - type: recall_at_1\n      value: 29.976000000000003\n    - type: recall_at_10\n      value: 60.99099999999999\n    - type: recall_at_100\n      value: 84.245\n    - type: recall_at_1000\n      value: 96.97200000000001\n    - type: recall_at_3\n      value: 43.794\n    - type: recall_at_5\n      value: 51.778999999999996\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 28.099166666666665\n    - type: map_at_10\n      value: 38.1365\n    - type: map_at_100\n      value: 39.44491666666667\n    - type: map_at_1000\n      value: 39.55858333333334\n    - type: map_at_3\n      value: 35.03641666666666\n    - type: map_at_5\n      value: 36.79833333333334\n    - type: mrr_at_1\n      value: 33.39966666666667\n    - type: mrr_at_10\n      value: 42.42583333333333\n    - type: mrr_at_100\n      value: 43.28575\n    - type: mrr_at_1000\n      value: 43.33741666666667\n    - type: mrr_at_3\n      value: 39.94975\n    - type: mrr_at_5\n      value: 41.41633333333334\n    - type: ndcg_at_1\n      value: 33.39966666666667\n    - type: ndcg_at_10\n      value: 43.81741666666667\n    - type: ndcg_at_100\n      value: 49.08166666666667\n    - type: ndcg_at_1000\n      value: 51.121166666666674\n    - type: ndcg_at_3\n      value: 38.73575\n    - type: ndcg_at_5\n      value: 41.18158333333333\n    - type: precision_at_1\n      value: 33.39966666666667\n    - type: precision_at_10\n      value: 7.738916666666667\n    - type: precision_at_100\n      value: 1.2265833333333331\n    - type: precision_at_1000\n      value: 0.15983333333333336\n    - type: precision_at_3\n      value: 17.967416666666665\n    - type: precision_at_5\n      value: 12.78675\n    - type: recall_at_1\n      value: 28.099166666666665\n    - type: recall_at_10\n      value: 56.27049999999999\n    - type: recall_at_100\n      value: 78.93291666666667\n    - type: recall_at_1000\n      value: 92.81608333333334\n    - type: recall_at_3\n      value: 42.09775\n    - type: recall_at_5\n      value: 48.42533333333334\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackStatsRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 23.663\n    - type: map_at_10\n      value: 30.377\n    - type: map_at_100\n      value: 31.426\n    - type: map_at_1000\n      value: 31.519000000000002\n    - type: map_at_3\n      value: 28.069\n    - type: map_at_5\n      value: 29.256999999999998\n    - type: mrr_at_1\n      value: 26.687\n    - type: mrr_at_10\n      value: 33.107\n    - type: mrr_at_100\n      value: 34.055\n    - type: mrr_at_1000\n      value: 34.117999999999995\n    - type: mrr_at_3\n      value: 31.058000000000003\n    - type: mrr_at_5\n      value: 32.14\n    - type: ndcg_at_1\n      value: 26.687\n    - type: ndcg_at_10\n      value: 34.615\n    - type: ndcg_at_100\n      value: 39.776\n    - type: ndcg_at_1000\n      value: 42.05\n    - type: ndcg_at_3\n      value: 30.322\n    - type: ndcg_at_5\n      value: 32.157000000000004\n    - type: precision_at_1\n      value: 26.687\n    - type: precision_at_10\n      value: 5.491\n    - type: precision_at_100\n      value: 0.877\n    - type: precision_at_1000\n      value: 0.11499999999999999\n    - type: precision_at_3\n      value: 13.139000000000001\n    - type: precision_at_5\n      value: 9.049\n    - type: recall_at_1\n      value: 23.663\n    - type: recall_at_10\n      value: 45.035\n    - type: recall_at_100\n      value: 68.554\n    - type: recall_at_1000\n      value: 85.077\n    - type: recall_at_3\n      value: 32.982\n    - type: recall_at_5\n      value: 37.688\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackTexRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 17.403\n    - type: map_at_10\n      value: 25.197000000000003\n    - type: map_at_100\n      value: 26.355\n    - type: map_at_1000\n      value: 26.487\n    - type: map_at_3\n      value: 22.733\n    - type: map_at_5\n      value: 24.114\n    - type: mrr_at_1\n      value: 21.37\n    - type: mrr_at_10\n      value: 29.091\n    - type: mrr_at_100\n      value: 30.018\n    - type: mrr_at_1000\n      value: 30.096\n    - type: mrr_at_3\n      value: 26.887\n    - type: mrr_at_5\n      value: 28.157\n    - type: ndcg_at_1\n      value: 21.37\n    - type: ndcg_at_10\n      value: 30.026000000000003\n    - type: ndcg_at_100\n      value: 35.416\n    - type: ndcg_at_1000\n      value: 38.45\n    - type: ndcg_at_3\n      value: 25.764\n    - type: ndcg_at_5\n      value: 27.742\n    - type: precision_at_1\n      value: 21.37\n    - type: precision_at_10\n      value: 5.609\n    - type: precision_at_100\n      value: 0.9860000000000001\n    - type: precision_at_1000\n      value: 0.14300000000000002\n    - type: precision_at_3\n      value: 12.423\n    - type: precision_at_5\n      value: 9.009\n    - type: recall_at_1\n      value: 17.403\n    - type: recall_at_10\n      value: 40.573\n    - type: recall_at_100\n      value: 64.818\n    - type: recall_at_1000\n      value: 86.53699999999999\n    - type: recall_at_3\n      value: 28.493000000000002\n    - type: recall_at_5\n      value: 33.660000000000004\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackUnixRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 28.639\n    - type: map_at_10\n      value: 38.951\n    - type: map_at_100\n      value: 40.238\n    - type: map_at_1000\n      value: 40.327\n    - type: map_at_3\n      value: 35.842\n    - type: map_at_5\n      value: 37.617\n    - type: mrr_at_1\n      value: 33.769\n    - type: mrr_at_10\n      value: 43.088\n    - type: mrr_at_100\n      value: 44.03\n    - type: mrr_at_1000\n      value: 44.072\n    - type: mrr_at_3\n      value: 40.656\n    - type: mrr_at_5\n      value: 42.138999999999996\n    - type: ndcg_at_1\n      value: 33.769\n    - type: ndcg_at_10\n      value: 44.676\n    - type: ndcg_at_100\n      value: 50.416000000000004\n    - type: ndcg_at_1000\n      value: 52.227999999999994\n    - type: ndcg_at_3\n      value: 39.494\n    - type: ndcg_at_5\n      value: 42.013\n    - type: precision_at_1\n      value: 33.769\n    - type: precision_at_10\n      value: 7.668\n    - type: precision_at_100\n      value: 1.18\n    - type: precision_at_1000\n      value: 0.145\n    - type: precision_at_3\n      value: 18.221\n    - type: precision_at_5\n      value: 12.966\n    - type: recall_at_1\n      value: 28.639\n    - type: recall_at_10\n      value: 57.687999999999995\n    - type: recall_at_100\n      value: 82.541\n    - type: recall_at_1000\n      value: 94.896\n    - type: recall_at_3\n      value: 43.651\n    - type: recall_at_5\n      value: 49.925999999999995\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackWebmastersRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 29.57\n    - type: map_at_10\n      value: 40.004\n    - type: map_at_100\n      value: 41.75\n    - type: map_at_1000\n      value: 41.97\n    - type: map_at_3\n      value: 36.788\n    - type: map_at_5\n      value: 38.671\n    - type: mrr_at_1\n      value: 35.375\n    - type: mrr_at_10\n      value: 45.121\n    - type: mrr_at_100\n      value: 45.994\n    - type: mrr_at_1000\n      value: 46.04\n    - type: mrr_at_3\n      value: 42.227\n    - type: mrr_at_5\n      value: 43.995\n    - type: ndcg_at_1\n      value: 35.375\n    - type: ndcg_at_10\n      value: 46.392\n    - type: ndcg_at_100\n      value: 52.196\n    - type: ndcg_at_1000\n      value: 54.274\n    - type: ndcg_at_3\n      value: 41.163\n    - type: ndcg_at_5\n      value: 43.813\n    - type: precision_at_1\n      value: 35.375\n    - type: precision_at_10\n      value: 8.676\n    - type: precision_at_100\n      value: 1.678\n    - type: precision_at_1000\n      value: 0.253\n    - type: precision_at_3\n      value: 19.104\n    - type: precision_at_5\n      value: 13.913\n    - type: recall_at_1\n      value: 29.57\n    - type: recall_at_10\n      value: 58.779\n    - type: recall_at_100\n      value: 83.337\n    - type: recall_at_1000\n      value: 95.979\n    - type: recall_at_3\n      value: 44.005\n    - type: recall_at_5\n      value: 50.975\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackWordpressRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 20.832\n    - type: map_at_10\n      value: 29.733999999999998\n    - type: map_at_100\n      value: 30.727\n    - type: map_at_1000\n      value: 30.843999999999998\n    - type: map_at_3\n      value: 26.834999999999997\n    - type: map_at_5\n      value: 28.555999999999997\n    - type: mrr_at_1\n      value: 22.921\n    - type: mrr_at_10\n      value: 31.791999999999998\n    - type: mrr_at_100\n      value: 32.666000000000004\n    - type: mrr_at_1000\n      value: 32.751999999999995\n    - type: mrr_at_3\n      value: 29.144\n    - type: mrr_at_5\n      value: 30.622\n    - type: ndcg_at_1\n      value: 22.921\n    - type: ndcg_at_10\n      value: 34.915\n    - type: ndcg_at_100\n      value: 39.744\n    - type: ndcg_at_1000\n      value: 42.407000000000004\n    - type: ndcg_at_3\n      value: 29.421000000000003\n    - type: ndcg_at_5\n      value: 32.211\n    - type: precision_at_1\n      value: 22.921\n    - type: precision_at_10\n      value: 5.675\n    - type: precision_at_100\n      value: 0.872\n    - type: precision_at_1000\n      value: 0.121\n    - type: precision_at_3\n      value: 12.753999999999998\n    - type: precision_at_5\n      value: 9.353\n    - type: recall_at_1\n      value: 20.832\n    - type: recall_at_10\n      value: 48.795\n    - type: recall_at_100\n      value: 70.703\n    - type: recall_at_1000\n      value: 90.187\n    - type: recall_at_3\n      value: 34.455000000000005\n    - type: recall_at_5\n      value: 40.967\n  - task:\n      type: Retrieval\n    dataset:\n      type: climate-fever\n      name: MTEB ClimateFEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 10.334\n    - type: map_at_10\n      value: 19.009999999999998\n    - type: map_at_100\n      value: 21.129\n    - type: map_at_1000\n      value: 21.328\n    - type: map_at_3\n      value: 15.152\n    - type: map_at_5\n      value: 17.084\n    - type: mrr_at_1\n      value: 23.453\n    - type: mrr_at_10\n      value: 36.099\n    - type: mrr_at_100\n      value: 37.069\n    - type: mrr_at_1000\n      value: 37.104\n    - type: mrr_at_3\n      value: 32.096000000000004\n    - type: mrr_at_5\n      value: 34.451\n    - type: ndcg_at_1\n      value: 23.453\n    - type: ndcg_at_10\n      value: 27.739000000000004\n    - type: ndcg_at_100\n      value: 35.836\n    - type: ndcg_at_1000\n      value: 39.242\n    - type: ndcg_at_3\n      value: 21.263\n    - type: ndcg_at_5\n      value: 23.677\n    - type: precision_at_1\n      value: 23.453\n    - type: precision_at_10\n      value: 9.199\n    - type: precision_at_100\n      value: 1.791\n    - type: precision_at_1000\n      value: 0.242\n    - type: precision_at_3\n      value: 16.2\n    - type: precision_at_5\n      value: 13.147\n    - type: recall_at_1\n      value: 10.334\n    - type: recall_at_10\n      value: 35.177\n    - type: recall_at_100\n      value: 63.009\n    - type: recall_at_1000\n      value: 81.938\n    - type: recall_at_3\n      value: 19.914\n    - type: recall_at_5\n      value: 26.077\n  - task:\n      type: Retrieval\n    dataset:\n      type: dbpedia-entity\n      name: MTEB DBPedia\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 8.212\n    - type: map_at_10\n      value: 17.386\n    - type: map_at_100\n      value: 24.234\n    - type: map_at_1000\n      value: 25.724999999999998\n    - type: map_at_3\n      value: 12.727\n    - type: map_at_5\n      value: 14.785\n    - type: mrr_at_1\n      value: 59.25\n    - type: mrr_at_10\n      value: 68.687\n    - type: mrr_at_100\n      value: 69.133\n    - type: mrr_at_1000\n      value: 69.14099999999999\n    - type: mrr_at_3\n      value: 66.917\n    - type: mrr_at_5\n      value: 67.742\n    - type: ndcg_at_1\n      value: 48.625\n    - type: ndcg_at_10\n      value: 36.675999999999995\n    - type: ndcg_at_100\n      value: 41.543\n    - type: ndcg_at_1000\n      value: 49.241\n    - type: ndcg_at_3\n      value: 41.373\n    - type: ndcg_at_5\n      value: 38.707\n    - type: precision_at_1\n      value: 59.25\n    - type: precision_at_10\n      value: 28.525\n    - type: precision_at_100\n      value: 9.027000000000001\n    - type: precision_at_1000\n      value: 1.8339999999999999\n    - type: precision_at_3\n      value: 44.833\n    - type: precision_at_5\n      value: 37.35\n    - type: recall_at_1\n      value: 8.212\n    - type: recall_at_10\n      value: 23.188\n    - type: recall_at_100\n      value: 48.613\n    - type: recall_at_1000\n      value: 73.093\n    - type: recall_at_3\n      value: 14.419\n    - type: recall_at_5\n      value: 17.798\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/emotion\n      name: MTEB EmotionClassification\n      config: default\n      split: test\n      revision: 4f58c6b202a23cf9a4da393831edf4f9183cad37\n    metrics:\n    - type: accuracy\n      value: 52.725\n    - type: f1\n      value: 46.50743309855908\n  - task:\n      type: Retrieval\n    dataset:\n      type: fever\n      name: MTEB FEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 55.086\n    - type: map_at_10\n      value: 66.914\n    - type: map_at_100\n      value: 67.321\n    - type: map_at_1000\n      value: 67.341\n    - type: map_at_3\n      value: 64.75800000000001\n    - type: map_at_5\n      value: 66.189\n    - type: mrr_at_1\n      value: 59.28600000000001\n    - type: mrr_at_10\n      value: 71.005\n    - type: mrr_at_100\n      value: 71.304\n    - type: mrr_at_1000\n      value: 71.313\n    - type: mrr_at_3\n      value: 69.037\n    - type: mrr_at_5\n      value: 70.35\n    - type: ndcg_at_1\n      value: 59.28600000000001\n    - type: ndcg_at_10\n      value: 72.695\n    - type: ndcg_at_100\n      value: 74.432\n    - type: ndcg_at_1000\n      value: 74.868\n    - type: ndcg_at_3\n      value: 68.72200000000001\n    - type: ndcg_at_5\n      value: 71.081\n    - type: precision_at_1\n      value: 59.28600000000001\n    - type: precision_at_10\n      value: 9.499\n    - type: precision_at_100\n      value: 1.052\n    - type: precision_at_1000\n      value: 0.11100000000000002\n    - type: precision_at_3\n      value: 27.503\n    - type: precision_at_5\n      value: 17.854999999999997\n    - type: recall_at_1\n      value: 55.086\n    - type: recall_at_10\n      value: 86.453\n    - type: recall_at_100\n      value: 94.028\n    - type: recall_at_1000\n      value: 97.052\n    - type: recall_at_3\n      value: 75.821\n    - type: recall_at_5\n      value: 81.6\n  - task:\n      type: Retrieval\n    dataset:\n      type: fiqa\n      name: MTEB FiQA2018\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 22.262999999999998\n    - type: map_at_10\n      value: 37.488\n    - type: map_at_100\n      value: 39.498\n    - type: map_at_1000\n      value: 39.687\n    - type: map_at_3\n      value: 32.529\n    - type: map_at_5\n      value: 35.455\n    - type: mrr_at_1\n      value: 44.907000000000004\n    - type: mrr_at_10\n      value: 53.239000000000004\n    - type: mrr_at_100\n      value: 54.086\n    - type: mrr_at_1000\n      value: 54.122\n    - type: mrr_at_3\n      value: 51.235\n    - type: mrr_at_5\n      value: 52.415\n    - type: ndcg_at_1\n      value: 44.907000000000004\n    - type: ndcg_at_10\n      value: 45.446\n    - type: ndcg_at_100\n      value: 52.429\n    - type: ndcg_at_1000\n      value: 55.169000000000004\n    - type: ndcg_at_3\n      value: 41.882000000000005\n    - type: ndcg_at_5\n      value: 43.178\n    - type: precision_at_1\n      value: 44.907000000000004\n    - type: precision_at_10\n      value: 12.931999999999999\n    - type: precision_at_100\n      value: 2.025\n    - type: precision_at_1000\n      value: 0.248\n    - type: precision_at_3\n      value: 28.652\n    - type: precision_at_5\n      value: 21.204\n    - type: recall_at_1\n      value: 22.262999999999998\n    - type: recall_at_10\n      value: 52.447\n    - type: recall_at_100\n      value: 78.045\n    - type: recall_at_1000\n      value: 94.419\n    - type: recall_at_3\n      value: 38.064\n    - type: recall_at_5\n      value: 44.769\n  - task:\n      type: Retrieval\n    dataset:\n      type: hotpotqa\n      name: MTEB HotpotQA\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 32.519\n    - type: map_at_10\n      value: 45.831\n    - type: map_at_100\n      value: 46.815\n    - type: map_at_1000\n      value: 46.899\n    - type: map_at_3\n      value: 42.836\n    - type: map_at_5\n      value: 44.65\n    - type: mrr_at_1\n      value: 65.037\n    - type: mrr_at_10\n      value: 72.16\n    - type: mrr_at_100\n      value: 72.51100000000001\n    - type: mrr_at_1000\n      value: 72.53\n    - type: mrr_at_3\n      value: 70.682\n    - type: mrr_at_5\n      value: 71.54599999999999\n    - type: ndcg_at_1\n      value: 65.037\n    - type: ndcg_at_10\n      value: 55.17999999999999\n    - type: ndcg_at_100\n      value: 58.888\n    - type: ndcg_at_1000\n      value: 60.648\n    - type: ndcg_at_3\n      value: 50.501\n    - type: ndcg_at_5\n      value: 52.977\n    - type: precision_at_1\n      value: 65.037\n    - type: precision_at_10\n      value: 11.530999999999999\n    - type: precision_at_100\n      value: 1.4460000000000002\n    - type: precision_at_1000\n      value: 0.168\n    - type: precision_at_3\n      value: 31.483\n    - type: precision_at_5\n      value: 20.845\n    - type: recall_at_1\n      value: 32.519\n    - type: recall_at_10\n      value: 57.657000000000004\n    - type: recall_at_100\n      value: 72.30199999999999\n    - type: recall_at_1000\n      value: 84.024\n    - type: recall_at_3\n      value: 47.225\n    - type: recall_at_5\n      value: 52.113\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/imdb\n      name: MTEB ImdbClassification\n      config: default\n      split: test\n      revision: 3d86128a09e091d6018b6d26cad27f2739fc2db7\n    metrics:\n    - type: accuracy\n      value: 88.3168\n    - type: ap\n      value: 83.80165516037135\n    - type: f1\n      value: 88.29942471066407\n  - task:\n      type: Retrieval\n    dataset:\n      type: msmarco\n      name: MTEB MSMARCO\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 20.724999999999998\n    - type: map_at_10\n      value: 32.736\n    - type: map_at_100\n      value: 33.938\n    - type: map_at_1000\n      value: 33.991\n    - type: map_at_3\n      value: 28.788000000000004\n    - type: map_at_5\n      value: 31.016\n    - type: mrr_at_1\n      value: 21.361\n    - type: mrr_at_10\n      value: 33.323\n    - type: mrr_at_100\n      value: 34.471000000000004\n    - type: mrr_at_1000\n      value: 34.518\n    - type: mrr_at_3\n      value: 29.453000000000003\n    - type: mrr_at_5\n      value: 31.629\n    - type: ndcg_at_1\n      value: 21.361\n    - type: ndcg_at_10\n      value: 39.649\n    - type: ndcg_at_100\n      value: 45.481\n    - type: ndcg_at_1000\n      value: 46.775\n    - type: ndcg_at_3\n      value: 31.594\n    - type: ndcg_at_5\n      value: 35.543\n    - type: precision_at_1\n      value: 21.361\n    - type: precision_at_10\n      value: 6.3740000000000006\n    - type: precision_at_100\n      value: 0.931\n    - type: precision_at_1000\n      value: 0.104\n    - type: precision_at_3\n      value: 13.514999999999999\n    - type: precision_at_5\n      value: 10.100000000000001\n    - type: recall_at_1\n      value: 20.724999999999998\n    - type: recall_at_10\n      value: 61.034\n    - type: recall_at_100\n      value: 88.062\n    - type: recall_at_1000\n      value: 97.86399999999999\n    - type: recall_at_3\n      value: 39.072\n    - type: recall_at_5\n      value: 48.53\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (en)\n      config: en\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 93.8919288645691\n    - type: f1\n      value: 93.57059586398059\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (en)\n      config: en\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 67.97993616051072\n    - type: f1\n      value: 48.244319183606535\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (en)\n      config: en\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 68.90047074646941\n    - type: f1\n      value: 66.48999056063725\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (en)\n      config: en\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.34566240753195\n    - type: f1\n      value: 73.54164154290658\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-p2p\n      name: MTEB MedrxivClusteringP2P\n      config: default\n      split: test\n      revision: e7a26af6f3ae46b30dde8737f02c07b1505bcc73\n    metrics:\n    - type: v_measure\n      value: 34.21866934757011\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-s2s\n      name: MTEB MedrxivClusteringS2S\n      config: default\n      split: test\n      revision: 35191c8c0dca72d8ff3efcd72aa802307d469663\n    metrics:\n    - type: v_measure\n      value: 32.000936217235534\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/mind_small\n      name: MTEB MindSmallReranking\n      config: default\n      split: test\n      revision: 3bdac13927fdc888b903db93b2ffdbd90b295a69\n    metrics:\n    - type: map\n      value: 31.68189362520352\n    - type: mrr\n      value: 32.69603637784303\n  - task:\n      type: Retrieval\n    dataset:\n      type: nfcorpus\n      name: MTEB NFCorpus\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 6.078\n    - type: map_at_10\n      value: 12.671\n    - type: map_at_100\n      value: 16.291\n    - type: map_at_1000\n      value: 17.855999999999998\n    - type: map_at_3\n      value: 9.610000000000001\n    - type: map_at_5\n      value: 11.152\n    - type: mrr_at_1\n      value: 43.963\n    - type: mrr_at_10\n      value: 53.173\n    - type: mrr_at_100\n      value: 53.718999999999994\n    - type: mrr_at_1000\n      value: 53.756\n    - type: mrr_at_3\n      value: 50.980000000000004\n    - type: mrr_at_5\n      value: 52.42\n    - type: ndcg_at_1\n      value: 42.415000000000006\n    - type: ndcg_at_10\n      value: 34.086\n    - type: ndcg_at_100\n      value: 32.545\n    - type: ndcg_at_1000\n      value: 41.144999999999996\n    - type: ndcg_at_3\n      value: 39.434999999999995\n    - type: ndcg_at_5\n      value: 37.888\n    - type: precision_at_1\n      value: 43.653\n    - type: precision_at_10\n      value: 25.014999999999997\n    - type: precision_at_100\n      value: 8.594\n    - type: precision_at_1000\n      value: 2.169\n    - type: precision_at_3\n      value: 37.049\n    - type: precision_at_5\n      value: 33.065\n    - type: recall_at_1\n      value: 6.078\n    - type: recall_at_10\n      value: 16.17\n    - type: recall_at_100\n      value: 34.512\n    - type: recall_at_1000\n      value: 65.447\n    - type: recall_at_3\n      value: 10.706\n    - type: recall_at_5\n      value: 13.158\n  - task:\n      type: Retrieval\n    dataset:\n      type: nq\n      name: MTEB NQ\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 27.378000000000004\n    - type: map_at_10\n      value: 42.178\n    - type: map_at_100\n      value: 43.32\n    - type: map_at_1000\n      value: 43.358000000000004\n    - type: map_at_3\n      value: 37.474000000000004\n    - type: map_at_5\n      value: 40.333000000000006\n    - type: mrr_at_1\n      value: 30.823\n    - type: mrr_at_10\n      value: 44.626\n    - type: mrr_at_100\n      value: 45.494\n    - type: mrr_at_1000\n      value: 45.519\n    - type: mrr_at_3\n      value: 40.585\n    - type: mrr_at_5\n      value: 43.146\n    - type: ndcg_at_1\n      value: 30.794\n    - type: ndcg_at_10\n      value: 50.099000000000004\n    - type: ndcg_at_100\n      value: 54.900999999999996\n    - type: ndcg_at_1000\n      value: 55.69499999999999\n    - type: ndcg_at_3\n      value: 41.238\n    - type: ndcg_at_5\n      value: 46.081\n    - type: precision_at_1\n      value: 30.794\n    - type: precision_at_10\n      value: 8.549\n    - type: precision_at_100\n      value: 1.124\n    - type: precision_at_1000\n      value: 0.12\n    - type: precision_at_3\n      value: 18.926000000000002\n    - type: precision_at_5\n      value: 14.16\n    - type: recall_at_1\n      value: 27.378000000000004\n    - type: recall_at_10\n      value: 71.842\n    - type: recall_at_100\n      value: 92.565\n    - type: recall_at_1000\n      value: 98.402\n    - type: recall_at_3\n      value: 49.053999999999995\n    - type: recall_at_5\n      value: 60.207\n  - task:\n      type: Retrieval\n    dataset:\n      type: quora\n      name: MTEB QuoraRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 70.557\n    - type: map_at_10\n      value: 84.729\n    - type: map_at_100\n      value: 85.369\n    - type: map_at_1000\n      value: 85.382\n    - type: map_at_3\n      value: 81.72\n    - type: map_at_5\n      value: 83.613\n    - type: mrr_at_1\n      value: 81.3\n    - type: mrr_at_10\n      value: 87.488\n    - type: mrr_at_100\n      value: 87.588\n    - type: mrr_at_1000\n      value: 87.589\n    - type: mrr_at_3\n      value: 86.53\n    - type: mrr_at_5\n      value: 87.18599999999999\n    - type: ndcg_at_1\n      value: 81.28999999999999\n    - type: ndcg_at_10\n      value: 88.442\n    - type: ndcg_at_100\n      value: 89.637\n    - type: ndcg_at_1000\n      value: 89.70700000000001\n    - type: ndcg_at_3\n      value: 85.55199999999999\n    - type: ndcg_at_5\n      value: 87.154\n    - type: precision_at_1\n      value: 81.28999999999999\n    - type: precision_at_10\n      value: 13.489999999999998\n    - type: precision_at_100\n      value: 1.54\n    - type: precision_at_1000\n      value: 0.157\n    - type: precision_at_3\n      value: 37.553\n    - type: precision_at_5\n      value: 24.708\n    - type: recall_at_1\n      value: 70.557\n    - type: recall_at_10\n      value: 95.645\n    - type: recall_at_100\n      value: 99.693\n    - type: recall_at_1000\n      value: 99.995\n    - type: recall_at_3\n      value: 87.359\n    - type: recall_at_5\n      value: 91.89699999999999\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering\n      name: MTEB RedditClustering\n      config: default\n      split: test\n      revision: 24640382cdbf8abc73003fb0fa6d111a705499eb\n    metrics:\n    - type: v_measure\n      value: 63.65060114776209\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering-p2p\n      name: MTEB RedditClusteringP2P\n      config: default\n      split: test\n      revision: 282350215ef01743dc01b456c7f5241fa8937f16\n    metrics:\n    - type: v_measure\n      value: 64.63271250680617\n  - task:\n      type: Retrieval\n    dataset:\n      type: scidocs\n      name: MTEB SCIDOCS\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 4.263\n    - type: map_at_10\n      value: 10.801\n    - type: map_at_100\n      value: 12.888\n    - type: map_at_1000\n      value: 13.224\n    - type: map_at_3\n      value: 7.362\n    - type: map_at_5\n      value: 9.149000000000001\n    - type: mrr_at_1\n      value: 21\n    - type: mrr_at_10\n      value: 31.416\n    - type: mrr_at_100\n      value: 32.513\n    - type: mrr_at_1000\n      value: 32.58\n    - type: mrr_at_3\n      value: 28.116999999999997\n    - type: mrr_at_5\n      value: 29.976999999999997\n    - type: ndcg_at_1\n      value: 21\n    - type: ndcg_at_10\n      value: 18.551000000000002\n    - type: ndcg_at_100\n      value: 26.657999999999998\n    - type: ndcg_at_1000\n      value: 32.485\n    - type: ndcg_at_3\n      value: 16.834\n    - type: ndcg_at_5\n      value: 15.204999999999998\n    - type: precision_at_1\n      value: 21\n    - type: precision_at_10\n      value: 9.84\n    - type: precision_at_100\n      value: 2.16\n    - type: precision_at_1000\n      value: 0.35500000000000004\n    - type: precision_at_3\n      value: 15.667\n    - type: precision_at_5\n      value: 13.62\n    - type: recall_at_1\n      value: 4.263\n    - type: recall_at_10\n      value: 19.922\n    - type: recall_at_100\n      value: 43.808\n    - type: recall_at_1000\n      value: 72.14500000000001\n    - type: recall_at_3\n      value: 9.493\n    - type: recall_at_5\n      value: 13.767999999999999\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sickr-sts\n      name: MTEB SICK-R\n      config: default\n      split: test\n      revision: a6ea5a8cab320b040a23452cc28066d9beae2cee\n    metrics:\n    - type: cos_sim_spearman\n      value: 81.27446313317233\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts12-sts\n      name: MTEB STS12\n      config: default\n      split: test\n      revision: a0d554a64d88156834ff5ae9920b964011b16384\n    metrics:\n    - type: cos_sim_spearman\n      value: 76.27963301217527\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts13-sts\n      name: MTEB STS13\n      config: default\n      split: test\n      revision: 7e90230a92c190f1bf69ae9002b8cea547a64cca\n    metrics:\n    - type: cos_sim_spearman\n      value: 88.18495048450949\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts14-sts\n      name: MTEB STS14\n      config: default\n      split: test\n      revision: 6031580fec1f6af667f0bd2da0a551cf4f0b2375\n    metrics:\n    - type: cos_sim_spearman\n      value: 81.91982338692046\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts15-sts\n      name: MTEB STS15\n      config: default\n      split: test\n      revision: ae752c7c21bf194d8b67fd573edf7ae58183cbe3\n    metrics:\n    - type: cos_sim_spearman\n      value: 89.00896818385291\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts16-sts\n      name: MTEB STS16\n      config: default\n      split: test\n      revision: 4d8694f8f0e0100860b497b999b3dbed754a0513\n    metrics:\n    - type: cos_sim_spearman\n      value: 85.48814644586132\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-en)\n      config: en-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_spearman\n      value: 90.30116926966582\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (en)\n      config: en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_spearman\n      value: 67.74132963032342\n  - task:\n      type: STS\n    dataset:\n      type: mteb/stsbenchmark-sts\n      name: MTEB STSBenchmark\n      config: default\n      split: test\n      revision: b0fddb56ed78048fa8b90373c8a3cfc37b684831\n    metrics:\n    - type: cos_sim_spearman\n      value: 86.87741355780479\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/scidocs-reranking\n      name: MTEB SciDocsRR\n      config: default\n      split: test\n      revision: d3c5e1fc0b855ab6097bf1cda04dd73947d7caab\n    metrics:\n    - type: map\n      value: 82.0019012295875\n    - type: mrr\n      value: 94.70267024188593\n  - task:\n      type: Retrieval\n    dataset:\n      type: scifact\n      name: MTEB SciFact\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 50.05\n    - type: map_at_10\n      value: 59.36\n    - type: map_at_100\n      value: 59.967999999999996\n    - type: map_at_1000\n      value: 60.023\n    - type: map_at_3\n      value: 56.515\n    - type: map_at_5\n      value: 58.272999999999996\n    - type: mrr_at_1\n      value: 53\n    - type: mrr_at_10\n      value: 61.102000000000004\n    - type: mrr_at_100\n      value: 61.476\n    - type: mrr_at_1000\n      value: 61.523\n    - type: mrr_at_3\n      value: 58.778\n    - type: mrr_at_5\n      value: 60.128\n    - type: ndcg_at_1\n      value: 53\n    - type: ndcg_at_10\n      value: 64.43100000000001\n    - type: ndcg_at_100\n      value: 66.73599999999999\n    - type: ndcg_at_1000\n      value: 68.027\n    - type: ndcg_at_3\n      value: 59.279\n    - type: ndcg_at_5\n      value: 61.888\n    - type: precision_at_1\n      value: 53\n    - type: precision_at_10\n      value: 8.767\n    - type: precision_at_100\n      value: 1.01\n    - type: precision_at_1000\n      value: 0.11100000000000002\n    - type: precision_at_3\n      value: 23.444000000000003\n    - type: precision_at_5\n      value: 15.667\n    - type: recall_at_1\n      value: 50.05\n    - type: recall_at_10\n      value: 78.511\n    - type: recall_at_100\n      value: 88.5\n    - type: recall_at_1000\n      value: 98.333\n    - type: recall_at_3\n      value: 64.117\n    - type: recall_at_5\n      value: 70.867\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/sprintduplicatequestions-pairclassification\n      name: MTEB SprintDuplicateQuestions\n      config: default\n      split: test\n      revision: d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46\n    metrics:\n    - type: cos_sim_accuracy\n      value: 99.72178217821782\n    - type: cos_sim_ap\n      value: 93.0728601593541\n    - type: cos_sim_f1\n      value: 85.6727976766699\n    - type: cos_sim_precision\n      value: 83.02063789868667\n    - type: cos_sim_recall\n      value: 88.5\n    - type: dot_accuracy\n      value: 99.72178217821782\n    - type: dot_ap\n      value: 93.07287396168348\n    - type: dot_f1\n      value: 85.6727976766699\n    - type: dot_precision\n      value: 83.02063789868667\n    - type: dot_recall\n      value: 88.5\n    - type: euclidean_accuracy\n      value: 99.72178217821782\n    - type: euclidean_ap\n      value: 93.07285657982895\n    - type: euclidean_f1\n      value: 85.6727976766699\n    - type: euclidean_precision\n      value: 83.02063789868667\n    - type: euclidean_recall\n      value: 88.5\n    - type: manhattan_accuracy\n      value: 99.72475247524753\n    - type: manhattan_ap\n      value: 93.02792973059809\n    - type: manhattan_f1\n      value: 85.7727737973388\n    - type: manhattan_precision\n      value: 87.84067085953879\n    - type: manhattan_recall\n      value: 83.8\n    - type: max_accuracy\n      value: 99.72475247524753\n    - type: max_ap\n      value: 93.07287396168348\n    - type: max_f1\n      value: 85.7727737973388\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering\n      name: MTEB StackExchangeClustering\n      config: default\n      split: test\n      revision: 6cbc1f7b2bc0622f2e39d2c77fa502909748c259\n    metrics:\n    - type: v_measure\n      value: 68.77583615550819\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering-p2p\n      name: MTEB StackExchangeClusteringP2P\n      config: default\n      split: test\n      revision: 815ca46b2622cec33ccafc3735d572c266efdb44\n    metrics:\n    - type: v_measure\n      value: 36.151636938606956\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/stackoverflowdupquestions-reranking\n      name: MTEB StackOverflowDupQuestions\n      config: default\n      split: test\n      revision: e185fbe320c72810689fc5848eb6114e1ef5ec69\n    metrics:\n    - type: map\n      value: 52.16607939471187\n    - type: mrr\n      value: 52.95172046091163\n  - task:\n      type: Summarization\n    dataset:\n      type: mteb/summeval\n      name: MTEB SummEval\n      config: default\n      split: test\n      revision: cda12ad7615edc362dbf25a00fdd61d3b1eaf93c\n    metrics:\n    - type: cos_sim_pearson\n      value: 31.314646669495666\n    - type: cos_sim_spearman\n      value: 31.83562491439455\n    - type: dot_pearson\n      value: 31.314590842874157\n    - type: dot_spearman\n      value: 31.83363065810437\n  - task:\n      type: Retrieval\n    dataset:\n      type: trec-covid\n      name: MTEB TRECCOVID\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 0.198\n    - type: map_at_10\n      value: 1.3010000000000002\n    - type: map_at_100\n      value: 7.2139999999999995\n    - type: map_at_1000\n      value: 20.179\n    - type: map_at_3\n      value: 0.528\n    - type: map_at_5\n      value: 0.8019999999999999\n    - type: mrr_at_1\n      value: 72\n    - type: mrr_at_10\n      value: 83.39999999999999\n    - type: mrr_at_100\n      value: 83.39999999999999\n    - type: mrr_at_1000\n      value: 83.39999999999999\n    - type: mrr_at_3\n      value: 81.667\n    - type: mrr_at_5\n      value: 83.06700000000001\n    - type: ndcg_at_1\n      value: 66\n    - type: ndcg_at_10\n      value: 58.059000000000005\n    - type: ndcg_at_100\n      value: 44.316\n    - type: ndcg_at_1000\n      value: 43.147000000000006\n    - type: ndcg_at_3\n      value: 63.815999999999995\n    - type: ndcg_at_5\n      value: 63.005\n    - type: precision_at_1\n      value: 72\n    - type: precision_at_10\n      value: 61.4\n    - type: precision_at_100\n      value: 45.62\n    - type: precision_at_1000\n      value: 19.866\n    - type: precision_at_3\n      value: 70\n    - type: precision_at_5\n      value: 68.8\n    - type: recall_at_1\n      value: 0.198\n    - type: recall_at_10\n      value: 1.517\n    - type: recall_at_100\n      value: 10.587\n    - type: recall_at_1000\n      value: 41.233\n    - type: recall_at_3\n      value: 0.573\n    - type: recall_at_5\n      value: 0.907\n  - task:\n      type: Retrieval\n    dataset:\n      type: webis-touche2020\n      name: MTEB Touche2020\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 1.894\n    - type: map_at_10\n      value: 8.488999999999999\n    - type: map_at_100\n      value: 14.445\n    - type: map_at_1000\n      value: 16.078\n    - type: map_at_3\n      value: 4.589\n    - type: map_at_5\n      value: 6.019\n    - type: mrr_at_1\n      value: 22.448999999999998\n    - type: mrr_at_10\n      value: 39.82\n    - type: mrr_at_100\n      value: 40.752\n    - type: mrr_at_1000\n      value: 40.771\n    - type: mrr_at_3\n      value: 34.354\n    - type: mrr_at_5\n      value: 37.721\n    - type: ndcg_at_1\n      value: 19.387999999999998\n    - type: ndcg_at_10\n      value: 21.563\n    - type: ndcg_at_100\n      value: 33.857\n    - type: ndcg_at_1000\n      value: 46.199\n    - type: ndcg_at_3\n      value: 22.296\n    - type: ndcg_at_5\n      value: 21.770999999999997\n    - type: precision_at_1\n      value: 22.448999999999998\n    - type: precision_at_10\n      value: 19.796\n    - type: precision_at_100\n      value: 7.142999999999999\n    - type: precision_at_1000\n      value: 1.541\n    - type: precision_at_3\n      value: 24.490000000000002\n    - type: precision_at_5\n      value: 22.448999999999998\n    - type: recall_at_1\n      value: 1.894\n    - type: recall_at_10\n      value: 14.931\n    - type: recall_at_100\n      value: 45.524\n    - type: recall_at_1000\n      value: 83.243\n    - type: recall_at_3\n      value: 5.712\n    - type: recall_at_5\n      value: 8.386000000000001\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/toxic_conversations_50k\n      name: MTEB ToxicConversationsClassification\n      config: default\n      split: test\n      revision: d7c0de2777da35d6aae2200a62c6e0e5af397c4c\n    metrics:\n    - type: accuracy\n      value: 71.049\n    - type: ap\n      value: 13.85116971310922\n    - type: f1\n      value: 54.37504302487686\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/tweet_sentiment_extraction\n      name: MTEB TweetSentimentExtractionClassification\n      config: default\n      split: test\n      revision: d604517c81ca91fe16a244d1248fc021f9ecee7a\n    metrics:\n    - type: accuracy\n      value: 64.1312959818902\n    - type: f1\n      value: 64.11413877009383\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/twentynewsgroups-clustering\n      name: MTEB TwentyNewsgroupsClustering\n      config: default\n      split: test\n      revision: 6125ec4e24fa026cec8a478383ee943acfbd5449\n    metrics:\n    - type: v_measure\n      value: 54.13103431861502\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/twittersemeval2015-pairclassification\n      name: MTEB TwitterSemEval2015\n      config: default\n      split: test\n      revision: 70970daeab8776df92f5ea462b6173c0b46fd2d1\n    metrics:\n    - type: cos_sim_accuracy\n      value: 87.327889372355\n    - type: cos_sim_ap\n      value: 77.42059895975699\n    - type: cos_sim_f1\n      value: 71.02706903250873\n    - type: cos_sim_precision\n      value: 69.75324344950394\n    - type: cos_sim_recall\n      value: 72.34828496042216\n    - type: dot_accuracy\n      value: 87.327889372355\n    - type: dot_ap\n      value: 77.4209479346677\n    - type: dot_f1\n      value: 71.02706903250873\n    - type: dot_precision\n      value: 69.75324344950394\n    - type: dot_recall\n      value: 72.34828496042216\n    - type: euclidean_accuracy\n      value: 87.327889372355\n    - type: euclidean_ap\n      value: 77.42096495861037\n    - type: euclidean_f1\n      value: 71.02706903250873\n    - type: euclidean_precision\n      value: 69.75324344950394\n    - type: euclidean_recall\n      value: 72.34828496042216\n    - type: manhattan_accuracy\n      value: 87.31000774870358\n    - type: manhattan_ap\n      value: 77.38930750711619\n    - type: manhattan_f1\n      value: 71.07935314027831\n    - type: manhattan_precision\n      value: 67.70957726295677\n    - type: manhattan_recall\n      value: 74.80211081794195\n    - type: max_accuracy\n      value: 87.327889372355\n    - type: max_ap\n      value: 77.42096495861037\n    - type: max_f1\n      value: 71.07935314027831\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/twitterurlcorpus-pairclassification\n      name: MTEB TwitterURLCorpus\n      config: default\n      split: test\n      revision: 8b6510b0b1fa4e4c4f879467980e9be563ec1cdf\n    metrics:\n    - type: cos_sim_accuracy\n      value: 89.58939729110878\n    - type: cos_sim_ap\n      value: 87.17594155025475\n    - type: cos_sim_f1\n      value: 79.21146953405018\n    - type: cos_sim_precision\n      value: 76.8918527109307\n    - type: cos_sim_recall\n      value: 81.67539267015707\n    - type: dot_accuracy\n      value: 89.58939729110878\n    - type: dot_ap\n      value: 87.17593963273593\n    - type: dot_f1\n      value: 79.21146953405018\n    - type: dot_precision\n      value: 76.8918527109307\n    - type: dot_recall\n      value: 81.67539267015707\n    - type: euclidean_accuracy\n      value: 89.58939729110878\n    - type: euclidean_ap\n      value: 87.17592466925834\n    - type: euclidean_f1\n      value: 79.21146953405018\n    - type: euclidean_precision\n      value: 76.8918527109307\n    - type: euclidean_recall\n      value: 81.67539267015707\n    - type: manhattan_accuracy\n      value: 89.62626615438352\n    - type: manhattan_ap\n      value: 87.16589873161546\n    - type: manhattan_f1\n      value: 79.25143598295348\n    - type: manhattan_precision\n      value: 76.39494177323712\n    - type: manhattan_recall\n      value: 82.32984293193716\n    - type: max_accuracy\n      value: 89.62626615438352\n    - type: max_ap\n      value: 87.17594155025475\n    - type: max_f1\n      value: 79.25143598295348\n---\n\n# hkunlp/instructor-large\nWe introduce **Instructor**👨‍🏫, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) ***by simply providing the task instruction, without any finetuning***. Instructor👨‍ achieves sota on 70 diverse embedding tasks ([MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard))!\nThe model is easy to use with **our customized** `sentence-transformer` library. For more details, check out [our paper](https://arxiv.org/abs/2212.09741) and [project page](https://instructor-embedding.github.io/)! \n\n**************************** **Updates** ****************************\n\n* 12/28: We released a new [checkpoint](https://huggingface.co/hkunlp/instructor-large) trained with hard negatives, which gives better performance.\n* 12/21: We released our [paper](https://arxiv.org/abs/2212.09741), [code](https://github.com/HKUNLP/instructor-embedding), [checkpoint](https://huggingface.co/hkunlp/instructor-large) and [project page](https://instructor-embedding.github.io/)! Check them out!\n\n## Quick start\n<hr />\n\n## Installation\n```bash\npip install InstructorEmbedding\n```\n\n## Compute your customized embeddings\nThen you can use the model like this to calculate domain-specific and task-aware embeddings:\n```python\nfrom InstructorEmbedding import INSTRUCTOR\nmodel = INSTRUCTOR(''hkunlp/instructor-large'')\nsentence = "3D ActionSLAM: wearable person tracking in multi-floor environments"\ninstruction = "Represent the Science title:"\nembeddings = model.encode([[instruction,sentence]])\nprint(embeddings)\n```\n\n## Use cases\n<hr />\n\n## Calculate embeddings for your customized texts\nIf you want to calculate customized embeddings for specific sentences, you may follow the unified template to write instructions: \n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Represent the `domain` `text_type` for `task_objective`:\n* `domain` is optional, and it specifies the domain of the text, e.g., science, finance, medicine, etc.\n* `text_type` is required, and it specifies the encoding unit, e.g., sentence, document, paragraph, etc.\n* `task_objective` is optional, and it specifies the objective of embedding, e.g., retrieve a document, classify the sentence, etc.\n\n## Calculate Sentence similarities\nYou can further use the model to compute similarities between two groups of sentences, with **customized embeddings**.\n```python\nfrom sklearn.metrics.pairwise import cosine_similarity\nsentences_a = [[''Represent the Science sentence: '',''Parton energy loss in QCD matter''], \n               [''Represent the Financial statement: '',''The Federal Reserve on Wednesday raised its benchmark interest rate.'']]\nsentences_b = [[''Represent the Science sentence: '',''The Chiral Phase Transition in Dissipative Dynamics''],\n               [''Represent the Financial statement: '',''The funds rose less than 0.5 per cent on Friday'']]\nembeddings_a = model.encode(sentences_a)\nembeddings_b = model.encode(sentences_b)\nsimilarities = cosine_similarity(embeddings_a,embeddings_b)\nprint(similarities)\n```\n\n## Information Retrieval\nYou can also use **customized embeddings** for information retrieval.\n```python\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nquery  = [[''Represent the Wikipedia question for retrieving supporting documents: '',''where is the food stored in a yam plant'']]\ncorpus = [[''Represent the Wikipedia document for retrieval: '',''Capitalism has been dominant in the Western world since the end of feudalism, but most feel[who?] that the term "mixed economies" more precisely describes most contemporary economies, due to their containing both private-owned and state-owned enterprises. In capitalism, prices determine the demand-supply scale. For example, higher demand for certain goods and services lead to higher prices and lower demand for certain goods lead to lower prices.''],\n          [''Represent the Wikipedia document for retrieval: '',"The disparate impact theory is especially controversial under the Fair Housing Act because the Act regulates many activities relating to housing, insurance, and mortgage loansâ€”and some scholars have argued that the theory''s use under the Fair Housing Act, combined with extensions of the Community Reinvestment Act, contributed to rise of sub-prime lending and the crash of the U.S. housing market and ensuing global economic recession"],\n          [''Represent the Wikipedia document for retrieval: '',''Disparate impact in United States labor law refers to practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another, even though rules applied by employers or landlords are formally neutral. Although the protected classes vary by statute, most federal civil rights laws protect based on race, color, religion, national origin, and sex as protected traits, and some laws include disability status and other traits as well.'']]\nquery_embeddings = model.encode(query)\ncorpus_embeddings = model.encode(corpus)\nsimilarities = cosine_similarity(query_embeddings,corpus_embeddings)\nretrieved_doc_id = np.argmax(similarities)\nprint(retrieved_doc_id)\n```\n\n## Clustering\nUse **customized embeddings** for clustering texts in groups.\n```python\nimport sklearn.cluster\nsentences = [[''Represent the Medicine sentence for clustering: '',''Dynamical Scalar Degree of Freedom in Horava-Lifshitz Gravity''],\n             [''Represent the Medicine sentence for clustering: '',''Comparison of Atmospheric Neutrino Flux Calculations at Low Energies''],\n             [''Represent the Medicine sentence for clustering: '',''Fermion Bags in the Massive Gross-Neveu Model''],\n             [''Represent the Medicine sentence for clustering: '',"QCD corrections to Associated t-tbar-H production at the Tevatron"],\n             [''Represent the Medicine sentence for clustering: '',''A New Analysis of the R Measurements: Resonance Parameters of the Higher,  Vector States of Charmonium'']]\nembeddings = model.encode(sentences)\nclustering_model = sklearn.cluster.MiniBatchKMeans(n_clusters=2)\nclustering_model.fit(embeddings)\ncluster_assignment = clustering_model.labels_\nprint(cluster_assignment)\n```', '{"pipeline_tag":"sentence-similarity","library_name":"sentence-transformers","framework":"sentence-transformers","params":null,"storage_bytes":6712370802,"files_count":14,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["T5EncoderModel"],"model_type":"t5","tokenizer_config":{"eos_token":"</s>","pad_token":"<pad>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:HKUNLP:instructor-embedding","source_url":"https://github.com/HKUNLP/instructor-embedding"},{"type":"based_on_paper","target_id":"arxiv:2212.09741","source_url":"https://arxiv.org/abs/2212.09741"}]', NULL, 'Apache-2.0', 'approved', 77.2, '49ab2746474e1f57324bb946d0e14c63', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-YoungMasterFromSect-Trauter-LoRAs', 'huggingface--youngmasterfromsect--trauter-loras', 'Trauter_LoRAs', 'YoungMasterFromSect', '--- tags: - anime --- NOTICE: My LoRAs require high amount of tags to look good, I will fix this later on and update all of my LoRAs if everything works out. - Overview - Installation - Usage - SocialMedia - Plans for the future Welcome to the place where I host my LoRAs. In short, LoRA is just a checkpoint trained on specific artstyle/subject that you load into your WebUI, that can be used with other models. Although you can use it with any model, the effects of LoRA will vary between them. ...', '["anime","region:us"]', 'other', 522, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- anime\n---\nNOTICE: My LoRAs require high amount of tags to look good, I will fix this later on and update all of my LoRAs if everything works out.\n\n# General Information\n\n- [Overview](#overview)\n- [Installation](#installation)\n- [Usage](#usage)\n- [SocialMedia](#socialmedia)\n- [Plans for the future](#plans-for-the-future)\n\n# Overview\n\nWelcome to the place where I host my LoRAs. In short, LoRA is just a checkpoint trained on specific artstyle/subject that you load into your WebUI, that can be used with other models.  \nAlthough you can use it with any model, the effects of LoRA will vary between them.\nMost of the previews use models that come from [WarriorMama777](https://huggingface.co/WarriorMama777/OrangeMixs) .  \nFor more information about them, you can visit the original LoRA repository: https://github.com/cloneofsimo/lora  \nEvery images posted here, or on the other sites have metadata in them that you can use in PNG Info tab in your WebUI to get access to the prompt of the image.  \nEverything I do here is for free of charge!  \nI don''t guarantee that my LoRAs will give you good results, if you think they are bad, don''t use them.\n\n# Installation\n\nTo use them in your WebUI, please install the extension linked under, following the installation guide:  \nhttps://github.com/kohya-ss/sd-webui-additional-networks#installation\n\n# Usage\n\nAll of my LoRAs are to be used with their original danbooru tag. For example:  \n```\nasuna \(blue archive\)\n```\nMy LoRAs will have sufixes that will tell you how much they were trained. Either by using words like "soft" and "hard",  \nwhere soft stands for lower amount of training and hard for higher amount of training.  \n\nMore trained LoRA is harder to modify but provides higher consistency in details and original outfits,  \nwhile lower trained one will be more flexible, but may get details wrong.\n\nAll the LoRAs that aren''t marked with PRUNED require tagging everything about the character to get the likness of it.\nYou have to tag every part of the character like: eyes,hair,breasts,accessories,special features,etc...\n\nIn theory, this should allow LoRAs to be more flexible, but it requires to prompt those things always, because character tag doesn''t have those features baked into it.\nFrom 1/16 I will test releasing pruned versions which will not require those prompting those things.\n\nThe usage of them is also explained in this guide:  \nhttps://github.com/kohya-ss/sd-webui-additional-networks#how-to-use\n\n# SocialMedia\n\nHere are some places where you can find my other stuff that I post, or if you feel like buying me a coffee:  \n[Twitter](https://twitter.com/Trauter8)  \n[Pixiv](https://www.pixiv.net/en/users/88153216)  \n[Buymeacoffee](https://www.buymeacoffee.com/Trauter) \n\n# Plans for the future\n\n- Remake all of my LoRAs into pruned versions which will be more user friendly and easier to use, and use 768x768 res. for training and better Learning Rate\n- After finishing all of my LoRA that I want to make, go over the old ones and try to make them better.\n- Accept suggestions for almost every character.\n- Maybe get motivation to actually tag outfits.\n  \n# LoRAs\n\n- [Genshin Impact](#genshin-impact)\n  - [Eula](#eula)\n  - [Barbara](#barbara)\n  - [Diluc](#diluc)\n  - [Mona](#mona)\n  - [Rosaria](#rosaria)\n  - [Yae Miko](#yae-miko)\n  - [Raiden Shogun](#raiden-shogun)\n  - [Kujou Sara](#kujou-sara)\n  - [Shenhe](#shenhe)\n  - [Yelan](#yelan)\n  - [Jean](#jean)\n  - [Lisa](#lisa)\n  - [Zhongli](#zhongli)\n  - [Yoimiya](#yoimiya)\n- [Blue Archive](#blue-archive)\n  - [Rikuhachima Aru](#rikuhachima-aru)\n  - [Ichinose Asuna](#ichinose-asuna)\n- [Fate Grand Order](#fate-grand-order)\n  - [Minamoto-no-Raikou](#minamoto-no-raikou)\n- [Misc. Characters](#misc.-characters)\n  - [Aponia](#aponia)\n  - [Reisalin Stout](#reisalin-stout)\n- [Artstyles](#artstyles)\n  - [Pozer](#pozer)\n\n\n# Genshin Impact\n  \n  - # Eula\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/1.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/1.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305293076)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Eula)\n  - # Barbara\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/bar.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/bar.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305435137)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Barbara)\n  - # Diluc\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/dil.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/dil.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305427945)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Diluc)\n  - # Mona\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/mon.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/mon.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305428050)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Mona)\n  - # Rosaria\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/ros.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/ros.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305428015)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Rosaria)\n  - # Yae Miko\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/yae.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/yae.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305448948)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/yae%20miko)\n  - # Raiden Shogun\n  - [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/ra.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/ra.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, raiden shogun, 1girl, breasts, solo, cleavage, kimono, bangs, sash, mole, obi, tassel, blush, large breasts, purple eyes, japanese clothes, long hair, looking at viewer, hand on own chest, hair ornament, purple hair, bridal gauntlets, closed mouth, purple kimono, blue hair, mole under eye, shoulder armor, long sleeves, wide sleeves, mitsudomoe (shape), tomoe (symbol), cowboy shot\nNegative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts,signature, watermark, username, blurry, artist name, from behind\nSteps: 30, Sampler: DPM++ 2M Karras, CFG scale: 4.5, Seed: 2544310848, Size: 704x384, Model hash: 2bba3136, Denoising strength: 0.5, Clip skip: 2, ENSD: 31337, Hires upscale: 2.05, Hires upscaler: 4x_foolhardy_Remacri\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305313633)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Raiden%20Shogun)\n  - # Kujou Sara\n  - [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/ku.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/ku.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, kujou sara, 1girl, solo, mask, gloves, bangs, bodysuit, gradient, sidelocks, signature, yellow eyes, bird mask, mask on head, looking at viewer, short hair, black hair, detached sleeves, simple background, japanese clothes, black gloves, black bodysuit, wide sleeves, white background, upper body, gradient background, closed mouth, hair ornament, artist name, elbow gloves\nNegative prompt: (worst quality, low quality:1.4)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 3966121353, Size: 512x768, Model hash: 931f9552, Denoising strength: 0.5, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires steps: 20, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305311498)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Kujou%20Sara)\n  - # Shenhe\n  - [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/sh.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/sh.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, shenhe \(genshin impact\), 1girl, solo, breasts, bodysuit, tassel, gloves, bangs, braid, outdoors, bird, jewelry, earrings, sky, breast curtain, long hair, hair over one eye, covered navel, blue eyes, looking at viewer, hair ornament, large breasts, shoulder cutout, clothing cutout, very long hair, hip vent, braided ponytail, partially fingerless gloves, black bodysuit, tassel earrings, black gloves, gold trim, cowboy shot, white hair\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 22, Sampler: DPM++ SDE Karras, CFG scale: 6.5, Seed: 573332187, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 2, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305307599)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Shenhe)\n  - # Yelan\n  - [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/10.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/10.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, yelan \(genshin impact\), 1girl, breasts, solo, bangs, armpits, smile, sky, cleavage, jewelry, gloves, jacket, dice, mole, cloud, grin, dress, blush, earrings, thighs, tassel, sleeveless, day, outdoors, large breasts, looking at viewer, green eyes, arms up, short hair, blue hair, vision (genshin impact), fur trim, white jacket, blue sky, mole on breast, arms behind head, bob cut, multicolored hair, black hair, fur-trimmed jacket, elbow gloves, bare shoulders, blue dress, parted lips, diagonal bangs, clothing cutout, pelvic curtain, asymmetrical gloves\nNegative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts,signature, watermark, username, blurry, artist name\nSteps: 23, Sampler: DPM++ SDE Karras, CFG scale: 6.5, Seed: 575500509, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.58, Clip skip: 2, ENSD: 31337, Hires upscale: 2.4, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305296897)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Yelan)\n  - # Jean\n  - [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/333.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/333.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, jean \(genshin impact\), 1girl, breasts, solo, cleavage, strapless, smile, ponytail, bangs, jewelry, earrings, bow, capelet, signature, sidelocks, cape, corset, shiny, blonde hair, long hair, upper body, detached sleeves, purple eyes, hair between eyes, hair bow, parted lips, looking to the side, large breasts, detached collar, medium breasts, blue capelet, white background, black bow, blue eyes, bare shoulders, simple background\nNegative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts,signature, watermark, username, blurry, artist name, (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 22, Sampler: DPM++ SDE Karras, CFG scale: 7.5, Seed: 32930253, Size: 512x768, Model hash: ffa7b160, Denoising strength: 0.59, Clip skip: 2, ENSD: 31337, Hires upscale: 1.85, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305307594)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Jean)\n  - # Lisa\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/lis.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/lis.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, lisa \(genshin impact\), 1girl, solo, hat, breasts, gloves, cleavage, flower, smile, bangs, dress, rose, jewelry, witch, capelet, green eyes, witch hat, brown hair, purple headwear, looking at viewer, white background, large breasts, long hair, simple background, black gloves, purple flower, hair between eyes, upper body, purple rose, parted lips, purple capelet, hat flower, multicolored dress, hair ornament, multicolored clothes, vision (genshin impact)\nNegative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts,signature, watermark, username, blurry, artist name, worst quality, low quality, extra digits, loli, loli face\nSteps: 23, Sampler: DPM++ SDE Karras, CFG scale: 6.5, Seed: 350134479, Size: 512x768, Model hash: ffa7b160, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.85, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305290865)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Lisa)\n\n  - # Zhongli\n[<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/zho.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/zho.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, zhongli \(genshin  impact\), solo, 1boy, bangs, jewelry, tassel, earrings, ponytail, low ponytail, gloves, necktie, jacket, shirt, formal, petals, suit, makeup, eyeliner, eyeshadow, male focus, long hair, brown hair, multicolored hair, long sleeves, tassel earrings, single earring, collared shirt, hair between eyes, black gloves, closed mouth, yellow eyes, gradient hair, orange hair, simple background\nNegative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts,signature, watermark, username, blurry, artist name, worst quality, low quality, extra digits, loli, loli face\nSteps: 22, Sampler: DPM++ SDE Karras, CFG scale: 7, Seed: 88418604, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.58, Clip skip: 2, ENSD: 31337, Hires upscale: 2, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305311423)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Zhongli)\n    \n  - # Yoimiya\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/Yoi.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/Yoi.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305448498)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Yoimiya)\n\n# Blue Archive\n  - # Rikuhachima Aru\n  - [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/22.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/22.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\naru \(blue archive\), masterpiece, best quality, 1girl, solo, horns, skirt, gloves, shirt, halo, window, breasts, blush, sweatdrop, ribbon, coat, bangs, :d, smile, indoors, standing, plant, thighs, sweat, jacket, day, sunlight, long hair, white shirt, white gloves, black skirt, looking at viewer, open mouth, long sleeves, red ribbon, fur trim, neck ribbon, red hair, fur-trimmed coat, collared shirt, orange eyes, medium breasts, brown coat, hands up, side slit, coat on shoulders, v-shaped eyebrows, yellow eyes, potted plant, fur collar, shirt tucked in, demon horns, high-waist skirt, dress shirt\nNegative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts,signature, watermark, username, blurry, artist name, (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 22, Sampler: DPM++ SDE Karras, CFG scale: 6.5, Seed: 1190296645, Size: 512x768, Model hash: ffa7b160, Denoising strength: 0.58, Clip skip: 2, ENSD: 31337, Hires upscale: 1.85, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305293051)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Blue-Archive/Rikuhachima%20Aru)\n  - # Ichinose Asuna\n  - [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/asu.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/asu.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nphotorealistic, (hyperrealistic:1.2), (extremely detailed CG unity 8k wallpaper), (ultra-detailed), (mature female:1.2), masterpiece, best quality, asuna \(blue archive\), 1girl, breasts, solo, gloves, pantyhose, ass, leotard, smile, tail, halo, grin, blush, bangs, sideboob, highleg, standing, mole, strapless, ribbon, thighs, animal ears, playboy bunny, rabbit ears, long hair, white gloves, very long hair, large breasts, high heels, blue leotard, hair over one eye, fake animal ears, blue eyes, looking at viewer, white footwear, rabbit tail, official alternate costume, full body, elbow gloves, simple background, white background, absurdly long hair, bare shoulders, detached collar, thighband pantyhose, leaning forward, highleg leotard, strapless leotard, hair ribbon, brown pantyhose, black pantyhose, mole on breast, light brown hair, brown hair, looking back, fake tail\nNegative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts,signature, watermark, username, blurry, artist name, (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 22, Sampler: DPM++ SDE Karras, CFG scale: 6.5, Seed: 2052579935, Size: 512x768, Model hash: ffa7b160, Clip skip: 2, ENSD: 31337\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305292996)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Blue-Archive/Ichinose%20Asuna)\n# Fate Grand Order\n  - # Minamoto-no-Raikou\n  - [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/3.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/3.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmature female, masterpiece, best quality, minamoto no raikou \(fate\), 1girl, breasts, solo, bodysuit, gloves, bangs, smile, rope, heart, blush, thighs, armor, kote, long hair, purple hair, fingerless gloves, purple eyes, large breasts, very long hair, looking at viewer, parted bangs, ribbed sleeves, black gloves, arm guards, covered navel, low-tied long hair, purple bodysuit, japanese armor\nNegative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts,signature, watermark, username, blurry, artist name, (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 22, Sampler: DPM++ SDE Karras, CFG scale: 7.5, Seed: 3383453781, Size: 512x768, Model hash: ffa7b160, Denoising strength: 0.59, Clip skip: 2, ENSD: 31337, Hires upscale: 2, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305290900)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Fate-Grand-Order/Minamoto-no-Raikou)\n\n# Misc. Characters\n  \n  - # Aponia\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/apo.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/apo.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305445819)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Misc.%20Characters/Aponia)\n\n  - # Reisalin Stout\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/ryza.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/ryza.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305448553)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Misc.%20Characters/reisalin%20stout)\n\n# Artstyles\n  \n  - # Pozer\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/art.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/art.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305445399)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Artstyles/Pozer)', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":12414717335,"files_count":136,"spaces_count":1,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:cloneofsimo:lora","source_url":"https://github.com/cloneofsimo/lora"},{"type":"has_code","target_id":"github:kohya-ss:sd-webui-additional-networks","source_url":"https://github.com/kohya-ss/sd-webui-additional-networks#installation"},{"type":"has_code","target_id":"github:kohya-ss:sd-webui-additional-networks","source_url":"https://github.com/kohya-ss/sd-webui-additional-networks#how-to-use"}]', NULL, NULL, 'pending', 67.2, '9e580948744255b03b864522a16e4638', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-liuhaotian-llava-v1.5-7b', 'huggingface--liuhaotian--llava-v1.5-7b', 'llava-v1.5-7b', 'liuhaotian', '--- inference: false pipeline_tag: image-text-to-text --- <br> <br> **Model type:** LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data. It is an auto-regressive language model, based on the transformer architecture. **Model date:** LLaVA-v1.5-7B was trained in September 2023. **Paper or resources for more information:** https://llava-vl.github.io/ Llama 2 is licensed under the LLAMA 2 Community License, Copyright (c) Meta...', '["transformers","pytorch","llava","text-generation","image-text-to-text","region:us"]', 'image-text-to-text', 522, 449679, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/liuhaotian/llava-v1.5-7b","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ninference: false\npipeline_tag: image-text-to-text\n---\n\n<br>\n<br>\n\n# LLaVA Model Card\n\n## Model details\n\n**Model type:**\nLLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.\nIt is an auto-regressive language model, based on the transformer architecture.\n\n**Model date:**\nLLaVA-v1.5-7B was trained in September 2023.\n\n**Paper or resources for more information:**\nhttps://llava-vl.github.io/\n\n## License\nLlama 2 is licensed under the LLAMA 2 Community License, \nCopyright (c) Meta Platforms, Inc. All Rights Reserved.\n\n**Where to send questions or comments about the model:**\nhttps://github.com/haotian-liu/LLaVA/issues\n\n## Intended use\n**Primary intended uses:**\nThe primary use of LLaVA is research on large multimodal models and chatbots.\n\n**Primary intended users:**\nThe primary intended users of the model are researchers and hobbyists in computer vision, natural language processing, machine learning, and artificial intelligence.\n\n## Training dataset\n- 558K filtered image-text pairs from LAION/CC/SBU, captioned by BLIP.\n- 158K GPT-generated multimodal instruction-following data.\n- 450K academic-task-oriented VQA data mixture.\n- 40K ShareGPT data.\n\n## Evaluation dataset\nA collection of 12 benchmarks, including 5 academic VQA benchmarks and 7 recent benchmarks specifically proposed for instruction-following LMMs.', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":27087556472,"files_count":11,"spaces_count":59,"gated":false,"private":false,"config":{"architectures":["LlavaLlamaForCausalLM"],"model_type":"llava","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<s>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"</s>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false},"pad_token":null,"unk_token":{"__type":"AddedToken","content":"<unk>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false}}}}', '[]', '[{"type":"has_code","target_id":"github:haotian-liu:LLaVA","source_url":"https://github.com/haotian-liu/LLaVA"}]', NULL, NULL, 'pending', 37.2, '9d3546e718c23cfe5ea490281cf54adc', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-HKUSTAudio-Llasa-3B', 'huggingface--hkustaudio--llasa-3b', 'Llasa-3B', 'HKUSTAudio', '--- license: cc-by-nc-4.0 language: - zh - en base_model: - meta-llama/Llama-3.2-3B-Instruct tags: - Text-to-Speech pipeline_tag: text-to-speech --- **Update （2025-05-10):** Sometimes I find that top_p=0.95 and temperature=0.9 produce more stable results. **Update (2025-02-13):** Add Llasa finetune instruction. **Update (2025-02-07):** Our paper has been released! LLaSA: Scaling Train-Time and Inference-Time Compute for LLaMA-based Speech Synthesis - **Train from Scratch**: If you want to tra...', '["safetensors","llama","text-to-speech","text-to-speech","zh","en","arxiv:2502.04128","base_model:meta-llama/llama-3.2-3b-instruct","license:cc-by-nc-4.0","region:us"]', 'text-to-speech', 522, 1714, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/HKUSTAudio/Llasa-3B","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: cc-by-nc-4.0\nlanguage:\n- zh\n- en\nbase_model:\n- meta-llama/Llama-3.2-3B-Instruct\ntags:\n- Text-to-Speech\npipeline_tag: text-to-speech\n---\n\n[![arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/abs/2502.04128)\n\n**Update （2025-05-10):** Sometimes I find that top_p=0.95 and temperature=0.9 produce more stable results.\n\n\n**Update (2025-02-13):** Add [Llasa finetune instruction](https://github.com/zhenye234/LLaSA_training/tree/main/finetune).\n\n\n**Update (2025-02-07):** Our paper has been released!\n\n\nLLaSA: Scaling Train-Time and Inference-Time Compute for LLaMA-based Speech Synthesis \n\n\n- **Train from Scratch**: If you want to train the model from scratch, use the [LLaSA Training Repository](https://github.com/zhenye234/LLaSA_training).\n\n- **Scale for Test-Time Computation**: If you want to experiment with scaling for test-time computation, use the [LLaSA Testing Repository](https://github.com/zhenye234/LLaSA_inference).\n\n## Model Information\nOur model, Llasa, is a text-to-speech (TTS) system that extends the text-based LLaMA (1B,3B, and 8B) language model by incorporating speech tokens from the XCodec2 codebook,\n which contains 65,536 tokens. We trained Llasa on a dataset comprising 250,000 hours of Chinese-English speech data.\n The model is capable of generating speech **either solely from input text or by utilizing a given speech prompt.**  \n\n The method is seamlessly compatible with the Llama framework, making training TTS similar as training LLM (convert audios into single-codebook tokens and simply view it as a special language). It opens the possiblity of existing method for compression, acceleration and finetuning for LLM to be applied. \n\n\n\n## How to use\nInstall [XCodec2](https://huggingface.co/HKUSTAudio/xcodec2).  \n\n**1. Speech synthesis solely from input text**\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport soundfile as sf\n\nllasa_3b =''HKUSTAudio/Llasa-3B''\n\ntokenizer = AutoTokenizer.from_pretrained(llasa_3b)\nmodel = AutoModelForCausalLM.from_pretrained(llasa_3b)\nmodel.eval() \nmodel.to(''cuda'')\n\nfrom xcodec2.modeling_xcodec2 import XCodec2Model\n \nmodel_path = "HKUSTAudio/xcodec2"  \n \nCodec_model = XCodec2Model.from_pretrained(model_path)\nCodec_model.eval().cuda()   \n\ninput_text = ''Dealing with family secrets is never easy. Yet, sometimes, omission is a form of protection, intending to safeguard some from the harsh truths. One day, I hope you understand the reasons behind my actions. Until then, Anna, please, bear with me.''\n# input_text = ''突然，身边一阵笑声。我看着他们，意气风发地挺直了胸膛，甩了甩那稍显肉感的双臂，轻笑道："我身上的肉，是为了掩饰我爆棚的魅力，否则，岂不吓坏了你们呢？"''\ndef ids_to_speech_tokens(speech_ids):\n \n    speech_tokens_str = []\n    for speech_id in speech_ids:\n        speech_tokens_str.append(f"<|s_{speech_id}|>")\n    return speech_tokens_str\n\ndef extract_speech_ids(speech_tokens_str):\n \n    speech_ids = []\n    for token_str in speech_tokens_str:\n        if token_str.startswith(''<|s_'') and token_str.endswith(''|>''):\n            num_str = token_str[4:-2]\n\n            num = int(num_str)\n            speech_ids.append(num)\n        else:\n            print(f"Unexpected token: {token_str}")\n    return speech_ids\n\n#TTS start!\nwith torch.no_grad():\n \n    formatted_text = f"<|TEXT_UNDERSTANDING_START|>{input_text}<|TEXT_UNDERSTANDING_END|>"\n\n    # Tokenize the text\n    chat = [\n        {"role": "user", "content": "Convert the text to speech:" + formatted_text},\n        {"role": "assistant", "content": "<|SPEECH_GENERATION_START|>"}\n    ]\n\n    input_ids = tokenizer.apply_chat_template(\n        chat, \n        tokenize=True, \n        return_tensors=''pt'', \n        continue_final_message=True\n    )\n    input_ids = input_ids.to(''cuda'')\n    speech_end_id = tokenizer.convert_tokens_to_ids(''<|SPEECH_GENERATION_END|>'')\n\n    # Generate the speech autoregressively\n    outputs = model.generate(\n        input_ids,\n        max_length=2048,  # We trained our model with a max length of 2048\n        eos_token_id= speech_end_id ,\n        do_sample=True,    \n        top_p=1,           #  Adjusts the diversity of generated content\n        temperature=0.8,   #  Controls randomness in output\n    )\n    # Extract the speech tokens\n    generated_ids = outputs[0][input_ids.shape[1]:-1]\n\n    speech_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)   \n\n    # Convert  token <|s_23456|> to int 23456 \n    speech_tokens = extract_speech_ids(speech_tokens)\n\n    speech_tokens = torch.tensor(speech_tokens).cuda().unsqueeze(0).unsqueeze(0)\n\n    # Decode the speech tokens to speech waveform\n    gen_wav = Codec_model.decode_code(speech_tokens) \n \n\nsf.write("gen.wav", gen_wav[0, 0, :].cpu().numpy(), 16000)\n```\n\n**2. Speech synthesis utilizing a given speech prompt**\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport soundfile as sf\n\nllasa_3b =''HKUSTAudio/Llasa-3B''\n\ntokenizer = AutoTokenizer.from_pretrained(llasa_3b)\nmodel = AutoModelForCausalLM.from_pretrained(llasa_3b)\nmodel.eval() \nmodel.to(''cuda'')\n\nfrom xcodec2.modeling_xcodec2 import XCodec2Model\n \nmodel_path = "HKUSTAudio/xcodec2"  \n \nCodec_model = XCodec2Model.from_pretrained(model_path)\nCodec_model.eval().cuda()   \n# only 16khz speech support!\nprompt_wav, sr = sf.read("太乙真人.wav")   # you can find wav in Files\n#prompt_wav, sr = sf.read("Anna.wav") # English prompt\nprompt_wav = torch.from_numpy(prompt_wav).float().unsqueeze(0)  \n\nprompt_text ="对，这就是我万人敬仰的太乙真人，虽然有点婴儿肥，但也掩不住我逼人的帅气。"\n#promt_text = "A chance to leave him alone, but... No. She just wanted to see him again. Anna, you don''t know how it feels to lose a sister. Anna, I''m sorry, but your father asked me not to tell you anything."\ntarget_text = ''突然，身边一阵笑声。我看着他们，意气风发地挺直了胸膛，甩了甩那稍显肉感的双臂，轻笑道："我身上的肉，是为了掩饰我爆棚的魅力，否则，岂不吓坏了你们呢？"''\n#target_text = "Dealing with family secrets is never easy. Yet, sometimes, omission is a form of protection, intending to safeguard some from the harsh truths. One day, I hope you understand the reasons behind my actions. Until then, Anna, please, bear with me."\ninput_text = prompt_text   + target_text\n\ndef ids_to_speech_tokens(speech_ids):\n \n    speech_tokens_str = []\n    for speech_id in speech_ids:\n        speech_tokens_str.append(f"<|s_{speech_id}|>")\n    return speech_tokens_str\n\ndef extract_speech_ids(speech_tokens_str):\n \n    speech_ids = []\n    for token_str in speech_tokens_str:\n        if token_str.startswith(''<|s_'') and token_str.endswith(''|>''):\n            num_str = token_str[4:-2]\n\n            num = int(num_str)\n            speech_ids.append(num)\n        else:\n            print(f"Unexpected token: {token_str}")\n    return speech_ids\n\n#TTS start!\nwith torch.no_grad():\n    # Encode the prompt wav\n    vq_code_prompt = Codec_model.encode_code(input_waveform=prompt_wav)\n    print("Prompt Vq Code Shape:", vq_code_prompt.shape )   \n\n    vq_code_prompt = vq_code_prompt[0,0,:]\n    # Convert int 12345 to token <|s_12345|>\n    speech_ids_prefix = ids_to_speech_tokens(vq_code_prompt)\n\n    formatted_text = f"<|TEXT_UNDERSTANDING_START|>{input_text}<|TEXT_UNDERSTANDING_END|>"\n\n    # Tokenize the text and the speech prefix\n    chat = [\n        {"role": "user", "content": "Convert the text to speech:" + formatted_text},\n        {"role": "assistant", "content": "<|SPEECH_GENERATION_START|>" + ''''.join(speech_ids_prefix)}\n    ]\n\n    input_ids = tokenizer.apply_chat_template(\n        chat, \n        tokenize=True, \n        return_tensors=''pt'', \n        continue_final_message=True\n    )\n    input_ids = input_ids.to(''cuda'')\n    speech_end_id = tokenizer.convert_tokens_to_ids(''<|SPEECH_GENERATION_END|>'')\n\n    # Generate the speech autoregressively\n    outputs = model.generate(\n        input_ids,\n        max_length=2048,  # We trained our model with a max length of 2048\n        eos_token_id= speech_end_id ,\n        do_sample=True,\n        top_p=1,           \n        temperature=0.8,\n    )\n    # Extract the speech tokens\n    generated_ids = outputs[0][input_ids.shape[1]-len(speech_ids_prefix):-1]\n\n    speech_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)   \n\n    # Convert  token <|s_23456|> to int 23456 \n    speech_tokens = extract_speech_ids(speech_tokens)\n\n    speech_tokens = torch.tensor(speech_tokens).cuda().unsqueeze(0).unsqueeze(0)\n\n    # Decode the speech tokens to speech waveform\n    gen_wav = Codec_model.decode_code(speech_tokens) \n\n    # if only need the generated part\n    # gen_wav = gen_wav[:,:,prompt_wav.shape[1]:]\n\nsf.write("gen.wav", gen_wav[0, 0, :].cpu().numpy(), 16000)\n```\n\n\n## Disclaimer\n\nThis model is licensed under the CC BY-NC 4.0 License, which prohibits free commercial use because of ethics and privacy concerns; detected violations will result in legal consequences.\n\nThis codebase is strictly prohibited from being used for any illegal purposes in any country or region. Please refer to your local laws about DMCA and other related laws.', '{"pipeline_tag":"text-to-speech","library_name":null,"framework":null,"params":4009454592,"storage_bytes":8060170434,"files_count":13,"spaces_count":26,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama"}}', '[]', '[{"type":"has_code","target_id":"github:zhenye234:LLaSA_training","source_url":"https://github.com/zhenye234/LLaSA_training"},{"type":"has_code","target_id":"github:zhenye234:LLaSA_training","source_url":"https://github.com/zhenye234/LLaSA_training"},{"type":"has_code","target_id":"github:zhenye234:LLaSA_inference","source_url":"https://github.com/zhenye234/LLaSA_inference"},{"type":"based_on_paper","target_id":"arxiv:2502.04128","source_url":"https://arxiv.org/abs/2502.04128"}]', NULL, 'CC-BY-NC-4.0', 'approved', 62.2, '480685c99844a6278e864ae7c067f2ef', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-VL-8B-Instruct', 'huggingface--qwen--qwen3-vl-8b-instruct', 'Qwen3-VL-8B-Instruct', 'Qwen', '--- license: apache-2.0 pipeline_tag: image-text-to-text library_name: transformers --- <a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> Meet Qwen3-VL — the most powerful vision-language model in the Qwen series to date. This generation delivers comprehensive upgrades across the board: superior text understanding ...', '["transformers","safetensors","qwen3_vl","image-to-text","image-text-to-text","conversational","arxiv:2505.09388","arxiv:2502.13923","arxiv:2409.12191","arxiv:2308.12966","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'image-text-to-text', 522, 2165060, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\npipeline_tag: image-text-to-text\nlibrary_name: transformers\n---\n<a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n\n# Qwen3-VL-8B-Instruct\n\n\nMeet Qwen3-VL — the most powerful vision-language model in the Qwen series to date.\n\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\n\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‑enhanced Thinking editions for flexible, on‑demand deployment.\n\n\n#### Key Enhancements:\n\n* **Visual Agent**: Operates PC/mobile GUIs—recognizes elements, understands functions, invokes tools, completes tasks.\n\n* **Visual Coding Boost**: Generates Draw.io/HTML/CSS/JS from images/videos.\n\n* **Advanced Spatial Perception**: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\n\n* **Long Context & Video Understanding**: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\n\n* **Enhanced Multimodal Reasoning**: Excels in STEM/Math—causal analysis and logical, evidence-based answers.\n\n* **Upgraded Visual Recognition**: Broader, higher-quality pretraining is able to “recognize everything”—celebrities, anime, products, landmarks, flora/fauna, etc.\n\n* **Expanded OCR**: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\n\n* **Text Understanding on par with pure LLMs**: Seamless text–vision fusion for lossless, unified comprehension.\n\n\n#### Model Architecture Updates:\n\n<p align="center">\n    <img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_arc.jpg" width="80%"/>\n<p>\n\n\n1. **Interleaved-MRoPE**: Full‑frequency allocation over time, width, and height via robust positional embeddings, enhancing long‑horizon video reasoning.\n\n2. **DeepStack**: Fuses multi‑level ViT features to capture fine‑grained details and sharpen image–text alignment.\n\n3. **Text–Timestamp Alignment:** Moves beyond T‑RoPE to precise, timestamp‑grounded event localization for stronger video temporal modeling.\n\nThis is the weight repository for Qwen3-VL-8B-Instruct.\n\n\n---\n\n## Model Performance\n\n**Multimodal performance**\n\n![](https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_4b_8b_vl_instruct.jpg)\n\n**Pure text performance**\n![](https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_4b_8b_text_instruct.jpg)\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen3-VL with 🤖 ModelScope and 🤗 Transformers.\n\nThe code of Qwen3-VL has been in the latest Hugging Face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\n```\n\n### Using 🤗 Transformers to Chat\n\nHere we show a code snippet to show how to use the chat model with `transformers`:\n\n```python\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n    "Qwen/Qwen3-VL-8B-Instruct", dtype="auto", device_map="auto"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     "Qwen/Qwen3-VL-8B-Instruct",\n#     dtype=torch.bfloat16,\n#     attn_implementation="flash_attention_2",\n#     device_map="auto",\n# )\n\nprocessor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-8B-Instruct")\n\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "image",\n                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",\n            },\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n\n# Preparation for inference\ninputs = processor.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_dict=True,\n    return_tensors="pt"\n)\ninputs = inputs.to(model.device)\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\n### Generation Hyperparameters\n#### VL\n```bash\nexport greedy=''false''\nexport top_p=0.8\nexport top_k=20\nexport temperature=0.7\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport out_seq_length=16384\n```\n\n#### Text\n```bash\nexport greedy=''false''\nexport top_p=1.0\nexport top_k=40\nexport repetition_penalty=1.0\nexport presence_penalty=2.0\nexport temperature=1.0\nexport out_seq_length=32768\n```\n\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n\n@article{Qwen2.5-VL,\n  title={Qwen2.5-VL Technical Report},\n  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\n  journal={arXiv preprint arXiv:2502.13923},\n  year={2025}\n}\n\n@article{Qwen2VL,\n  title={Qwen2-VL: Enhancing Vision-Language Model''s Perception of the World at Any Resolution},\n  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\n  journal={arXiv preprint arXiv:2409.12191},\n  year={2024}\n}\n\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":8767123696,"storage_bytes":17534339512,"files_count":16,"spaces_count":37,"gated":false,"private":false,"config":{"architectures":["Qwen3VLForConditionalGeneration"],"model_type":"qwen3_vl","processor_config":{"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {%- if messages[0].content is string %}\n            {{- messages[0].content }}\n        {%- else %}\n            {%- for content in messages[0].content %}\n                {%- if ''text'' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' }}\n        {%- if messages[0].content is string %}\n            {{- messages[0].content }}\n        {%- else %}\n            {%- for content in messages[0].content %}\n                {%- if ''text'' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- set image_count = namespace(value=0) %}\n{%- set video_count = namespace(value=0) %}\n{%- for message in messages %}\n    {%- if message.role == \"user\" %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- else %}\n            {%- for content in message.content %}\n                {%- if content.type == ''image'' or ''image'' in content or ''image_url'' in content %}\n                    {%- set image_count.value = image_count.value + 1 %}\n                    {%- if add_vision_id %}Picture {{ image_count.value }}: {% endif -%}\n                    <|vision_start|><|image_pad|><|vision_end|>\n                {%- elif content.type == ''video'' or ''video'' in content %}\n                    {%- set video_count.value = video_count.value + 1 %}\n                    {%- if add_vision_id %}Video {{ video_count.value }}: {% endif -%}\n                    <|vision_start|><|video_pad|><|vision_end|>\n                {%- elif ''text'' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- else %}\n            {%- for content_item in message.content %}\n                {%- if ''text'' in content_item %}\n                    {{- content_item.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and message.content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- else %}\n            {%- for content in message.content %}\n                {%- if content.type == ''image'' or ''image'' in content or ''image_url'' in content %}\n                    {%- set image_count.value = image_count.value + 1 %}\n                    {%- if add_vision_id %}Picture {{ image_count.value }}: {% endif -%}\n                    <|vision_start|><|image_pad|><|vision_end|>\n                {%- elif content.type == ''video'' or ''video'' in content %}\n                    {%- set video_count.value = video_count.value + 1 %}\n                    {%- if add_vision_id %}Video {{ video_count.value }}: {% endif -%}\n                    <|vision_start|><|video_pad|><|vision_end|>\n                {%- elif ''text'' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n"},"tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {%- if messages[0].content is string %}\n            {{- messages[0].content }}\n        {%- else %}\n            {%- for content in messages[0].content %}\n                {%- if ''text'' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' }}\n        {%- if messages[0].content is string %}\n            {{- messages[0].content }}\n        {%- else %}\n            {%- for content in messages[0].content %}\n                {%- if ''text'' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- set image_count = namespace(value=0) %}\n{%- set video_count = namespace(value=0) %}\n{%- for message in messages %}\n    {%- if message.role == \"user\" %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- else %}\n            {%- for content in message.content %}\n                {%- if content.type == ''image'' or ''image'' in content or ''image_url'' in content %}\n                    {%- set image_count.value = image_count.value + 1 %}\n                    {%- if add_vision_id %}Picture {{ image_count.value }}: {% endif -%}\n                    <|vision_start|><|image_pad|><|vision_end|>\n                {%- elif content.type == ''video'' or ''video'' in content %}\n                    {%- set video_count.value = video_count.value + 1 %}\n                    {%- if add_vision_id %}Video {{ video_count.value }}: {% endif -%}\n                    <|vision_start|><|video_pad|><|vision_end|>\n                {%- elif ''text'' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- else %}\n            {%- for content_item in message.content %}\n                {%- if ''text'' in content_item %}\n                    {{- content_item.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and message.content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- else %}\n            {%- for content in message.content %}\n                {%- if content.type == ''image'' or ''image'' in content or ''image_url'' in content %}\n                    {%- set image_count.value = image_count.value + 1 %}\n                    {%- if add_vision_id %}Picture {{ image_count.value }}: {% endif -%}\n                    <|vision_start|><|image_pad|><|vision_end|>\n                {%- elif content.type == ''video'' or ''video'' in content %}\n                    {%- set video_count.value = video_count.value + 1 %}\n                    {%- if add_vision_id %}Video {{ video_count.value }}: {% endif -%}\n                    <|vision_start|><|video_pad|><|vision_end|>\n                {%- elif ''text'' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"based_on_paper","target_id":"arxiv:2505.09388","source_url":"https://arxiv.org/abs/2505.09388"},{"type":"based_on_paper","target_id":"arxiv:2502.13923","source_url":"https://arxiv.org/abs/2502.13923"},{"type":"based_on_paper","target_id":"arxiv:2409.12191","source_url":"https://arxiv.org/abs/2409.12191"},{"type":"based_on_paper","target_id":"arxiv:2308.12966","source_url":"https://arxiv.org/abs/2308.12966"}]', NULL, 'Apache-2.0', 'approved', 62.2, '9cac3b35bff1706aee98d51463e129e8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-tensorflow-tensorflow', 'github--tensorflow--tensorflow', 'tensorflow', 'tensorflow', '<div align="center"> <img src="https://www.tensorflow.org/images/tf_logo_horizontal.png"> </div> **** | ------------------- | | TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries, and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML-powered applications. TensorFlow was originally developed by researchers and engineers working within the Machine ...', '["deep-learning","deep-neural-networks","distributed","machine-learning","ml","neural-network","python","tensorflow","c++"]', 'other', 192706, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/tensorflow/tensorflow","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n  <img src="https://www.tensorflow.org/images/tf_logo_horizontal.png">\n</div>\n\n[![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg)](https://badge.fury.io/py/tensorflow)\n[![PyPI](https://badge.fury.io/py/tensorflow.svg)](https://badge.fury.io/py/tensorflow)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4724125.svg)](https://doi.org/10.5281/zenodo.4724125)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1486/badge)](https://bestpractices.coreinfrastructure.org/projects/1486)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/tensorflow/tensorflow/badge)](https://securityscorecards.dev/viewer/?uri=github.com/tensorflow/tensorflow)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow-py.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow-py)\n[![OSSRank](https://shields.io/endpoint?url=https://ossrank.com/shield/44)](https://ossrank.com/p/44)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md)\n\n**`Documentation`** |\n------------------- |\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/api_docs/) |\n\n[TensorFlow](https://www.tensorflow.org/) is an end-to-end open source platform\nfor machine learning. It has a comprehensive, flexible ecosystem of\n[tools](https://www.tensorflow.org/resources/tools),\n[libraries](https://www.tensorflow.org/resources/libraries-extensions), and\n[community](https://www.tensorflow.org/community) resources that lets\nresearchers push the state-of-the-art in ML and developers easily build and\ndeploy ML-powered applications.\n\nTensorFlow was originally developed by researchers and engineers working within\nthe Machine Intelligence team at Google Brain to conduct research in machine\nlearning and neural networks. However, the framework is versatile enough to be\nused in other areas as well.\n\nTensorFlow provides stable [Python](https://www.tensorflow.org/api_docs/python)\nand [C++](https://www.tensorflow.org/api_docs/cc) APIs, as well as a\nnon-guaranteed backward compatible API for\n[other languages](https://www.tensorflow.org/api_docs).\n\nKeep up-to-date with release announcements and security updates by subscribing\nto\n[announce@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/announce).\nSee all the [mailing lists](https://www.tensorflow.org/community/forums).\n\n## Install\n\nSee the [TensorFlow install guide](https://www.tensorflow.org/install) for the\n[pip package](https://www.tensorflow.org/install/pip), to\n[enable GPU support](https://www.tensorflow.org/install/gpu), use a\n[Docker container](https://www.tensorflow.org/install/docker), and\n[build from source](https://www.tensorflow.org/install/source).\n\nTo install the current release, which includes support for\n[CUDA-enabled GPU cards](https://www.tensorflow.org/install/gpu) *(Ubuntu and\nWindows)*:\n\n```\n$ pip install tensorflow\n```\n\nOther devices (DirectX and MacOS-metal) are supported using\n[Device Plugins](https://www.tensorflow.org/install/gpu_plugins#available_devices).\n\nA smaller CPU-only package is also available:\n\n```\n$ pip install tensorflow-cpu\n```\n\nTo update TensorFlow to the latest version, add `--upgrade` flag to the above\ncommands.\n\n*Nightly binaries are available for testing using the\n[tf-nightly](https://pypi.python.org/pypi/tf-nightly) and\n[tf-nightly-cpu](https://pypi.python.org/pypi/tf-nightly-cpu) packages on PyPI.*\n\n#### *Try your first TensorFlow program*\n\n```shell\n$ python\n```\n\n```python\n>>> import tensorflow as tf\n>>> tf.add(1, 2).numpy()\n3\n>>> hello = tf.constant(''Hello, TensorFlow!'')\n>>> hello.numpy()\nb''Hello, TensorFlow!''\n```\n\nFor more examples, see the\n[TensorFlow Tutorials](https://www.tensorflow.org/tutorials/).\n\n## Contribution guidelines\n\n**If you want to contribute to TensorFlow, be sure to review the\n[Contribution Guidelines](CONTRIBUTING.md). This project adheres to TensorFlow''s\n[Code of Conduct](CODE_OF_CONDUCT.md). By participating, you are expected to\nuphold this code.**\n\n**We use [GitHub Issues](https://github.com/tensorflow/tensorflow/issues) for\ntracking requests and bugs, please see\n[TensorFlow Forum](https://discuss.tensorflow.org/) for general questions and\ndiscussion, and please direct specific questions to\n[Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow).**\n\nThe TensorFlow project strives to abide by generally accepted best practices in\nopen-source software development.\n\n## Patching guidelines\n\nFollow these steps to patch a specific version of TensorFlow, for example, to\napply fixes to bugs or security vulnerabilities:\n\n*   Clone the TensorFlow repository and switch to the appropriate branch for\n    your desired version—for example, `r2.8` for version 2.8.\n*   Apply the desired changes (i.e., cherry-pick them) and resolve any code\n    conflicts.\n*   Run TensorFlow tests and ensure they pass.\n*   [Build](https://www.tensorflow.org/install/source) the TensorFlow pip\n    package from source.\n\n## Continuous build status\n\nYou can find more community-supported platforms and configurations in the\n[TensorFlow SIG Build Community Builds Table](https://github.com/tensorflow/build#community-supported-tensorflow-builds).\n\n### Official Builds\n\nBuild Type                    | Status                                                                                                                                                                           | Artifacts\n----------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------\n**Linux CPU**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.html)           | [PyPI](https://pypi.org/project/tf-nightly/)\n**Linux GPU**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.html) | [PyPI](https://pypi.org/project/tf-nightly-gpu/)\n**Linux XLA**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.html)         | TBA\n**macOS**                     | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html)     | [PyPI](https://pypi.org/project/tf-nightly/)\n**Windows CPU**               | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.html)       | [PyPI](https://pypi.org/project/tf-nightly/)\n**Windows GPU**               | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.html)       | [PyPI](https://pypi.org/project/tf-nightly-gpu/)\n**Android**                   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.html)               | [Download](https://bintray.com/google/tensorflow/tensorflow/_latestVersion)\n**Raspberry Pi 0 and 1**      | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.html)           | [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv6l.whl)\n**Raspberry Pi 2 and 3**      | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.html)           | [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl)\n**Libtensorflow MacOS CPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/macos/latest/macos_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Linux CPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/cpu/ubuntu_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Linux GPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/gpu/ubuntu_gpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Windows CPU** | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/cpu/windows_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Windows GPU** | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/gpu/windows_gpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n\n## Resources\n\n*   [TensorFlow.org](https://www.tensorflow.org)\n*   [TensorFlow Tutorials](https://www.tensorflow.org/tutorials/)\n*   [TensorFlow Official Models](https://github.com/tensorflow/models/tree/master/official)\n*   [TensorFlow Examples](https://github.com/tensorflow/examples)\n*   [TensorFlow Codelabs](https://codelabs.developers.google.com/?cat=TensorFlow)\n*   [TensorFlow Blog](https://blog.tensorflow.org)\n*   [Learn ML with TensorFlow](https://www.tensorflow.org/resources/learn-ml)\n*   [TensorFlow Twitter](https://twitter.com/tensorflow)\n*   [TensorFlow YouTube](https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ)\n*   [TensorFlow model optimization roadmap](https://www.tensorflow.org/model_optimization/guide/roadmap)\n*   [TensorFlow White Papers](https://www.tensorflow.org/about/bib)\n*   [TensorBoard Visualization Toolkit](https://github.com/tensorflow/tensorboard)\n*   [TensorFlow Code Search](https://cs.opensource.google/tensorflow/tensorflow)\n\nLearn more about the\n[TensorFlow Community](https://www.tensorflow.org/community) and how to\n[Contribute](https://www.tensorflow.org/community/contribute).\n\n## Courses\n\n* [Coursera](https://www.coursera.org/search?query=TensorFlow)\n* [Udacity](https://www.udacity.com/courses/all?search=TensorFlow)\n* [Edx](https://www.edx.org/search?q=TensorFlow)\n\n## License\n\n[Apache License 2.0](LICENSE)\n', '{"language":"C++","stars":192706,"forks":75032,"watchers":192706,"open_issues":2741,"topics":["deep-learning","deep-neural-networks","distributed","machine-learning","ml","neural-network","python","tensorflow"],"default_branch":"master","size_kb":1232123,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:tensorflow:tensorflow","source_url":"https://github.com/tensorflow/tensorflow"},{"type":"has_code","target_id":"github:tensorflow:build","source_url":"https://github.com/tensorflow/build#community-supported-tensorflow-builds"},{"type":"has_code","target_id":"github:tensorflow:models","source_url":"https://github.com/tensorflow/models"},{"type":"has_code","target_id":"github:tensorflow:examples","source_url":"https://github.com/tensorflow/examples"},{"type":"has_code","target_id":"github:tensorflow:tensorboard","source_url":"https://github.com/tensorflow/tensorboard"}]', NULL, 'Apache-2.0', 'approved', 80, '5602baa1a7add8fb8514019a9a31004c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-tensorflow-tensorflow from https://github.com/tensorflow.png
Image converted to WebP: data/images/github-tensorflow-tensorflow.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-huggingface-transformers', 'github--huggingface--transformers', 'transformers', 'huggingface', '<!--- Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the Licens...', '["audio","deep-learning","deepseek","gemma","glm","hacktoberfest","llm","machine-learning","model-hub","natural-language-processing","nlp","pretrained-models","python","pytorch","pytorch-transformers","qwen","speech-recognition","transformer","vlm","python"]', 'other', 153588, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/huggingface/transformers","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the "License");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an "AS IS" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align="center">\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg">\n    <source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg">\n    <img alt="Hugging Face Transformers Library" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg" width="352" height="59" style="max-width: 100%;">\n  </picture>\n  <br/>\n  <br/>\n</p>\n\n<p align="center">\n    <a href="https://huggingface.com/models"><img alt="Checkpoints on Hub" src="https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen"></a>\n    <a href="https://circleci.com/gh/huggingface/transformers"><img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main"></a>\n    <a href="https://github.com/huggingface/transformers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"></a>\n    <a href="https://huggingface.co/docs/transformers/index"><img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"></a>\n    <a href="https://github.com/huggingface/transformers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg"></a>\n    <a href="https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"></a>\n    <a href="https://zenodo.org/badge/latestdoi/155220641"><img src="https://zenodo.org/badge/155220641.svg" alt="DOI"></a>\n</p>\n\n<h4 align="center">\n    <p>\n        <b>English</b> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md">简体中文</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md">繁體中文</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md">한국어</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_es.md">Español</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md">日本語</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md">हिन्दी</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md">Русский</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md">Português</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_te.md">తెలుగు</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md">Français</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_de.md">Deutsch</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_it.md">Italiano</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md">Tiếng Việt</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md">العربية</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md">اردو</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_bn.md">বাংলা</a> |\n    </p>\n</h4>\n\n<h3 align="center">\n    <p>State-of-the-art pretrained models for inference and training</p>\n</h3>\n\n<h3 align="center">\n    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_as_a_model_definition.png"/>\n</h3>\n\nTransformers acts as the model-definition framework for state-of-the-art machine learning with text, computer\nvision, audio, video, and multimodal models, for both inference and training.\n\nIt centralizes the model definition so that this definition is agreed upon across the ecosystem. `transformers` is the\npivot across frameworks: if a model definition is supported, it will be compatible with the majority of training\nframeworks (Axolotl, Unsloth, DeepSpeed, FSDP, PyTorch-Lightning, ...), inference engines (vLLM, SGLang, TGI, ...),\nand adjacent modeling libraries (llama.cpp, mlx, ...) which leverage the model definition from `transformers`.\n\nWe pledge to help support new state-of-the-art models and democratize their usage by having their model definition be\nsimple, customizable, and efficient.\n\nThere are over 1M+ Transformers [model checkpoints](https://huggingface.co/models?library=transformers&sort=trending) on the [Hugging Face Hub](https://huggingface.com/models) you can use.\n\nExplore the [Hub](https://huggingface.com/) today to find a model and use Transformers to help you get started right away.\n\n## Installation\n\nTransformers works with Python 3.9+, and [PyTorch](https://pytorch.org/get-started/locally/) 2.1+.\n\nCreate and activate a virtual environment with [venv](https://docs.python.org/3/library/venv.html) or [uv](https://docs.astral.sh/uv/), a fast Rust-based Python package and project manager.\n\n```py\n# venv\npython -m venv .my-env\nsource .my-env/bin/activate\n# uv\nuv venv .my-env\nsource .my-env/bin/activate\n```\n\nInstall Transformers in your virtual environment.\n\n```py\n# pip\npip install "transformers[torch]"\n\n# uv\nuv pip install "transformers[torch]"\n```\n\nInstall Transformers from source if you want the latest changes in the library or are interested in contributing. However, the *latest* version may not be stable. Feel free to open an [issue](https://github.com/huggingface/transformers/issues) if you encounter an error.\n\n```shell\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\n\n# pip\npip install ''.[torch]''\n\n# uv\nuv pip install ''.[torch]''\n```\n\n## Quickstart\n\nGet started with Transformers right away with the [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) API. The `Pipeline` is a high-level inference class that supports text, audio, vision, and multimodal tasks. It handles preprocessing the input and returns the appropriate output.\n\nInstantiate a pipeline and specify model to use for text generation. The model is downloaded and cached so you can easily reuse it again. Finally, pass some text to prompt the model.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task="text-generation", model="Qwen/Qwen2.5-1.5B")\npipeline("the secret to baking a really good cake is ")\n[{''generated_text'': ''the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.''}]\n```\n\nTo chat with a model, the usage pattern is the same. The only difference is you need to construct a chat history (the input to `Pipeline`) between you and the system.\n\n> [!TIP]\n> You can also chat with a model directly from the command line, as long as [`transformers serve` is running](https://huggingface.co/docs/transformers/main/en/serving).\n> ```shell\n> transformers chat Qwen/Qwen2.5-0.5B-Instruct\n> ```\n\n```py\nimport torch\nfrom transformers import pipeline\n\nchat = [\n    {"role": "system", "content": "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986."},\n    {"role": "user", "content": "Hey, can you tell me any fun things to do in New York?"}\n]\n\npipeline = pipeline(task="text-generation", model="meta-llama/Meta-Llama-3-8B-Instruct", dtype=torch.bfloat16, device_map="auto")\nresponse = pipeline(chat, max_new_tokens=512)\nprint(response[0]["generated_text"][-1]["content"])\n```\n\nExpand the examples below to see how `Pipeline` works for different modalities and tasks.\n\n<details>\n<summary>Automatic speech recognition</summary>\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task="automatic-speech-recognition", model="openai/whisper-large-v3")\npipeline("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")\n{''text'': '' I have a dream that one day this nation will rise up and live out the true meaning of its creed.''}\n```\n\n</details>\n\n<details>\n<summary>Image classification</summary>\n\n<h3 align="center">\n    <a><img src="https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png"></a>\n</h3>\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task="image-classification", model="facebook/dinov2-small-imagenet1k-1-layer")\npipeline("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")\n[{''label'': ''macaw'', ''score'': 0.997848391532898},\n {''label'': ''sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita'',\n  ''score'': 0.0016551691805943847},\n {''label'': ''lorikeet'', ''score'': 0.00018523589824326336},\n {''label'': ''African grey, African gray, Psittacus erithacus'',\n  ''score'': 7.85409429227002e-05},\n {''label'': ''quail'', ''score'': 5.502637941390276e-05}]\n```\n\n</details>\n\n<details>\n<summary>Visual question answering</summary>\n\n<h3 align="center">\n    <a><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg"></a>\n</h3>\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task="visual-question-answering", model="Salesforce/blip-vqa-base")\npipeline(\n    image="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg",\n    question="What is in the image?",\n)\n[{''answer'': ''statue of liberty''}]\n```\n\n</details>\n\n## Why should I use Transformers?\n\n1. Easy-to-use state-of-the-art models:\n    - High performance on natural language understanding & generation, computer vision, audio, video, and multimodal tasks.\n    - Low barrier to entry for researchers, engineers, and developers.\n    - Few user-facing abstractions with just three classes to learn.\n    - A unified API for using all our pretrained models.\n\n1. Lower compute costs, smaller carbon footprint:\n    - Share trained models instead of training from scratch.\n    - Reduce compute time and production costs.\n    - Dozens of model architectures with 1M+ pretrained checkpoints across all modalities.\n\n1. Choose the right framework for every part of a models lifetime:\n    - Train state-of-the-art models in 3 lines of code.\n    - Move a single model between PyTorch/JAX/TF2.0 frameworks at will.\n    - Pick the right framework for training, evaluation, and production.\n\n1. Easily customize a model or an example to your needs:\n    - We provide examples for each architecture to reproduce the results published by its original authors.\n    - Model internals are exposed as consistently as possible.\n    - Model files can be used independently of the library for quick experiments.\n\n<a target="_blank" href="https://huggingface.co/enterprise">\n    <img alt="Hugging Face Enterprise Hub" src="https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925">\n</a><br>\n\n## Why shouldn''t I use Transformers?\n\n- This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.\n- The training API is optimized to work with PyTorch models provided by Transformers. For generic machine learning loops, you should use another library like [Accelerate](https://huggingface.co/docs/accelerate).\n- The [example scripts](https://github.com/huggingface/transformers/tree/main/examples) are only *examples*. They may not necessarily work out-of-the-box on your specific use case and you''ll need to adapt the code for it to work.\n\n## 100 projects using Transformers\n\nTransformers is more than a toolkit to use pretrained models, it''s a community of projects built around it and the\nHugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyone\nelse to build their dream projects.\n\nIn order to celebrate Transformers 100,000 stars, we wanted to put the spotlight on the\ncommunity with the [awesome-transformers](./awesome-transformers.md) page which lists 100\nincredible projects built with Transformers.\n\nIf you own or use a project that you believe should be part of the list, please open a PR to add it!\n\n## Example models\n\nYou can test most of our models directly on their [Hub model pages](https://huggingface.co/models).\n\nExpand each modality below to see a few example models for various use cases.\n\n<details>\n<summary>Audio</summary>\n\n- Audio classification with [Whisper](https://huggingface.co/openai/whisper-large-v3-turbo)\n- Automatic speech recognition with [Moonshine](https://huggingface.co/UsefulSensors/moonshine)\n- Keyword spotting with [Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)\n- Speech to speech generation with [Moshi](https://huggingface.co/kyutai/moshiko-pytorch-bf16)\n- Text to audio with [MusicGen](https://huggingface.co/facebook/musicgen-large)\n- Text to speech with [Bark](https://huggingface.co/suno/bark)\n\n</details>\n\n<details>\n<summary>Computer vision</summary>\n\n- Automatic mask generation with [SAM](https://huggingface.co/facebook/sam-vit-base)\n- Depth estimation with [DepthPro](https://huggingface.co/apple/DepthPro-hf)\n- Image classification with [DINO v2](https://huggingface.co/facebook/dinov2-base)\n- Keypoint detection with [SuperPoint](https://huggingface.co/magic-leap-community/superpoint)\n- Keypoint matching with [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor)\n- Object detection with [RT-DETRv2](https://huggingface.co/PekingU/rtdetr_v2_r50vd)\n- Pose Estimation with [VitPose](https://huggingface.co/usyd-community/vitpose-base-simple)\n- Universal segmentation with [OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_swin_large)\n- Video classification with [VideoMAE](https://huggingface.co/MCG-NJU/videomae-large)\n\n</details>\n\n<details>\n<summary>Multimodal</summary>\n\n- Audio or text to text with [Qwen2-Audio](https://huggingface.co/Qwen/Qwen2-Audio-7B)\n- Document question answering with [LayoutLMv3](https://huggingface.co/microsoft/layoutlmv3-base)\n- Image or text to text with [Qwen-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)\n- Image captioning [BLIP-2](https://huggingface.co/Salesforce/blip2-opt-2.7b)\n- OCR-based document understanding with [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)\n- Table question answering with [TAPAS](https://huggingface.co/google/tapas-base)\n- Unified multimodal understanding and generation with [Emu3](https://huggingface.co/BAAI/Emu3-Gen)\n- Vision to text with [Llava-OneVision](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf)\n- Visual question answering with [Llava](https://huggingface.co/llava-hf/llava-1.5-7b-hf)\n- Visual referring expression segmentation with [Kosmos-2](https://huggingface.co/microsoft/kosmos-2-patch14-224)\n\n</details>\n\n<details>\n<summary>NLP</summary>\n\n- Masked word completion with [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base)\n- Named entity recognition with [Gemma](https://huggingface.co/google/gemma-2-2b)\n- Question answering with [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)\n- Summarization with [BART](https://huggingface.co/facebook/bart-large-cnn)\n- Translation with [T5](https://huggingface.co/google-t5/t5-base)\n- Text generation with [Llama](https://huggingface.co/meta-llama/Llama-3.2-1B)\n- Text classification with [Qwen](https://huggingface.co/Qwen/Qwen2.5-0.5B)\n\n</details>\n\n## Citation\n\nWe now have a [paper](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) you can cite for the 🤗 Transformers library:\n```bibtex\n@inproceedings{wolf-etal-2020-transformers,\n    title = "Transformers: State-of-the-Art Natural Language Processing",\n    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",\n    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",\n    month = oct,\n    year = "2020",\n    address = "Online",\n    publisher = "Association for Computational Linguistics",\n    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",\n    pages = "38--45"\n}\n```\n', '{"language":"Python","stars":153588,"forks":31342,"watchers":153588,"open_issues":2135,"topics":["audio","deep-learning","deepseek","gemma","glm","hacktoberfest","llm","machine-learning","model-hub","natural-language-processing","nlp","pretrained-models","python","pytorch","pytorch-transformers","qwen","speech-recognition","transformer","vlm"],"default_branch":"main","size_kb":417578,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers.git","source_url":"https://github.com/huggingface/transformers.git"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"}]', NULL, 'Apache-2.0', 'approved', 80, '433a4d9d6a4a71d2583223d91ad5a58c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-huggingface-transformers from https://github.com/huggingface.png
Image converted to WebP: data/images/github-huggingface-transformers.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-pytorch-pytorch', 'github--pytorch--pytorch', 'pytorch', 'pytorch', '!PyTorch Logo -------------------------------------------------------------------------------- PyTorch is a Python package that provides two high-level features: - Tensor computation (like NumPy) with strong GPU acceleration - Deep neural networks built on a tape-based autograd system You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed. Our trunk health (Continuous Integration signals) can be found at hud.pytorch.org. <!-- toc --> - More ...', '["autograd","deep-learning","gpu","machine-learning","neural-network","numpy","python","tensor","python"]', 'other', 95688, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/pytorch/pytorch","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '![PyTorch Logo](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png)\n\n--------------------------------------------------------------------------------\n\nPyTorch is a Python package that provides two high-level features:\n- Tensor computation (like NumPy) with strong GPU acceleration\n- Deep neural networks built on a tape-based autograd system\n\nYou can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.\n\nOur trunk health (Continuous Integration signals) can be found at [hud.pytorch.org](https://hud.pytorch.org/ci/pytorch/pytorch/main).\n\n<!-- toc -->\n\n- [More About PyTorch](#more-about-pytorch)\n  - [A GPU-Ready Tensor Library](#a-gpu-ready-tensor-library)\n  - [Dynamic Neural Networks: Tape-Based Autograd](#dynamic-neural-networks-tape-based-autograd)\n  - [Python First](#python-first)\n  - [Imperative Experiences](#imperative-experiences)\n  - [Fast and Lean](#fast-and-lean)\n  - [Extensions Without Pain](#extensions-without-pain)\n- [Installation](#installation)\n  - [Binaries](#binaries)\n    - [NVIDIA Jetson Platforms](#nvidia-jetson-platforms)\n  - [From Source](#from-source)\n    - [Prerequisites](#prerequisites)\n      - [NVIDIA CUDA Support](#nvidia-cuda-support)\n      - [AMD ROCm Support](#amd-rocm-support)\n      - [Intel GPU Support](#intel-gpu-support)\n    - [Get the PyTorch Source](#get-the-pytorch-source)\n    - [Install Dependencies](#install-dependencies)\n    - [Install PyTorch](#install-pytorch)\n      - [Adjust Build Options (Optional)](#adjust-build-options-optional)\n  - [Docker Image](#docker-image)\n    - [Using pre-built images](#using-pre-built-images)\n    - [Building the image yourself](#building-the-image-yourself)\n  - [Building the Documentation](#building-the-documentation)\n    - [Building a PDF](#building-a-pdf)\n  - [Previous Versions](#previous-versions)\n- [Getting Started](#getting-started)\n- [Resources](#resources)\n- [Communication](#communication)\n- [Releases and Contributing](#releases-and-contributing)\n- [The Team](#the-team)\n- [License](#license)\n\n<!-- tocstop -->\n\n## More About PyTorch\n\n[Learn the basics of PyTorch](https://pytorch.org/tutorials/beginner/basics/intro.html)\n\nAt a granular level, PyTorch is a library that consists of the following components:\n\n| Component | Description |\n| ---- | --- |\n| [**torch**](https://pytorch.org/docs/stable/torch.html) | A Tensor library like NumPy, with strong GPU support |\n| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | A tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |\n| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | A compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |\n| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | A neural networks library deeply integrated with autograd designed for maximum flexibility |\n| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |\n| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |\n\nUsually, PyTorch is used either as:\n\n- A replacement for NumPy to use the power of GPUs.\n- A deep learning research platform that provides maximum flexibility and speed.\n\nElaborating Further:\n\n### A GPU-Ready Tensor Library\n\nIf you use NumPy, then you have used Tensors (a.k.a. ndarray).\n\n![Tensor illustration](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/tensor_illustration.png)\n\nPyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the\ncomputation by a huge amount.\n\nWe provide a wide variety of tensor routines to accelerate and fit your scientific computation needs\nsuch as slicing, indexing, mathematical operations, linear algebra, reductions.\nAnd they are fast!\n\n### Dynamic Neural Networks: Tape-Based Autograd\n\nPyTorch has a unique way of building neural networks: using and replaying a tape recorder.\n\nMost frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world.\nOne has to build a neural network and reuse the same structure again and again.\nChanging the way the network behaves means that one has to start from scratch.\n\nWith PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to\nchange the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes\nfrom several research papers on this topic, as well as current and past work such as\n[torch-autograd](https://github.com/twitter/torch-autograd),\n[autograd](https://github.com/HIPS/autograd),\n[Chainer](https://chainer.org), etc.\n\nWhile this technique is not unique to PyTorch, it''s one of the fastest implementations of it to date.\nYou get the best of speed and flexibility for your crazy research.\n\n![Dynamic graph](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif)\n\n### Python First\n\nPyTorch is not a Python binding into a monolithic C++ framework.\nIt is built to be deeply integrated into Python.\nYou can use it naturally like you would use [NumPy](https://www.numpy.org/) / [SciPy](https://www.scipy.org/) / [scikit-learn](https://scikit-learn.org) etc.\nYou can write your new neural network layers in Python itself, using your favorite libraries\nand use packages such as [Cython](https://cython.org/) and [Numba](http://numba.pydata.org/).\nOur goal is to not reinvent the wheel where appropriate.\n\n### Imperative Experiences\n\nPyTorch is designed to be intuitive, linear in thought, and easy to use.\nWhen you execute a line of code, it gets executed. There isn''t an asynchronous view of the world.\nWhen you drop into a debugger or receive error messages and stack traces, understanding them is straightforward.\nThe stack trace points to exactly where your code was defined.\nWe hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.\n\n### Fast and Lean\n\nPyTorch has minimal framework overhead. We integrate acceleration libraries\nsuch as [Intel MKL](https://software.intel.com/mkl) and NVIDIA ([cuDNN](https://developer.nvidia.com/cudnn), [NCCL](https://developer.nvidia.com/nccl)) to maximize speed.\nAt the core, its CPU and GPU Tensor and neural network backends\nare mature and have been tested for years.\n\nHence, PyTorch is quite fast — whether you run small or large neural networks.\n\nThe memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives.\nWe''ve written custom memory allocators for the GPU to make sure that\nyour deep learning models are maximally memory efficient.\nThis enables you to train bigger deep learning models than before.\n\n### Extensions Without Pain\n\nWriting new neural network modules, or interfacing with PyTorch''s Tensor API was designed to be straightforward\nand with minimal abstractions.\n\nYou can write new neural network layers in Python using the torch API\n[or your favorite NumPy-based libraries such as SciPy](https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html).\n\nIf you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate.\nNo wrapper code needs to be written. You can see [a tutorial here](https://pytorch.org/tutorials/advanced/cpp_extension.html) and [an example here](https://github.com/pytorch/extension-cpp).\n\n\n## Installation\n\n### Binaries\nCommands to install binaries via Conda or pip wheels are on our website: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)\n\n\n#### NVIDIA Jetson Platforms\n\nPython wheels for NVIDIA''s Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX, and Jetson AGX Orin are provided [here](https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048) and the L4T container is published [here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch)\n\nThey require JetPack 4.2 and above, and [@dusty-nv](https://github.com/dusty-nv) and [@ptrblck](https://github.com/ptrblck) are maintaining them.\n\n\n### From Source\n\n#### Prerequisites\nIf you are installing from source, you will need:\n- Python 3.10 or later\n- A compiler that fully supports C++17, such as clang or gcc (gcc 9.4.0 or newer is required, on Linux)\n- Visual Studio or Visual Studio Build Tool (Windows only)\n\n\* PyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise,\nProfessional, or Community Editions. You can also install the build tools from\nhttps://visualstudio.microsoft.com/visual-cpp-build-tools/. The build tools *do not*\ncome with Visual Studio Code by default.\n\nAn example of environment setup is shown below:\n\n* Linux:\n\n```bash\n$ source <CONDA_INSTALL_DIR>/bin/activate\n$ conda create -y -n <CONDA_NAME>\n$ conda activate <CONDA_NAME>\n```\n\n* Windows:\n\n```bash\n$ source <CONDA_INSTALL_DIR>\Scripts\activate.bat\n$ conda create -y -n <CONDA_NAME>\n$ conda activate <CONDA_NAME>\n$ call "C:\Program Files\Microsoft Visual Studio\<VERSION>\Community\VC\Auxiliary\Build\vcvarsall.bat" x64\n```\n\nA conda environment is not required.  You can also do a PyTorch build in a\nstandard virtual environment, e.g., created with tools like `uv`, provided\nyour system has installed all the necessary dependencies unavailable as pip\npackages (e.g., CUDA, MKL.)\n\n##### NVIDIA CUDA Support\nIf you want to compile with CUDA support, [select a supported version of CUDA from our support matrix](https://pytorch.org/get-started/locally/), then install the following:\n- [NVIDIA CUDA](https://developer.nvidia.com/cuda-downloads)\n- [NVIDIA cuDNN](https://developer.nvidia.com/cudnn) v8.5 or above\n- [Compiler](https://gist.github.com/ax3l/9489132) compatible with CUDA\n\nNote: You could refer to the [cuDNN Support Matrix](https://docs.nvidia.com/deeplearning/cudnn/backend/latest/reference/support-matrix.html) for cuDNN versions with the various supported CUDA, CUDA driver, and NVIDIA hardware.\n\nIf you want to disable CUDA support, export the environment variable `USE_CUDA=0`.\nOther potentially useful environment variables may be found in `setup.py`.  If\nCUDA is installed in a non-standard location, set PATH so that the nvcc you\nwant to use can be found (e.g., `export PATH=/usr/local/cuda-12.8/bin:$PATH`).\n\nIf you are building for NVIDIA''s Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are [available here](https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/)\n\n##### AMD ROCm Support\nIf you want to compile with ROCm support, install\n- [AMD ROCm](https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html) 4.0 and above installation\n- ROCm is currently supported only for Linux systems.\n\nBy default the build system expects ROCm to be installed in `/opt/rocm`. If ROCm is installed in a different directory, the `ROCM_PATH` environment variable must be set to the ROCm installation directory. The build system automatically detects the AMD GPU architecture. Optionally, the AMD GPU architecture can be explicitly set with the `PYTORCH_ROCM_ARCH` environment variable [AMD GPU architecture](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus)\n\nIf you want to disable ROCm support, export the environment variable `USE_ROCM=0`.\nOther potentially useful environment variables may be found in `setup.py`.\n\n##### Intel GPU Support\nIf you want to compile with Intel GPU support, follow these\n- [PyTorch Prerequisites for Intel GPUs](https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html) instructions.\n- Intel GPU is supported for Linux and Windows.\n\nIf you want to disable Intel GPU support, export the environment variable `USE_XPU=0`.\nOther potentially useful environment variables may be found in `setup.py`.\n\n#### Get the PyTorch Source\n\n```bash\ngit clone https://github.com/pytorch/pytorch\ncd pytorch\n# if you are updating an existing checkout\ngit submodule sync\ngit submodule update --init --recursive\n```\n\n#### Install Dependencies\n\n**Common**\n\n```bash\n# Run this command from the PyTorch directory after cloning the source code using the “Get the PyTorch Source“ section above\npip install --group dev\n```\n\n**On Linux**\n\n```bash\npip install mkl-static mkl-include\n# CUDA only: Add LAPACK support for the GPU if needed\n# magma installation: run with active conda environment. specify CUDA version to install\n.ci/docker/common/install_magma_conda.sh 12.4\n\n# (optional) If using torch.compile with inductor/triton, install the matching version of triton\n# Run from the pytorch directory after cloning\n# For Intel GPU support, please explicitly `export USE_XPU=1` before running command.\nmake triton\n```\n\n**On MacOS**\n\n```bash\n# Add this package on intel x86 processor machines only\npip install mkl-static mkl-include\n# Add these packages if torch.distributed is needed\nconda install pkg-config libuv\n```\n\n**On Windows**\n\n```bash\npip install mkl-static mkl-include\n# Add these packages if torch.distributed is needed.\n# Distributed package support on Windows is a prototype feature and is subject to changes.\nconda install -c conda-forge libuv=1.51\n```\n\n#### Install PyTorch\n\n**On Linux**\n\nIf you''re compiling for AMD ROCm then first run this command:\n\n```bash\n# Only run this if you''re compiling for ROCm\npython tools/amd_build/build_amd.py\n```\n\nInstall PyTorch\n\n```bash\n# the CMake prefix for conda environment\nexport CMAKE_PREFIX_PATH="${CONDA_PREFIX:-''$(dirname $(which conda))/../''}:${CMAKE_PREFIX_PATH}"\npython -m pip install --no-build-isolation -v -e .\n\n# the CMake prefix for non-conda environment, e.g. Python venv\n# call following after activating the venv\nexport CMAKE_PREFIX_PATH="${VIRTUAL_ENV}:${CMAKE_PREFIX_PATH}"\n```\n\n**On macOS**\n\n```bash\npython -m pip install --no-build-isolation -v -e .\n```\n\n**On Windows**\n\nIf you want to build legacy python code, please refer to [Building on legacy code and CUDA](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda)\n\n**CPU-only builds**\n\nIn this mode PyTorch computations will run on your CPU, not your GPU.\n\n```cmd\npython -m pip install --no-build-isolation -v -e .\n```\n\nNote on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you''ll need to manually download the library and set up the building environment by tweaking `CMAKE_INCLUDE_PATH` and `LIB`. The instruction [here](https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source) is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.\n\n**CUDA based build**\n\nIn this mode PyTorch computations will leverage your GPU via CUDA for faster number crunching\n\n[NVTX](https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm) is needed to build Pytorch with CUDA.\nNVTX is a part of CUDA distributive, where it is called "Nsight Compute". To install it onto an already installed CUDA run CUDA installation once again and check the corresponding checkbox.\nMake sure that CUDA with Nsight Compute is installed after Visual Studio.\n\nCurrently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If `ninja.exe` is detected in `PATH`, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019.\n<br/> If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.\n\nAdditional libraries such as\n[Magma](https://developer.nvidia.com/magma), [oneDNN, a.k.a. MKLDNN or DNNL](https://github.com/oneapi-src/oneDNN), and [Sccache](https://github.com/mozilla/sccache) are often needed. Please refer to the [installation-helper](https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers) to install them.\n\nYou can refer to the [build_pytorch.bat](https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat) script for some other environment variables configurations\n\n```cmd\ncmd\n\n:: Set the environment variables after you have downloaded and unzipped the mkl package,\n:: else CMake would throw an error as `Could NOT find OpenMP`.\nset CMAKE_INCLUDE_PATH={Your directory}\mkl\include\nset LIB={Your directory}\mkl\lib;%LIB%\n\n:: Read the content in the previous section carefully before you proceed.\n:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.\n:: "Visual Studio 2019 Developer Command Prompt" will be run automatically.\n:: Make sure you have CMake >= 3.12 before you do this when you use the Visual Studio generator.\nset CMAKE_GENERATOR_TOOLSET_VERSION=14.27\nset DISTUTILS_USE_SDK=1\nfor /f "usebackq tokens=*" %i in (`"%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe" -version [15^,17^) -products * -latest -property installationPath`) do call "%i\VC\Auxiliary\Build\vcvarsall.bat" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%\n\n:: [Optional] If you want to override the CUDA host compiler\nset CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe\n\npython -m pip install --no-build-isolation -v -e .\n```\n\n**Intel GPU builds**\n\nIn this mode PyTorch with Intel GPU support will be built.\n\nPlease make sure [the common prerequisites](#prerequisites) as well as [the prerequisites for Intel GPU](#intel-gpu-support) are properly installed and the environment variables are configured prior to starting the build. For build tool support, `Visual Studio 2022` is required.\n\nThen PyTorch can be built with the command:\n\n```cmd\n:: CMD Commands:\n:: Set the CMAKE_PREFIX_PATH to help find corresponding packages\n:: %CONDA_PREFIX% only works after `conda activate custom_env`\n\nif defined CMAKE_PREFIX_PATH (\n    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library;%CMAKE_PREFIX_PATH%"\n) else (\n    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library"\n)\n\npython -m pip install --no-build-isolation -v -e .\n```\n\n##### Adjust Build Options (Optional)\n\nYou can adjust the configuration of cmake variables optionally (without building first), by doing\nthe following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done\nwith such a step.\n\nOn Linux\n\n```bash\nexport CMAKE_PREFIX_PATH="${CONDA_PREFIX:-''$(dirname $(which conda))/../''}:${CMAKE_PREFIX_PATH}"\nCMAKE_ONLY=1 python setup.py build\nccmake build  # or cmake-gui build\n```\n\nOn macOS\n\n```bash\nexport CMAKE_PREFIX_PATH="${CONDA_PREFIX:-''$(dirname $(which conda))/../''}:${CMAKE_PREFIX_PATH}"\nMACOSX_DEPLOYMENT_TARGET=11.0 CMAKE_ONLY=1 python setup.py build\nccmake build  # or cmake-gui build\n```\n\n### Docker Image\n\n#### Using pre-built images\n\nYou can also pull a pre-built docker image from Docker Hub and run with docker v19.03+\n\n```bash\ndocker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest\n```\n\nPlease note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.\nfor multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you\nshould increase shared memory size either with `--ipc=host` or `--shm-size` command line options to `nvidia-docker run`.\n\n#### Building the image yourself\n\n**NOTE:** Must be built with a docker version > 18.06\n\nThe `Dockerfile` is supplied to build images with CUDA 11.1 support and cuDNN v8.\nYou can pass `PYTHON_VERSION=x.y` make variable to specify which Python version is to be used by Miniconda, or leave it\nunset to use the default.\n\n```bash\nmake -f docker.Makefile\n# images are tagged as docker.io/${your_docker_username}/pytorch\n```\n\nYou can also pass the `CMAKE_VARS="..."` environment variable to specify additional CMake variables to be passed to CMake during the build.\nSee [setup.py](./setup.py) for the list of available variables.\n\n```bash\nmake -f docker.Makefile\n```\n\n### Building the Documentation\n\nTo build documentation in various formats, you will need [Sphinx](http://www.sphinx-doc.org)\nand the pytorch_sphinx_theme2.\n\nBefore you build the documentation locally, ensure `torch` is\ninstalled in your environment. For small fixes, you can install the\nnightly version as described in [Getting Started](https://pytorch.org/get-started/locally/).\n\nFor more complex fixes, such as adding a new module and docstrings for\nthe new module, you might need to install torch [from source](#from-source).\nSee [Docstring Guidelines](https://github.com/pytorch/pytorch/wiki/Docstring-Guidelines)\nfor docstring conventions.\n\n```bash\ncd docs/\npip install -r requirements.txt\nmake html\nmake serve\n```\n\nRun `make` to get a list of all available output formats.\n\nIf you get a katex error run `npm install katex`.  If it persists, try\n`npm install -g katex`\n\n> [!NOTE]\n> If you installed `nodejs` with a different package manager (e.g.,\n> `conda`) then `npm` will probably install a version of `katex` that is not\n> compatible with your version of `nodejs` and doc builds will fail.\n> A combination of versions that is known to work is `node@6.13.1` and\n> `katex@0.13.18`. To install the latter with `npm` you can run\n> ```npm install -g katex@0.13.18```\n\n> [!NOTE]\n> If you see a numpy incompatibility error, run:\n> ```\n> pip install ''numpy<2''\n> ```\n\nWhen you make changes to the dependencies run by CI, edit the\n`.ci/docker/requirements-docs.txt` file.\n\n#### Building a PDF\n\nTo compile a PDF of all PyTorch documentation, ensure you have\n`texlive` and LaTeX installed. On macOS, you can install them using:\n\n```\nbrew install --cask mactex\n```\n\nTo create the PDF:\n\n1. Run:\n\n   ```\n   make latexpdf\n   ```\n\n   This will generate the necessary files in the `build/latex` directory.\n\n2. Navigate to this directory and execute:\n\n   ```\n   make LATEXOPTS="-interaction=nonstopmode"\n   ```\n\n   This will produce a `pytorch.pdf` with the desired content. Run this\n   command one more time so that it generates the correct table\n   of contents and index.\n\n> [!NOTE]\n> To view the Table of Contents, switch to the **Table of Contents**\n> view in your PDF viewer.\n\n\n### Previous Versions\n\nInstallation instructions and binaries for previous PyTorch versions may be found\non [our website](https://pytorch.org/get-started/previous-versions).\n\n\n## Getting Started\n\nThree pointers to get you started:\n- [Tutorials: get you started with understanding and using PyTorch](https://pytorch.org/tutorials/)\n- [Examples: easy to understand PyTorch code across all domains](https://github.com/pytorch/examples)\n- [The API Reference](https://pytorch.org/docs/)\n- [Glossary](https://github.com/pytorch/pytorch/blob/main/GLOSSARY.md)\n\n## Resources\n\n* [PyTorch.org](https://pytorch.org/)\n* [PyTorch Tutorials](https://pytorch.org/tutorials/)\n* [PyTorch Examples](https://github.com/pytorch/examples)\n* [PyTorch Models](https://pytorch.org/hub/)\n* [Intro to Deep Learning with PyTorch from Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188)\n* [Intro to Machine Learning with PyTorch from Udacity](https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229)\n* [Deep Neural Networks with PyTorch from Coursera](https://www.coursera.org/learn/deep-neural-networks-with-pytorch)\n* [PyTorch Twitter](https://twitter.com/PyTorch)\n* [PyTorch Blog](https://pytorch.org/blog/)\n* [PyTorch YouTube](https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw)\n\n## Communication\n* Forums: Discuss implementations, research, etc. https://discuss.pytorch.org\n* GitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts, etc.\n* Slack: The [PyTorch Slack](https://pytorch.slack.com/) hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration, etc. If you are a beginner looking for help, the primary medium is [PyTorch Forums](https://discuss.pytorch.org). If you need a slack invite, please fill this form: https://goo.gl/forms/PP1AGvNHpSaJP8to1\n* Newsletter: No-noise, a one-way email newsletter with important announcements about PyTorch. You can sign-up here: https://eepurl.com/cbG0rv\n* Facebook Page: Important announcements about PyTorch. https://www.facebook.com/pytorch\n* For brand guidelines, please visit our website at [pytorch.org](https://pytorch.org/)\n\n## Releases and Contributing\n\nTypically, PyTorch has three minor releases a year. Please let us know if you encounter a bug by [filing an issue](https://github.com/pytorch/pytorch/issues).\n\nWe appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.\n\nIf you plan to contribute new features, utility functions, or extensions to the core, please first open an issue and discuss the feature with us.\nSending a PR without discussion might end up resulting in a rejected PR because we might be taking the core in a different direction than you might be aware of.\n\nTo learn more about making a contribution to Pytorch, please see our [Contribution page](CONTRIBUTING.md). For more information about PyTorch releases, see [Release page](RELEASE.md).\n\n## The Team\n\nPyTorch is a community-driven project with several skillful engineers and researchers contributing to it.\n\nPyTorch is currently maintained by [Soumith Chintala](http://soumith.ch), [Gregory Chanan](https://github.com/gchanan), [Dmytro Dzhulgakov](https://github.com/dzhulgakov), [Edward Yang](https://github.com/ezyang), [Alban Desmaison](https://github.com/albanD), [Piotr Bialecki](https://github.com/ptrblck) and [Nikita Shulga](https://github.com/malfet) with major contributions coming from hundreds of talented individuals in various forms and means.\nA non-exhaustive but growing list needs to mention: [Trevor Killeen](https://github.com/killeent), [Sasank Chilamkurthy](https://github.com/chsasank), [Sergey Zagoruyko](https://github.com/szagoruyko), [Adam Lerer](https://github.com/adamlerer), [Francisco Massa](https://github.com/fmassa), [Alykhan Tejani](https://github.com/alykhantejani), [Luca Antiga](https://github.com/lantiga), [Alban Desmaison](https://github.com/albanD), [Andreas Koepf](https://github.com/andreaskoepf), [James Bradbury](https://github.com/jekbradbury), [Zeming Lin](https://github.com/ebetica), [Yuandong Tian](https://github.com/yuandong-tian), [Guillaume Lample](https://github.com/glample), [Marat Dukhan](https://github.com/Maratyszcza), [Natalia Gimelshein](https://github.com/ngimel), [Christian Sarofeen](https://github.com/csarofeen), [Martin Raison](https://github.com/martinraison), [Edward Yang](https://github.com/ezyang), [Zachary Devito](https://github.com/zdevito). <!-- codespell:ignore -->\n\nNote: This project is unrelated to [hughperkins/pytorch](https://github.com/hughperkins/pytorch) with the same name. Hugh is a valuable contributor to the Torch community and has helped with many things Torch and PyTorch.\n\n## License\n\nPyTorch has a BSD-style license, as found in the [LICENSE](LICENSE) file.\n', '{"language":"Python","stars":95688,"forks":26136,"watchers":95688,"open_issues":17812,"topics":["autograd","deep-learning","gpu","machine-learning","neural-network","numpy","python","tensor"],"default_branch":"main","size_kb":1172135,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:twitter:torch-autograd","source_url":"https://github.com/twitter/torch-autograd"},{"type":"has_code","target_id":"github:HIPS:autograd","source_url":"https://github.com/HIPS/autograd"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:extension-cpp","source_url":"https://github.com/pytorch/extension-cpp"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:oneapi-src:oneDNN","source_url":"https://github.com/oneapi-src/oneDNN"},{"type":"has_code","target_id":"github:mozilla:sccache","source_url":"https://github.com/mozilla/sccache"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:examples","source_url":"https://github.com/pytorch/examples"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:examples","source_url":"https://github.com/pytorch/examples"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:hughperkins:pytorch","source_url":"https://github.com/hughperkins/pytorch"}]', NULL, 'NOASSERTION', 'approved', 80, 'fc830e2694c6bb7369bd5674475ab371', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-pytorch-pytorch from https://github.com/pytorch.png
Image converted to WebP: data/images/github-pytorch-pytorch.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-rasbt-LLMs-from-scratch', 'github--rasbt--llms-from-scratch', 'LLMs-from-scratch', 'rasbt', 'This repository contains the code for developing, pretraining, and finetuning a GPT-like LLM and is the official code repository for the book Build a Large Language Model (From Scratch). <br> <br> <a href="https://amzn.to/4fqvn0D"><img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/cover.jpg?123" width="250px"></a> <br> In *Build a Large Language Model (From Scratch)*, you''ll learn and understand how large language models (LLMs) work from the inside out by coding them from ...', '["ai","artificial-intelligence","chatbot","chatgpt","deep-learning","from-scratch","generative-ai","gpt","language-model","large-language-models","llm","machine-learning","neural-networks","python","pytorch","transformers","jupyter notebook"]', 'other', 80647, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/rasbt/LLMs-from-scratch","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '# Build a Large Language Model (From Scratch)\n\nThis repository contains the code for developing, pretraining, and finetuning a GPT-like LLM and is the official code repository for the book [Build a Large Language Model (From Scratch)](https://amzn.to/4fqvn0D).\n\n<br>\n<br>\n\n<a href="https://amzn.to/4fqvn0D"><img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/cover.jpg?123" width="250px"></a>\n\n<br>\n\nIn [*Build a Large Language Model (From Scratch)*](http://mng.bz/orYv), you''ll learn and understand how large language models (LLMs) work from the inside out by coding them from the ground up, step by step. In this book, I''ll guide you through creating your own LLM, explaining each stage with clear text, diagrams, and examples.\n\nThe method described in this book for training and developing your own small-but-functional model for educational purposes mirrors the approach used in creating large-scale foundational models such as those behind ChatGPT. In addition, this book includes code for loading the weights of larger pretrained models for finetuning.\n\n- Link to the official [source code repository](https://github.com/rasbt/LLMs-from-scratch)\n- [Link to the book at Manning (the publisher''s website)](http://mng.bz/orYv)\n- [Link to the book page on Amazon.com](https://www.amazon.com/gp/product/1633437167)\n- ISBN 9781633437166\n\n<a href="http://mng.bz/orYv#reviews"><img src="https://sebastianraschka.com//images/LLMs-from-scratch-images/other/reviews.png" width="220px"></a>\n\n\n<br>\n<br>\n\nTo download a copy of this repository, click on the [Download ZIP](https://github.com/rasbt/LLMs-from-scratch/archive/refs/heads/main.zip) button or execute the following command in your terminal:\n\n```bash\ngit clone --depth 1 https://github.com/rasbt/LLMs-from-scratch.git\n```\n\n<br>\n\n(If you downloaded the code bundle from the Manning website, please consider visiting the official code repository on GitHub at [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch) for the latest updates.)\n\n<br>\n<br>\n\n\n# Table of Contents\n\nPlease note that this `README.md` file is a Markdown (`.md`) file. If you have downloaded this code bundle from the Manning website and are viewing it on your local computer, I recommend using a Markdown editor or previewer for proper viewing. If you haven''t installed a Markdown editor yet, [Ghostwriter](https://ghostwriter.kde.org) is a good free option.\n\nYou can alternatively view this and other files on GitHub at [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch) in your browser, which renders Markdown automatically.\n\n<br>\n<br>\n\n\n> **Tip:**\n> If you''re seeking guidance on installing Python and Python packages and setting up your code environment, I suggest reading the [README.md](setup/README.md) file located in the [setup](setup) directory.\n\n<br>\n<br>\n\n[![Code tests Linux](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-linux-uv.yml/badge.svg)](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-linux-uv.yml)\n[![Code tests Windows](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-windows-uv-pip.yml/badge.svg)](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-windows-uv-pip.yml)\n[![Code tests macOS](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-macos-uv.yml/badge.svg)](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-macos-uv.yml)\n\n\n\n| Chapter Title                                              | Main Code (for Quick Access)                                                                                                    | All Code + Supplementary      |\n|------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|-------------------------------|\n| [Setup recommendations](setup) <br/>[How to best read this book](https://sebastianraschka.com/blog/2025/reading-books.html)                            | -                                                                                                                               | -                             |\n| Ch 1: Understanding Large Language Models                  | No code                                                                                                                         | -                             |\n| Ch 2: Working with Text Data                               | - [ch02.ipynb](ch02/01_main-chapter-code/ch02.ipynb)<br/>- [dataloader.ipynb](ch02/01_main-chapter-code/dataloader.ipynb) (summary)<br/>- [exercise-solutions.ipynb](ch02/01_main-chapter-code/exercise-solutions.ipynb)               | [./ch02](./ch02)            |\n| Ch 3: Coding Attention Mechanisms                          | - [ch03.ipynb](ch03/01_main-chapter-code/ch03.ipynb)<br/>- [multihead-attention.ipynb](ch03/01_main-chapter-code/multihead-attention.ipynb) (summary) <br/>- [exercise-solutions.ipynb](ch03/01_main-chapter-code/exercise-solutions.ipynb)| [./ch03](./ch03)             |\n| Ch 4: Implementing a GPT Model from Scratch                | - [ch04.ipynb](ch04/01_main-chapter-code/ch04.ipynb)<br/>- [gpt.py](ch04/01_main-chapter-code/gpt.py) (summary)<br/>- [exercise-solutions.ipynb](ch04/01_main-chapter-code/exercise-solutions.ipynb) | [./ch04](./ch04)           |\n| Ch 5: Pretraining on Unlabeled Data                        | - [ch05.ipynb](ch05/01_main-chapter-code/ch05.ipynb)<br/>- [gpt_train.py](ch05/01_main-chapter-code/gpt_train.py) (summary) <br/>- [gpt_generate.py](ch05/01_main-chapter-code/gpt_generate.py) (summary) <br/>- [exercise-solutions.ipynb](ch05/01_main-chapter-code/exercise-solutions.ipynb) | [./ch05](./ch05)              |\n| Ch 6: Finetuning for Text Classification                   | - [ch06.ipynb](ch06/01_main-chapter-code/ch06.ipynb)  <br/>- [gpt_class_finetune.py](ch06/01_main-chapter-code/gpt_class_finetune.py)  <br/>- [exercise-solutions.ipynb](ch06/01_main-chapter-code/exercise-solutions.ipynb) | [./ch06](./ch06)              |\n| Ch 7: Finetuning to Follow Instructions                    | - [ch07.ipynb](ch07/01_main-chapter-code/ch07.ipynb)<br/>- [gpt_instruction_finetuning.py](ch07/01_main-chapter-code/gpt_instruction_finetuning.py) (summary)<br/>- [ollama_evaluate.py](ch07/01_main-chapter-code/ollama_evaluate.py) (summary)<br/>- [exercise-solutions.ipynb](ch07/01_main-chapter-code/exercise-solutions.ipynb) | [./ch07](./ch07)  |\n| Appendix A: Introduction to PyTorch                        | - [code-part1.ipynb](appendix-A/01_main-chapter-code/code-part1.ipynb)<br/>- [code-part2.ipynb](appendix-A/01_main-chapter-code/code-part2.ipynb)<br/>- [DDP-script.py](appendix-A/01_main-chapter-code/DDP-script.py)<br/>- [exercise-solutions.ipynb](appendix-A/01_main-chapter-code/exercise-solutions.ipynb) | [./appendix-A](./appendix-A) |\n| Appendix B: References and Further Reading                 | No code                                                                                                                         | [./appendix-B](./appendix-B) |\n| Appendix C: Exercise Solutions                             | - [list of exercise solutions](appendix-C)                                                                 | [./appendix-C](./appendix-C) |\n| Appendix D: Adding Bells and Whistles to the Training Loop | - [appendix-D.ipynb](appendix-D/01_main-chapter-code/appendix-D.ipynb)                                                          | [./appendix-D](./appendix-D)  |\n| Appendix E: Parameter-efficient Finetuning with LoRA       | - [appendix-E.ipynb](appendix-E/01_main-chapter-code/appendix-E.ipynb)                                                          | [./appendix-E](./appendix-E) |\n\n<br>\n&nbsp;\n\nThe mental model below summarizes the contents covered in this book.\n\n<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/mental-model.jpg" width="650px">\n\n\n<br>\n&nbsp;\n\n## Prerequisites\n\nThe most important prerequisite is a strong foundation in Python programming.\nWith this knowledge, you will be well prepared to explore the fascinating world of LLMs\nand understand the concepts and code examples presented in this book.\n\nIf you have some experience with deep neural networks, you may find certain concepts more familiar, as LLMs are built upon these architectures.\n\nThis book uses PyTorch to implement the code from scratch without using any external LLM libraries. While proficiency in PyTorch is not a prerequisite, familiarity with PyTorch basics is certainly useful. If you are new to PyTorch, Appendix A provides a concise introduction to PyTorch. Alternatively, you may find my book, [PyTorch in One Hour: From Tensors to Training Neural Networks on Multiple GPUs](https://sebastianraschka.com/teaching/pytorch-1h/), helpful for learning about the essentials.\n\n\n\n<br>\n&nbsp;\n\n## Hardware Requirements\n\nThe code in the main chapters of this book is designed to run on conventional laptops within a reasonable timeframe and does not require specialized hardware. This approach ensures that a wide audience can engage with the material. Additionally, the code automatically utilizes GPUs if they are available. (Please see the [setup](https://github.com/rasbt/LLMs-from-scratch/blob/main/setup/README.md) doc for additional recommendations.)\n\n\n&nbsp;\n## Video Course\n\n[A 17-hour and 15-minute companion video course](https://www.manning.com/livevideo/master-and-build-large-language-models) where I code through each chapter of the book. The course is organized into chapters and sections that mirror the book''s structure so that it can be used as a standalone alternative to the book or complementary code-along resource.\n\n<a href="https://www.manning.com/livevideo/master-and-build-large-language-models"><img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/video-screenshot.webp?123" width="350px"></a>\n\n\n&nbsp;\n\n\n## Companion Book / Sequel\n\n[*Build A Reasoning Model (From Scratch)*](https://mng.bz/lZ5B), while a standalone book, can be considered as a sequel to *Build A Large Language Model (From Scratch)*.\n\nIt starts with a pretrained model and implements different reasoning approaches, including inference-time scaling, reinforcement learning, and distillation, to improve the model''s reasoning capabilities.\n\nSimilar to *Build A Large Language Model (From Scratch)*, [*Build A Reasoning Model (From Scratch)*](https://mng.bz/lZ5B) takes a hands-on approach implementing these methods from scratch.\n\n<a href="https://mng.bz/lZ5B"><img src="https://sebastianraschka.com/images/reasoning-from-scratch-images/cover.webp?123" width="120px"></a>\n\n- Amazon link (TBD)\n- [Manning link](https://mng.bz/lZ5B)\n- [GitHub repository](https://github.com/rasbt/reasoning-from-scratch)\n\n<br>\n\n&nbsp;\n## Exercises\n\nEach chapter of the book includes several exercises. The solutions are summarized in Appendix C, and the corresponding code notebooks are available in the main chapter folders of this repository (for example,  [./ch02/01_main-chapter-code/exercise-solutions.ipynb](./ch02/01_main-chapter-code/exercise-solutions.ipynb).\n\nIn addition to the code exercises, you can download a free 170-page PDF titled  [Test Yourself On Build a Large Language Model (From Scratch)](https://www.manning.com/books/test-yourself-on-build-a-large-language-model-from-scratch) from the Manning website. It contains approximately 30 quiz questions and solutions per chapter to help you test your understanding.\n\n<a href="https://www.manning.com/books/test-yourself-on-build-a-large-language-model-from-scratch"><img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/test-yourself-cover.jpg?123" width="150px"></a>\n\n&nbsp;\n## Bonus Material\n\nSeveral folders contain optional materials as a bonus for interested readers:\n- **Setup**\n  - [Python Setup Tips](setup/01_optional-python-setup-preferences)\n  - [Installing Python Packages and Libraries Used in This Book](setup/02_installing-python-libraries)\n  - [Docker Environment Setup Guide](setup/03_optional-docker-environment)\n\n- **Chapter 2: Working With Text Data**\n  - [Byte Pair Encoding (BPE) Tokenizer From Scratch](ch02/05_bpe-from-scratch/bpe-from-scratch-simple.ipynb)\n  - [Comparing Various Byte Pair Encoding (BPE) Implementations](ch02/02_bonus_bytepair-encoder)\n  - [Understanding the Difference Between Embedding Layers and Linear Layers](ch02/03_bonus_embedding-vs-matmul)\n  - [Dataloader Intuition With Simple Numbers](ch02/04_bonus_dataloader-intuition)\n\n- **Chapter 3: Coding Attention Mechanisms**\n  - [Comparing Efficient Multi-Head Attention Implementations](ch03/02_bonus_efficient-multihead-attention/mha-implementations.ipynb)\n  - [Understanding PyTorch Buffers](ch03/03_understanding-buffers/understanding-buffers.ipynb)\n\n- **Chapter 4: Implementing a GPT Model From Scratch**\n  - [FLOPs Analysis](ch04/02_performance-analysis/flops-analysis.ipynb)\n  - [KV Cache](ch04/03_kv-cache)\n  - [Attention Alternatives](ch04/#attention-alternatives)\n    - [Grouped-Query Attention](ch04/04_gqa)\n    - [Multi-Head Latent Attention](ch04/05_mla)\n    - [Sliding Window Attention](ch04/06_swa)\n    - [Gated DeltaNet](ch04/08_deltanet)\n  - [Mixture-of-Experts (MoE)](ch04/07_moe)\n\n- **Chapter 5: Pretraining on Unlabeled Data**\n  - [Alternative Weight Loading Methods](ch05/02_alternative_weight_loading/)\n  - [Pretraining GPT on the Project Gutenberg Dataset](ch05/03_bonus_pretraining_on_gutenberg)\n  - [Adding Bells and Whistles to the Training Loop](ch05/04_learning_rate_schedulers)\n  - [Optimizing Hyperparameters for Pretraining](ch05/05_bonus_hparam_tuning)\n  - [Building a User Interface to Interact With the Pretrained LLM](ch05/06_user_interface)\n  - [Converting GPT to Llama](ch05/07_gpt_to_llama)\n  - [Memory-efficient Model Weight Loading](ch05/08_memory_efficient_weight_loading/memory-efficient-state-dict.ipynb)\n  - [Extending the Tiktoken BPE Tokenizer with New Tokens](ch05/09_extending-tokenizers/extend-tiktoken.ipynb)\n  - [PyTorch Performance Tips for Faster LLM Training](ch05/10_llm-training-speed)\n  - [LLM Architectures](ch05/#llm-architectures-from-scratch)\n    - [Llama 3.2 From Scratch](ch05/07_gpt_to_llama/standalone-llama32.ipynb)\n    - [Qwen3 Dense and Mixture-of-Experts (MoE) From Scratch](ch05/11_qwen3/)\n    - [Gemma 3 From Scratch](ch05/12_gemma3/)\n    - [Olmo 3 From Scratch](ch05/13_olmo3/)\n- **Chapter 6: Finetuning for classification**\n  - [Additional experiments finetuning different layers and using larger models](ch06/02_bonus_additional-experiments)\n  - [Finetuning different models on 50k IMDb movie review dataset](ch06/03_bonus_imdb-classification)\n  - [Building a User Interface to Interact With the GPT-based Spam Classifier](ch06/04_user_interface)\n- **Chapter 7: Finetuning to follow instructions**\n  - [Dataset Utilities for Finding Near Duplicates and Creating Passive Voice Entries](ch07/02_dataset-utilities)\n  - [Evaluating Instruction Responses Using the OpenAI API and Ollama](ch07/03_model-evaluation)\n  - [Generating a Dataset for Instruction Finetuning](ch07/05_dataset-generation/llama3-ollama.ipynb)\n  - [Improving a Dataset for Instruction Finetuning](ch07/05_dataset-generation/reflection-gpt4.ipynb)\n  - [Generating a Preference Dataset With Llama 3.1 70B and Ollama](ch07/04_preference-tuning-with-dpo/create-preference-data-ollama.ipynb)\n  - [Direct Preference Optimization (DPO) for LLM Alignment](ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb)\n  - [Building a User Interface to Interact With the Instruction-Finetuned GPT Model](ch07/06_user_interface)\n\nMore bonus material from the [Reasoning From Scratch](https://github.com/rasbt/reasoning-from-scratch) repository:\n\n- **Qwen3 (From Scratch) Basics**\n  - [Qwen3 Source Code Walkthrough](https://github.com/rasbt/reasoning-from-scratch/blob/main/chC/01_main-chapter-code/chC_main.ipynb)\n  - [Optimized Qwen3](https://github.com/rasbt/reasoning-from-scratch/tree/main/ch02/03_optimized-LLM)\n\n- **Evaluation**\n  - [Verifier-Based Evaluation (MATH-500)](https://github.com/rasbt/reasoning-from-scratch/tree/main/ch03)\n  - [Multiple-Choice Evaluation (MMLU)](https://github.com/rasbt/reasoning-from-scratch/blob/main/chF/02_mmlu)\n  - [LLM Leaderboard Evaluation](https://github.com/rasbt/reasoning-from-scratch/blob/main/chF/03_leaderboards)\n  - [LLM-as-a-Judge Evaluation](https://github.com/rasbt/reasoning-from-scratch/blob/main/chF/04_llm-judge)\n\n<br>\n&nbsp;\n\n## Questions, Feedback, and Contributing to This Repository\n\n\nI welcome all sorts of feedback, best shared via the [Manning Forum](https://livebook.manning.com/forum?product=raschka&page=1) or [GitHub Discussions](https://github.com/rasbt/LLMs-from-scratch/discussions). Likewise, if you have any questions or just want to bounce ideas off others, please don''t hesitate to post these in the forum as well.\n\nPlease note that since this repository contains the code corresponding to a print book, I currently cannot accept contributions that would extend the contents of the main chapter code, as it would introduce deviations from the physical book. Keeping it consistent helps ensure a smooth experience for everyone.\n\n\n&nbsp;\n## Citation\n\nIf you find this book or code useful for your research, please consider citing it.\n\nChicago-style citation:\n\n> Raschka, Sebastian. *Build A Large Language Model (From Scratch)*. Manning, 2024. ISBN: 978-1633437166.\n\nBibTeX entry:\n\n```\n@book{build-llms-from-scratch-book,\n  author       = {Sebastian Raschka},\n  title        = {Build A Large Language Model (From Scratch)},\n  publisher    = {Manning},\n  year         = {2024},\n  isbn         = {978-1633437166},\n  url          = {https://www.manning.com/books/build-a-large-language-model-from-scratch},\n  github       = {https://github.com/rasbt/LLMs-from-scratch}\n}\n```\n', '{"language":"Jupyter Notebook","stars":80647,"forks":12026,"watchers":80647,"open_issues":1,"topics":["ai","artificial-intelligence","chatbot","chatgpt","deep-learning","from-scratch","generative-ai","gpt","language-model","large-language-models","llm","machine-learning","neural-networks","python","pytorch","transformers"],"default_branch":"main","size_kb":15419,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch.git","source_url":"https://github.com/rasbt/LLMs-from-scratch.git"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch](https:","source_url":"https://github.com/rasbt/LLMs-from-scratch](https:"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch](https:","source_url":"https://github.com/rasbt/LLMs-from-scratch](https:"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:reasoning-from-scratch","source_url":"https://github.com/rasbt/reasoning-from-scratch"},{"type":"has_code","target_id":"github:rasbt:reasoning-from-scratch","source_url":"https://github.com/rasbt/reasoning-from-scratch"},{"type":"has_code","target_id":"github:rasbt:reasoning-from-scratch","source_url":"https://github.com/rasbt/reasoning-from-scratch"},{"type":"has_code","target_id":"github:rasbt:reasoning-from-scratch","source_url":"https://github.com/rasbt/reasoning-from-scratch"},{"type":"has_code","target_id":"github:rasbt:reasoning-from-scratch","source_url":"https://github.com/rasbt/reasoning-from-scratch"},{"type":"has_code","target_id":"github:rasbt:reasoning-from-scratch","source_url":"https://github.com/rasbt/reasoning-from-scratch"},{"type":"has_code","target_id":"github:rasbt:reasoning-from-scratch","source_url":"https://github.com/rasbt/reasoning-from-scratch"},{"type":"has_code","target_id":"github:rasbt:reasoning-from-scratch","source_url":"https://github.com/rasbt/reasoning-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch}","source_url":"https://github.com/rasbt/LLMs-from-scratch}"}]', NULL, 'NOASSERTION', 'approved', 80, '204e58f8972fa4521d6437a94ba6c183', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-rasbt-LLMs-from-scratch from https://github.com/rasbt.png
Image converted to WebP: data/images/github-rasbt-LLMs-from-scratch.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-microsoft-ML-For-Beginners', 'github--microsoft--ml-for-beginners', 'ML-For-Beginners', 'microsoft', '<!-- CO-OP TRANSLATOR LANGUAGES TABLE START --> Arabic | Bengali | Bulgarian | Burmese (Myanmar) | Chinese (Simplified) | Chinese (Traditional, Hong Kong) | Chinese (Traditional, Macau) | Chinese (Traditional, Taiwan) | Croatian | Czech | Danish | Dutch | Estonian | Finnish | French | German | Greek | Hebrew | Hindi | Hungarian | Indonesian | Italian | Japanese | Korean | Lithuanian | Malay | Marathi | Nepali | Norwegian | Persian (Farsi) | Polish | Portuguese (Brazil) | Portuguese (Portugal)...', '["data-science","education","machine-learning","machine-learning-algorithms","machinelearning","machinelearning-python","microsoft-for-beginners","ml","python","r","scikit-learn","scikit-learn-python","jupyter notebook"]', 'other', 80316, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/microsoft/ML-For-Beginners","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '[![GitHub license](https://img.shields.io/github/license/microsoft/ML-For-Beginners.svg)](https://github.com/microsoft/ML-For-Beginners/blob/master/LICENSE)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/graphs/contributors/)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/issues/)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/pulls/)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ML-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ML-For-Beginners/watchers/)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n\n### 🌐 Multi-Language Support\n\n#### Supported via GitHub Action (Automated & Always Up-to-Date)\n\n<!-- CO-OP TRANSLATOR LANGUAGES TABLE START -->\n[Arabic](./translations/ar/README.md) | [Bengali](./translations/bn/README.md) | [Bulgarian](./translations/bg/README.md) | [Burmese (Myanmar)](./translations/my/README.md) | [Chinese (Simplified)](./translations/zh/README.md) | [Chinese (Traditional, Hong Kong)](./translations/hk/README.md) | [Chinese (Traditional, Macau)](./translations/mo/README.md) | [Chinese (Traditional, Taiwan)](./translations/tw/README.md) | [Croatian](./translations/hr/README.md) | [Czech](./translations/cs/README.md) | [Danish](./translations/da/README.md) | [Dutch](./translations/nl/README.md) | [Estonian](./translations/et/README.md) | [Finnish](./translations/fi/README.md) | [French](./translations/fr/README.md) | [German](./translations/de/README.md) | [Greek](./translations/el/README.md) | [Hebrew](./translations/he/README.md) | [Hindi](./translations/hi/README.md) | [Hungarian](./translations/hu/README.md) | [Indonesian](./translations/id/README.md) | [Italian](./translations/it/README.md) | [Japanese](./translations/ja/README.md) | [Korean](./translations/ko/README.md) | [Lithuanian](./translations/lt/README.md) | [Malay](./translations/ms/README.md) | [Marathi](./translations/mr/README.md) | [Nepali](./translations/ne/README.md) | [Norwegian](./translations/no/README.md) | [Persian (Farsi)](./translations/fa/README.md) | [Polish](./translations/pl/README.md) | [Portuguese (Brazil)](./translations/br/README.md) | [Portuguese (Portugal)](./translations/pt/README.md) | [Punjabi (Gurmukhi)](./translations/pa/README.md) | [Romanian](./translations/ro/README.md) | [Russian](./translations/ru/README.md) | [Serbian (Cyrillic)](./translations/sr/README.md) | [Slovak](./translations/sk/README.md) | [Slovenian](./translations/sl/README.md) | [Spanish](./translations/es/README.md) | [Swahili](./translations/sw/README.md) | [Swedish](./translations/sv/README.md) | [Tagalog (Filipino)](./translations/tl/README.md) | [Tamil](./translations/ta/README.md) | [Thai](./translations/th/README.md) | [Turkish](./translations/tr/README.md) | [Ukrainian](./translations/uk/README.md) | [Urdu](./translations/ur/README.md) | [Vietnamese](./translations/vi/README.md)\n<!-- CO-OP TRANSLATOR LANGUAGES TABLE END -->\n\n #### Join Our Community\n\n[![Microsoft Foundry Discord](https://dcbadge.limes.pink/api/server/nTYy5BXMWG)](https://discord.gg/nTYy5BXMWG)\n\nWe have a Discord learn with AI series ongoing, learn more and join us at [Learn with AI Series](https://aka.ms/learnwithai/discord) from 18 - 30 September, 2025. You will get tips and tricks of using GitHub Copilot for Data Science.\n\n![Learn with AI series](/images/3.png)\n\n# Machine Learning for Beginners - A Curriculum\n\n> 🌍 Travel around the world as we explore Machine Learning by means of world cultures 🌍\n\nCloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about **Machine Learning**. In this curriculum, you will learn about what is sometimes called **classic machine learning**, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our [AI for Beginners'' curriculum](https://aka.ms/ai4beginners). Pair these lessons with our [''Data Science for Beginners'' curriculum](https://aka.ms/ds4beginners), as well!\n\nTravel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment, and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to ''stick''.\n\n**✍️ Hearty thanks to our authors** Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, Ruth Yakubu and Amy Boyd\n\n**🎨 Thanks as well to our illustrators** Tomomi Imura, Dasani Madipalli, and Jen Looper\n\n**🙏 Special thanks 🙏 to our Microsoft Student Ambassador authors, reviewers, and content contributors**, notably Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, and Snigdha Agarwal\n\n**🤩 Extra gratitude to Microsoft Student Ambassadors Eric Wanjau, Jasleen Sondhi, and Vidushi Gupta for our R lessons!**\n\n# Getting Started\n\nFollow these steps:\n1. **Fork the Repository**: Click on the "Fork" button at the top-right corner of this page.\n2. **Clone the Repository**:   `git clone https://github.com/microsoft/ML-For-Beginners.git`\n\n> [find all additional resources for this course in our Microsoft Learn collection](https://learn.microsoft.com/en-us/collections/qrqzamz1nn2wx3?WT.mc_id=academic-77952-bethanycheum)\n\n> 🔧 **Need help?** Check our [Troubleshooting Guide](TROUBLESHOOTING.md) for solutions to common issues with installation, setup, and running lessons.\n\n\n**[Students](https://aka.ms/student-page)**, to use this curriculum, fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:\n\n- Start with a pre-lecture quiz.\n- Read the lecture and complete the activities, pausing and reflecting at each knowledge check.\n- Try to create the projects by comprehending the lessons rather than running the solution code; however that code is available in the `/solution` folders in each project-oriented lesson.\n- Take the post-lecture quiz.\n- Complete the challenge.\n- Complete the assignment.\n- After completing a lesson group, visit the [Discussion Board](https://github.com/microsoft/ML-For-Beginners/discussions) and "learn out loud" by filling out the appropriate PAT rubric. A ''PAT'' is a Progress Assessment Tool that is a rubric you fill out to further your learning. You can also react to other PATs so we can learn together.\n\n> For further study, we recommend following these [Microsoft Learn](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-77952-leestott) modules and learning paths.\n\n**Teachers**, we have [included some suggestions](for-teachers.md) on how to use this curriculum.\n\n---\n\n## Video walkthroughs\n\nSome of the lessons are available as short form video. You can find all these in-line in the lessons, or on the [ML for Beginners playlist on the Microsoft Developer YouTube channel](https://aka.ms/ml-beginners-videos) by clicking the image below.\n\n[![ML for beginners banner](./images/ml-for-beginners-video-banner.png)](https://aka.ms/ml-beginners-videos)\n\n---\n\n## Meet the Team\n\n[![Promo video](./images/ml.gif)](https://youtu.be/Tj1XWrDSYJU)\n\n**Gif by** [Mohit Jaisal](https://linkedin.com/in/mohitjaisal)\n\n> 🎥 Click the image above for a video about the project and the folks who created it!\n\n---\n\n## Pedagogy\n\nWe have chosen two pedagogical tenets while building this curriculum: ensuring that it is hands-on **project-based** and that it includes **frequent quizzes**. In addition, this curriculum has a common **theme** to give it cohesion.\n\nBy ensuring that the content aligns with projects, the process is made more engaging for students and retention of concepts will be augmented. In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 12-week cycle. This curriculum also includes a postscript on real-world applications of ML, which can be used as extra credit or as a basis for discussion.\n\n> Find our [Code of Conduct](CODE_OF_CONDUCT.md), [Contributing](CONTRIBUTING.md), [Translation](TRANSLATIONS.md), and [Troubleshooting](TROUBLESHOOTING.md) guidelines. We welcome your constructive feedback!\n\n## Each lesson includes\n\n- optional sketchnote\n- optional supplemental video\n- video walkthrough (some lessons only)\n- [pre-lecture warmup quiz](https://ff-quizzes.netlify.app/en/ml/)\n- written lesson\n- for project-based lessons, step-by-step guides on how to build the project\n- knowledge checks\n- a challenge\n- supplemental reading\n- assignment\n- [post-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)\n\n> **A note about languages**: These lessons are primarily written in Python, but many are also available in R. To complete an R lesson, go to the `/solution` folder and look for R lessons. They include an .rmd extension that represents an **R Markdown** file which can be simply defined as an embedding of `code chunks` (of R or other languages) and a `YAML header` (that guides how to format outputs such as PDF) in a `Markdown document`. As such, it serves as an exemplary authoring framework for data science since it allows you to combine your code, its output, and your thoughts by allowing you to write them down in Markdown. Moreover, R Markdown documents can be rendered to output formats such as PDF, HTML, or Word.\n\n> **A note about quizzes**: All quizzes are contained in [Quiz App folder](./quiz-app/), for 52 total quizzes of three questions each. They are linked from within the lessons but the quiz app can be run locally; follow the instruction in the `quiz-app` folder to locally host or deploy to Azure.\n\n| Lesson Number |                             Topic                              |                   Lesson Grouping                   | Learning Objectives                                                                                                             |                                                              Linked Lesson                                                               |                        Author                        |\n| :-----------: | :------------------------------------------------------------: | :-------------------------------------------------: | ------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------: |\n|      01       |                Introduction to machine learning                |      [Introduction](1-Introduction/README.md)       | Learn the basic concepts behind machine learning                                                                                |                                             [Lesson](1-Introduction/1-intro-to-ML/README.md)                                             |                       Muhammad                       |\n|      02       |                The History of machine learning                 |      [Introduction](1-Introduction/README.md)       | Learn the history underlying this field                                                                                         |                                            [Lesson](1-Introduction/2-history-of-ML/README.md)                                            |                     Jen and Amy                      |\n|      03       |                 Fairness and machine learning                  |      [Introduction](1-Introduction/README.md)       | What are the important philosophical issues around fairness that students should consider when building and applying ML models? |                                              [Lesson](1-Introduction/3-fairness/README.md)                                               |                        Tomomi                        |\n|      04       |                Techniques for machine learning                 |      [Introduction](1-Introduction/README.md)       | What techniques do ML researchers use to build ML models?                                                                       |                                          [Lesson](1-Introduction/4-techniques-of-ML/README.md)                                           |                    Chris and Jen                     |\n|      05       |                   Introduction to regression                   |        [Regression](2-Regression/README.md)         | Get started with Python and Scikit-learn for regression models                                                                  |         [Python](2-Regression/1-Tools/README.md) • [R](2-Regression/1-Tools/solution/R/lesson_1.html)         |      Jen • Eric Wanjau       |\n|      06       |                North American pumpkin prices 🎃                |        [Regression](2-Regression/README.md)         | Visualize and clean data in preparation for ML                                                                                  |          [Python](2-Regression/2-Data/README.md) • [R](2-Regression/2-Data/solution/R/lesson_2.html)          |      Jen • Eric Wanjau       |\n|      07       |                North American pumpkin prices 🎃                |        [Regression](2-Regression/README.md)         | Build linear and polynomial regression models                                                                                   |        [Python](2-Regression/3-Linear/README.md) • [R](2-Regression/3-Linear/solution/R/lesson_3.html)        |      Jen and Dmitry • Eric Wanjau       |\n|      08       |                North American pumpkin prices 🎃                |        [Regression](2-Regression/README.md)         | Build a logistic regression model                                                                                               |     [Python](2-Regression/4-Logistic/README.md) • [R](2-Regression/4-Logistic/solution/R/lesson_4.html)      |      Jen • Eric Wanjau       |\n|      09       |                          A Web App 🔌                          |           [Web App](3-Web-App/README.md)            | Build a web app to use your trained model                                                                                       |                                                 [Python](3-Web-App/1-Web-App/README.md)                                                  |                         Jen                          |\n|      10       |                 Introduction to classification                 |    [Classification](4-Classification/README.md)     | Clean, prep, and visualize your data; introduction to classification                                                            | [Python](4-Classification/1-Introduction/README.md) • [R](4-Classification/1-Introduction/solution/R/lesson_10.html)  | Jen and Cassie • Eric Wanjau |\n|      11       |             Delicious Asian and Indian cuisines 🍜             |    [Classification](4-Classification/README.md)     | Introduction to classifiers                                                                                                     | [Python](4-Classification/2-Classifiers-1/README.md) • [R](4-Classification/2-Classifiers-1/solution/R/lesson_11.html) | Jen and Cassie • Eric Wanjau |\n|      12       |             Delicious Asian and Indian cuisines 🍜             |    [Classification](4-Classification/README.md)     | More classifiers                                                                                                                | [Python](4-Classification/3-Classifiers-2/README.md) • [R](4-Classification/3-Classifiers-2/solution/R/lesson_12.html) | Jen and Cassie • Eric Wanjau |\n|      13       |             Delicious Asian and Indian cuisines 🍜             |    [Classification](4-Classification/README.md)     | Build a recommender web app using your model                                                                                    |                                              [Python](4-Classification/4-Applied/README.md)                                              |                         Jen                          |\n|      14       |                   Introduction to clustering                   |        [Clustering](5-Clustering/README.md)         | Clean, prep, and visualize your data; Introduction to clustering                                                                |         [Python](5-Clustering/1-Visualize/README.md) • [R](5-Clustering/1-Visualize/solution/R/lesson_14.html)         |      Jen • Eric Wanjau       |\n|      15       |              Exploring Nigerian Musical Tastes 🎧              |        [Clustering](5-Clustering/README.md)         | Explore the K-Means clustering method                                                                                           |           [Python](5-Clustering/2-K-Means/README.md) • [R](5-Clustering/2-K-Means/solution/R/lesson_15.html)           |      Jen • Eric Wanjau       |\n|      16       |        Introduction to natural language processing ☕️         |   [Natural language processing](6-NLP/README.md)    | Learn the basics about NLP by building a simple bot                                                                             |                                             [Python](6-NLP/1-Introduction-to-NLP/README.md)                                              |                       Stephen                        |\n|      17       |                      Common NLP Tasks ☕️                      |   [Natural language processing](6-NLP/README.md)    | Deepen your NLP knowledge by understanding common tasks required when dealing with language structures                          |                                                    [Python](6-NLP/2-Tasks/README.md)                                                     |                       Stephen                        |\n|      18       |             Translation and sentiment analysis ♥️              |   [Natural language processing](6-NLP/README.md)    | Translation and sentiment analysis with Jane Austen                                                                             |                                            [Python](6-NLP/3-Translation-Sentiment/README.md)                                             |                       Stephen                        |\n|      19       |                  Romantic hotels of Europe ♥️                  |   [Natural language processing](6-NLP/README.md)    | Sentiment analysis with hotel reviews 1                                                                                         |                                               [Python](6-NLP/4-Hotel-Reviews-1/README.md)                                                |                       Stephen                        |\n|      20       |                  Romantic hotels of Europe ♥️                  |   [Natural language processing](6-NLP/README.md)    | Sentiment analysis with hotel reviews 2                                                                                         |                                               [Python](6-NLP/5-Hotel-Reviews-2/README.md)                                                |                       Stephen                        |\n|      21       |            Introduction to time series forecasting             |        [Time series](7-TimeSeries/README.md)        | Introduction to time series forecasting                                                                                         |                                             [Python](7-TimeSeries/1-Introduction/README.md)                                              |                      Francesca                       |\n|      22       | ⚡️ World Power Usage ⚡️ - time series forecasting with ARIMA |        [Time series](7-TimeSeries/README.md)        | Time series forecasting with ARIMA                                                                                              |                                                 [Python](7-TimeSeries/2-ARIMA/README.md)                                                 |                      Francesca                       |\n|      23       |  ⚡️ World Power Usage ⚡️ - time series forecasting with SVR  |        [Time series](7-TimeSeries/README.md)        | Time series forecasting with Support Vector Regressor                                                                           |                                                  [Python](7-TimeSeries/3-SVR/README.md)                                                  |                       Anirban                        |\n|      24       |             Introduction to reinforcement learning             | [Reinforcement learning](8-Reinforcement/README.md) | Introduction to reinforcement learning with Q-Learning                                                                          |                                             [Python](8-Reinforcement/1-QLearning/README.md)                                              |                        Dmitry                        |\n|      25       |                 Help Peter avoid the wolf! 🐺                  | [Reinforcement learning](8-Reinforcement/README.md) | Reinforcement learning Gym                                                                                                      |                                                [Python](8-Reinforcement/2-Gym/README.md)                                                 |                        Dmitry                        |\n|  Postscript   |            Real-World ML scenarios and applications            |      [ML in the Wild](9-Real-World/README.md)       | Interesting and revealing real-world applications of classical ML                                                               |                                             [Lesson](9-Real-World/1-Applications/README.md)                                              |                         Team                         |\n|  Postscript   |            Model Debugging in ML using RAI dashboard          |      [ML in the Wild](9-Real-World/README.md)       | Model Debugging in Machine Learning using Responsible AI dashboard components                                                              |                                             [Lesson](9-Real-World/2-Debugging-ML-Models/README.md)                                              |                         Ruth Yakubu                       |\n\n> [find all additional resources for this course in our Microsoft Learn collection](https://learn.microsoft.com/en-us/collections/qrqzamz1nn2wx3?WT.mc_id=academic-77952-bethanycheum)\n\n## Offline access\n\nYou can run this documentation offline by using [Docsify](https://docsify.js.org/#/). Fork this repo, [install Docsify](https://docsify.js.org/#/quickstart) on your local machine, and then in the root folder of this repo, type `docsify serve`. The website will be served on port 3000 on your localhost: `localhost:3000`.\n\n## PDFs\n\nFind a pdf of the curriculum with links [here](https://microsoft.github.io/ML-For-Beginners/pdf/readme.pdf).\n\n\n## 🎒 Other Courses \n\nOur team produces other courses! Check out:\n\n<!-- CO-OP TRANSLATOR OTHER COURSES START -->\n### Azure / Edge / MCP / Agents\n[![AZD for Beginners](https://img.shields.io/badge/AZD%20for%20Beginners-0078D4?style=for-the-badge&labelColor=E5E7EB&color=0078D4)](https://github.com/microsoft/AZD-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![Edge AI for Beginners](https://img.shields.io/badge/Edge%20AI%20for%20Beginners-00B8E4?style=for-the-badge&labelColor=E5E7EB&color=00B8E4)](https://github.com/microsoft/edgeai-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![MCP for Beginners](https://img.shields.io/badge/MCP%20for%20Beginners-009688?style=for-the-badge&labelColor=E5E7EB&color=009688)](https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![AI Agents for Beginners](https://img.shields.io/badge/AI%20Agents%20for%20Beginners-00C49A?style=for-the-badge&labelColor=E5E7EB&color=00C49A)](https://github.com/microsoft/ai-agents-for-beginners?WT.mc_id=academic-105485-koreyst)\n\n---\n \n### Generative AI Series\n[![Generative AI for Beginners](https://img.shields.io/badge/Generative%20AI%20for%20Beginners-8B5CF6?style=for-the-badge&labelColor=E5E7EB&color=8B5CF6)](https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![Generative AI (.NET)](https://img.shields.io/badge/Generative%20AI%20(.NET)-9333EA?style=for-the-badge&labelColor=E5E7EB&color=9333EA)](https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst)\n[![Generative AI (Java)](https://img.shields.io/badge/Generative%20AI%20(Java)-C084FC?style=for-the-badge&labelColor=E5E7EB&color=C084FC)](https://github.com/microsoft/generative-ai-for-beginners-java?WT.mc_id=academic-105485-koreyst)\n[![Generative AI (JavaScript)](https://img.shields.io/badge/Generative%20AI%20(JavaScript)-E879F9?style=for-the-badge&labelColor=E5E7EB&color=E879F9)](https://github.com/microsoft/generative-ai-with-javascript?WT.mc_id=academic-105485-koreyst)\n\n---\n \n### Core Learning\n[![ML for Beginners](https://img.shields.io/badge/ML%20for%20Beginners-22C55E?style=for-the-badge&labelColor=E5E7EB&color=22C55E)](https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst)\n[![Data Science for Beginners](https://img.shields.io/badge/Data%20Science%20for%20Beginners-84CC16?style=for-the-badge&labelColor=E5E7EB&color=84CC16)](https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst)\n[![AI for Beginners](https://img.shields.io/badge/AI%20for%20Beginners-A3E635?style=for-the-badge&labelColor=E5E7EB&color=A3E635)](https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst)\n[![Cybersecurity for Beginners](https://img.shields.io/badge/Cybersecurity%20for%20Beginners-F97316?style=for-the-badge&labelColor=E5E7EB&color=F97316)](https://github.com/microsoft/Security-101?WT.mc_id=academic-96948-sayoung)\n[![Web Dev for Beginners](https://img.shields.io/badge/Web%20Dev%20for%20Beginners-EC4899?style=for-the-badge&labelColor=E5E7EB&color=EC4899)](https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst)\n[![IoT for Beginners](https://img.shields.io/badge/IoT%20for%20Beginners-14B8A6?style=for-the-badge&labelColor=E5E7EB&color=14B8A6)](https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst)\n[![XR Development for Beginners](https://img.shields.io/badge/XR%20Development%20for%20Beginners-38BDF8?style=for-the-badge&labelColor=E5E7EB&color=38BDF8)](https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst)\n\n---\n \n### Copilot Series\n[![Copilot for AI Paired Programming](https://img.shields.io/badge/Copilot%20for%20AI%20Paired%20Programming-FACC15?style=for-the-badge&labelColor=E5E7EB&color=FACC15)](https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst)\n[![Copilot for C#/.NET](https://img.shields.io/badge/Copilot%20for%20C%23/.NET-FBBF24?style=for-the-badge&labelColor=E5E7EB&color=FBBF24)](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst)\n[![Copilot Adventure](https://img.shields.io/badge/Copilot%20Adventure-FDE68A?style=for-the-badge&labelColor=E5E7EB&color=FDE68A)](https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst)\n<!-- CO-OP TRANSLATOR OTHER COURSES END -->\n\n## Getting Help\n\nIf you get stuck or have any questions about building AI apps. Join fellow learners and experienced developers in discussions about MCP. It''s a supportive community where questions are welcome and knowledge is shared freely.\n\n[![Microsoft Foundry Discord](https://dcbadge.limes.pink/api/server/nTYy5BXMWG)](https://discord.gg/nTYy5BXMWG)\n\nIf you have product feedback or errors while building visit:\n\n[![Microsoft Foundry Developer Forum](https://img.shields.io/badge/GitHub-Microsoft_Foundry_Developer_Forum-blue?style=for-the-badge&logo=github&color=000000&logoColor=fff)](https://aka.ms/foundry/forum)\n', '{"language":"Jupyter Notebook","stars":80316,"forks":18851,"watchers":80316,"open_issues":6,"topics":["data-science","education","machine-learning","machine-learning-algorithms","machinelearning","machinelearning-python","microsoft-for-beginners","ml","python","r","scikit-learn","scikit-learn-python"],"default_branch":"main","size_kb":1628865,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:microsoft:ML-For-Beginners","source_url":"https://github.com/microsoft/ML-For-Beginners"},{"type":"has_code","target_id":"github:microsoft:ML-For-Beginners.git`","source_url":"https://github.com/microsoft/ML-For-Beginners.git`"},{"type":"has_code","target_id":"github:microsoft:ML-For-Beginners","source_url":"https://github.com/microsoft/ML-For-Beginners"},{"type":"has_code","target_id":"github:microsoft:AZD-for-beginners","source_url":"https://github.com/microsoft/AZD-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:edgeai-for-beginners","source_url":"https://github.com/microsoft/edgeai-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:mcp-for-beginners","source_url":"https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:ai-agents-for-beginners","source_url":"https://github.com/microsoft/ai-agents-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:generative-ai-for-beginners","source_url":"https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:Generative-AI-for-beginners-dotnet","source_url":"https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:generative-ai-for-beginners-java","source_url":"https://github.com/microsoft/generative-ai-for-beginners-java?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:generative-ai-with-javascript","source_url":"https://github.com/microsoft/generative-ai-with-javascript?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:Security-101","source_url":"https://github.com/microsoft/Security-101?WT.mc_id=academic-96948-sayoung"},{"type":"has_code","target_id":"github:microsoft:xr-development-for-beginners","source_url":"https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:mastering-github-copilot-for-dotnet-csharp-developers","source_url":"https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:CopilotAdventures","source_url":"https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst"}]', NULL, 'MIT', 'approved', 80, 'a5fb71fa6b711cc8cb5a0b720165ba52', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-microsoft-ML-For-Beginners from https://github.com/microsoft.png
Image converted to WebP: data/images/github-microsoft-ML-For-Beginners.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-netdata-netdata', 'github--netdata--netdata', 'netdata', 'netdata', '<p align="center"> <a href="https://www.netdata.cloud#gh-light-mode-only"> <img src="https://www.netdata.cloud/img/readme-images/netdata_readme_logo_light.png" alt="Netdata" width="300"/> </a> <a href="https://www.netdata.cloud#gh-dark-mode-only"> <img src="https://www.netdata.cloud/img/readme-images/netdata_readme_logo_dark.png" alt="Netdata" width="300"/> </a> </p> <h3 align="center">X-Ray Vision for your infrastructure!</h3> <h4 align="center">Every Metric, Every Second. No BS.</h4> <br />...', '["ai","alerting","cncf","data-visualization","database","devops","docker","grafana","influxdb","kubernetes","linux","machine-learning","mcp","mongodb","monitoring","mysql","netdata","observability","postgresql","prometheus","c"]', 'other', 76940, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/netdata/netdata","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n<a href="https://www.netdata.cloud#gh-light-mode-only">\n  <img src="https://www.netdata.cloud/img/readme-images/netdata_readme_logo_light.png" alt="Netdata" width="300"/>\n</a>\n<a href="https://www.netdata.cloud#gh-dark-mode-only">\n  <img src="https://www.netdata.cloud/img/readme-images/netdata_readme_logo_dark.png" alt="Netdata" width="300"/>\n</a>\n</p>\n<h3 align="center">X-Ray Vision for your infrastructure!</h3>\n<h4 align="center">Every Metric, Every Second. No BS.</h4>\n\n<br />\n<p align="center">\n  <a href="https://github.com/netdata/netdata/"><img src="https://img.shields.io/github/stars/netdata/netdata?style=social" alt="GitHub Stars"></a>\n  <br />\n  <a href="https://app.netdata.cloud/spaces/netdata-demo?utm_campaign=github_readme_demo_badge"><img src="https://img.shields.io/badge/Live%20Demo-green" alt="Live Demo"></a>\n  <a href="https://github.com/netdata/netdata/releases/latest"><img src="https://img.shields.io/github/release/netdata/netdata.svg" alt="Latest release"></a>\n  <a href="https://github.com/netdata/netdata-nightlies/releases/latest"><img src="https://img.shields.io/github/release/netdata/netdata-nightlies.svg" alt="Latest nightly build"></a>\n  <br/>\n  <a href="https://community.netdata.cloud"><img alt="Discourse topics" src="https://img.shields.io/discourse/topics?server=https%3A%2F%2Fcommunity.netdata.cloud%2F&logo=discourse&label=discourse%20forum"></a>\n  <a href="https://github.com/netdata/netdata/discussions"><img alt="GitHub Discussions" src="https://img.shields.io/github/discussions/netdata/netdata?logo=github&label=github%20discussions"></a>\n  <br/>\n  <a href="https://bestpractices.coreinfrastructure.org/projects/2231"><img src="https://bestpractices.coreinfrastructure.org/projects/2231/badge" alt="CII Best Practices"></a>\n  <a href="https://scan.coverity.com/projects/netdata-netdata?tab=overview"><img alt="Coverity Scan" src="https://img.shields.io/coverity/scan/netdata"></a>\n</p>\n\n<p align="center">\n  <a href="https://registry.my-netdata.io/#menu_netdata_submenu_registry"><img src="https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_entries&dimensions=persons&label=user%20base&units=M&value_color=blue&precision=2&divide=1000000&options=unaligned&tier=1&v44" alt="User base"></a>\n  <a href="https://registry.my-netdata.io/#menu_netdata_submenu_registry"><img src="https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_entries&dimensions=machines&label=servers%20monitored&units=M&divide=1000000&value_color=orange&precision=2&options=unaligned&tier=1&v44" alt="Servers monitored"></a>\n  <a href="https://registry.my-netdata.io/#menu_netdata_submenu_registry"><img src="https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_sessions&label=sessions%20served&units=M&value_color=yellowgreen&precision=2&divide=1000000&options=unaligned&tier=1&v44" alt="Sessions served"></a>\n  <a href="https://hub.docker.com/r/netdata/netdata"><img src="https://registry.my-netdata.io/api/v3/badge.svg?chart=dockerhub.pulls_sum&divide=1000000&precision=1&units=M&label=docker+hub+pulls&options=unaligned&tier=1&v44" alt="Docker Hub pulls"></a>\n</p>\n<p align="center"><b>Visit our <a href="https://www.netdata.cloud">Home Page</a></b></p>\n\n<hr class="solid">\n\nMENU: **[WHO WE ARE](#who-we-are)** | **[KEY FEATURES](#key-features)** | **[GETTING STARTED](#getting-started)** | **[HOW IT WORKS](#how-it-works)** | **[FAQ](#faq)** | **[DOCS](#book-documentation)** | **[COMMUNITY](#tada-community)** | **[CONTRIBUTE](#pray-contribute)** | **[LICENSE](#scroll-license)**\n\n\n> [!WARNING]\n> People **get addicted to Netdata.**\n> Once you use it on your systems, *there''s no going back.*\n\n[![Platforms](https://img.shields.io/badge/Platforms-Linux%20%7C%20macOS%20%7C%20FreeBSD%20%7C%20Windows-blue)]()\n\n---\n\n## WHO WE ARE\n\nNetdata is an open-source, real-time infrastructure monitoring platform. Monitor, detect, and act across your entire infrastructure.\n\n**Core Advantages:**\n\n* **Instant Insights** – With Netdata you can access per-second metrics and visualizations.\n* **Zero Configuration** – You can deploy immediately without complex setup.\n* **ML-Powered** – You can detect anomalies, predict issues, and automate analysis.\n* **Efficient** – You can monitor with minimal resource usage and maximum scalability.\n* **Secure & Distributed** – You can keep your data local with no central collection needed.\n\nWith Netdata, you get real-time, per-second updates. Clear **insights at a glance**, no complexity.\n\n<details>\n  <summary><strong>All heroes have a great origin story. Click to discover ours.</strong></summary>\n  <br/>\n\nIn 2013, at the company where Costa Tsaousis was COO, a significant percentage of their cloud-based transactions failed silently, severely impacting business performance.\n\nCosta and his team tried every troubleshooting tool available at the time. None could identify the root cause. As Costa later wrote:\n\n“*I couldn’t believe that monitoring systems provide so few metrics and with such low resolution, scale so badly, and cost so much to run.*”\n\nFrustrated, he decided to build his own monitoring tool, starting from scratch.\n\nThat decision led to countless late nights and weekends. It also sparked a fundamental shift in how infrastructure monitoring and troubleshooting are approached, both in method and in cost.\n</details>\n\n### Most Energy-Efficient Monitoring Tool\n\n<p align="center">\n<a href="https://www.ivanomalavolta.com/files/papers/ICSOC_2023.pdf#gh-dark-mode-only">\n  <img src="https://github.com/netdata/netdata/assets/139226121/7118757a-38fb-48d7-b12a-53e709a8e8c0" alt="Energy Efficiency" width="800"/>\n</a>\n<a href="https://www.ivanomalavolta.com/files/papers/ICSOC_2023.pdf#gh-light-mode-only">\n  <img src="https://github.com/netdata/netdata/assets/139226121/4f64cbb6-05e4-48e3-b7c0-d1b79e37e219" alt="Energy efficiency" width="800"/>\n</a>\n</p>\n\nAccording to the [University of Amsterdam study](https://www.ivanomalavolta.com/files/papers/ICSOC_2023.pdf), Netdata is the most energy-efficient tool for monitoring Docker-based systems. The study also shows Netdata excels in CPU usage, RAM usage, and execution time compared to other monitoring solutions.\n\n---\n\n## Key Features\n\n| Feature                    | Description                               | What Makes It Unique                                     |\n|----------------------------|-------------------------------------------|----------------------------------------------------------|\n| **Real-Time**              | Per-second data collection and processing | Works in a beat – click and see results instantly        |\n| **Zero-Configuration**     | Automatic detection and discovery         | Auto-discovers everything on the nodes it runs           |\n| **ML-Powered**             | Unsupervised anomaly detection            | Trains multiple ML models per metric at the edge         |\n| **Long-Term Retention**    | High-performance storage                  | ~0.5 bytes per sample with tiered storage for archiving  |\n| **Advanced Visualization** | Rich, interactive dashboards              | Slice and dice data without query language               |\n| **Extreme Scalability**    | Native horizontal scaling                 | Parent-Child centralization with multi-million samples/s |\n| **Complete Visibility**    | From infrastructure to applications       | Simplifies operations and eliminates silos               |\n| **Edge-Based**             | Processing at your premises               | Distributes code instead of centralizing data            |\n\n> [!NOTE]  \n> Want to put Netdata to the test against Prometheus?\n> Explore the [full comparison](https://www.netdata.cloud/blog/netdata-vs-prometheus-2025/).\n\n---\n\n## Netdata Ecosystem\n\nThis three-part architecture enables you to scale from single nodes to complex multi-cloud environments:\n\n| Component         | Description                                                                                                                                                 | License                                         |\n|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------|\n| **Netdata Agent** | • Core monitoring engine<br>• Handles collection, storage, ML, alerts, exports<br>• Runs on servers, cloud, K8s, IoT<br>• Zero production impact            | [GPL v3+](https://www.gnu.org/licenses/gpl-3.0) |\n| **Netdata Cloud** | • Enterprise features<br>• User management, RBAC, horizontal scaling<br>• Centralized alerts<br>• Free community tier<br>• No metric storage centralization |                                                 |\n| **Netdata UI**    | • Dashboards and visualizations<br>• Free to use<br>• Included in standard packages<br>• Latest version via CDN                                             | [NCUL1](https://app.netdata.cloud/LICENSE.txt)  |\n\n## What You Can Monitor\n\nWith Netdata you can monitor all these components across platforms:\n\n|                                                                                                   Component |              Linux               | FreeBSD | macOS |                      Windows                      |\n|------------------------------------------------------------------------------------------------------------:|:--------------------------------:|:-------:|:-----:|:-------------------------------------------------:|\n|                             **System Resources**<small><br/>CPU, Memory and system shared resources</small> |               Full               |   Yes   |  Yes  |                        Yes                        |\n|                                **Storage**<small><br/>Disks, Mount points, Filesystems, RAID arrays</small> |               Full               |   Yes   |  Yes  |                        Yes                        |\n|                                 **Network**<small><br/>Network Interfaces, Protocols, Firewall, etc</small> |               Full               |   Yes   |  Yes  |                        Yes                        |\n|                        **Hardware & Sensors**<small><br/>Fans, Temperatures, Controllers, GPUs, etc</small> |               Full               |  Some   | Some  |                       Some                        |\n|                                       **O/S Services**<small><br/>Resources, Performance and Status</small> | Yes<small><br/>`systemd`</small> |    -    |   -   |                         -                         |\n|                                      **Processes**<small><br/>Resources, Performance, OOM, and more</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n|                                                                             System and Application **Logs** | Yes<small><br/>`systemd`-journal |    -    |   -   | Yes<small><br/>`Windows Event Log`, `ETW`</small> |\n|                                 **Network Connections**<small><br/>Live TCP and UDP sockets per PID</small> |               Yes                |    -    |   -   |                         -                         |\n|                               **Containers**<small><br/>Docker/containerd, LXC/LXD, Kubernetes, etc</small> |               Yes                |    -    |   -   |                         -                         |\n|                                 **VMs** (from the host)<small><br/>KVM, qemu, libvirt, Proxmox, etc</small> | Yes<small><br/>`cgroups`</small> |    -    |   -   |         Yes<small><br/>`Hyper-V`</small>          |\n|                       **Synthetic Checks**<small><br/>Test APIs, TCP ports, Ping, Certificates, etc</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n| **Packaged Applications**<small><br/>nginx, apache, postgres, redis, mongodb,<br/>and hundreds more</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n|                              **Cloud Provider Infrastructure**<small><br/>AWS, GCP, Azure, and more</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n|                       **Custom Applications**<small><br/>OpenMetrics, StatsD and soon OpenTelemetry</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n\nOn Linux, you can continuously monitor all kernel features and hardware sensors for errors, including Intel/AMD/Nvidia GPUs, PCI AER, RAM EDAC, IPMI, S.M.A.R.T, Intel RAPL, NVMe, fans, power supplies, and voltage readings.\n\n---\n\n## Getting Started\n\nYou can install Netdata on all major operating systems. To begin:\n\n### 1. Install Netdata\n\nChoose your platform and follow the installation guide:\n\n* [Linux Installation](https://learn.netdata.cloud/docs/installing/one-line-installer-for-all-linux-systems)\n* [macOS](https://learn.netdata.cloud/docs/installing/macos)\n* [FreeBSD](https://learn.netdata.cloud/docs/installing/freebsd)\n* [Windows](https://learn.netdata.cloud/docs/netdata-agent/installation/windows)\n* [Docker Guide](/packaging/docker/README.md)\n* [Kubernetes Setup](https://learn.netdata.cloud/docs/installation/install-on-specific-environments/kubernetes)\n\n> [!NOTE]\n> You can access the Netdata UI at `http://localhost:19999` (or `http://NODE:19999` if remote).\n\n### 2. Configure Collectors\n\nNetdata auto-discovers most metrics, but you can manually configure some collectors:\n\n* [All collectors](https://learn.netdata.cloud/docs/data-collection/)\n* [SNMP monitoring](https://learn.netdata.cloud/docs/data-collection/monitor-anything/networking/snmp)\n\n### 3. Configure Alerts\n\nYou can use hundreds of built-in alerts and integrate with:\n\n`email`, `Slack`, `Telegram`, `PagerDuty`, `Discord`, `Microsoft Teams`, and more.\n\n> [!NOTE]  \n> Email alerts work by default if there''s a configured MTA.\n\n### 4. Configure Parents\n\nYou can centralize dashboards, alerts, and storage with Netdata Parents:\n\n* [Streaming Reference](https://learn.netdata.cloud/docs/streaming/streaming-configuration-reference)\n\n> [!NOTE]  \n> You can use Netdata Parents for central dashboards, longer retention, and alert configuration.\n\n### 5. Connect to Netdata Cloud\n\n[Sign in to Netdata Cloud](https://app.netdata.cloud/sign-in) and connect your nodes for:\n\n* Access from anywhere\n* Horizontal scalability and multi-node dashboards\n* UI configuration for alerts and data collection\n* Role-based access control\n* Free tier available\n\n> [!NOTE]  \n> Netdata Cloud is optional. Your data stays in your infrastructure.\n\n## Live Demo Sites\n\n<p align="center">\n  <b>See Netdata in action</b><br/>\n  <a href="https://frankfurt.netdata.rocks"><b>FRANKFURT</b></a> |\n  <a href="https://newyork.netdata.rocks"><b>NEWYORK</b></a> |\n  <a href="https://atlanta.netdata.rocks"><b>ATLANTA</b></a> |\n  <a href="https://sanfrancisco.netdata.rocks"><b>SANFRANCISCO</b></a> |\n  <a href="https://toronto.netdata.rocks"><b>TORONTO</b></a> |\n  <a href="https://singapore.netdata.rocks"><b>SINGAPORE</b></a> |\n  <a href="https://bangalore.netdata.rocks"><b>BANGALORE</b></a>\n  <br/>\n  <i>These demo clusters run with default configuration and show real monitoring data.</i>\n  <br/>\n  <i>Choose the instance closest to you for the best performance.</i>\n</p>\n\n---\n\n## How It Works\n\nWith Netdata you can run a modular pipeline for metrics collection, processing, and visualization.\n\n```mermaid\nflowchart TB\n  A[Netdata Agent]:::mainNode\n  A1(Collect):::green --> A\n  A2(Store):::green --> A\n  A3(Learn):::green --> A\n  A4(Detect):::green --> A\n  A5(Check):::green --> A\n  A6(Stream):::green --> A\n  A7(Archive):::green --> A\n  A8(Query):::green --> A\n  A9(Score):::green --> A\n\n  classDef green fill:#bbf3bb,stroke:#333,stroke-width:1px,color:#000\n  classDef mainNode fill:#f0f0f0,stroke:#333,stroke-width:1px,color:#333\n```\n\nWith each Agent you can:\n\n1. **Collect** – Gather metrics from systems, containers, apps, logs, APIs, and synthetic checks.\n2. **Store** – Save metrics to a high-efficiency, tiered time-series database.\n3. **Learn** – Train ML models per metric using recent behavior.\n4. **Detect** – Identify anomalies using trained ML models.\n5. **Check** – Evaluate metrics against pre-set or custom alert rules.\n6. **Stream** – Send metrics to Netdata Parents in real time.\n7. **Archive** – Export metrics to Prometheus, InfluxDB, OpenTSDB, Graphite, and others.\n8. **Query** – Access metrics via an API for dashboards or third-party tools.\n9. **Score** – Use a scoring engine to find patterns and correlations across metrics.\n\n> [!NOTE]  \n> Learn more: [Netdata''s architecture](https://learn.netdata.cloud/docs/netdata-agent/#distributed-observability-pipeline)\n\n## Agent Capabilities\n\nWith the Netdata Agent, you can use these core capabilities out-of-the-box:\n\n| Capability                   | Description                                                                                                                                   |\n|------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|\n| **Comprehensive Collection** | • 800+ integrations<br>• Systems, containers, VMs, hardware sensors<br>• OpenMetrics, StatsD, and logs<br>• OpenTelemetry support coming soon |\n| **Performance & Precision**  | • Per-second collection<br>• Real-time visualization with 1-second latency<br>• High-resolution metrics                                       |\n| **Edge-Based ML**            | • ML models trained at the edge<br>• Automatic anomaly detection per metric<br>• Pattern recognition based on historical behavior             |\n| **Advanced Log Management**  | • Direct systemd-journald and Windows Event Log integration<br>• Process logs at the edge<br>• Rich log visualization                         |\n| **Observability Pipeline**   | • Parent-Child relationships<br>• Flexible centralization<br>• Multi-level replication and retention                                          |\n| **Automated Visualization**  | • NIDL data model<br>• Auto-generated dashboards<br>• No query language needed                                                                |\n| **Smart Alerting**           | • Pre-configured alerts<br>• Multiple notification methods<br>• Proactive detection                                                           |\n| **Low Maintenance**          | • Auto-detection<br>• Zero-touch ML<br>• Easy scalability<br>• CI/CD friendly                                                                 |\n| **Open & Extensible**        | • Modular architecture<br>• Easy to customize<br>• Integrates with existing tools                                                             |\n\n---\n\n## CNCF Membership\n\n<p align="center">\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/white/cncf-white.svg">\n    <source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/color/cncf-color.svg">\n    <img alt="CNCF Logo" src="https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/color/cncf-color.svg" width="300">\n  </picture>\n  <br />\n  Netdata actively supports and is a member of the Cloud Native Computing Foundation (CNCF).<br />\n  It is one of the most starred projects in the <a href="https://landscape.cncf.io/?item=observability-and-analysis--observability--netdata">CNCF landscape</a>.\n</p>\n\n---\n\n## FAQ\n\n<details>\n<summary><strong>Is Netdata secure?</strong></summary>\n<br/>\n\nYes. Netdata follows [OpenSSF best practices](https://bestpractices.coreinfrastructure.org/en/projects/2231), has a security-first design, and is regularly audited by the community.\n\n* [Security design](https://learn.netdata.cloud/docs/security-and-privacy-design)\n* [Security policies and advisories](https://github.com/netdata/netdata/security)\n\n</details>\n\n<details>\n<summary><strong>Does Netdata use a lot of resources?</strong></summary>\n<br/>\n\nNo. Even with ML and per-second metrics, Netdata uses minimal resources.\n\n* \~5% CPU and 150MiB RAM by default on production systems\n* <1% CPU and \~100MiB RAM when ML and alerts are disabled and using ephemeral storage\n* Parents scale to millions of metrics per second with appropriate hardware\n\n> You can use the **Netdata Monitoring** section in the dashboard to inspect its resource usage.\n\n</details>\n\n<details>\n<summary><strong>How much data retention is possible?</strong></summary>\n<br/>\n\nAs much as your disk allows.\n\nWith Netdata you can use tiered retention:\n\n* Tier 0: per-second resolution\n* Tier 1: per-minute resolution\n* Tier 2: per-hour resolution\n\nThese are queried automatically based on the zoom level.\n</details>\n\n<details>\n<summary><strong>Can Netdata scale to many servers?</strong></summary>\n<br/>\n\nYes. With Netdata you can:\n\n* Scale horizontally with many Agents\n* Scale vertically with powerful Parents\n* Scale infinitely via Netdata Cloud\n\n> You can use Netdata Cloud to merge many independent infrastructures into one logical view.\n\n</details>\n\n<details>\n<summary><strong>Is disk I/O a concern?</strong></summary>\n<br/>\n\nNo. Netdata minimizes disk usage:\n\n* Metrics are flushed to disk every 17 minutes, spread out evenly\n* Uses direct I/O and compression (ZSTD)\n* Can run entirely in RAM or stream to a Parent\n\n> You can use `alloc` or `ram` mode for no disk writes.\n\n</details>\n\n<details>\n<summary><strong>How is Netdata different from Prometheus + Grafana?</strong></summary>\n<br/>\n\nWith Netdata you get a complete monitoring solution—not just tools.\n\n* No manual setup or dashboards needed\n* Built-in ML, alerts, dashboards, and correlations\n* More efficient and easier to deploy\n\n> [Performance comparison](https://blog.netdata.cloud/netdata-vs-prometheus-performance-analysis/)\n\n</details>\n\n<details>\n<summary><strong>How is Netdata different from commercial SaaS tools?</strong></summary>\n<br/>\n\nWith Netdata you can store all metrics on your infrastructure—no sampling, no aggregation, no loss.\n\n* High-resolution metrics by default\n* ML per metric, not shared models\n* Unlimited scalability without skyrocketing cost\n\n</details>\n\n<details>\n<summary><strong>Can Netdata run alongside Nagios, Zabbix, etc.?</strong></summary>\n<br/>\n\nYes. You can use Netdata together with traditional tools.\n\nWith Netdata you get:\n\n* Real-time, high-resolution monitoring\n* Zero configuration and auto-generated dashboards\n* Anomaly detection and advanced visualization\n\n</details>\n\n<details>\n<summary><strong>What if I feel overwhelmed?</strong></summary>\n<br/>\n\nYou can start small:\n\n* Use the dashboard''s table of contents and search\n* Explore anomaly scoring ("AR" toggle)\n* Create custom dashboards in Netdata Cloud\n\n> [Docs and guides](https://learn.netdata.cloud/guides)\n\n</details>\n\n<details>\n<summary><strong>Do I have to use Netdata Cloud?</strong></summary>\n<br/>\n\nNo. Netdata Cloud is optional.\n\nNetdata works without it, but with Cloud you can:\n\n* Access remotely with SSO\n* Save dashboard customizations\n* Configure alerts centrally\n* Collaborate with role-based access\n\n</details>\n\n<details>\n<summary><strong>What telemetry does Netdata collect?</strong></summary>\n<br/>\n\nAnonymous telemetry helps improve the product. You can disable it:\n\n* Add `--disable-telemetry` to the installer, or\n* Create `/etc/netdata/.opt-out-from-anonymous-statistics` and restart Netdata\n\n> Telemetry helps us understand usage, not track users. No private data is collected.\n\n</details>\n\n<details>\n<summary><strong>Who uses Netdata?</strong></summary>\n<br/>\n\nYou''ll join users including:\n\n* Major companies (Amazon, ABN AMRO Bank, Facebook, Google, IBM, Intel, Netflix, Samsung)\n* Universities (NYU, Columbia, Seoul National, UCL)\n* Government organizations worldwide\n* Infrastructure-intensive organizations\n* Technology operators\n* Startups and freelancers\n* SysAdmins and DevOps professionals\n\n</details>\n\n---\n\n## \:book: Documentation\n\nVisit [Netdata Learn](https://learn.netdata.cloud) for full documentation and guides.\n\n> [!NOTE]  \n> Includes deployment, configuration, alerting, exporting, troubleshooting, and more.\n\n---\n\n## \:tada: Community\n\nJoin the Netdata community:\n\n* [Discord](https://discord.com/invite/2mEmfW735j)\n* [Forum](https://community.netdata.cloud)\n* [GitHub Discussions](https://github.com/netdata/netdata/discussions)\n\n> [!NOTE]  \n> [Code of Conduct](https://github.com/netdata/.github/blob/main/CODE_OF_CONDUCT.md)\n\nFollow us on:\n[Twitter](https://twitter.com/netdatahq) | [Reddit](https://www.reddit.com/r/netdata/) | [YouTube](https://www.youtube.com/c/Netdata) | [LinkedIn](https://www.linkedin.com/company/netdata-cloud/)\n\n---\n\n## \:pray: Contribute\n\nWe welcome your contributions.\n\nWays you help us stay sharp:\n\n* Share best practices and monitoring insights\n* Report issues or missing features\n* Improve documentation\n* Develop new integrations or collectors\n* Help users in forums and chats\n\n> [!NOTE]  \n> [Contribution guide](https://github.com/netdata/.github/blob/main/CONTRIBUTING.md)\n\n---\n\n## \:scroll: License\n\nThe Netdata ecosystem includes:\n\n* **Netdata Agent** – Open-source core (GPLv3+). **Includes** data collection, storage, ML, alerting, APIs and **redistributes** several other open-source tools and libraries.\n    * [Netdata Agent License](https://github.com/netdata/netdata/blob/master/LICENSE)\n    * [Netdata Agent Redistributed](https://github.com/netdata/netdata/blob/master/REDISTRIBUTED.md)\n* **Netdata UI** – Closed-source but free to use with Netdata Agent and Cloud. Delivered via CDN. It integrates third-party open-source components.\n    * [Netdata Cloud UI License](https://app.netdata.cloud/LICENSE.txt)\n    * [Netdata UI third-party licenses](https://app.netdata.cloud/3D_PARTY_LICENSES.txt)\n* **Netdata Cloud** – Closed-source, with free and paid tiers. Adds remote access, SSO, scalability.\n', '{"language":"C","stars":76940,"forks":6258,"watchers":76940,"open_issues":246,"topics":["ai","alerting","cncf","data-visualization","database","devops","docker","grafana","influxdb","kubernetes","linux","machine-learning","mcp","mongodb","monitoring","mysql","netdata","observability","postgresql","prometheus"],"default_branch":"master","size_kb":245763,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:netdata:netdata","source_url":"https://github.com/netdata/netdata"},{"type":"has_code","target_id":"github:netdata:netdata","source_url":"https://github.com/netdata/netdata"},{"type":"has_code","target_id":"github:netdata:netdata-nightlies","source_url":"https://github.com/netdata/netdata-nightlies"},{"type":"has_code","target_id":"github:netdata:netdata","source_url":"https://github.com/netdata/netdata"},{"type":"has_code","target_id":"github:netdata:netdata","source_url":"https://github.com/netdata/netdata"},{"type":"has_code","target_id":"github:netdata:netdata","source_url":"https://github.com/netdata/netdata"},{"type":"has_code","target_id":"github:netdata:netdata","source_url":"https://github.com/netdata/netdata"},{"type":"has_code","target_id":"github:netdata:netdata","source_url":"https://github.com/netdata/netdata"},{"type":"has_code","target_id":"github:netdata:.github","source_url":"https://github.com/netdata/.github"},{"type":"has_code","target_id":"github:netdata:.github","source_url":"https://github.com/netdata/.github"},{"type":"has_code","target_id":"github:netdata:netdata","source_url":"https://github.com/netdata/netdata"},{"type":"has_code","target_id":"github:netdata:netdata","source_url":"https://github.com/netdata/netdata"}]', NULL, 'GPL-3.0', 'approved', 80, '7fdacde065c8cfc971cb3bdfbc2af887', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-netdata-netdata from https://github.com/netdata.png
Image converted to WebP: data/images/github-netdata-netdata.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-d2l-ai-d2l-zh', 'github--d2l-ai--d2l-zh', 'd2l-zh', 'd2l-ai', '第二版：zh.D2L.ai | 第一版：zh-v1.D2L.ai | 安装和使用书中源代码： 第二版 第一版 <h5 align="center"><i>理解深度学习的最佳方法是学以致用。</i></h5> <p align="center"> <img width="200" src="static/frontpage/_images/eq.jpg"> <img width="200" src="static/frontpage/_images/figure.jpg"> <img width="200" src="static/frontpage/_images/code.jpg"> <img width="200" src="static/frontpage/_images/notebook.gif"> </p> 本开源项目代表了我们的一种尝试：我们将教给读者概念、背景知识和代码；我们将在同一个地方阐述剖析问题所需的批判性思维、解决问题所需的数学知识，以及实现解决方案所需的工程技能。 我们的目标是创建一个为实现以下目标的统一资源： 1. 所有人均可在网上免费获取； 1. 提供...', '["book","chinese","computer-vision","deep-learning","machine-learning","natural-language-processing","notebook","python","python"]', 'other', 74263, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/d2l-ai/d2l-zh","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# 动手学深度学习（Dive into Deep Learning，D2L.ai）\n\n[第二版：zh.D2L.ai](https://zh.d2l.ai)  | [第一版：zh-v1.D2L.ai](https://zh-v1.d2l.ai/) |  安装和使用书中源代码： [第二版](https://zh.d2l.ai/chapter_installation/index.html) [第一版](https://zh-v1.d2l.ai/chapter_prerequisite/install.html)\n\n<h5 align="center"><i>理解深度学习的最佳方法是学以致用。</i></h5>\n\n<p align="center">\n  <img width="200"  src="static/frontpage/_images/eq.jpg">\n  <img width="200"  src="static/frontpage/_images/figure.jpg">\n  <img width="200"  src="static/frontpage/_images/code.jpg">\n  <img width="200"  src="static/frontpage/_images/notebook.gif">\n</p>\n\n本开源项目代表了我们的一种尝试：我们将教给读者概念、背景知识和代码；我们将在同一个地方阐述剖析问题所需的批判性思维、解决问题所需的数学知识，以及实现解决方案所需的工程技能。\n\n我们的目标是创建一个为实现以下目标的统一资源：\n1. 所有人均可在网上免费获取；\n1. 提供足够的技术深度，从而帮助读者实际成为深度学习应用科学家：既理解数学原理，又能够实现并不断改进方法；\n1. 包含可运行的代码，为读者展示如何在实际中解决问题。这样不仅直接将数学公式对应成实际代码，而且可以修改代码、观察结果并及时获取经验；\n1. 允许我们和整个社区不断快速迭代内容，从而紧跟仍在高速发展的深度学习领域；\n1. 由包含有关技术细节问答的论坛作为补充，使大家可以相互答疑并交换经验。\n\n<h5 align="center">将本书（中英文版）用作教材或参考书的大学</h5>\n<p align="center">\n  <img width="400"  src="https://d2l.ai/_images/map.png">\n</p>\n\n如果本书对你有帮助，请Star (★) 本仓库或引用本书的英文版：\n\n```\n@book{zhang2023dive,\n    title={Dive into Deep Learning},\n    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},\n    publisher={Cambridge University Press},\n    note={\url{https://D2L.ai}},\n    year={2023}\n}\n```\n\n## 本书的英文版\n\n虽然纸质书已出版，但深度学习领域依然在迅速发展。为了得到来自更广泛的英文开源社区的帮助，从而提升本书质量，本书的新版将继续用英文编写，并搬回中文版。\n\n欢迎关注本书的[英文开源项目](https://github.com/d2l-ai/d2l-en)。\n\n## 中英文教学资源\n\n加州大学伯克利分校 2019 年春学期 [*Introduction to Deep Learning* 课程](http://courses.d2l.ai/berkeley-stat-157/index.html)教材（同时提供含教学视频地址的[中文版课件](https://github.com/d2l-ai/berkeley-stat-157/tree/master/slides-zh)）。\n\n## 学术界推荐\n\n> <p>"Dive into this book if you want to dive into deep learning!"</p>\n> <b>&mdash; 韩家炜，ACM 院士、IEEE 院士，美国伊利诺伊大学香槟分校计算机系 Michael Aiken Chair 教授</b>\n\n> <p>"This is a highly welcome addition to the machine learning literature."</p>\n> <b>&mdash; Bernhard Schölkopf，ACM 院士、德国国家科学院院士，德国马克斯•普朗克研究所智能系统院院长</b>\n\n> <p>"书中代码可谓‘所学即所用’。"</p>\n> <b>&mdash; 周志华，ACM 院士、IEEE 院士、AAAS 院士，南京大学计算机科学与技术系主任</b>\n\n> <p>"这本书可以帮助深度学习实践者快速提升自己的能力。"</p>\n> <b>&mdash; 张潼，ASA 院士、IMS 院士，香港科技大学计算机系和数学系教授</b>\n\n## 工业界推荐\n\n> <p>"一本优秀的深度学习教材，值得任何想了解深度学习何以引爆人工智能革命的人关注。"</p>\n> <b>&mdash; 黄仁勋，NVIDIA创始人 & CEO</b>\n\n> <p>"《动手学深度学习》是最适合工业界研发工程师学习的。我毫无保留地向广大的读者们强烈推荐。"</p>\n> <b>&mdash; 余凯，地平线公司创始人 & CEO</b>\n\n> <p>"强烈推荐这本书！我特别赞赏这种手脑一体的学习方式。"</p>\n> <b>&mdash; 漆远，复旦大学“浩清”教授、人工智能创新与产业研究院院长</b>\n\n> <p>"《动手学深度学习》是一本很容易让学习者上瘾的书。"</p>\n> <b>&mdash; 沈强，将门创投创始合伙人</b>\n\n## 贡献\n\n感谢[社区贡献者们](https://github.com/d2l-ai/d2l-zh/graphs/contributors)为每一位读者改进这本开源书。\n\n[如何贡献](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html) | [致谢](https://zh.d2l.ai/chapter_preface/index.html) | [讨论或报告问题](https://discuss.d2l.ai/c/chinese-version/16) | [其他](INFO.md)\n', '{"language":"Python","stars":74263,"forks":12019,"watchers":74263,"open_issues":119,"topics":["book","chinese","computer-vision","deep-learning","machine-learning","natural-language-processing","notebook","python"],"default_branch":"master","size_kb":316965,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:d2l-ai:d2l-en","source_url":"https://github.com/d2l-ai/d2l-en"},{"type":"has_code","target_id":"github:d2l-ai:berkeley-stat-157","source_url":"https://github.com/d2l-ai/berkeley-stat-157"},{"type":"has_code","target_id":"github:d2l-ai:d2l-zh","source_url":"https://github.com/d2l-ai/d2l-zh"}]', NULL, 'Apache-2.0', 'approved', 65, 'f8a684d85b0c9e6ded8510777eeb94ee', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-d2l-ai-d2l-zh from https://github.com/d2l-ai.png
Image converted to WebP: data/images/github-d2l-ai-d2l-zh.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-tesseract-ocr-tesseract', 'github--tesseract-ocr--tesseract', 'tesseract', 'tesseract-ocr', '\ * Tesseract OCR * About * Brief history * Installing Tesseract * Running Tesseract * For developers * Support * License * Dependencies * Latest Version of README This package contains an **OCR engine** - and a **command line program** - . Tesseract 4 adds a new neural net (LSTM) based OCR engine which is focused on line recognition, but also still supports the legacy Tesseract OCR engine of Tesseract 3 which works by recognizing character patterns. Compatibility with Tesseract 3 is enabled ...', '["hacktoberfest","lstm","machine-learning","ocr","ocr-engine","tesseract","tesseract-ocr","c++"]', 'other', 71281, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/tesseract-ocr/tesseract","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Tesseract OCR\n\n[![Coverity Scan Build Status](https://scan.coverity.com/projects/tesseract-ocr/badge.svg)](https://scan.coverity.com/projects/tesseract-ocr)\n[![CodeQL](https://github.com/tesseract-ocr/tesseract/workflows/CodeQL/badge.svg)](https://github.com/tesseract-ocr/tesseract/security/code-scanning)\n[![OSS-Fuzz](https://img.shields.io/badge/oss--fuzz-fuzzing-brightgreen)](https://issues.oss-fuzz.com/issues?q=is:open%20title:tesseract-ocr)\n\\n[![GitHub license](https://img.shields.io/badge/license-Apache--2.0-blue.svg)](https://raw.githubusercontent.com/tesseract-ocr/tesseract/main/LICENSE)\n[![Downloads](https://img.shields.io/badge/download-all%20releases-brightgreen.svg)](https://github.com/tesseract-ocr/tesseract/releases/)\n\n## Table of Contents\n\n* [Tesseract OCR](#tesseract-ocr)\n  * [About](#about)\n  * [Brief history](#brief-history)\n  * [Installing Tesseract](#installing-tesseract)\n  * [Running Tesseract](#running-tesseract)\n  * [For developers](#for-developers)\n  * [Support](#support)\n  * [License](#license)\n  * [Dependencies](#dependencies)\n  * [Latest Version of README](#latest-version-of-readme)\n\n## About\n\nThis package contains an **OCR engine** - `libtesseract` and a **command line program** - `tesseract`.\n\nTesseract 4 adds a new neural net (LSTM) based [OCR engine](https://en.wikipedia.org/wiki/Optical_character_recognition) which is focused on line recognition, but also still supports the legacy Tesseract OCR engine of Tesseract 3 which works by recognizing character patterns. Compatibility with Tesseract 3 is enabled by using the Legacy OCR Engine mode (--oem 0).\nIt also needs [traineddata](https://tesseract-ocr.github.io/tessdoc/Data-Files.html) files which support the legacy engine, for example those from the [tessdata](https://github.com/tesseract-ocr/tessdata) repository.\n\nStefan Weil is the current lead developer. Ray Smith was the lead developer until 2017. The maintainer is Zdenko Podobny. For a list of contributors see [AUTHORS](https://github.com/tesseract-ocr/tesseract/blob/main/AUTHORS)\nand GitHub''s log of [contributors](https://github.com/tesseract-ocr/tesseract/graphs/contributors).\n\nTesseract has **unicode (UTF-8) support**, and can **recognize [more than 100 languages](https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html)** "out of the box".\n\nTesseract supports **[various image formats](https://tesseract-ocr.github.io/tessdoc/InputFormats)** including PNG, JPEG and TIFF.\n\nTesseract supports **various output formats**: plain text, hOCR (HTML), PDF, invisible-text-only PDF, TSV, ALTO and PAGE.\n\nYou should note that in many cases, in order to get better OCR results, you''ll need to **[improve the quality](https://tesseract-ocr.github.io/tessdoc/ImproveQuality.html) of the image** you are giving Tesseract.\n\nThis project **does not include a GUI application**. If you need one, please see the [3rdParty](https://tesseract-ocr.github.io/tessdoc/User-Projects-%E2%80%93-3rdParty.html) documentation.\n\nTesseract **can be trained to recognize other languages**.\nSee [Tesseract Training](https://tesseract-ocr.github.io/tessdoc/Training-Tesseract.html) for more information.\n\n## Brief history\n\nTesseract was originally developed at Hewlett-Packard Laboratories Bristol UK and at Hewlett-Packard Co, Greeley Colorado USA between 1985 and 1994, with some more changes made in 1996 to port to Windows, and some C++izing in 1998. In 2005 Tesseract was open sourced by HP. From 2006 until August 2017 it was developed by Google.\n\nMajor version 5 is the current stable version and started with release\n[5.0.0](https://github.com/tesseract-ocr/tesseract/releases/tag/5.0.0) on November 30, 2021. Newer minor versions and bugfix versions are available from\n[GitHub](https://github.com/tesseract-ocr/tesseract/releases/).\n\nLatest source code is available from [main branch on GitHub](https://github.com/tesseract-ocr/tesseract/tree/main).\nOpen issues can be found in [issue tracker](https://github.com/tesseract-ocr/tesseract/issues),\nand [planning documentation](https://tesseract-ocr.github.io/tessdoc/Planning.html).\n\nSee **[Release Notes](https://tesseract-ocr.github.io/tessdoc/ReleaseNotes.html)**\nand **[Change Log](https://github.com/tesseract-ocr/tesseract/blob/main/ChangeLog)** for more details of the releases.\n\n## Installing Tesseract\n\nYou can either [Install Tesseract via pre-built binary package](https://tesseract-ocr.github.io/tessdoc/Installation.html)\nor [build it from source](https://tesseract-ocr.github.io/tessdoc/Compiling.html).\n\nBefore building Tesseract from source, please check that your system has a compiler which is one of the [supported compilers](https://tesseract-ocr.github.io/tessdoc/supported-compilers.html).\n\n## Running Tesseract\n\nBasic **[command line usage](https://tesseract-ocr.github.io/tessdoc/Command-Line-Usage.html)**:\n\n    tesseract imagename outputbase [-l lang] [--oem ocrenginemode] [--psm pagesegmode] [configfiles...]\n\nFor more information about the various command line options use `tesseract --help` or `man tesseract`.\n\nExamples can be found in the [documentation](https://tesseract-ocr.github.io/tessdoc/Command-Line-Usage.html#simplest-invocation-to-ocr-an-image).\n\n## For developers\n\nDevelopers can use `libtesseract` [C](https://github.com/tesseract-ocr/tesseract/blob/main/include/tesseract/capi.h) or\n[C++](https://github.com/tesseract-ocr/tesseract/blob/main/include/tesseract/baseapi.h) API to build their own application. If you need bindings to `libtesseract` for other programming languages, please see the\n[wrapper](https://tesseract-ocr.github.io/tessdoc/AddOns.html#tesseract-wrappers) section in the AddOns documentation.\n\nDocumentation of Tesseract generated from source code by doxygen can be found on [tesseract-ocr.github.io](https://tesseract-ocr.github.io/).\n\n## Support\n\nBefore you submit an issue, please review **[the guidelines for this repository](https://github.com/tesseract-ocr/tesseract/blob/main/CONTRIBUTING.md)**.\n\nFor support, first read the [documentation](https://tesseract-ocr.github.io/tessdoc/),\nparticularly the [FAQ](https://tesseract-ocr.github.io/tessdoc/FAQ.html) to see if your problem is addressed there.\nIf not, search the [Tesseract user forum](https://groups.google.com/g/tesseract-ocr), the [Tesseract developer forum](https://groups.google.com/g/tesseract-dev) and [past issues](https://github.com/tesseract-ocr/tesseract/issues), and if you still can''t find what you need, ask for support in the mailing-lists.\n\nMailing-lists:\n\n* [tesseract-ocr](https://groups.google.com/g/tesseract-ocr) - For tesseract users.\n* [tesseract-dev](https://groups.google.com/g/tesseract-dev) - For tesseract developers.\n\nPlease report an issue only for a **bug**, not for asking questions.\n\n## License\n\n    The code in this repository is licensed under the Apache License, Version 2.0 (the "License");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an "AS IS" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n**NOTE**: This software depends on other packages that may be licensed under different open source licenses.\n\nTesseract uses [Leptonica library](http://leptonica.com/) which essentially\nuses a [BSD 2-clause license](http://leptonica.com/about-the-license.html).\n\n## Dependencies\n\nTesseract uses [Leptonica library](https://github.com/DanBloomberg/leptonica)\nfor opening input images (e.g. not documents like pdf).\nIt is suggested to use leptonica with built-in support for [zlib](https://zlib.net),\n[png](https://sourceforge.net/projects/libpng) and\n[tiff](http://www.simplesystems.org/libtiff) (for multipage tiff).\n\n## Latest Version of README\n\nFor the latest online version of the README.md see:\n\n<https://github.com/tesseract-ocr/tesseract/blob/main/README.md>\n', '{"language":"C++","stars":71281,"forks":10414,"watchers":71281,"open_issues":463,"topics":["hacktoberfest","lstm","machine-learning","ocr","ocr-engine","tesseract","tesseract-ocr"],"default_branch":"main","size_kb":53713,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tessdata","source_url":"https://github.com/tesseract-ocr/tessdata"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:DanBloomberg:leptonica","source_url":"https://github.com/DanBloomberg/leptonica"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"}]', NULL, 'Apache-2.0', 'approved', 65, '771d5700c042315b910b5599d47c464b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-tesseract-ocr-tesseract from https://github.com/tesseract-ocr.png
Image converted to WebP: data/images/github-tesseract-ocr-tesseract.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-Developer-Y-cs-video-courses', 'github--developer-y--cs-video-courses', 'cs-video-courses', 'Developer-Y', '<!-- omit in toc --> <!-- omit in toc --> - Please check NOTES for general information about this list. - Please refer CONTRIBUTING.md for contribution guidelines. - Please feel free to raise any genuine issue you may have, however, it has been noticed that few people open empty issues to raise their GitHub contribution on their account. Such spammers will be blocked. - You are welcome to contribute, please create PR for actual college/University level courses. Please do not add links for sma...', '["algorithms","bioinformatics","computational-biology","computational-physics","computer-architecture","computer-science","computer-vision","database-systems","databases","deep-learning","embedded-systems","machine-learning","quantum-computing","reinforcement-learning","robotics","security","systems","web-development"]', 'other', 70337, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/Developer-Y/cs-video-courses","fetched_at":"2025-12-08T10:39:52.041Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<!-- omit in toc -->\n# Computer Science courses with video lectures\n\n<!-- omit in toc -->\n## Introduction\n\n- Please check [NOTES](https://github.com/Developer-Y/cs-video-courses/blob/master/NOTES.md) for general information about this list.\n- Please refer [CONTRIBUTING.md](https://github.com/Developer-Y/cs-video-courses/blob/master/CONTRIBUTING.md) for contribution guidelines.\n- Please feel free to raise any genuine issue you may have, however, it has been noticed that few people open empty issues to raise their GitHub contribution on their account. Such spammers will be blocked. \n- You are welcome to contribute, please create PR for actual college/University level courses. Please do not add links for small MOOCs, basic tutorials, or advertisements for some sites/channels.\n\n------------------------------\n\nTable of Contents\n\n------------------------------\n- [Introduction to Computer Science](#introduction-to-computer-science)\n- [Data Structures and Algorithms](#data-structures-and-algorithms)\n- [Systems Programming](#systems-programming)\n  * [Operating Systems](#operating-systems)\n  * [Distributed Systems](#distributed-systems)\n  * [Real-Time Systems](#real-time-systems) \n- [Database Systems](#database-systems)\n- [Software Engineering](#software-engineering)\n  * [Object Oriented Design](#object-oriented-design)\n  * [Software Engineering](#software-engineering)\n  * [Software Architecture](#software-architecture)\n  * [Concurrency](#concurrency)\n  * [Mobile Application Development](#mobile-application-development)\n- [Artificial Intelligence](#artificial-intelligence)\n- [Machine Learning](#machine-learning)\n  * [Introduction to Machine Learning](#introduction-to-machine-learning)\n  * [Data Mining](#data-mining)\n  * [Probabilistic Graphical Modeling](#probabilistic-graphical-modeling)\n  * [Deep Learning](#deep-learning)\n  * [Reinforcement Learning](#reinforcement-learning)\n  * [Advanced Machine Learning](#advanced-machine-learning)\n  * [Natural Language Processing](#natural-language-processing)\n  * [Generative AI and LLMs](#generative-ai-and-llms)\n  * [Computer Vision](#computer-vision)\n  * [Time Series Analysis](#time-series-analysis)\n  * [Optimization](#optimization)\n  * [Unsupervised Learning](#unsupervised-learning)\n  * [Misc Machine Learning Topics](#misc-machine-learning-topics)\n- [Computer Networks](#computer-networks)\n- [Math for Computer Scientist](#math-for-computer-scientist)\n- [Web Programming and Internet Technologies](#web-programming-and-internet-technologies)\n- [Theoretical CS and Programming Languages](#theoretical-cs-and-programming-languages)\n- [Embedded Systems](#embedded-systems)\n- [Real time system evaluation](#real-time-system-evaluation)\n- [Computer Organization and Architecture](#computer-organization-and-architecture)\n- [Security](#security)\n- [Computer Graphics](#computer-graphics)\n- [Image Processing and Computer Vision](#image-processing-and-computer-vision)\n- [Computational Physics](#computational-physics)\n- [Computational Biology](#computational-biology)\n- [Quantum Computing](#quantum-computing)\n- [Robotics and Control](#robotics-and-control)\n- [Computational Finance](#computational-finance)\n- [Network Science](#network-science)\n- [Blockchain Development](#blockchain-development)\n- [Misc](#misc)\n\n<!-- omit in toc -->\n## Courses\n\n------------------------------\n\n### Introduction to Computer Science\n\n- [CS 10 - The Beauty and Joy of Computing - Spring 2015 - Dan Garcia - UC Berkeley InfoCoBuild](http://www.infocobuild.com/education/audio-video-courses/computer-science/cs10-spring2015-berkeley.html)\n- [6.0001 - Introduction to Computer Science and Programming in Python - MIT OCW](https://ocw.mit.edu/courses/6-0001-introduction-to-computer-science-and-programming-in-python-fall-2016/video_galleries/lecture-videos/)\n- [6.001 - Structure and Interpretation of Computer Programs, MIT](https://ocw.mit.edu/courses/6-001-structure-and-interpretation-of-computer-programs-spring-2005/video_galleries/video-lectures/)\n- [Introduction to Computational Thinking - MIT](https://computationalthinking.mit.edu/Fall22/)\n- [CS 50 - Introduction to Computer Science, Harvard University](https://online-learning.harvard.edu/course/cs50-introduction-computer-science) ([cs50.tv](http://cs50.tv/2017/fall/))\n- [CS50R - Introduction to Programming with R](https://cs50.harvard.edu/r/2024/) ([Lecture Videos](https://www.youtube.com/playlist?list=PLhQjrBD2T382yfNp_-xzX244d-O9W6YmD))\n- [CS50: Introduction to Computer Science with Python - Harvard (David J. Malan)](https://www.youtube.com/playlist?list=PLhQjrBD2T3817j24-GogXmWqO5Q5vYy0V)\n- [CS 61A - Structure and Interpretation of Computer Programs [Python], UC Berkeley](https://cs61a.org/)\n- [CPSC 110 - Systematic Program Design [Racket], University of British Columbia](https://www.youtube.com/channel/UC7dEjIUwSxSNcW4PqNRQW8w/playlists?view=1&flow=grid&sort=da)\n- [CS50''s Understanding Technology](https://www.youtube.com/playlist?list=PLhQjrBD2T382p8amnvUp1rws1p7n7gJ2p)\n- [CSE 142 Computer Programming I (Java Programming), Spring 2016 - University of Washington](https://courses.cs.washington.edu/courses/cse142/16sp/calendar.shtml)\n- [CS 1301 Intro to computing - Gatech](https://www.cc.gatech.edu/classes/AY2016/cs1301c_fall/)\n- [CS 106A - Programming Methodology, Stanford University](https://see.stanford.edu/Course/CS106A) ([Lecture Videos](https://www.youtube.com/playlist?list=PL84A56BC7F4A1F852))\n- [CS 106B - Programming Abstractions, Stanford University](https://see.stanford.edu/Course/CS106B) ([Lecture Videos](https://www.youtube.com/playlist?list=PLnfg8b9vdpLn9exZweTJx44CII1bYczuk))\n- [CS 106L - Standard C++ Programming](https://web.stanford.edu/class/cs106l/)([Lecture Videos](https://www.youtube.com/playlist?list=PLCgD3ws8aVdolCexlz8f3U-RROA0s5jWA))\n- [CS 106X - Programming Abstractions in C++](http://web.stanford.edu/class/cs106x/) ([Lecture Videos](https://www.youtube.com/playlist?list=PLrivl8gTKLcpIJ-ktHCxMEgWOn8LawYhb))\n- [CS 107 - Programming Paradigms, Stanford University](https://see.stanford.edu/Course/CS107)\n- [CmSc 150 - Introduction to Programming with Arcade Games, Simpson College](http://ProgramArcadeGames.com)\n- [IN2377 - Concepts of C++ programming (Winter 2023), TUM](https://live.rbg.tum.de/?year=2023&term=W&slug=cpp&view=3) ([Winter 2022](https://live.rbg.tum.de/?year=2022&term=W&slug=cpp&view=3)) ([Summer 2022](https://live.rbg.tum.de/?year=2022&term=S&slug=ccppprog&view=3)) ([Summer 2021](https://live.rbg.tum.de/?year=2021&term=S&slug=ccppprog&view=3))\n- [IN1503 - Advanced C++ Programming, TUM](https://live.rbg.tum.de/?year=2023&term=W&slug=AdvProg&view=3)\n- [LINFO 1104 - Paradigms of computer programming, Peter Van Roy, Université catholique de Louvain, Belgium - EdX](https://www.youtube.com/playlist?list=PLw454N-VXALSIzIe_eL5U8L4S68v2X_ak)\n- [FP 101x - Introduction to Functional Programming, TU Delft](https://ocw.tudelft.nl/courses/introduction-to-functional-programming/)\n- [Introduction to Problem Solving and Programming - IIT Kanpur](https://nptel.ac.in/courses/106104074/)\n- [Introduction to programming in C - IIT Kanpur](https://nptel.ac.in/courses/106104128/)\n- [Programming in C++ - IIT Kharagpur](https://nptel.ac.in/courses/106105151/)\n- [Python Boot Camp Fall 2016 - Berkeley Institute for Data Science (BIDS)](https://www.youtube.com/playlist?list=PLKW2Azk23ZtSeBcvJi0JnL7PapedOvwz9)\n- [CS 101 - Introduction to Computer Science - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPmjFQ2w9j05WDX8Jtg5RXWW)\n- [6.00SC - Introduction to Computer Science and Programming (Spring 2011) - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-00sc-introduction-to-computer-science-and-programming-spring-2011/)\n- [6.00 - Introduction to Computer Science and Programming (Fall 2008) - MIT OCW](https://ocw.mit.edu/courses/6-00-introduction-to-computer-science-and-programming-fall-2008/video_galleries/video-lectures/)\n- [6.01SC - Introduction to Electrical Engineering and Computer Science I - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-01sc-introduction-to-electrical-engineering-and-computer-science-i-spring-2011/)\n- [Modern C++ Course (2018) - Bonn University](https://www.youtube.com/playlist?list=PLgnQpQtFTOGR50iIOtO36nK6aNPtVq98C)\n- [Modern C++ (Lecture & Tutorials, 2020, Vizzo & Stachniss) - University of Bonn](https://www.youtube.com/playlist?list=PLgnQpQtFTOGRM59sr3nSL8BmeMZR9GCIA)\n- [UW Madison CS 368 C++ for Java Programmers Fall 2020, by Michael Doescher](https://www.youtube.com/playlist?list=PLXY5xcFHqg33srpQjC7q7jqITLxcErPCM)\n- [UW Madison CS 354 Machine Organization and Programming spring 2020, 2021, by Michael Doescher](https://www.youtube.com/playlist?list=PLXY5xcFHqg32r5MZ-HfpA2Tr8Ke2lDYwI)\n- [Cornell CS 1110 Introduction to Computing using Python fall 2020, by Walker White](https://www.cs.cornell.edu/courses/cs1110/2020fa/lessons/) ([Lecture Videos](https://vod.video.cornell.edu/channel/CS+1110+Fall+2020/179890731))\n- [Cornell ECE 4960 Computational and Software Engineering spring 2017, by Edwin Kan](https://www.youtube.com/playlist?list=PLcVqWUh-bHiFN2CY1KMTw0-L39iDXlemi)\n\n------------------------------\n\n### Data Structures and Algorithms\n\n- [ECS 36C - Data Structures and Algorithms (C++) - Spring 2020 - Joël Porquet-Lupine - UC Davis](https://lupteach.gitlab.io/courses/ucd-ecs36c/online/)\n- [Programming and Data Structures with Python, 2021-2022, Sem I - by Prof. Madhavan Mukund, CMI](https://www.cmi.ac.in/~madhavan/courses/python2021sep/)\n- [Graph Algorithms - Robert Sedgewick - Princeton University](https://www.youtube.com/watch?v=0qF7tPSQdCg)\n- [6.006 - Introduction to Algorithms, MIT OCW](https://ocw.mit.edu/courses/6-006-introduction-to-algorithms-spring-2020/video_galleries/lecture-videos/)\n- [MIT 6.006 Introduction to Algorithms, Spring 2020](https://www.youtube.com/playlist?list=PLUl4u3cNGP63EdVPNLG3ToM6LaEUuStEY)\n- [Algorithms: Design and Analysis 1 - Stanford University](https://www.youtube.com/playlist?list=PLXFMmlk03Dt7Q0xr1PIAriY5623cKiH7V)\n- [Algorithms: Design and Analysis 2 - Stanford University](https://www.youtube.com/playlist?list=PLXFMmlk03Dt5EMI2s2WQBsLsZl7A5HEK6)\n- [COS 226 Algorithms, Youtube, Princeton - by Robert Sedgewick and Kevin Wayne](https://www.youtube.com/watch?v=1QZDe28peZk&list=PLRdD1c6QbAqJn0606RlOR6T3yUqFWKwmX&index=1)\n- [CSE 331 Introduction to Algorithm Design and Analysis, SUNY University at Buffalo, NY - Fall 2017](http://www-student.cse.buffalo.edu/~atri/cse331/fall17/index.html) ([Lectures](https://www.youtube.com/playlist?list=PLZBCR-EGqNpoiHeO17FlLADJ38Kb3EiPU)) ([Homework Walkthroughs](https://www.youtube.com/playlist?list=PLZBCR-EGqNpoVyQCIUDHiXnL-zdFD_ixk))\n- [CSE 373 - Analysis of Algorithms, Stony Brook - Prof Skiena](http://www.cs.sunysb.edu/~algorith/video-lectures/)\n- [COP 3530 Data Structures and Algorithms, Prof Sahni, UFL](http://www.cise.ufl.edu/~sahni/cop3530/) ([Videos](http://www.cise.ufl.edu/academics/courses/preview/cop3530sahni/))\n- [CS225 - Data Structures - University of Illinois at Urbana-Champaign](https://cs.illinois.edu/courses/profile/CS225)([Video lectures](https://www.youtube.com/playlist?list=PLRdSp8jtJxBqG3KNQPKKB-0Z2hh9omoDo))\n- [CS2 - Data Structures and Algorithms - Richard Buckland - UNSW](https://www.youtube.com/playlist?list=PLE621E25B3BF8B9D1)\n- [Data Structures - Pepperdine University](https://itunes.apple.com/us/course/data-structures/id546468797)\n- [CS 161 - Design and Analysis of Algorithms, Prof. Tim Roughgarden, Stanford University](http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=IntroToAlgorithms)\n- [6.046J - Introduction to Algorithms - Fall 2005, MIT OCW](https://ocw.mit.edu/courses/6-046j-introduction-to-algorithms-sma-5503-fall-2005/)\n- [Introduction to Algorithms (Spring 2020), MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-spring-2020/)\n- [6.046 - Design and Analysis of Algorithms, Spring 2015 - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/lecture-videos/)\n- [CS 473 - Algorithms - University of Illinois at Urbana-Champaign](https://courses.engr.illinois.edu/cs473/sp2016/lectures.html) ([Notes - Jeff Erickson](http://jeffe.cs.illinois.edu/teaching/algorithms/)) ([YouTube](https://www.youtube.com/playlist?list=PL0v718LJg-78SFq81e4kJh_rS8XbKZ7Kn))\n- [COMP300E - Programming Challenges, Prof Skiena, Hong Kong University of Science and Technology - 2009](https://www.youtube.com/playlist?list=PL07B3F10B48592010)\n- [16s-4102 - Algorithms, University of Virginia](http://www.cs.virginia.edu/~shelat/16s-4102/) ([Youtube](https://www.youtube.com/channel/UCxXYk53cSZof2bR_Ax0uJYQ/videos))\n- [CS 61B - Data Structures (Java) - UC Berkeley](https://inst.eecs.berkeley.edu/~cs61b/)([Youtube](https://www.youtube.com/watch?v=gG4--V_PpEk&list=PLjuu7kFWxFtZBm-5GifiVpqdAxeW7Hsax))\n- [CS 170 Algorithms - UCBerkeley](https://cs170.org/) [Fall 2019, Youtube](https://www.youtube.com/playlist?list=PLIygTcviGPKD4TU_QsvJI0G7QnrIS_7Wn) [Fall 2018, Youtube](https://www.youtube.com/watch?v=fd5P-8IQwMY&list=PLkFD6_40KJIx8lWWbE-Uk069aZ1R-W-VU&index=2&t=0s) [Fall 2018,Bilibili](https://www.bilibili.com/video/av43955743/?p=1) [2013 Bilibili](https://www.bilibili.com/video/av26670685/)\n- [CS 159 Data-Driven Algorithm Design - Caltech](https://sites.google.com/view/cs-159-spring-2020/lectures?authuser=0) [Spring 2020, Youtube](https://www.youtube.com/playlist?list=PLuz4CTPOUNi4Dz6zBPypcI8I3oJUjFKk4)\n- [ECS 122A - Algorithm Design and Analysis, UC Davis](http://web.cs.ucdavis.edu/~gusfield/cs122f10/videolist.html)\n- [CSE 373 - Data Structures and Algorithms, Winter 2024 - University of Washington](https://courses.cs.washington.edu/courses/cse373/24wi/) ([Winter 2024, Youtube](https://www.youtube.com/playlist?list=PLEcoVsAaONjd5n69K84sSmAuvTrTQT_Nl)) ([Spring 2023, Notes](https://courses.cs.washington.edu/courses/cse373/23sp/)) ([Spring 2023, Youtube](https://www.youtube.com/playlist?list=PLEcoVsAaONjfHSAbP1AsVjAxIOFue6uWh))\n- [CSEP 521 - Applied Algorithms, Winter 2013 - University of Washington](https://courses.cs.washington.edu/courses/csep521/13wi/) ([Videos](https://courses.cs.washington.edu/courses/csep521/13wi/video/))\n- [Data Structures And Algorithms - IIT Delhi](https://nptel.ac.in/courses/106102064/)\n- [Design and Analysis of Algorithms - IIT Bombay](https://nptel.ac.in/courses/106101060/)\n- [Programming, Data Structures and Algorithms - IIT Madras](https://nptel.ac.in/courses/106106127/)\n- [Design and Analysis of Algorithms - IIT Madras](https://nptel.ac.in/courses/106106131/)\n- [Fundamental Algorithms:Design and Analysis - IIT Kharagpur](https://nptel.ac.in/courses/106105157/)\n- [Programming and Data Structure - IIT Kharagpur](https://nptel.ac.in/courses/106105085/)\n- [Programming, Data structures and Algorithms - IIT Madras](https://nptel.ac.in/courses/106106133/)\n- [Programming, Data Structures and Algorithms in Python - IIT Madras](https://nptel.ac.in/courses/106106145/)\n- [Programming and Data structures (PDS) - IIT Madras](https://nptel.ac.in/courses/106106130/)\n- [COP 5536 Advanced Data Structures, Prof Sahni - UFL](http://www.cise.ufl.edu/~sahni/cop5536/index.html) ([Videos](http://www.cise.ufl.edu/academics/courses/preview/cop5536sahni/))\n- [CS 261 - A Second Course in Algorithms, Stanford University](http://theory.stanford.edu/~tim/w16/w16.html) ([Youtube](https://www.youtube.com/playlist?list=PLEGCF-WLh2RJh2yDxlJJjnKswWdoO8gAc))\n- [CS 224 - Advanced Algorithms, Harvard University](http://people.seas.harvard.edu/~minilek/cs224/fall14/index.html) ([Lecture Videos](http://people.seas.harvard.edu/~minilek/cs224/fall14/lec.html)) ([Youtube](https://www.youtube.com/playlist?list=PL2SOU6wwxB0uP4rJgf5ayhHWgw7akUWSf))\n- [CS 6150 - Advanced Algorithms (Fall 2016), University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCp8X9FHOglnLqFjyvqGLftx)\n- [CS 6150 - Advanced Algorithms (Fall 2017), University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCqS9Z419eky9m6gJP7zfhO9)\n- [ECS 222A - Graduate Level Algorithm Design and Analysis, UC Davis](http://web.cs.ucdavis.edu/~gusfield/cs222f07/videolist.html)\n- [6.851 - Advanced Data Structures, MIT](http://courses.csail.mit.edu/6.851/spring14/lectures/) ([MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-851-advanced-data-structures-spring-2012/lecture-videos/))\n- [6.854 - Advanced Algorithms, MIT](https://www.youtube.com/playlist?list=PL6ogFv-ieghdoGKGg2Bik3Gl1glBTEu8c) ([Prof. Karger lectures](https://www.youtube.com/channel/UCtv9PiQVUDzsT4yl7524DCg/videos))\n- [CS264 Beyond Worst-Case Analysis, Fall 2014 - Tim Roughgarden Lecture](http://theory.stanford.edu/~tim/f14/f14.html) ([Youtube](https://www.youtube.com/playlist?list=PLEGCF-WLh2RL8jsZpaf2tLHa5LotFEt5b))\n- [CS364A Algorithmic Game Theory, Fall 2013 - Tim Roughgarden Lectures](https://www.youtube.com/playlist?list=PLEGCF-WLh2RJBqmxvZ0_ie-mleCFhi2N4)\n- [CS364B Advanced Mechanism Design, Winter 2014 - Tim Roughgarden Lectures](https://www.youtube.com/playlist?list=PLEGCF-WLh2RI77PL4gwLld_OU9Zh3TCX9)\n- [Algorithms - Aduni](http://aduni.org/courses/algorithms/index.php?view=cw)\n- [6.889 - Algorithms for Planar Graphs and Beyond (Fall 2011) MIT](http://courses.csail.mit.edu/6.889/fall11/lectures/)\n- [6.890 Algorithmic Lower Bounds: Fun with Hardness Proofs - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-890-algorithmic-lower-bounds-fun-with-hardness-proofs-fall-2014/)\n- [Computer Algorithms - 2 - IIT Kanpur](https://nptel.ac.in/courses/106104019/)\n- [Parallel Algorithm - IIT Kanpur](https://nptel.ac.in/courses/106104120/)\n- [Graph Theory - IISC Bangalore](https://nptel.ac.in/courses/106108054/)\n- [Data Structures - mycodeschool](https://www.youtube.com/watch?v=92S4zgXN17o&list=PL2_aWCzGMAwI3W_JlcBbtYTwiQSsOTa6P)\n- [Algorithmic Game Theory, Winter 2020/21 - Uni Bonn](https://www.youtube.com/playlist?list=PLyzcvvgje7aD_DjpmhFzQ9DVS8zzhrgp6)\n- [NETS 4120: Algorithmic Game Theory, Spring 2023 - UPenn](https://www.youtube.com/playlist?list=PLlIlhe_rS4U1MfB0NzG4IWb7CM0xKkx4d)\n- [Introduction to Game Theory and Mechanism Design - IIT Kanpur](https://www.youtube.com/playlist?list=PL3eEm6KzZ3lF2TlVOnPyJHyGWJhUogn-D)\n- [15-850 Advanced Algorithms - CMU Spring 2023](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%2253c58248-7fd4-4f71-8774-af85013a570a%22&page=1)\n- [CS 270. Combinatorial Algorithms and Data Structures, Spring 2021](https://people.eecs.berkeley.edu/~prasad/spring2021.html) ([Youtube](https://www.youtube.com/playlist?list=PLfkeJ2f4i0AfWApBP8X8YvQfN4WbRQTC3))\n- [CMU 15 850 Advanced Algorithms spring 2023, by Anupam Gupta](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%2253c58248-7fd4-4f71-8774-af85013a570a%22&page=1)\n- [UC Berkeley CS 294-165 Sketching Algorithms fall 2020, by Jelani Nelson](https://www.sketchingbigdata.org/fall20/lec/) ([Youtube](https://www.youtube.com/playlist?list=PLIygTcviGPKCdx1AVD-CAzNk5uXDu9wIA))\n- [UIUC CS 498 ABD / CS 598 CSC Algorithms for Big Data fall 2020, by Chandra Chekuri](https://www.youtube.com/playlist?list=PL682UO4IMem_OA8_wY3nnSDLOSWr3PgYa)\n- [Algorithms for Data Science spring 2021, by Anil Maheshwari ](https://people.scs.carleton.ca/~maheshwa/courses/ADS/ADS-S20.html)\n- [CMU 15 859 Algorithms for Big Data fall 2020, by David Woodruff](http://www.cs.cmu.edu/~dwoodruf/teaching/15859-fall20/index.html)\n- [CO 642 Graph Theory - University of Waterloo](https://www.youtube.com/playlist?list=PL2BdWtDKMS6mplieDd_vls0TBX9Fq2jht)\n- [COMS W4241 Numerical Algorithms spring 2006, by Henryk Wozniakowski - Columbia](https://www.youtube.com/playlist?list=PL682UO4IMem98vm26lNUJ0TV0-EFrcUJb)\n- [Bonn Algorithms and Uncertainty summer 2021, by Thomas Kesselheim](https://www.youtube.com/playlist?list=PLyzcvvgje7aDZRFMJZgaVgOW5t5KLvD1-)\n- [Harvard Information Theory 2022, by Gregory Falkovich](https://www.youtube.com/playlist?list=PLDEN2FPNHwVZKAFqfFl1b_NNAESTJwV9o)\n- [Math 510 - Linear Programming and Network Flows - Colorado State University](https://www.math.colostate.edu/~adams/teaching/math510fall2020/)\n- [LINFO 2266 Advanced Algorithms for Optimization 2021, by Pierre Schaus - UCLouvain](https://www.youtube.com/playlist?list=PL682UO4IMem-wgYnJl5yMswlNkve_8oGU)\n- [MIT 6.5210 / 6.854 / 18.415 Advanced Algorithms Fall 2013, 2020, 2021, 2022, by David Karger](https://6.5210.csail.mit.edu/materials) ([Spring 2016, by Ankur Moitra](https://www.youtube.com/playlist?list=PL6ogFv-ieghdoGKGg2Bik3Gl1glBTEu8c))\n- [CMU 10 801 Advanced Optimization and Randomized Algorithms spring 2014, by Suvrit Sra and Alex Smola](https://www.cs.cmu.edu/~suvrit/teach/)\n- [Purdue CS 381 Fundamental Algorithms, by Kent Quanrud](https://fas22.s3.amazonaws.com/fas22-book.pdf) ([Spring 2022](https://www.youtube.com/playlist?list=PL0YFU3y0Z_gb6P9jG0-40AgZapdGGXPTD))\n- [Purdue CS 390 ATA Fundamental Algorithms Advanced, by Kent Quanrud](https://fas25.s3.amazonaws.com/fas25.pdf) ([Spring 2025](https://www.youtube.com/playlist?list=PL0YFU3y0Z_gYOC4uogZcu54XKq57TVEs3))\n- [Purdue CS 580 Graduate Algorithms, by Kent Quanrud](https://fas23.s3.amazonaws.com/fas23.pdf) ([Spring 2023](https://www.youtube.com/playlist?list=PL0YFU3y0Z_gYAwo5Tg4kP91ifXPF_FIQ1)) ([Spring 2024](https://www.youtube.com/playlist?list=PL0YFU3y0Z_gaGD6jZLeuLHTRW0ISFn6AU))\n- [Purdue CS 588 Randomized Algorithms, by Kent Quanrud](https://ras24.s3.amazonaws.com/ras24.pdf) ([Fall 2022](https://www.youtube.com/playlist?list=PL0YFU3y0Z_gbjvT1yDQkwRU9UXahd0BP1)) ([Spring 2024](https://www.youtube.com/playlist?list=PL0YFU3y0Z_gZxc-FeLhbOFS99ZzlWl4He))\n- [UC Santa Cruz CSE 101 Intro to Data Structures and Algorithms fall 2022, by Seshadhri Comandur](https://www.youtube.com/playlist?list=PLOQjlWvnI0fY1BCDxdiUSwkRHjnNI73G6) ([Fall 2020](https://www.youtube.com/playlist?list=PLOQjlWvnI0fZGffr1_MqCoaC5nUVtQIWz))\n- [UC Santa Cruz CSE 201 Analysis of Algorithms winter 2022, by Seshadhri Comandur](https://www.youtube.com/playlist?list=PLOQjlWvnI0fYmOmrAAN-g1d4nFB2uz6tU)\n- [UC Santa Cruz CSE 202 Combinatorial Algorithms spring 2021, by Seshadhri Comandur](https://www.youtube.com/playlist?list=PLOQjlWvnI0fbn9zAJfvJoQF1nc50KQR9g)\n- [UC Santa Cruz CSE 104, 204 Computational Complexity spring 2022, by Seshadhri Comandur](https://www.youtube.com/playlist?list=PLOQjlWvnI0fYMPFnJeVZ0kt4KPwWcbF0o) ([Fall 2020](https://www.youtube.com/playlist?list=PLOQjlWvnI0fas529oXenovd3MyafNQbKl))\n- [UC Santa Cruz CSE 290A Randomized Algorithms spring 2020, by Seshadhri Comandur](https://www.youtube.com/playlist?list=PLOQjlWvnI0faRpH2oJcyW4CuM5Clt8a2n)\n- [University of Maryland CMSC351 Introduction to Algorithms, by Mohammad Hajiaghayi](https://www.youtube.com/playlist?list=PLx7SjCaKZzEJLQ9RubHk2zFuddXTD5_ac)\n- [University of Maryland CMSC858F Network Algorithms and Approximations, by Mohammad Hajiaghayi](https://www.cs.umd.edu/~hajiagha/NetDsgn11/courseNetworkDesign.html) ([YouTube playlists](https://www.youtube.com/playlist?list=PLx7SjCaKZzEIeJxOlTuXveAE5eY7WOYB9))\n- [University of Maryland CMSC858M Algorithmic Lower Bounds: Fun with Hardness Proofs, by Mohammad Hajiaghayi](https://www.cs.umd.edu/~hajiagha/ALB19/ALB19.html) ([YouTube playlists](https://www.youtube.com/playlist?list=PLx7SjCaKZzEKwynDkSTacgJGivjoB2ksn))\n- [University of Maryland UMD DATA602 / MSML602 Principles of Data Science spring 2024, by Mohammad Hajiaghayi](https://www.youtube.com/playlist?list=PLx7SjCaKZzEIzzVsO03ozPOCP7Yc-LNq_)\n- [Algorithms for Big-Data (Fall 2020) - Saket Saurabh](https://sites.google.com/view/sakethome/teaching/algorithms-for-big-data-fall-2020)\n- [CS498ABD - Algorithms for Big Data - UIUC, Fall 2020](https://courses.engr.illinois.edu/cs498abd/fa2020/schedule.html)\n- [Advanced Data Structures](https://www.youtube.com/playlist?list=PLN-ShipRKQ0h6jIphD381pHdQtj_APRM8)\n- [CS60025 Algorithmic Game Theory - IIT KGP - Winter 2020](http://cse.iitkgp.ac.in/~palash/Courses/2020AlgorithmicGameTheory/agt2020.html)\n- [CS60083 Parameterized Algorithms - IIT KGP](http://cse.iitkgp.ac.in/~palash/Courses/2020ParameterizedAlgo/paramAlgo.html)\n- [Parameterized Complexity](https://sites.google.com/view/sakethome/teaching/parameterized-complexity)\n- [Structural Graph Theory - IIT Madras](https://www.youtube.com/playlist?list=PLtDHG-2klXcEedB8L-jjvb17OIUZbF3gW)\n- [Information Theory - IISC Bangalore](https://nptel.ac.in/courses/108/108/108108168/)\n\n\n\n\n------------------------------\n\n### Systems Programming\n\n- [15-213 Introduction to Computer Systems, Fall 2015  - CMU](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22b96d90ae-9871-4fae-91e2-b1627b43e25e%22&maxResults=150)\n- [Computer Systems: A programmer''s Perspective](https://www.youtube.com/playlist?list=PLyboo2CCDSWnhzzzzDQ3OBPrRiIjl-aIE)\n- [CS361 - COMPUTER SYSTEMS - UIC](https://www.cs.uic.edu/~ckanich/cs361/f20/)\n- [CS 3650 - Computer Systems - Fall 2020 - Nat Tuck - NEU](https://web.archive.org/web/20210423030302/https://ntuck-neu.site/2020-09/cs3650/) ([Lectures - YouTube](https://www.youtube.com/playlist?list=PLtg_A_3rzLAtBuwQp6mA3WveYw9Q7GzIZ))\n- [CS 4400 – Computer Systems   Fall 2016 - UoUtah](https://www.eng.utah.edu/~cs4400/)\n- [Systems - Aduni](http://aduni.org/courses/systems/index.php?view=cw)\n- [CS110: Principles of Computer Systems - Stanford](https://web.stanford.edu/class/archive/cs/cs110/cs110.1202/)\n- #### **Operating Systems**\n  - [ECS 150 - Operating Systems and Systems Programming - Fall 2020 - Joël Porquet-Lupine - UC Davis](https://lupteach.gitlab.io/courses/ucd-ecs150/online/)\n  - [CS124 Operating Systems - California Institute of Technology, Fall 2018 - Youtube](https://www.youtube.com/playlist?list=PL3swII2vlVoVbav6FV98pidq6BsTN4u56)\n  - [CS 162 Operating Systems and Systems Programming, Spring 2015 - University of California, Berkeley](https://archive.org/details/ucberkeley-webcast-PL-XXv-cvA_iBDyz-ba4yDskqMDY6A1w_c?sort=titleSorter) ([Fall 2020 - YouTube](https://www.youtube.com/playlist?list=PLF2K2xZjNEf97A_uBCwEl61sdxWVP7VWC))\n  - [CS 4414 - Operating Systems, University of Virginia (rust-class)](http://rust-class.org/pages/classes.html)\n  - [CS 4414 Operating Systems, Fall 2018 - University of Virginia](https://www.cs.virginia.edu/~cr4bd/4414/F2018/schedule.html)\n  - [CSE 421/521 - Introduction to Operating Systems, SUNY University at Buffalo, NY - Spring 2016](https://www.ops-class.org/courses/buffalo/CSE421_Spring2016/) ([Lectures - YouTube](https://www.youtube.com/playlist?list=PLE6LEE8y2Jp-kbEcVR2W3vfx0Pdca0BD3)) ([Recitations 2016](https://www.youtube.com/playlist?list=PLE6LEE8y2Jp_YJn8wu9aJTPOgeWqiaJDF)) ([Assignment walkthroughs](https://www.youtube.com/playlist?list=PLE6LEE8y2Jp9PC8fyzc2meL4XvrVSyP8O))\n  - [CS 377 - Operating Systems, Fall 16 - Umass OS](https://www.youtube.com/playlist?list=PLacuG5pysFbDTmsCRGWsMW_PzIOpXnckw)\n  - [CS 577 - Operating Systems, Spring 20 - Umass OS](https://www.youtube.com/playlist?list=PLacuG5pysFbB2_z9EkSfQIjq3yNzy8igs)\n  - [6.828 - Operating System Engineering [Fall 2014]](https://www.youtube.com/playlist?list=PLfciLKR3SgqNJKKIKUliWoNBBH1VHL3AP)\n  - [6.S081 - Operating System Engineering [Fall 2020]](https://pdos.csail.mit.edu/6.828/2020/schedule.html)\n  - [CSE 30341 - Operating Systems, Spr 2008](https://www.youtube.com/playlist?list=PLAB7D5CA7E262B0E2)\n  - [CSEP 551 Operating Systems Autumn 2014 - University of Washington](https://courses.cs.washington.edu/courses/csep551/14au/video/)\n  - [Introduction to Operating Systems - IIT Madras](https://nptel.ac.in/courses/106106144/)\n  - [CS194 Advanced Operating Systems Structures and Implementation, Spring 2013 InfoCoBuild, UC Berkeley](http://www.infocobuild.com/education/audio-video-courses/computer-science/cs194-spring2013-berkeley.html)\n  - [CSE 60641 - Graduate Operating Systems, Fall 08](https://www.youtube.com/view_play_list?p=22B10D854588E20C)\n  - [Advanced Programming in the UNIX Environment](https://stevens.netmeister.org/631/)\n  - [Operating System - IIT Madras](https://www.youtube.com/playlist?list=PLZ2ps__7DhBYcwlZ7GPCBzbowmiiF4BYR)\n- #### **Distributed Systems**\n  - [CS 677 - Distributed Operating Systems, Spring 24 - Umass OS](https://www.youtube.com/playlist?list=PLacuG5pysFbBpWHfKUU9Dfdk8RmQ7B9EH)\n  - [CS 436 - Distributed Computer Systems - U Waterloo](https://www.youtube.com/playlist?list=PLawkBQ15NDEkDJ5IyLIJUTZ1rRM9YQq6N)\n  - [6.824 - Distributed Systems, Spring 2015 - MIT](https://www.youtube.com/playlist?list=PLkcQbKbegkMqiWf7nF8apfMRL4P4sw8UL)\n  - [6.824 Distributed Systems - Spring 2020 - MIT](https://pdos.csail.mit.edu/6.824/schedule.html) ([Youtube](https://www.youtube.com/playlist?list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB))\n  - [Distributed Systems Lecture Series](https://www.youtube.com/playlist?list=PLeKd45zvjcDFUEv_ohr_HdUFe97RItdiB)\n  - [Distributed Algorithms, https://canvas.instructure.com/courses/902299](https://www.youtube.com/playlist?list=PL700757A5D4B3F368)\n  - [CSEP 552 - PMP Distributed Systems, Spring 2013 - University of Washington](https://courses.cs.washington.edu/courses/csep552/13sp/) ([Videos](https://courses.cs.washington.edu/courses/csep552/13sp/video/))\n  - [CSE 490H - Scalable Systems: Design, Implementation and Use of Large Scale Clusters, Autumn 2008 - University of Washington](https://courses.cs.washington.edu/courses/cse490h/08au/lectures.htm) ([Videos](https://courses.cs.washington.edu/courses/cse490h/08au/video.htm))\n  - [MOOC - Cloud Computing Concepts - UIUC](https://www.youtube.com/playlist?list=PLFd87qVsaLhOkTLvfp6MC94iFa_1c9wrU)\n  - [Distributed Systems (Prof. Pallab Dasgupta)](https://www.youtube.com/playlist?list=PLUJ7JmcrTifBROWODSG8wgyl20XgBuE-N)\n  - [EdX KTHx ID2203 Reliable Distributed Algorithms](https://www.youtube.com/playlist?list=PLx3mQFFeHPjndmQ0iP9j6C58b90hqGa0X)\n  - [Distributed Data Management - Technische Universität Braunschweig, Germany](http://www.ifis.cs.tu-bs.de/teaching/ss-15/ddm)\n  - [Information Retrieval and Web Search Engines - Technische Universität Braunschweig, Germany](http://www.ifis.cs.tu-bs.de/teaching/ws-1516/IRWS)\n  - [Middleware and Distributed Systems (WS 2009/10) - Dr. Martin von Löwis - HPI](https://www.tele-task.de/series/729/)\n  - [CSE 138 - Distributed Systems - UC Santa Cruz, Spring 2020](https://www.youtube.com/playlist?list=PLNPUF5QyWU8O0Wd8QDh9KaM1ggsxspJ31) ([2021](https://www.youtube.com/playlist?list=PLNPUF5QyWU8PydLG2cIJrCvnn5I_exhYx))\n  - [CMU 15 440 / 640 Distributed Systems Spring 2022, by Mahadev Satyanarayanan, Padmanabhan Pillai](https://www.youtube.com/playlist?list=PLIygTcviGPKAp30J9kcVW9jPzFC7Otpol)\n  - [UNC Comp533 - Distributed Systems Spring 2020](https://www.youtube.com/playlist?list=PLH5XTBxCO2hzgww9p5sew30lx3ngJsxcB)\n  - [Brown CSCI 1380 Distributed Computer Systems spring 2016, by Tom Doeppner & Rodrigo Fonseca](https://cs.brown.edu/courses/cs138/s16/syllabus.html)\n  - [Distributed Systems lecture series - Martin Kleppmann](https://www.youtube.com/playlist?list=PLeKd45zvjcDFUEv_ohr_HdUFe97RItdiB)\n  - [Distributed Algorithms - Jukka Suomela](https://www.youtube.com/playlist?list=PL2RY7P3JxZN8g9hFCasNqzuDhZbIbAj54)\n  - [Programming Parallel Computers - Jukka Suomela](https://www.youtube.com/playlist?list=PL2RY7P3JxZN-Pz1nwvnoJ9uEHmOmv4jmi)\n- #### **Real-Time Systems**\n  - [CPCS 663 - Real-Time Systems: Video Material - TAMU](http://faculty.cs.tamu.edu/bettati/Courses/663/Video/presentation.html)\n  - [Real Time Systems - IIT Kharagpur](https://nptel.ac.in/courses/106105036/)\n- [6.172 Performance Engineering of Software Systems - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-172-performance-engineering-of-software-systems-fall-2018/)\n- [Performance Evaluation of Computer Systems - IIT Madras](https://nptel.ac.in/courses/106106048/)\n- [Storage Systems - IISC Bangalore](https://nptel.ac.in/courses/106108058/)\n- [MAP6264 - Queueing Theory - FAU](http://www.cse.fau.edu/~bob/courses/map6264/)([Video Lectures](https://vimeo.com/album/171324/))\n- [EE 380 Colloquium on Computer Systems - Stanford University](http://web.stanford.edu/class/ee380/) ([Lecture videos](https://www.youtube.com/playlist?list=PLoROMvodv4rMWw6rRoeSpkiseTHzWj6vu))\n\n------------------------------\n\n### Database Systems\n\n- [CMPSC 431W Database Management Systems, Fall 2015 - Penn State University](http://www.cse.psu.edu/~wul2/cmpsc431w/) [Lectures - YouTube](https://www.youtube.com/playlist?list=PLstRzn3gXZMdXqAiVJ1NN2CoyXHqma7pQ)\n- [CS121 - Introduction to Relational Database Systems, Fall 2016 - Caltech](http://users.cms.caltech.edu/~donnie/cs121/)\n- [CS 5530 - Database Systems, Spring 2016 - University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCrercQNP9tTsjjPdgRVYvC7)\n- [Distributed Data Management (WT 2018/19) - HPI University of Potsdam](https://www.tele-task.de/series/1224/)\n- [MOOC - Database Stanford Dbclass](https://www.youtube.com/playlist?list=PL6hGtHedy2Z4EkgY76QOcueU8lAC4o6c3)\n- [CSEP 544, Database Management Systems, Au 2015 - University of Washington](https://www.youtube.com/playlist?list=PLTPQEx-31JXjQYrUKvAjUTWgCYluHGs_L)\n- [Database Design - IIT Madras](https://nptel.ac.in/courses/106106093/)\n- [Fundamentals of Database Systems - IIT Kanpur](https://nptel.ac.in/courses/106104135/)\n- [Principles of Database Management, Bart Baesens](https://www.youtube.com/playlist?list=PLdQddgMBv5zEhlpqdiUcf9aTNEtmESgyl)\n- [FIT9003 Database Systems Design - Monash University](https://itunes.apple.com/us/podcast/fit9003-database-systems-design/id306569364)\n- [15-445 - Introduction to Database Systems, CMU](https://15445.courses.cs.cmu.edu/fall2025/) ([YouTube-2017](https://www.youtube.com/playlist?list=PLSE8ODhjZXjYutVzTeAds8xUt1rcmyT7x)), ([YouTube-2018](https://www.youtube.com/playlist?list=PLSE8ODhjZXja3hgmuwhf89qboV1kOxMx7)),([YouTube-2019](https://www.youtube.com/playlist?list=PLSE8ODhjZXjbohkNBWQs_otTrBTrjyohi)), ([YouTube-2021](https://www.youtube.com/playlist?list=PLSE8ODhjZXjZaHA6QcxDfJ0SIWBzQFKEG)), ([YouTube-2022](https://www.youtube.com/playlist?list=PLSE8ODhjZXjaKScG3l0nuOiDTTqpfnWFf)),([YouTube-2023](https://youtube.com/playlist?list=PLSE8ODhjZXjbj8BMuIrRcacnQh20hmY9g&si=R7F_J9zbXsG07PjR)),([YouTube-2024](https://youtube.com/playlist?list=PLSE8ODhjZXjYDBpQnSymaectKjxCy6BYq&si=H-wfgjoLz6ifhZqS)),([YouTube-2025](https://youtube.com/playlist?list=PLSE8ODhjZXjYMAgsGH-GtY5rJYZ6zjsd5&si=KKXgy16Zm20utGyH))\n- [15-721 - Advanced Database Systems, CMU](http://15721.courses.cs.cmu.edu/spring2024) ([YouTube-2024](https://youtube.com/playlist?list=PLSE8ODhjZXjYa_zX-KeMJui7pcN1rIaIJ&si=J9cH2uZ0pFUu8q6f), [YouTube-2023](https://youtube.com/playlist?list=PLSE8ODhjZXjYzlLMbX3cR0sxWnRM7CLFn&si=O78E7wsQlVhgwE_u), [YouTube-2022](https://youtube.com/playlist?list=PLSE8ODhjZXjasmrEd2_Yi1deeE360zv5O&si=DsDfVhDp6j0n981J))\n- [CS122 - Relational Database System Implementation, Winter 2014-2015 - Caltech](http://users.cms.caltech.edu/~donnie/cs122/)\n- [CS 186 - Database Systems, UC Berkeley, Spring 2015](http://www.infocobuild.com/education/audio-video-courses/computer-science/cs186-spring2015-berkeley.html)\n- [CS 6530 - Graduate-level Database Systems, Fall 2016, University of Utah](https://www.cs.utah.edu/~lifeifei/cs6530/) ([Lectures - YouTube](https://www.youtube.com/playlist?list=PLbuogVdPnkCqwHUcieMrytP453Ep0y6eI))\n- [6.830/6.814 - Database Systems Fall 2014](https://www.youtube.com/playlist?list=PLfciLKR3SgqOxCy1TIXXyfTqKzX2enDjK)\n- [Informatics 1 - Data & Analysis 2014/15- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2014/da.htm)\n- [Database Management Systems, Aduni](http://aduni.org/courses/databases/index.php?view=cw)\n- [D4M - Signal Processing on Databases](https://ocw.mit.edu/resources/res-ll-005-d4m-signal-processing-on-databases-fall-2012/)\n- [In-Memory Data Management (2013)Prof. Hasso Plattner - HPI](https://open.hpi.de/courses/imdb2013/items/72j6pftms3dOSunM98JhfW)\n- [Distributed Data Management (WT 2019/20) - Dr. Thorsten Papenbrock - HPI](https://www.tele-task.de/series/1285/)\n- [CS122d - NoSQL Data Management (Spring 21) - Prof. Mike Carey - UC Irvine](https://uci.yuja.com/V/PlayList?node=9933576&a=1583628376&autoplay=1)\n\n------------------------------\n\n### Software Engineering\n\n- #### **Object Oriented Design**\n  - [ECE 462 Object-Oriented Programming using C++ and Java - Purdue](https://engineering.purdue.edu/OOSD/F2008/F2008.html)\n  - [Object-oriented Program Design and Software Engineering - Aduni](http://aduni.org/courses/java/index.php?view=cw)\n  - [OOSE - Object-Oriented Software Engineering, Dr. Tim Lethbridge](https://www.youtube.com/playlist?list=PL6iDJCG2nkhfNlig8NY5ePPfGvtQX6yLa)\n  - [Object Oriented Systems Analysis and Design (Systems Analysis and Design in a Changing World)](https://www.youtube.com/playlist?list=PL6XklZATqYx9dj72MKG6wLYjljeB2odra)\n  - [CS 251 - Intermediate Software Design (C++ version) - Vanderbilt University](https://www.youtube.com/playlist?list=PLZ9NgFYEMxp4ZsvD10uXmClGnukcu3Uff)\n  - [OOSE - Software Dev Using UML and Java](https://www.youtube.com/playlist?list=PLJ9pm_Rc9HesnkwKlal_buSIHA-jTZMpO)\n  - [Object-Oriented Analysis and Design - IIT Kharagpur](https://nptel.ac.in/courses/106105153/)\n  - [CS3 - Design in Computing - Richard Buckland UNSW](https://www.youtube.com/playlist?list=PL0C5D85DBA20E685C)\n  - [Informatics 1 - Object-Oriented Programming 2014/15- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2014/inf1op.htm)\n  - [Software Engineering with Objects and Components 2015/16- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2015/seoc.htm)\n- #### **Software Engineering**\n  - [Computer Science 169- Software Engineering - Spring 2015 - UCBerkeley](https://youtube.com/playlist?list=PLVEFwJhglgHJQEQ6RjMMjcclix94gp1k2)\n  - [Computer Science 169- Software Engineering - Fall 2019 - UCBerkeley](https://www.youtube.com/playlist?list=PLkFD6_40KJIxCKgzL0uysjsAtfY3JawLS)\n  - [CS 5150 -  Software Engineering, Fall 2014 - Cornell University](http://www.cs.cornell.edu/courses/cs5150/2014fa/materials.html)\n  - [Introduction to Service Design and Engineering - University of Trento, Italy](https://www.youtube.com/playlist?list=PLBdajHWwi0JCn87QuFT3e58mekU0-6WUT)\n  - [CS 164 Software Engineering - Harvard](https://www.youtube.com/watch?v=3zdfCR6c8vw&list=PLuhjguFxSeVLvKvWwTUIpVwXdLtZPX1ZS)\n  - [System Analysis and Design - IISC Bangalore](https://nptel.ac.in/courses/106108102/)\n  - [Software Engineering - IIT Bombay](https://nptel.ac.in/courses/106101061/)\n  - [Dependable Systems (SS 2014)- HPI University of Potsdam](https://www.tele-task.de/series/1005/)\n  - [Automated Software Testing - ETH Zürich | Spring 2024](https://video.ethz.ch/lectures/d-infk/2024/spring/263-2815-00L/9c81df65-d04d-411a-bea4-cbd32eb249e5.html)\n  - [Software Testing - IIT Kharagpur](https://nptel.ac.in/courses/106105150/)\n  - [Software Testing - Udacity, course-cs258 | 2015](https://www.youtube.com/playlist?list=PLAwxTw4SYaPkWVHeC_8aSIbSxE_NXI76g)\n  - [Software Debugging - Udacity, course-cs259 | 2015](https://www.youtube.com/playlist?list=PLAwxTw4SYaPkxK63TiT88oEe-AIBhr96A)\n  - [Software Engineering - Bauhaus-Uni Weimar](https://www.youtube.com/watch?v=jouBM4AH0jw&list=PLjEglKdMOevU2STTGq79duxTXDFuO-k1H&index=2)\n  - [CMU 17-445 Software Engineering for AI-Enabled Systems summer 2020, by Christian Kaestner](https://www.youtube.com/playlist?list=PLDS2JMJnJzdkQPdkhcuwcbJpjB84g9ffX)\n- #### **Software Architecture**\n  - [CS 411 - Software Architecture Design - Bilkent University](http://video.bilkent.edu.tr/course_videos.php?courseid=10)\n  - [MOOC - Software Architecture & Design - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPkMTetlG7xKWaI5ZAZFX8fL)\n  - [CS-310 Scalable Software Architectures](https://www.youtube.com/playlist?list=PLWl7jvxH18r0u5VRZsOjhghNXc_Ec4dZz)\n- #### **Concurrency**\n  - [CS176 - Multiprocessor Synchronization - Brown University](http://cs.brown.edu/courses/cs176/course_information.shtml) ([Videos from 2012](http://www.brown.edu/cis/sta/dev/herlihy_csci1760_fa12/#vid))\n  - [CS 282 (2014): Concurrent Java Network Programming in Android](https://www.youtube.com/playlist?list=PLZ9NgFYEMxp4KSJPUyaQCj7x--NQ6kvcX)\n  - [CSE P 506 – Concurrency, Spring 2011 - University of Washington](https://courses.cs.washington.edu/courses/csep506/11sp/Home.html) ([Videos](https://courses.cs.washington.edu/courses/csep506/11sp/Videos.html))\n  - [CSEP 524 - Parallel Computation - University of Washington](https://courses.cs.washington.edu/courses/csep524/10sp/) ([Videos](https://courses.cs.washington.edu/courses/csep524/10sp/lectures/video.html))\n  - [Parallel Programming Concepts (WT 2013/14) - HPI University of Potsdam](https://www.tele-task.de/series/977/)\n  - [Parallel Programming Concepts (WT 2012/13) - HPI University of Potsdam](https://www.tele-task.de/series/924/)\n  - [UIUC ECE 408 / CS 408 Applied Parallel Programming fall 2022, by Wen-mei Hwu, Sanjay Patel](https://www.youtube.com/playlist?list=PL6RdenZrxrw-UKfRL5smPfFFpeqwN3Dsz) ([Spring 2018](https://www.youtube.com/playlist?list=PLRRuQYjFhpmvu5ODQoY2l7D0ADgWEcYAX))\n  - [UIUC ECE 508 / CS 508 Manycore Parallel Algorithms spring 2019, by Wen-mei Hwu](https://www.youtube.com/playlist?list=PLRRuQYjFhpmspsME4LmLbuCG1VHbJmIcy)\n  - [UIUC CS 420 / ECE 492 / CSE 402 Introduction to Parallel Programming for Scientists and Engineers fall 2015, by Sanjay Kale](https://www.youtube.com/playlist?list=PL682UO4IMem9cAjfy_RPjAc6k-HPYpTa9)\n  - [Stanford CME 213 Introduction to Parallel Computing using MPI, openMP, and CUDA winter 2020, by Eric Darve](https://www.youtube.com/playlist?list=PLAtMgFDMfGy2mysjPHN_d1cf9sR1muRkq)\n- #### **Mobile Application Development**\n  - [MOOC Programming Mobile Applications for Android Handheld Systems - University of Maryland](https://www.youtube.com/playlist?list=PLkHsKoi6eZnwilGXUc95CqS7Vw4uLLDLG)\n  - [CS 193p - Developing Applications for iOS, Stanford University](https://cs193p.sites.stanford.edu/)\n  - [CS S-76 Building Mobile Applications - Harvard](http://cs76.tv/2013/summer/)\n  - [CS 251 (2015): Intermediate Software Design](https://www.youtube.com/playlist?list=PLZ9NgFYEMxp7lylj-XC8h1kjatOjbh9ne)\n  - [Android App Development for Beginners Playlist - thenewboston](https://www.youtube.com/playlist?list=PL6gx4Cwl9DGBsvRxJJOzG4r4k_zLKrnxl)\n  - [Android Application Development Tutorials - thenewboston](https://www.youtube.com/playlist?list=PL2F07DBCDCC01493A)\n  - [MOOC - Developing Android Apps - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPnMwH5-FNkErnnq_aSy706S)\n  - [MOOC - Advanced Android App Development - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPmETCT07vnDSiIaUBuyut0X)\n  - [CSSE490 Android Development Rose-Hulman Winter 2010-2011, Dave Fisher](https://www.youtube.com/playlist?list=PLF3EEB647F6B52F03)\n  - [iOS Course, Dave Fisher](https://www.youtube.com/playlist?list=PL96C635E4DCD393A8)\n  - [Developing iPad Applications for Visualization and Insight - Carnegie Mellon University](https://itunes.apple.com/us/course/developing-ipad-applications/id499050344)\n  - [Mobile Computing - IIT Madras](https://nptel.ac.in/courses/106106147/)\n  - [Mobile Information Systems - Bauhaus-Uni Weimar](https://www.youtube.com/watch?v=8EmbrZJwMOI&list=PLjEglKdMOevWv4zPW0diw7iJFdT7s4sTP)\n\n------------------------------\n\n### Artificial Intelligence\n\n- [CS50 - Introduction to Artificial Intelligence with Python (and Machine Learning), Harvard OCW](https://cs50.harvard.edu/ai/2023/)\n- [CS 188 - Introduction to Artificial Intelligence, UC Berkeley - Spring 2025, by John Canny, Oliver Grillmeyer](https://inst.eecs.berkeley.edu/~cs188/sp25/) ([Spring 2024](https://inst.eecs.berkeley.edu/~cs188/sp24/)) ([Spring 2023](https://www.youtube.com/playlist?list=PLp8QV47qJEg7WWVg_5eOECzVPpy23UjJz))\n- [6.034 Artificial Intelligence, MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/)\n- [CS221: Artificial Intelligence: Principles and Techniques - Autumn 2019 - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rO1NB9TD4iUZ3qghGEGtqNX)\n- [15-780 - Graduate Artificial Intelligence, Spring 14, CMU](http://www.cs.cmu.edu/~zkolter/course/15-780-s14/lectures.html)\n- [CSE 592 Applications of Artificial Intelligence, Winter 2003 - University of Washington](https://courses.cs.washington.edu/courses/csep573/03wi/lectures/index.htm)\n- [CS322 - Introduction to Artificial Intelligence, Winter 2012-13 - UBC](http://www.cs.ubc.ca/~mack/CS322/) ([YouTube](https://www.youtube.com/playlist?list=PLDPnGbm0sUmpzvcGvktbz446SLdFbfZVU))\n- [CS 4804: Introduction to Artificial Intelligence, Fall 2016](https://www.youtube.com/playlist?list=PLUenpfvlyoa1iiSbGy9BBewgiXjzxVgBd)\n- [CS 5804: Introduction to Artificial Intelligence, Spring 2015](https://www.youtube.com/playlist?list=PLUenpfvlyoa0PB6_kqJ9WU7m6i6z1RhfJ)\n- [Artificial Intelligence, Fall 2023 - FAU](https://www.fau.tv/course/id/3595) ([Spring 2023](https://www.fau.tv/course/id/3386)) ([Fall 2022](https://www.fau.tv/course/id/3180)) ([Spring 2021](https://www.fau.tv/course/id/2095)) ([Fall 2020](https://www.fau.tv/course/id/1690)) ([Fall 2018](https://www.fau.tv/course/id/713)) ([Spring 2018](https://www.fau.tv/course/id/657))\n- [Artificial Intelligence - IIT Kharagpur](https://nptel.ac.in/courses/106105077/)\n- [Artificial Intelligence - IIT Madras](https://nptel.ac.in/courses/106106126/)\n- [Artificial Intelligence(Prof.P.Dasgupta) - IIT Kharagpur](https://nptel.ac.in/courses/106105079/)\n- [MOOC - Intro to Artificial Intelligence - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPlqMkzr4xyuD6cXTIgPuzgn)\n- [MOOC - Artificial Intelligence for Robotics - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPkCSYXw6-a_aAoXVKLDwnHK)\n- [Graduate Course in Artificial Intelligence, Autumn 2012 - University of Washington](https://www.youtube.com/playlist?list=PLbQ3Aya0VERDoDdbMogU9EASJGWris9qG)\n- [Agent-Based Systems 2015/16- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2015/abs.htm)\n- [Informatics 2D - Reasoning and Agents 2014/15- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2014/inf2d.htm)\n- [Artificial Intelligence - Hochschule Ravensburg-Weingarten](https://www.youtube.com/playlist?list=PL39B5D3AFC249556A)\n- [Deductive Databases and Knowledge-Based Systems - Technische Universität Braunschweig, Germany](http://www.ifis.cs.tu-bs.de/teaching/ws-1516/KBS)\n- [Artificial Intelligence: Knowledge Representation and Reasoning - IIT Madras](https://nptel.ac.in/courses/106106140/)\n- [Semantic Web Technologies by Dr. Harald Sack - HPI](https://www.youtube.com/playlist?list=PLoOmvuyo5UAeihlKcWpzVzB51rr014TwD)\n- [Knowledge Engineering with Semantic Web Technologies by Dr. Harald Sack - HPI](https://www.youtube.com/playlist?list=PLoOmvuyo5UAcBXlhTti7kzetSsi1PpJGR)\n- [T81-558: Applications of Deep Neural Networks by Jeff Heaton, 2022, Washington University in St. Louis](https://sites.wustl.edu/jeffheaton/t81-558/)\n- [MSU programming for AI](https://www.youtube.com/playlist?list=PLZ-krWGO-UEz84TseDMIlx2Set6xZp0YP)\n\n------------------------------\n\n### Machine Learning\n\n- #### **Introduction to Machine Learning**\n  - [Introduction to Machine Learning for Coders](https://course18.fast.ai/ml)\n  - [MOOC - Statistical Learning, Stanford University](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/)\n  - [Statistical Learning with Python - Stanford Online](https://www.youtube.com/playlist?list=PLoROMvodv4rPP6braWoRt5UCXYZ71GZIQ)\n  - [Foundations of Machine Learning Boot Camp, Berkeley Simons Institute](https://www.youtube.com/playlist?list=PLgKuh-lKre11GbZWneln-VZDLHyejO7YD)\n  - [CS 155 - Machine Learning & Data Mining, 2023 - Caltech](https://www.youtube.com/playlist?list=PLu5yXJ11s4r-nBlojSQC9seeUF1IxX-_z) ([Notes-2020](http://www.yisongyue.com/courses/cs155/2020_winter/)) ([YouTube-2020](https://www.youtube.com/playlist?list=PLuz4CTPOUNi67hPzb9zJXH1cbeN7LKNiD)) ([Notes-2019](http://www.yisongyue.com/courses/cs155/2019_winter/)) ([YouTube-2019](https://www.youtube.com/playlist?list=PLuz4CTPOUNi7r2trKGgwaedY17MADTay4)) ([Notes-2018](http://www.yisongyue.com/courses/cs155/2018_winter/)) ([YouTube-2018](https://www.youtube.com/playlist?list=PLuz4CTPOUNi644ypoxzP1frkPYVHdjDJU)) ([Notes-2017](http://www.yisongyue.com/courses/cs155/2017_winter/)) ([YouTube-2017](https://www.youtube.com/playlist?list=PLuz4CTPOUNi6BfMrltePqMAHdl5W33-bC)) ([Notes-2016](http://www.yisongyue.com/courses/cs155/2016_winter/)) ([YouTube-2016](https://www.youtube.com/playlist?list=PL5HdMttxBY0BVTP9y7qQtzTgmcjQ3P0mb))\n  - [CS 156 - Learning from Data, Caltech](https://work.caltech.edu/lectures.html)\n  - [10-601 - Introduction to Machine Learning (MS) - Tom Mitchell - 2015, CMU](http://www.cs.cmu.edu/~ninamf/courses/601sp15/lectures.shtml) ([YouTube](https://www.youtube.com/playlist?list=PLAJ0alZrN8rD63LD0FkzKFiFgkOmEtltQ))\n  - [10-601 Machine Learning | CMU | Fall 2017](https://www.youtube.com/playlist?list=PL7k0r4t5c10-g7CWCnHfZOAxLaiNinChk)\n  - [10-701 - Introduction to Machine Learning (PhD) - Tom Mitchell, Spring 2011, CMU](http://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml) ([Fall 2014](https://www.youtube.com/playlist?list=PL7y-1rk2cCsDZCVz2xS7LrExqidHpJM3B)) ([Spring 2015 by Alex Smola](https://www.youtube.com/playlist?list=PLZSO_6-bSqHTTV7w9u7grTXBHMH-mw3qn)) ([Fall 2020 by Ziv Bar-Joseph, Eric Xing](https://www.youtube.com/playlist?list=PLsWN0V-b507g7dbQTUvFkKZEqdHR5Fh4P))\n  - [10 - 301/601 - Introduction to Machine Learning - Fall 2023 - CMU](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22d5bf275d-ff88-4bf6-a865-b065010f55c2%22)\n  - [6.036 - Machine Learning, Broderick - MIT Fall 2020](https://www.youtube.com/playlist?list=PLxC_ffO4q_rW0bqQB80_vcQB09HOA3ClV)\n  - [Mediterranean Machine Learning summer school 2024](https://www.youtube.com/playlist?list=PLF-wkqRv4u1bV4Zd1UepYfyZXkv6Bz6ra) ([YouTube-2023](https://www.youtube.com/playlist?list=PLF-wkqRv4u1Y-Bret-wrcPypPCZ3Gg_3L)) ([YouTube-2022](https://www.youtube.com/playlist?list=PLF-wkqRv4u1agtfVaDsDUaMHmToP84Fk6)) ([YouTube-2021](https://www.youtube.com/playlist?list=PLF-wkqRv4u1YRbfnwN8cXXyrmXld-sked))\n  - [LxMLS Lisbon Machine Learning School 2024](https://www.youtube.com/playlist?list=PLQl_xdhSmQeh4eRfAwETbtJJLPKcDLrzw) ([YouTube-2023](https://www.youtube.com/playlist?list=PLQl_xdhSmQeikRCf-wJ8NCK51JOMHGOCP)) ([YouTube-2022](https://www.youtube.com/playlist?list=PLQl_xdhSmQejdxQL7qI5aJkLcAQJ68Abp)) ([YouTube-2021](https://www.youtube.com/playlist?list=PLQl_xdhSmQegzsLin54NbfePFAuTUEmUj)) ([YouTube-2020](https://www.youtube.com/playlist?list=PLQl_xdhSmQehE6aAk774yWMag4NNBGr5k))\n  - [Applied Machine Learning (Cornell Tech CS 5787, Fall 2020)](https://www.youtube.com/playlist?list=PL2UML_KCiC0UlY7iCQDSiGDMovaupqc83)\n  - [Stanford CS229: Machine Learning Course | Summer 2019 (Anand Avati)](https://www.youtube.com/playlist?list=PLoROMvodv4rNH7qL6-efu_q2_bPuy0adh) ([Spring 2022](https://www.youtube.com/playlist?list=PLoROMvodv4rNyWOpJg_Yh4NSqI4Z4vOYy))\n  - [CMS 165 Foundations of Machine Learning - 2019 - Caltech](http://tensorlab.cms.caltech.edu/users/anima/cms165-2019.html) ([Youtube](https://www.youtube.com/playlist?list=PLVNifWxslHCA5GUh0o92neMiWiQiGVFqp))\n  - [CMS 165 Foundations of Machine Learning and Statistical Inference - 2020 - Caltech](https://www.youtube.com/playlist?list=PLVNifWxslHCDlbyitaLLYBOAEPbmF1AHg)\n  - [Microsoft Research - Machine Learning Course](https://www.youtube.com/playlist?list=PL34iyE0uXtxo7vPXGFkmm6KbgZQwjf9Kf)\n  - [CS 446 - Machine Learning, Fall 2016, UIUC](https://www.youtube.com/playlist?list=PLQcasX5-oG91TgY6A_gz-IW7YSpwdnD2O)\n  - [CS 582 - Machine Learning for Bioinformatics, Fall 2024, UIUC](https://www.youtube.com/playlist?list=PLIygTcviGPKBtxPX875Z8suCYGJkrPQ2Z)\n  - [ECE 364 - Programming Methods for Machine Learning, Spring 2025, UIUC](https://www.youtube.com/playlist?list=PLIygTcviGPKD0WT5NEsD6AIqaeV4azJwN)\n  - [undergraduate machine learning at UBC 2012, Nando de Freitas](https://www.youtube.com/playlist?list=PLE6Wd9FR--Ecf_5nCbnSQMHqORpiChfJf)\n  - [CS 229 - Machine Learning - Stanford University](https://see.stanford.edu/Course/CS229) ([Autumn 2018](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU))\n  - [CS 189/289A Introduction to Machine Learning, Prof Jonathan Shewchuk - UCBerkeley](https://people.eecs.berkeley.edu/~jrs/189/)\n  - [CPSC 340: Machine Learning and Data Mining (2018) - UBC](https://www.youtube.com/playlist?list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b)\n  - [CS391L Machine Learning, Spring 2025 - UT Austin](https://utcs-ml-course.github.io/main/Lectures/)\n  - [CS4780/5780 Machine Learning, Fall 2013 - Cornell University](http://www.cs.cornell.edu/courses/cs4780/2013fa/)\n  - [CS4780/5780 Machine Learning, Fall 2018 - Cornell University](http://www.cs.cornell.edu/courses/cs4780/2018fa/page18/index.html) ([Youtube](https://www.youtube.com/playlist?list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS))\n  - [CSE474/574 Introduction to Machine Learning - SUNY University at Buffalo](https://www.youtube.com/playlist?list=PLEQDy5tl3xkMzk_zlo2DPzXteCquHA8bQ)\n  - [CS 5350/6350 - Machine Learning, Spring 2024, University of Utah](https://svivek.com/teaching/machine-learning/spring2024/) ([Youtube](https://www.youtube.com/playlist?list=PLbuogVdPnkCr-ANNi5GZid3MvSkzm_wnM))\n  - [ECE 4252/8803 Fundamentals of Machine Learning (FunML), Spring 2024 - Georgia Tech](https://alregib.ece.gatech.edu/georgia-tech-courses/funml/)\n  - [ECE 5984 Introduction to Machine Learning, Spring 2015 - Virginia Tech](https://filebox.ece.vt.edu/~s15ece5984/)\n  - [CSx824/ECEx242 Machine Learning, Bert Huang, Fall 2015 - Virginia Tech](https://www.youtube.com/playlist?list=PLUenpfvlyoa0rMoE5nXA8kdctBKE9eSob)\n  - [STA 4273H - Large Scale Machine Learning, Winter 2015 - University of Toronto](http://www.cs.toronto.edu/~rsalakhu/STA4273_2015/lectures.html)\n  - [CSC 2515 Introduction to Machine Learning, Amir-massoud Farahmand, Fall 2021, University of Toronto](https://www.youtube.com/playlist?list=PLCveiXxL2xNZRg7PVp-JM4teSmaBETksy)\n  - [ECE 421 Introduction to Machine Learning, Amir Ashouri, Winter 2019, University of Toronto](https://www.youtube.com/playlist?list=PL-Mfq5QS-s8iS9XqKuApPE1TSlnZblFHF)\n  - [EECS 4404E/5327 Introduction to Machine Learning, Amir Ashouri, Fall 2019, York University](https://www.youtube.com/playlist?list=PL-Mfq5QS-s8horb94sQH4xcL85zDkpL9w)\n  - [CS 480/680 Introduction to Machine Learning, Gautam Kamath, University of Waterloo](http://www.gautamkamath.com/courses/CS480-sp2021.html) ([Spring 2021](https://www.youtube.com/playlist?list=PLmd_zeMNzSvSzRRc4Q29qEcpxbhdwjMOx))\n  - [CS 480/680 Introduction to Machine Learning, Kathryn Simone, University of Waterloo](https://github.com/kpc-simone/cs480-f24) ([Fall 2024](https://www.youtube.com/playlist?list=PLH84ETHrlsC8sbfb8WaOeXSx9ySH2Qoyc))\n  - [CS 485/685 Machine Learning, Shai Ben-David, University of Waterloo](https://www.youtube.com/channel/UCR4_akQ1HYMUcDszPQ6jh8Q/videos)\n  - [STAT 441/841 Classification Winter 2017 , Waterloo](https://www.youtube.com/playlist?list=PLehuLRPyt1HzXDemu7K4ETcF0Ld_B5adG)\n  - [10-605 - Machine Learning with Large Datasets, Fall 2016 - CMU](https://www.youtube.com/channel/UCIE4UdPoCJZMAZrTLuq-CPQ/videos)\n  - [Information Theory, Pattern Recognition, and Neural Networks - University of Cambridge](https://www.youtube.com/playlist?list=PLruBu5BI5n4aFpG32iMbdWoRVAA-Vcso6)\n  - [Pattern Analysis (2018) - FAU](https://www.fau.tv/course/id/655) ([Class 2017](https://www.fau.tv/course/id/544)) ([Class 2016](https://www.fau.tv/course/id/449)) ([Class 2015](https://www.fau.tv/course/id/355)) ([Class 2009](https://www.fau.tv/course/id/2))\n  - [Pattern Recognition (2020-2021) - FAU](https://www.fau.tv/course/id/1579) ([Class 2012-2013](https://www.fau.tv/course/id/173))\n  - [Beyond the Patterns (2020-2021) - FAU](https://www.fau.tv/course/id/1868)\n  - [Python and machine learning - Stanford Crowd Course Initiative](https://www.youtube.com/playlist?list=PLVxFQjPUB2cnYGZPAGG52OQc9SpWVKjjB)\n  - [MOOC - Machine Learning Part 1a - Udacity/Georgia Tech](https://www.youtube.com/playlist?list=PLAwxTw4SYaPl0N6-e1GvyLp5-MUMUjOKo) ([Part 1b](https://www.youtube.com/playlist?list=PLAwxTw4SYaPlkESDcHD-0oqVx5sAIgz7O) [Part 2](https://www.youtube.com/playlist?list=PLAwxTw4SYaPmaHhu-Lz3mhLSj-YH-JnG7) [Part 3](https://www.youtube.com/playlist?list=PLAwxTw4SYaPnidDwo9e2c7ixIsu_pdSNp))\n  - [Pattern Recognition Class (2012)- Universität Heidelberg](https://www.youtube.com/playlist?list=PLuRaSnb3n4kRDZVU6wxPzGdx1CN12fn0w)\n  - [Introduction to Machine Learning and Pattern Recognition - CBCSL OSU](https://www.youtube.com/playlist?list=PLcXJymqaE9PPGGtFsTNoDWKl-VNVX5d6b)\n  - [Introduction to Machine Learning - IIT Kharagpur](https://nptel.ac.in/courses/106105152/)\n  - [Introduction to Machine Learning - IIT Madras](https://nptel.ac.in/courses/106106139/)\n  - [Pattern Recognition - IISC Bangalore](https://nptel.ac.in/courses/117108048/)\n  - [Pattern Recognition and Application - IIT Kharagpur](https://nptel.ac.in/courses/117105101/)\n  - [Pattern Recognition - IIT Madras](https://nptel.ac.in/courses/106106046/)\n  - [Machine Learning Summer School 2013 - Max Planck Institute for Intelligent Systems Tübingen](https://www.youtube.com/playlist?list=PLqJm7Rc5-EXFv6RXaPZzzlzo93Hl0v91E)\n  - [Machine Learning - Professor Kogan (Spring 2016) - Rutgers](https://www.youtube.com/playlist?list=PLauepKFT6DK_1_plY78bXMDj-bshv7UsQ)\n  - [CS273a: Introduction to Machine Learning](http://sli.ics.uci.edu/Classes/2015W-273a) ([YouTube](https://www.youtube.com/playlist?list=PLkWzaBlA7utJMRi89i9FAKMopL0h0LBMk))\n  - [Machine Learning Crash Course 2015](https://www.youtube.com/playlist?list=PLyGKBDfnk-iD5dK8N7UBUFVVDBBtznenR)\n  - [COM4509/COM6509 Machine Learning and Adaptive Intelligence 2015-16](http://inverseprobability.com/mlai2015/)\n  - [Introduction to Machine Learning - Spring 2018 - ETH Zurich](https://www.youtube.com/playlist?list=PLzn6LN6WhlN273tsqyfdrBUsA-o5nUESV)\n  - [Machine Learning - Pedro Domingos- University of Washington](https://www.youtube.com/user/UWCSE/playlists?view=50&sort=dd&shelf_id=16)\n  - [CSE 446/546 - Machine Learning, Spring 2020 - University of Washington](https://courses.cs.washington.edu/courses/cse446/20sp/schedule/) ([Videos](https://www.youtube.com/playlist?list=PLrE1feouzSWr7LBFAeRZIb7CN9H6dB9Jt))\n  - [Machine Learning (COMP09012)](https://www.youtube.com/playlist?list=PLyH-5mHPFffFwz7Twap0XuVeUJ8vuco9t)\n  - [Probabilistic Machine Learning 2020 - University of Tübingen](https://www.youtube.com/playlist?list=PL05umP7R6ij1tHaOFY96m5uX3J21a6yNd)\n  - [Statistical Machine Learning 2020 - Ulrike von Luxburg - University of Tübingen](https://www.youtube.com/playlist?list=PL05umP7R6ij2XCvrRzLokX6EoHWaGA2cC)\n  - [COMS W4995 - Applied Machine Learning - Spring 2020 - Columbia University](https://www.cs.columbia.edu/~amueller/comsw4995s20/schedule/)\n  - [Machine Learning for Engineers 2022](https://apmonitor.com/pds) ([YouTube](https://www.youtube.com/watch?v=Gh5rbBLh4JY&list=PLLBUgWXdTBDg1K1bu60lHypSzSP-WSBmx))\n  - [10-418 / 10-618 (Fall 2019) Machine Learning for Structured Data](https://www.youtube.com/playlist?list=PL4CxkUJbvNVihRKP4bXufvRLIWzeS-ieP)\n  - [ORIE 4741/5741: Learning with Big Messy Data - Cornell](https://people.orie.cornell.edu/mru8/orie4741/lectures.html)\n  - [Machine Learning in IoT](https://www.youtube.com/playlist?list=PLeZoXD_TLsLbW_ILvL9TlhBYdW8wJyON-)\n  - [Stanford CS229M: Machine Learning Theory - Fall 2021](https://www.youtube.com/playlist?list=PLoROMvodv4rP8nAmISxFINlGKSK4rbLKh)\n  - [Intro to Machine Learning and Statistical Pattern Classification - Prof Sebastian Raschka](https://www.youtube.com/playlist?list=PLTKMiZHVd_2KyGirGEvKlniaWeLOHhUF3)\n  - [CMU''s Multimodal Machine Learning course (11-777), Fall 2020](https://www.youtube.com/playlist?list=PL-Fhd_vrvisNup9YQs_TdLW7DQz-lda0G)\n  - [EE104: Introduction to Machine Learning - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rN_Uy7_wmS051_q1d6akXmK)\n  - [CPSC 330: Applied Machine Learning (2020) - UBC](https://www.youtube.com/playlist?list=PLWmXHcz_53Q2BXsWviGgEqdlSHmfsjSzC)\n  - [Machine Learning 2013 - Nando de Freitas, UBC](https://www.youtube.com/playlist?list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6)\n  - [Machine Learning, 2014-2015, University of Oxford](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/)\n  - [10-702/36-702 - Statistical Machine Learning - Larry Wasserman, Spring 2016, CMU](https://www.stat.cmu.edu/~ryantibs/statml/) ([Spring 2015](https://www.youtube.com/playlist?list=PLjbUi5mgii6BWEUZf7He6nowWvGne_Y8r))\n  - [10-715 Advanced Introduction to Machine Learning - CMU](http://www.cs.cmu.edu/~bapoczos/Classes/ML10715_2015Fall/) ([YouTube](https://www.youtube.com/playlist?list=PL4DwY1suLMkcu-wytRDbvBNmx57CdQ2pJ))\n  - [CS 281B - Scalable Machine Learning, Alex Smola, UC Berkeley](http://alex.smola.org/teaching/berkeley2012/syllabus.html)\n  - [100 Days of Machine Learning - CampusX (Hindi)](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)\n  - [CampusX Data Science Mentorship Program 2022-23 (Hindi)](https://www.youtube.com/playlist?list=PLKnIA16_RmvbAlyx4_rdtR66B7EHX5k3z)\n  - [Statistical Machine Learning - S2023 - Benyamin Ghojogh](https://www.youtube.com/playlist?list=PLPrxGIUWsqP2g7cpk0nFFt0c4aRcREq2s)\n  - [MIT 6.5940 EfficientML.ai Lecture, Fall 2023](https://www.youtube.com/playlist?list=PL80kAHvQbh-pT4lCkDT53zT8DKmhE0idB)\n  - [TinyML - Tiny Machine Learning at UPenn](https://www.youtube.com/playlist?list=PL7rtKJAz_mPe6kAbiH6Ucq02Vpa95qvBJ)\n  - [ECE 4760 (Digital Systems Design Using Microcontrollers) at Cornell for the Fall, 2022](https://www.youtube.com/playlist?list=PLDqMkB5cbBA5oDg8VXM110GKc-CmvUqEZ) ([Spring 2021](https://www.youtube.com/playlist?list=PLDqMkB5cbBA6FEJuj94gl-9vw8xcHu9Gp))\n  - [EfficientML.ai Lecture, Fall 2023, MIT 6.5940](https://www.youtube.com/playlist?list=PL80kAHvQbh-pT4lCkDT53zT8DKmhE0idB)\n  - [SFU CMPT 727 Statistical Machine Learning, by Maxwell Libbrecht](https://coursys.sfu.ca/2023sp-cmpt-727-g1/pages/) ([Spring 2023](https://www.youtube.com/playlist?list=PL_5SuHtr8fsrK9NqWWSL4YL8urMAHLsvU)) ([Spring 2022](https://www.youtube.com/playlist?list=PL_5SuHtr8fsp95AhIKeTHbpcVdhlhB9h6))\n  - [UC Berkeley CS 189 / 289A Introduction to Machine Learning fall 2023, by Jennifer Listgarten & Jitendra Malik](https://eecs189.org/)\n  - [UC Berkeley CS 189 Introduction to Machine Learning (CDSS offering) spring 2022, by Marvin Zhang](https://www.youtube.com/playlist?list=PLCuQm2FL98HTlRmlwMk2AuFEM9n1c06HE)\n  - [UC San Diego/edX DSE 220X Machine Learning Fundamentals, by Sanjoy Dasgupta](https://www.youtube.com/playlist?list=PLUPLKa8g2P75vrLVe6HKdgAwfuU89RfqB)\n  - [MIT 6.036 Introduction to Machine Learning spring 2019, by Leslie Kaelbling](https://www.youtube.com/playlist?list=PLQEw29vp6f1Ae9dp8vkKB8H6sF1PHvP5N)\n  - [LMU Munich Introduction to Machine Learning](https://slds-lmu.github.io/i2ml/)\n  - [CMU 15 388 / 15 688 Practical Data Science, by Zico Kolter](https://www.datasciencecourse.org/lectures/) ([Fall 2019](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22618ea253-ca45-4b14-9f1d-aab501543bd2%22)) ([Spring 2018](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22912b80a3-625d-405d-8905-a8620133666b%22))\n  - [UW Madison CS 320 Data Programming II spring 2021, by Tyler R. Caraza-Harter](https://tyler.caraza-harter.com/cs320/s21/schedule.html)\n  - [UC San Diego COGS9 Introduction to Data Science fall 2020, by Jason Fleischer](https://www.youtube.com/playlist?list=PLaaNbhBDEsoFarUB58v9s7pUVoyAahMeQ)\n  - [UCLA Stats 15 Introduction to Data Science fall 2022, by Miles Chen](https://www.youtube.com/playlist?list=PLIygTcviGPKCEzYDpJha6hP6ne-50sT1o)\n  - [UCLA Stats 21 Python and Other Technologies for Data Science spring 2024, by Miles Chen](https://www.youtube.com/playlist?list=PLIygTcviGPKAdsiprcbdk5HHPxS6PloWE) ([Spring 2021](https://www.youtube.com/playlist?list=PLKR7271tMEmgBPgu4LtjDhX3ywpTxda5g))\n  - [UCLA Stats C161/C261 Introduction to Pattern Recognition and Machine Learning winter 2024, by Arash Amini](https://www.youtube.com/playlist?list=PLN_qg0-2-0SxQ2vlXxlZVMKkt4gI1YYP8) ([Winter 2023](https://www.youtube.com/playlist?list=PLN_qg0-2-0SwLCXGUyM3FNSRwG6GNgONr))\n  - [UCLA Stats 231C Theories of Machine Learning spring 2022, by Arash Amini](https://www.youtube.com/playlist?list=PLN_qg0-2-0SxKyZLv_FotPDED5ET_rQmo)\n  - [MSU Machine Learning](https://www.youtube.com/watch?v=kMf0qDtQ_PM&list=PLZ-krWGO-UEyPHsZfOjYH03_TyIN2pPhl&pp=iAQB)\n  - [Data Science for Dynamical Systems, by Oliver Wallscheid & Sebastian Peitz](https://github.com/DS-4-DS/DS4DS_Course) ([YouTube](https://www.youtube.com/@DataScience4DynamicalSystems/playlists))\n  - [Cambridge Statistical Learning in Practice 2021, by Alberto J. Coca](https://www.youtube.com/playlist?list=PLn1JSlh3WT_b7sMBktkAgV9-cP052JFhb)\n  - [Data 8: The Foundations of Data Science - UC Berkeley](http://data8.org/) ([Spring 23](https://www.data8.org/sp23/)) ([Fall 22](https://www.data8.org/fa22/)) ([Spring 22](https://www.data8.org/sp22/)) ([Summer 17](http://data8.org/su17/))\n  - [Data 144: Foundations of Data Science spring 2021 - Vassar College](https://www.youtube.com/playlist?list=PLIygTcviGPKB9hHuywn56TraSMv2pRumr) ([Course materials](https://github.com/jwaterman/data144-materials-sp21))\n  - [CSE519 - Data Science Fall 2016 - Skiena, SBU](https://www.youtube.com/playlist?list=PLOtl7M3yp-DVBdLYatrltDJr56AKZ1qXo)\n  - [CS 109 Data Science, Harvard University](http://cs109.github.io/2015/pages/videos.html) ([YouTube](https://www.youtube.com/playlist?list=PLb4G5axmLqiuneCqlJD2bYFkBwHuOzKus))\n  - [6.0002 Introduction to Computational Thinking and Data Science - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/lecture-videos/)\n  - [Data 100: Principles and Techniques of Data Science - UC Berkeley](https://ds100.org/fa24/) ([Fall 24](https://www.youtube.com/playlist?list=PLIygTcviGPKBzDKR72ILypzPQZ_Cz6HcH)) ([Spring 24](https://ds100.org/sp24/)) ([Summer 19](https://www.youtube.com/playlist?list=PLPHXc20GewP8J56CisONS_mFZWZAfa7jR))\n  - [Data 102 - Spring 21- UC Berkeley](https://data102.org/sp21/#lecture-week-14) ([YouTube](https://www.youtube.com/playlist?list=PLIygTcviGPKAEbY32OXjmkQbD0uRhge9Y))\n  - [Distributed Data Analytics (WT 2017/18) - HPI University of Potsdam](https://www.tele-task.de/series/1179/)\n  - [Data Profiling and Data Cleansing (WS 2014/15) - HPI University of Potsdam](https://www.tele-task.de/series/1027/)\n  - [CS 229r - Algorithms for Big Data, Harvard University](http://people.seas.harvard.edu/~minilek/cs229r/fall15/lec.html) ([Youtube](https://www.youtube.com/playlist?list=PL2SOU6wwxB0v1kQTpqpuu5kEJo2i-iUyf))\n  - [Algorithms for Big Data - IIT Madras](https://nptel.ac.in/courses/106106142/)\n  - [Python Data Science with the TCLab](https://github.com/APMonitor/data_science) ([YouTube](https://www.youtube.com/watch?v=pAgW_bZVo88&list=PLLBUgWXdTBDg1Qgmwt4jKtVn9BWh5-zgy))\n  - [Foundations of Data Analysis (Fall 2020)- University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCqB1sx1eheVmLtp2EN7osYt)\n\n  \n- #### **Data Mining**\n  - [CSEP 546, Data Mining - Pedro Domingos, Sp 2016 - University of Washington](https://courses.cs.washington.edu/courses/csep546/16sp/) ([YouTube](https://www.youtube.com/playlist?list=PLTPQEx-31JXgtDaC6-3HxWcp7fq4N8YGr))\n  - [CS 5140/6140 - Data Mining, Spring 2020, University of Utah by Prof. Jeff Phillips](https://users.cs.utah.edu/~jeffp/teaching/cs5140-S20/cs5140.html) ([Youtube](https://www.youtube.com/playlist?list=PLbuogVdPnkCrEf65zrd3J1UG3LT6TcDlt))\n  - [CS 5140/6140 - Data Mining, Spring 2023, University of Utah by Prof. Ana Marasović](https://utah-data-mining-spring23.github.io/) ([Youtube](https://www.youtube.com/playlist?list=PLbuogVdPnkCrnLNqZPnTuG_s19TNDoad0))\n  - [CS 5955/6955 - Data Mining, University of Utah](http://www.cs.utah.edu/~jeffp/teaching/cs5955.html) ([YouTube](https://www.youtube.com/channel/UCcrlwW88yMcXujhGjSP2WBg/videos))\n  - [Statistics 202 - Statistical Aspects of Data Mining, Summer 2007 - Google](http://www.stats202.com/original_index.html) ([YouTube](https://www.youtube.com/playlist?list=PLFE776F2C513A744E))\n  - [MOOC - Text Mining and Analytics by ChengXiang Zhai](https://www.youtube.com/playlist?list=PLLssT5z_DsK8Xwnh_0bjN4KNT81bekvtt)\n  - [Information Retrieval SS 2014, iTunes - HPI](https://itunes.apple.com/us/itunes-u/information-retrieval-ss-2014/id874200291)\n  - [MOOC - Data Mining with Weka](https://www.youtube.com/playlist?list=PLm4W7_iX_v4NqPUjceOGd-OKNVO4c_cPD)\n  - [CS 290 DataMining Lectures](https://www.youtube.com/playlist?list=PLB4CCA346A5741C4C)\n  - [CS246 - Mining Massive Data Sets, Winter 2016, Stanford University](https://web.stanford.edu/class/cs246/) ([YouTube](https://www.youtube.com/channel/UC_Oao2FYkLAUlUVkBfze4jg/videos))\n  - [Information Retrieval - Spring 2018 - ETH Zurich](https://www.youtube.com/playlist?list=PLzn6LN6WhlN1ktkDvNurPSDwTQ_oGQisn)\n  - [Information Retrieval - WS 2022/23 - Universität Freiburg](https://ad-wiki.informatik.uni-freiburg.de/teaching/InformationRetrievalWS2223)\n  - [CAP6673 - Data Mining and Machine Learning - FAU](http://www.cse.fau.edu/~taghi/classes/cap6673/)([Video lectures](https://vimeo.com/album/1505953))\n  - [CS 412 - Introduction to Data Mining - UIUC](https://www.youtube.com/playlist?list=PLIygTcviGPKDZi44-yuH2XH9UaHdaJkxs)\n  - [CS 512 - Data Mining Principles - UIUC](https://github.com/spacemanidol/CS512DM/tree/main/lectures) ([YouTube](https://www.youtube.com/playlist?list=PLIygTcviGPKABUzEm9v1PuKLlb0AQ9tdH))\n  \n- #### **Probabilistic Graphical Modeling**\n  - [CS 6190 - Probabilistic Modeling, Spring 2016, University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCpvxdF-Gy3gwaBObx7AnQut)\n  - [10-708 - Probabilistic Graphical Models, Carnegie Mellon University](https://www.cs.cmu.edu/~epxing/Class/10708-20/lectures.html)\n  - [Probabilistic Graphical Models, Daphne Koller, Stanford University](http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=ProbabilisticGraphicalModels)\n  - [Probabilistic Graphical Models, Spring 2018 - Notre Dame](https://www.youtube.com/playlist?list=PLd-PuDzW85AcV4bgdu7wHPL37hm60W4RM)\n  \n- #### **Deep Learning**\n  - [Full Stack Deep Learning - Course 2022](https://www.youtube.com/watch?v=-Iob-FW5jVM&list=PL1T8fO7ArWleMMI8KPJ_5D5XSlovTW_Ur)\n  - [Full Stack Deep Learning - Course 2021](https://www.youtube.com/watch?v=fGxWfEuUu0w&list=PL1T8fO7ArWlcWg04OgNiJy91PywMKT2lv)\n  - [NYU Deep Learning Spring 2020](https://www.youtube.com/playlist?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq)\n  - [NYU Deep Learning Spring 2021](https://www.youtube.com/playlist?list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI)\n  - [6.S191: Introduction to Deep Learning - MIT](http://introtodeeplearning.com/)\n  - [Intro to Deep Learning and Generative Models Course - Prof Sebastian Raschka](https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51)\n  - [Deep Learning CMU](https://www.youtube.com/channel/UC8hYZGEkI2dDO8scT8C5UQA/videos)\n  - [CS231n Deep Learning for Computer Vision - Stanford University](https://cs231n.stanford.edu/schedule.html) ([Spring 2025](https://www.youtube.com/playlist?list=PLoROMvodv4rOmsNzYBMe0gJY2XS8AQg16)) ([Winter 2016 Andrej Karpathy](https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC))\n  - [Deep Learning: CS 182 Spring 2021](https://www.youtube.com/playlist?list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A)\n  - [10-414/714: Deep Learning Systems - CMU](https://dlsyscourse.org/lectures/) ([Youtube](https://www.youtube.com/@deeplearningsystemscourse1116/videos))\n  - [11-785: Introduction to Deep Learning - CMU](https://deeplearning.cs.cmu.edu/S24/index.html) ([Lectures - YouTube-2024](https://www.youtube.com/playlist?list=PLp-0K3kfddPxUJzAW0KxNNjGiK_hISFas), [Recitations - YouTube-2024](https://www.youtube.com/playlist?list=PLp-0K3kfddPzNnco9QQAoTx_sVhZgHK-n))\n  - [Part 1: Practical Deep Learning for Coders, v3 - fast.ai](https://course.fast.ai/)\n  - [Part 2: Deep Learning from the Foundations - fast.ai](https://course19.fast.ai/part2)\n  - [Deep learning at Oxford 2015 - Nando de Freitas](https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu)\n  - [Self-Driving Cars — Andreas Geiger, 2021/22](https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/lectures/self-driving-cars/) ([YouTube](https://www.youtube.com/watch?v=wAaSJUAKPuY&list=PL05umP7R6ij321zzKXK6XCQXAaaYjQbzr))\n  - [6.S094: Deep Learning for Self-Driving Cars - MIT](https://www.youtube.com/playlist?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf)\n  - [CS294-129 Designing, Visualizing and Understanding Deep Neural Networks](https://bcourses.berkeley.edu/courses/1453965/pages/cs294-129-designing-visualizing-and-understanding-deep-neural-networks) ([YouTube](https://www.youtube.com/playlist?list=PLkFD6_40KJIxopmdJF_CLNqG3QuDFHQUm))\n  - [CS230: Deep Learning - Autumn 2018 - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb)\n  - [STAT-157 Deep Learning 2019 - UC Berkeley](https://www.youtube.com/playlist?list=PLZSO_6-bSqHQHBCoGaObUljoXAyyqhpFW)\n  - [Deep Learning, Stanford University](http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=DeepLearning)\n  - [MOOC - Neural Networks for Machine Learning, Geoffrey Hinton 2016 - Coursera](https://www.youtube.com/playlist?list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9)\n  - [Stat 946 Deep Learning - University of Waterloo](https://www.youtube.com/playlist?list=PLehuLRPyt1Hyi78UOkMPWCGRxGcA9NVOE)\n  - [EECS 298 Theory of Computational Neural Networks and Machine Learning (Fall 2020) - UC Irvine](https://grandcentral.eee.uci.edu/syllabus/download/F-2020/17815) ([YouTube](https://www.youtube.com/playlist?list=PLIygTcviGPKBAh1HWXPOI3Rw59WdRlJXu))\n  - [ECE 1508 Applied Deep Learning - University of Toronto](https://www.bereyhi.com/deep-learning) ([Winter 2025](https://www.youtube.com/playlist?list=PLcFgNUo9s_AgsMOnniTMIWmLpjj9vZ1Wm)) ([Fall 2024](https://www.youtube.com/playlist?list=PLcFgNUo9s_Ajz1l4rBDIApwdcEKgzoMko))\n  - [Neural networks class - Université de Sherbrooke](http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html) ([YouTube](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH))\n  - [DLCV - Deep Learning for Computer Vision - UPC Barcelona](https://www.youtube.com/playlist?list=PL-5eMc3HQTBavDoZpFcX-bff5WgQqSLzR)\n  - [DLAI - Deep Learning for Artificial Intelligence @ UPC Barcelona](https://www.youtube.com/playlist?list=PL-5eMc3HQTBagIUjKefjcTbnXC0wXC_vd)\n  - [Neural Networks and Applications - IIT Kharagpur](https://nptel.ac.in/courses/117105084/)\n  - [UVA DEEP LEARNING COURSE](http://uvadlc.github.io/#lecture)\n  - [Deep Learning - Winter 2020-21 - Tübingen Machine Learning](https://www.youtube.com/playlist?list=PL05umP7R6ij3NTWIdtMbfvX7Z-4WEXRqD)\n  - [Geometric Deep Learning - AMMI](https://www.youtube.com/playlist?list=PLn2-dEmQeTfQ8YVuHBOvAhUlnIPYxkeu3)\n  - [Math for Deep Learning — Andreas Geiger](https://www.youtube.com/playlist?list=PL05umP7R6ij0bo4UtMdzEJ6TiLOqj4ZCm)\n  - [Applied Deep Learning 2022 - TU Wien](https://www.youtube.com/playlist?list=PLNsFwZQ_pkE_QaTwYxoTmmRJHtMXyIAU6)\n  - [Neural Networks: Zero to Hero - Andrej Karpathy](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)\n  - [CIS 522 - Deep Learning - U Penn](https://www.youtube.com/@cis522-deeplearning8/playlists)\n  - [UVA DEEP LEARNING COURSE](http://uvadlc.github.io/#lecture)\n  - [Deep Learning (Fall 2020) - FAU](https://www.fau.tv/course/id/1600) ([Spring 2020](https://www.fau.tv/course/id/925)) ([Fall 2019](https://www.fau.tv/course/id/849)) ([Spring 2019](https://www.fau.tv/course/id/758)) ([Fall 2018](https://www.fau.tv/course/id/701)) ([Spring 2018](https://www.fau.tv/course/id/662))\n  - [Deep Learning (Fall 2020) - Georgia Tech](https://www.youtube.com/playlist?list=PL-fZD610i7yB7gDnPDpFcKpHI9X8z3OQ7)\n  - [Mathematics of Deep Learning (2021) - FAU](https://www.fau.tv/course/id/878)\n  - [CS7015 - Deep Learning - Prof. Mitesh M. Khapra - IIT Madras](https://www.youtube.com/playlist?list=PLyqSpQzTE6M9gCgajvQbc68Hk_JKGBAYT)\n  - [ETH Zürich | Deep Learning in Scientific Computing 2023](https://www.youtube.com/playlist?list=PLJkYEExhe7rYY5HjpIJbgo-tDZ3bIAqAm)\n  - [Applied Deep Learning Maziar Raissi](https://www.youtube.com/playlist?list=PLoEMreTa9CNmuxQeIKWaz7AVFd_ZeAcy4)\n  - [UC Berkeley CS 182 / 282a Deep Learning spring 2023, by Anant Sahai](https://www.youtube.com/playlist?list=PLIygTcviGPKAaj_UAJcazYN4964xZ7Lt1)\n  - [Foundations of Deep Learning - UMD](https://www.youtube.com/playlist?list=PLHgjs9ncvHi80UCSlSvQe-TK_uOyDv_Jf)\n  - [TUM IN2346 Introduction to Deep Learning Fall 2024, by Daniel Cremers](https://cvg.cit.tum.de/teaching/ws2024/i2dl) ([Summer 2023](https://www.youtube.com/playlist?list=PLQ8Y4kIIbzy_pGm2QAwF625E6nmcRu2sU))\n  - [UT Austin - Advances in Deep Learning](https://ut.philkr.net/advances_in_deeplearning/)\n  - [HKU - Data 8014 Principles of Deep Representation Learning Fall 2025, by Yi Ma](https://ma-lab-berkeley.github.io/deep-representation-learning-book/community.html)\n  \n- #### **Reinforcement Learning**\n  - [CS234: Reinforcement Learning - Spring 2024 - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rN4wG6Nk6sNpTEbuOSosZdX) ([Winter 2019](https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u))\n  - [CSE 542: Reinforcement Learning - Spring 2024 - University of Washington](https://courses.cs.washington.edu/courses/cse542/24sp/schedule/)\n  - [CSE 579: Reinforcement Learning - Autumn 2024 - University of Washington](https://courses.cs.washington.edu/courses/cse579/24au/schedule/)\n  - [CSC 2547: Introduction to Reinforcement Learning - Spring 2021 - University of Toronto](https://amfarahmand.github.io/IntroRL/) ([YouTube](https://www.youtube.com/playlist?list=PLCveiXxL2xNbiDq51a8iJwPRq2aO0ykrq))\n  - [Introduction to reinforcement learning - UCL](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)\n  - [Reinforcement Learning - IIT Madras](https://www.youtube.com/playlist?list=PLyqSpQzTE6M_FwzHFAyf4LSkz_IjMyjD9) ([TA - Manav Mishra](https://www.youtube.com/playlist?list=PLpKrAXMumEsjAR1Ybb0qbTKGYd9RY0vxa), [TA - Prabhleen Kukreja](https://www.youtube.com/playlist?list=PLTt_oMmEiDJhBPjy_aedcoeIY-IsaDBOn), [TA - Sandarbh Yadav ](https://www.youtube.com/playlist?list=PLoo_WPzXEM1ShdfOxv-adi3JdhRX4rA7F), [TA - Avik Kar](https://www.youtube.com/playlist?list=PLz2x4RAIbeXkTJFEipkD_ds3z0qx8_5D7))\n  - [Special topics in ML (Reinforcement Learning) IIT madras](https://www.youtube.com/playlist?list=PLZ2ps__7DhBbDiVplM2I9q2XNso1Qfj62)\n  - [CS885 Reinforcement Learning - Spring 2018 - University of Waterloo](https://www.youtube.com/playlist?list=PLdAoL1zKcqTXFJniO3Tqqn6xMBBL07EDc)\n  - [CS 285 - Deep Reinforcement Learning- UC Berkeley](https://www.youtube.com/playlist?list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A)\n  - [CS 294 112 - Reinforcement Learning](https://www.youtube.com/playlist?list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37)\n  - [NUS CS 6101 - Deep Reinforcement Learning](https://www.youtube.com/playlist?list=PLllwxvcS7ca5wOmRLKm6ri-OaC0INYehv)\n  - [ECE 8851: Reinforcement Learning](https://www.youtube.com/playlist?list=PL_Nk3YvgORJs1tCLQnlnSRsOJArj_cP9u)\n  - [CS294-112, Deep Reinforcement Learning Sp17](http://rll.berkeley.edu/deeprlcourse/) ([YouTube](https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX))\n  - [UCL Course 2015 on Reinforcement Learning by David Silver from DeepMind](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) ([YouTube](https://www.youtube.com/watch?v=2pWv7GOvuf0))\n  - [Deep RL Bootcamp - Berkeley Aug 2017](https://sites.google.com/view/deep-rl-bootcamp/lectures)\n  - [Reinforcement Learning - IIT Madras](https://www.youtube.com/playlist?list=PLyqSpQzTE6M_FwzHFAyf4LSkz_IjMyjD9)\n  - [Reinforcement Learning Course at KTH (FDD3359 - 2022)](https://www.youtube.com/playlist?list=PL21JFJEtbq0JLNo53UIkbIwkc2njCVUUR)\n  - [Reinforcement Learning Course at ASU, Spring 2022](https://www.youtube.com/playlist?list=PLmH30BG15SIoXhxLldoio0BhsIY84YMDj)\n  - [CS 4789/5789: Introduction to Reinforcement Learning - Cornell](https://www.youtube.com/playlist?list=PLQVNhPb8ajtCjWSKUvKU8cX5lueYP9s3X)\n  - [S20/IE613 - Online (Machine) Learning/ Bandit Algorithms](https://www.youtube.com/playlist?list=PLDREIwGwrHBdiBm1q0cVJLZn4Cn6Hig2s)\n  - [Reinforcement Learning - Fall 2021 chandar-lab](https://www.youtube.com/playlist?list=PLImtCgowF_ES_JdF_UcM60EXTcGZg67Ua)\n  - [CMU 10 703 Deep Reinforcement Learning & Control fall 2022, by Katerina Fragkiadaki](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22ee5794a2-cb54-4edc-836b-aefc01023243%22)\n  - [ECE524 Foundations of Reinforcement Learning at Princeton University, Spring 2024](https://www.youtube.com/playlist?list=PLYXvCE1En13epbogBmgafC_Yyyk9oQogl)\n  - [REINFORCEMENT LEARNING AND OPTIMAL CONTROL - Dimitri P. Bertsekas, ASU](https://web.mit.edu/dimitrib/www/RLbook.html)\n  - [CMU 16 745 Optimal Control and Reinforcement Learning spring by Zac Manchester](https://www.youtube.com/@roboticexplorationlab3724/playlists)\n  - [CMU 16 899 Adaptive Control and Reinforcement Learning fall 2020, by  Changliu Liu](https://www.youtube.com/playlist?list=PLZL5VXraKdz-0zByoPNzNTqSirR4veU8z)\n  - [Jadavpur University, 2025: Introduction to Reinforcement Learning](https://www.youtube.com/playlist?list=PLcNLn_ApooUzGJW60RcD2HRrL7sm0HG-w)\n  - [EE675 (2024) Introduction to Reinforcement Learning Course | IIT Kanpur](https://www.youtube.com/playlist?list=PLZAmMLcSnKRICBNyjraAhQdtdJFFgyRL5)\n  - [Reinforcement Learning Course by Frédéric Godin - Concordia University](https://www.youtube.com/playlist?list=PLFskzTP727yc5eXXm09Xst7bIk5fi7mD6)\n  - [CS 285: Deep RL, 2023](https://www.youtube.com/playlist?list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps)\n  - [Mathematical Foundations of Reinforcement Learning - WINDY Lab](https://www.youtube.com/playlist?list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8)\n  - [Reinforcement Learning (HMC CS 181V)—Spring, 2020 - Neil Rhodes](https://www.youtube.com/playlist?list=PL2Yggtk_pK69evEzfwQHm9ASOCbXPlXPS)\n  - [Reinforcement Learning Course: Lectures (Summer 2023) by Paderborn University](https://www.youtube.com/playlist?list=PL4GzQQuIDBGv-IFxRSgydCR7OrOM_xKqN)\n  - [CS292F (Spring 2021) Statistical Foundation of Reinforcement Learning - UCSD](https://cseweb.ucsd.edu/~yuxiangw/classes/RLCourse-2021Spring/)\n  - [Algorithmic Foundations of Interactive Learning - CMU](https://interactive-learning-algos.github.io/)\n  \n- #### **Advanced Machine Learning**\n  - [Advanced Machine Learning, 2021-2022, Sem I - by Prof. Madhavan Mukund, CMI](https://www.cmi.ac.in/~madhavan/courses/aml2021)\n  - [18.409 Algorithmic Aspects of Machine Learning Spring 2015 - MIT](https://www.youtube.com/playlist?list=PLB3sDpSRdrOvI1hYXNsa6Lety7K8FhPpx)\n  - [CS 330 - Deep Multi-Task and Meta Learning - Fall 2019 - Stanford University](https://cs330.stanford.edu/) ([Youtube](https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5))\n  - [Stanford CS330: Deep Multi-Task and Meta Learning I Autumn 2022](https://www.youtube.com/playlist?list=PLoROMvodv4rNjRoawgt72BBNwL2V7doGI)\n  - [ES 661 (2023): Probabilistic Machine Learning - IIT Gandhinagar](https://www.youtube.com/playlist?list=PLftoLyLEwECBEJyfRBJoSBd0UaTjEcs3I)\n  - [Information Retrieval in High Dimensional Data](https://www.youtube.com/playlist?list=PLaE1lKCe0jH3ePp9wCU1ygTquVOXY-UYv)\n  - [Trustworthy Machine Learning - Winter Semester 2023-2024, University of Tübingen](https://scalabletrustworthyai.github.io/courses/tml_winter_2324/)\n  - [Trustworthy Machine Learning - Winter Semester 2024-2025, University of Tübingen](https://scalabletrustworthyai.github.io/courses/tml_winter_2425/)\n  - [ETH Zürich Advanced Machine Learning fall 2019, by Joachim M. Buhmann](https://video.ethz.ch/lectures/d-infk/2019/autumn/252-0535-00L.html)\n  - [CS 159 Advanced Topics in Machine Learning, Spring 2021 - Caltech](https://1five9.github.io/)\n  - [CS 229br Advanced Topics in the theory of machine learning, Spring 2021 - Harvard](https://boazbk.github.io/mltheoryseminar/cs229br.html)\n  \n  \n- #### **Natural Language Processing**\n  - [CS 224N -Natural Language Processing with Deep Learning - Stanford University](http://web.stanford.edu/class/cs224n/) ([Lectures -  Winter 2019](https://youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)) ([Lectures -  Winter 2021](https://youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)) ([Lectures - Spring 2024](https://www.youtube.com/playlist?list=PLoROMvodv4rOaMFbaqxPDoLWjDaRAdP9D))\n  - [CS 224N - Natural Language Processing, Stanford University](https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html) ([Lecture videos](https://academictorrents.com/details/d2c8f8f1651740520b7dfab23438d89bc8c0c0ab))\n  - [Stanford XCS224U: Natural Language Understanding I Spring 2023](https://www.youtube.com/playlist?list=PLoROMvodv4rOwvldxftJTmoR3kRcWkJBp)\n  - [CS388: Natural Language Processing - UT Austin](https://www.cs.utexas.edu/~gdurrett/courses/online-course/materials.html)\n  - [CS 124 - From Languages to Information - Stanford University](https://www.youtube.com/channel/UC_48v322owNVtORXuMeRmpA/playlists?view=50&sort=dd&shelf_id=2)\n  - [CS 6340/5340 - Natural Language Processing - University of Utah - Spring 2024](https://utah-intro-nlp.github.io/) ([Youtube](https://www.youtube.com/playlist?list=PLbuogVdPnkCrPZ4Vc-GRnk730SLhC1L43))\n  - [CSE 447/517 - Natural Language Processing - University of Washington - Winter 2024](https://safe-fernleaf-26d.notion.site/Winter-24-CSE-447-517-Natural-Language-Processing-4142333a001143d2be5ecff1a535c4ab)\n  - [Neural Networks: Zero to Hero - Andrej Karpathy](https://karpathy.ai/zero-to-hero.html)\n  - [fast.ai Code-First Intro to Natural Language Processing](https://www.youtube.com/playlist?list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9) ([Github](https://github.com/fastai/course-nlp))\n  - [MOOC - Natural Language Processing - Coursera, University of Michigan](https://www.youtube.com/playlist?list=PLLssT5z_DsK8BdawOVCCaTCO99Ya58ryR)\n  - [Natural Language Processing at UT Austin (Greg Durrett)](https://www.youtube.com/playlist?list=PLofp2YXfp7Tbk88uH4jejfXPd2OpWuSLq)\n  - [CS224U: Natural Language Understanding - Spring 2019 - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rObpMCir6rNNUlFAn56Js20)\n  - [Deep Learning for Natural Language Processing, 2017 - Oxford University](https://github.com/oxford-cs-deepnlp-2017/lectures)\n  - [Natural Language Processing - IIT Bombay](https://nptel.ac.in/courses/106101007/)\n  - [CMU Advanced NLP Fall 2024](https://phontron.com/class/anlp-fall2024/schedule/) ([Lectures - Fall 2024](https://www.youtube.com/playlist?list=PL8PYTP1V4I8D4BeyjwWczukWq9d8PNyZp)) ([Lectures - Fall 2021](https://www.youtube.com/playlist?list=PL8PYTP1V4I8AYSXn_GKVgwXVluCT9chJ6))\n  - [CMU Neural Nets for NLP 2021](https://www.youtube.com/playlist?list=PL8PYTP1V4I8AkaHEJ7lOOrlex-pcxS-XV)\n  - [Natural Language Processing - Michael Collins - Columbia University](https://www.youtube.com/playlist?list=PLA212ij5XG8OTDRl8IWFiJgHR9Ve2k9pv)\n  - [CMU CS11-711 - Advanced Natural Language Processing](https://cmu-l3.github.io/anlp-spring2025/) ([Lectures - Spring 2025](https://www.youtube.com/playlist?list=PLqC25OT8ZpD3WxQ0FwWMGPS_BcWdcKyZy))\n  - [CMU CS11-737 - Multilingual Natural Language Processing](https://www.youtube.com/playlist?list=PL8PYTP1V4I8CHhppU6n1Q9-04m96D9gt5)\n  - [UMass CS685: Advanced Natural Language Processing (Spring 2022)](https://www.youtube.com/playlist?list=PLWnsVgP6CzadI4-FT2Po4wsEK7MHCIQ-d)\n  - [Natural Language Processing (CMSC 470)](https://www.youtube.com/playlist?list=PLwrUPjGidcJ4UkSoi7_rmn-1kcedLqgdL)\n  - [Stanford CS25 - Transformers United 2023](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM)\n  - [Natural Language Processing (IN2361) - TUM](https://live.rbg.tum.de/?year=2019&term=W&slug=nlp&view=3)\n  - [Natural Language Processing (Spring 2024) - University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCrPZ4Vc-GRnk730SLhC1L43)\n  - [Multilingual NLP 2020 - CMU](https://www.youtube.com/playlist?list=PL8PYTP1V4I8CHhppU6n1Q9-04m96D9gt5)\n  - [Speech Technology - IIT Madras](https://www.youtube.com/playlist?list=PLZ2ps__7DhBaI_g3_V-CFrgFIf-0Yksiv)\n  \n- #### **Generative AI and LLMs**\n  - [Stanford CS236: Deep Generative Models I 2023 I Stefano Ermon](https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8)\n  - [CS 6785 - Deep Generative Models - Cornell Tech, Spring 2023)](https://www.youtube.com/playlist?list=PL2UML_KCiC0UPzjW9BjO-IW6dqliu9O4B)\n  - [MIT 6.S184 Flow Matching and Diffusion Models, 2025](https://diffusion.csail.mit.edu)\n  - [Course on Diffusion Models for Generative AI - UT Austin](https://www.youtube.com/playlist?list=PL8lIiiIWuabLxhJreBRZwNW-d02dJwbMb)\n  - [Stanford CS336 Language Modeling from Scratch I 2025 - Stanford](https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_)\n  - [Stanford CME295 Transformers & LLMs - Autumn 2025 - Stanford](https://www.youtube.com/playlist?list=PLoROMvodv4rObv1FMizXqumgVVdzX4_05)\n  - [Introduction to large language models - IIT Madras](https://www.youtube.com/playlist?list=PLZ2ps__7DhBbaMNZoyW2Hizl8DG6ikkjo)\n  - [Build a Large Language Model (From Scratch) by Sebastian Raschka](https://www.youtube.com/playlist?list=PLTKMiZHVd_2IIEsoJrWACkIxLRdfMlw11)\n  - [Reinforcement Learning of Large Language Models - UCLA](https://www.youtube.com/playlist?list=PLir0BWtR5vRp5dqaouyMU-oTSzaU5LK9r)\n  - [WING NUS CS6101 Large Language Models (T2310)](https://www.youtube.com/playlist?list=PLzIZxnJJT7ORSBnYrXJMYBVnYeLryJtl7)\n  - [CS 886: Recent Advances on Foundation Models Winter 2024 - University of Waterloo](https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/)\n  - [UC Berkeley CS 194/294-196 Large Language Model Agents Fall 2024, by Dawn Song & Xinyun Chen](https://rdi.berkeley.edu/llm-agents/f24) ([YouTube playlist](https://www.youtube.com/playlist?list=PLS01nW3RtgopsNLeM936V4TNSsvvVglLc))\n  - [UC Berkeley CS 194/294-267 Understanding Large Language Models Foundations and Safety spring 2024, by Dawn Song & Dan Hendrycks](https://www.youtube.com/playlist?list=PLJ66BAXN6D8H_gRQJGjmbnS5qCWoxJNfe)\n  - [Introduction to Large Language Models (LLMs), IIT Delhi](https://www.youtube.com/playlist?list=PLqGkIjcOyrGnjyBHl4GE2S9kX47X96FH-)\n  \n- #### **Computer Vision**\n  - [CS 231n - Convolutional Neural Networks for Visual Recognition, Stanford University](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)\n  - [CS 198-126: Modern Computer Vision Fall 2022 (UC Berkeley)](https://www.youtube.com/playlist?list=PLzWRmD0Vi2KVsrCqA4VnztE4t71KnTnP5)\n  - [Machine Learning for Robotics and Computer Vision, WS 2013/2014 - TU München](https://vision.in.tum.de/teaching/ws2013/ml_ws13) ([YouTube](https://www.youtube.com/playlist?list=PLTBdjV_4f-EIiongKlS9OKrBEp8QR47Wl))\n  - [COGSCI 1 - Intro to Cognitive Science Summer 2022 - UC Berkeley](https://www.youtube.com/playlist?list=PLaMjLYzDGxvz1oT5gpFiY6rJZnlJ-1Xu-)\n  - [Informatics 1 - Cognitive Science 2015/16- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2015/inf1cs.htm)\n  - [Informatics 2A - Processing Formal and Natural Languages 2016-17 - University of Edinburgh](http://www.inf.ed.ac.uk/teaching/courses/inf2a/schedule.html)\n  - [NOC:Deep Learning For Visual Computing - IIT Kharagpur](https://nptel.ac.in/courses/108/105/108105103/)\n  - [Extreme Classification ](https://www.youtube.com/playlist?list=PLXtAHOcKKDTk43wjXud9GQS-l-QA5DQxH)\n  - [EECS 498/598 - Deep Learning for Computer Vision - University of Michigan - Fall 2019](https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/) ([Youtube](https://www.youtube.com/playlist?list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r))\n  - [Computer Vision - FAU Spring 2021](https://www.fau.tv/course/id/2306) ([Spring 2018](https://www.fau.tv/course/id/734))\n  - [CAP5415 Computer Vision - UCF Fall 2023](https://www.youtube.com/playlist?list=PLd3hlSJsX_InWyCQtwqQ7y6KnwhxNCgRf)\n  - [CAP6412 Advanced Computer Vision - UCF Spring 2024](https://www.crcv.ucf.edu/courses/cap6412-spring-2024/schedule/) ([Youtube](https://www.youtube.com/playlist?list=PLd3hlSJsX_IlSr0ua4v8WQezazAMXEJE4))\n  - [Advanced Deep Learning for Computer vision (ADL4CV) (IN2364) - TU Munich](https://dvl.in.tum.de/teaching/adl4cv-ss20/) ([Youtube](https://www.youtube.com/playlist?list=PLog3nOPCjKBnjhuHMIXu4ISE4Z4f2jm39))\n  - [Computer Vision III: Detection, Segmentation and Tracking (CV3DST) (IN2375) - TU Munich](https://www.youtube.com/playlist?list=PLog3nOPCjKBkamdw8F6Hw_4YbRiDRb2rb)\n  \n- #### **Time Series Analysis**\n  - [02417 Time Series Analysis](https://www.youtube.com/playlist?list=PLtiTxpFJ4k6TZ0g496fVcQpt_-XJRNkbi)\n  - [Applied Time Series Analysis](https://www.youtube.com/playlist?list=PLl0FT6O_WWDBm-4W-eoK34omYmEMseQDX)\n  \n- #### **Optimization**\n  - [Optimisation for Machine Learning: Theory and Implementation (Hindi) - IIT](https://www.youtube.com/playlist?list=PLyqSpQzTE6M-pmLzCoMu_ANU6atEFyyJl)\n  - [Rochester DSCC 435 Optimization for Machine Learning fall 2023, by Jiaming Liang](https://www.youtube.com/playlist?list=PLuJY91x7h5orCtyh6mChurVQ2ZZef9qPF)\n  - [Princeton ELE539/COS512 Optimization for Machine Learning spring 2021, by Chi Jin](https://sites.google.com/view/cjin/teaching/ece539cos512-2021-ver)\n  - [UT Dallas CS 7301 Advanced Topics in Optimization for Machine Learning spring 2021, by Rishabh Iyer](https://github.com/rishabhk108/AdvancedOptML) ([YouTube](https://www.youtube.com/playlist?list=PLGod0_zT9w92_evaYrf3-rE67AmgPJoUU))\n  - [Convex Analysis, Summer 2021 - TU Braunschweig](https://www.tu-braunschweig.de/index.php?eID=dumpFile&t=f&f=128341&token=3c40ee32c6c029df85f7e552522f4a87470e3401) ([YouTube](https://www.youtube.com/playlist?list=PLPomPKAI5ZlJBThiwc3bya7qngzw9-0QD))\n  - [EE364a: Convex Optimization I - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rMJqxxviPa4AmDClvcbHi6h)\n  - [10-725 Convex Optimization, Spring 2015 - CMU](http://www.stat.cmu.edu/~ryantibs/convexopt-S15/)\n  - [10-725 Convex Optimization: Fall 2016 - CMU](http://www.stat.cmu.edu/~ryantibs/convexopt/)\n  - [10-725 Optimization Fall 2012 - CMU](http://www.cs.cmu.edu/~ggordon/10725-F12/schedule.html)\n  - [10-801 Advanced Optimization and Randomized Methods - CMU](http://www.cs.cmu.edu/~suvrit/teach/aopt.html) ([YouTube](https://www.youtube.com/playlist?list=PLjTcdlvIS6cjdA8WVXNIk56X_SjICxt0d))\n  - [AM 207 - Stochastic Methods for Data Analysis, Inference and Optimization, Harvard University](http://am207.github.io/2016/index.html)\n  - [MIT 6.S098 Applied Convex Optimization IAP 2022, by Alexandre Amice, Benoit Legat](https://alexandreamice.github.io/teaching/convex_optimization/) ([YouTube](https://www.youtube.com/playlist?list=PL5SG6ajT9NZKxdvM1jQOLXmeKO7MfyLxR))\n  - [University of Twente Discrete Optimization, by Marc Uetz](https://marcuetz.personalweb.utwente.nl/do/) ([Fall 2020](https://www.youtube.com/playlist?list=PLIygTcviGPKBUY9e1MCruRr5-I24LNm4g))\n  - [UC Davis MAT 168 Optimization winter 2024, by Matthias Köppe](https://video.ucdavis.edu/channel/MAT+168+Optimization+%28Matthias+K%C3%B6ppe%3B+Winter+2024%29/329241352)\n  - [Purdue University CHE 597 Computational Optimization spring 2025, by Can Li](https://canli1.github.io/courses)\n  - [UCSD CS292F Convex Optimization Spring 2020, by Yu-Xiang Wang](https://cseweb.ucsd.edu/~yuxiangw/classes/CS292F-2020Spring/) ([Youtube](https://www.youtube.com/playlist?list=PLTN4aNO9NiB5VxYILKPBXoy9g1tUqmnBx))\n  - [UIUC ECE 490 Introduction to Optimization fall 2020, by Venugopal V. Veeravalli](https://courses.grainger.illinois.edu/ece490/fa2020/) ([YouTube](https://www.youtube.com/playlist?list=PLIygTcviGPKC5wSXtE6s2qdSYutRH1AUU))\n  - [University of Wisconsin-Madison CS/ECE/ISyE 524 Introduction to Optimization spring 2017-18, by Laurent Lessard](https://laurentlessard.com/teaching/524-intro-to-optimization/)\n  - [University of Wisconsin-Madison ISyE/Math/CS/Stat 525 Linear Optimization fall 2021, by Alberto Del Pia](https://www.youtube.com/playlist?list=PLeO_PhASIA0Ot69TqANAnNxoykHGOQp2Y)\n  - [University of Wisconsin-Madison ISyE/Math/CS 728 Integer Optimization (second part of the course) spring 2020](https://www.youtube.com/playlist?list=PLeO_PhASIA0NlDNF9y-SsgVEYcvAMj2CY)\n  - [Columbia IEOR E4007 Optimization Models and Methods 2005, by Garud Iyengar](https://www.youtube.com/playlist?list=PLIygTcviGPKCNQ2xHRwrLOxkEWv9OfGiF)\n \n- #### **Unsupervised Learning**\n  - [CS294 Deep Unsupervised Learning Spring 2024](https://sites.google.com/view/berkeley-cs294-158-sp24/home)\n  - [Deep Unsupervised Learning -- Berkeley Spring 2020](https://www.youtube.com/playlist?list=PLwRJQ4m4UJjPiJP3691u-qWwPGVKzSlNP)\n  - [CS294-158 Deep Unsupervised Learning SP19](https://www.youtube.com/channel/UCf4SX8kAZM_oGcZjMREsU9w/videos)\n  - [UC San Diego COGS 118A Supervised Machine Learning fall 2020, by Jason Fleischer](https://www.youtube.com/playlist?list=PLaaNbhBDEsoF_ad5N2mlOsvB-RwSTYUjQ)\n  - [UC San Diego COGS 118B Unsupervised Machine Learning winter 2024, by Jason Fleischer](https://www.youtube.com/playlist?list=PLaaNbhBDEsoEU9RRbwCqJWFAZH2LjNwBN)\n  - [UIUC STAT 437 Unsupervised Learning spring 2024, by Tori Ellison](https://www.youtube.com/playlist?list=PLIygTcviGPKB133Vh7zxsxFoblyfS4P5Y)\n  - [Johns Hopkins Unsupervised Learning spring 2017, by Rene Vidal](https://www.youtube.com/playlist?list=PLaBAmmD3yH4Nta9Y6g9hOV4dcnpTzeW4q)\n  - [Unsupervised Learning (STAT 841), Winter 2017](https://www.youtube.com/playlist?list=PLehuLRPyt1HzQoXEhtNuYTmd0aNQvtyAK)\n  \n- #### **Misc Machine Learning Topics**\n  - [Quantum Machine Learning | 2021 Qiskit Global Summer School](https://www.youtube.com/playlist?list=PLOFEBzvs-VvqJwybFxkTiDzhf5E11p8BI)\n  - [CS 6955 - Clustering, Spring 2015, University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCpRvi-qSMCdOwyn4UYoPxTI)\n  - [Info 290 - Analyzing Big Data with Twitter, UC Berkeley school of information](http://blogs.ischool.berkeley.edu/i290-abdt-s12/) ([YouTube](https://www.youtube.com/playlist?list=PLE8C1256A28C1487F))\n  - [CS224W Machine Learning with Graphs | Spring 2021 | Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn)\n  - [9.520 - Statistical Learning Theory and Applications, Fall 2015 - MIT](https://www.youtube.com/playlist?list=PLyGKBDfnk-iDj3FBd0Avr_dLbrU8VG73O)\n  - [Statistical Learning Theory, Spring 2019 - ETH Zürich](https://video.ethz.ch/lectures/d-infk/2019/spring/252-0526-00L.html)\n  - [Course on the Statistical Learning Theory, University of São Paulo, ICMC](https://www.youtube.com/playlist?list=PLKWX1jIoUZaVpVhMfevAE7iYNcDHPEJI_)\n  - [Reinforcement Learning - UCL](https://www.youtube.com/playlist?list=PLacBNHqv7n9gp9cBMrA6oDbzz_8JqhSKo)\n  - [Regularization Methods for Machine Learning 2016](http://academictorrents.com/details/493251615310f9b6ae1f483126292378137074cd) ([YouTube](https://www.youtube.com/playlist?list=PLbF0BXX_6CPJ20Gf_KbLFnPWj', '{"language":null,"stars":70337,"forks":9421,"watchers":70337,"open_issues":2,"topics":["algorithms","bioinformatics","computational-biology","computational-physics","computer-architecture","computer-science","computer-vision","database-systems","databases","deep-learning","embedded-systems","machine-learning","quantum-computing","reinforcement-learning","robotics","security","systems","web-development"],"default_branch":"master","size_kb":861,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:Developer-Y:cs-video-courses","source_url":"https://github.com/Developer-Y/cs-video-courses"},{"type":"has_code","target_id":"github:Developer-Y:cs-video-courses","source_url":"https://github.com/Developer-Y/cs-video-courses"},{"type":"has_code","target_id":"github:kpc-simone:cs480-f24","source_url":"https://github.com/kpc-simone/cs480-f24"},{"type":"has_code","target_id":"github:DS-4-DS:DS4DS_Course","source_url":"https://github.com/DS-4-DS/DS4DS_Course"},{"type":"has_code","target_id":"github:jwaterman:data144-materials-sp21","source_url":"https://github.com/jwaterman/data144-materials-sp21"},{"type":"has_code","target_id":"github:APMonitor:data_science","source_url":"https://github.com/APMonitor/data_science"},{"type":"has_code","target_id":"github:spacemanidol:CS512DM","source_url":"https://github.com/spacemanidol/CS512DM"},{"type":"has_code","target_id":"github:fastai:course-nlp","source_url":"https://github.com/fastai/course-nlp"},{"type":"has_code","target_id":"github:oxford-cs-deepnlp-2017:lectures","source_url":"https://github.com/oxford-cs-deepnlp-2017/lectures"},{"type":"has_code","target_id":"github:rishabhk108:AdvancedOptML","source_url":"https://github.com/rishabhk108/AdvancedOptML"}]', NULL, NULL, 'pending', 70, '9460a68a8fadb891177d11dea8f7fea6', NULL, NULL, CURRENT_TIMESTAMP);
