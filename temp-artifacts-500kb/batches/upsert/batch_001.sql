-- Auto-generated upsert SQL
-- Args: Args { input: "../../data/merged.json" }
-- R2_BUCKET env: Err(NotPresent)
-- CLOUDFLARE_ACCOUNT_ID env: Ok("46307fa431173267a9c52d6954d5a2df")
-- R2_PUBLIC_URL_PREFIX env: Ok("https://cdn.free2aitools.com")
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-R1', 'huggingface--deepseek-ai--deepseek-r1', 'DeepSeek-R1', 'deepseek-ai', '--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/d...', '["transformers","safetensors","deepseek_v3","text-generation","conversational","custom_code","arxiv:2501.12948","license:mit","text-generation-inference","endpoints_compatible","fp8","region:us"]', 'text-generation', 12896, 1209122, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-R1","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n\n<p align="center">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Paper Link</b>üëÅÔ∏è</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align="center">\n  <img width="80%" src="figures/benchmark.jpg">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model''s reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align="center">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align="center">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align="center">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align="center">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek''s official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button "DeepThink"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face''s Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: "Please reason step by step, and put your final answer within \boxed{}."\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting "\<think\>\n\n\</think\>") when responding to certain queries, which can adversely affect the model''s performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with "\<think\>\n" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":684531386000,"storage_bytes":688624501744,"files_count":174,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["DeepseekV3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_deepseek.DeepseekV3Config","AutoModel":"modeling_deepseek.DeepseekV3Model","AutoModelForCausalLM":"modeling_deepseek.DeepseekV3ForCausalLM"},"model_type":"deepseek_v3","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''', is_first_sp=true) %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message[''content''] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + ''\\n\\n'' + message[''content''] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{{''<ÔΩúUserÔΩú>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and ''tool_calls'' in message %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls''] %}{%- if not ns.is_first %}{%- if message[''content''] is none %}{{''<ÔΩúAssistantÔΩú><ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- else %}{{''<ÔΩúAssistantÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{''\\n'' + ''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- endfor %}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- if message[''role''] == ''assistant'' and ''tool_calls'' not in message %}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'' + message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message[''content''] %}{% if ''</think>'' in content %}{% set content = content.split(''</think>'')[-1] %}{% endif %}{{''<ÔΩúAssistantÔΩú>'' + content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- set ns.is_output_first = false %}{%- else %}{{''<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{''<ÔΩúAssistantÔΩú><think>\\n''}}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2501.12948","source_url":"https://arxiv.org/abs/2501.12948"}]', NULL, 'MIT', 'approved', 100, 'edaca41abf4103afb376162fd64dee2f', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-R1/resolve/main/figures/benchmark.jpg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-R1 from https://huggingface.co/deepseek-ai/DeepSeek-R1/resolve/main/figures/benchmark.jpg
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-R1.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-black-forest-labs-FLUX.1-dev', 'huggingface--black-forest-labs--flux.1-dev', 'FLUX.1-dev', 'black-forest-labs', '', '["diffusers","safetensors","text-to-image","image-generation","flux","en","license:other","endpoints_compatible","diffusers:fluxpipeline","region:us"]', 'text-to-image', 11977, 1154994, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/black-forest-labs/FLUX.1-dev","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":68212484318,"files_count":29,"spaces_count":100,"gated":"auto","private":false,"config":{"diffusers":{"_class_name":"FluxPipeline"}}}', '[]', '[]', NULL, 'Other', 'approved', 40, '8e95969b65749e4817d4c14628830c6a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-stable-diffusion-xl-base-1.0', 'huggingface--stabilityai--stable-diffusion-xl-base-1.0', 'stable-diffusion-xl-base-1.0', 'stabilityai', '--- license: openrail++ tags: - text-to-image - stable-diffusion --- !row01 !pipeline SDXL consists of an ensemble of experts pipeline for latent diffusion: In a first step, the base model is used to generate (noisy) latents, which are then further processed with a refinement model (available here: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/) specialized for the final denoising steps. Note that the base model can be used as a standalone module. Alternatively, we can us...', '["diffusers","onnx","safetensors","text-to-image","stable-diffusion","arxiv:2307.01952","arxiv:2211.01324","arxiv:2108.01073","arxiv:2112.10752","license:openrail++","endpoints_compatible","diffusers:stablediffusionxlpipeline","deploy:azure","region:us"]', 'text-to-image', 7192, 2467601, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: openrail++\ntags:\n- text-to-image\n- stable-diffusion\n---\n# SD-XL 1.0-base Model Card\n![row01](01.png)\n\n## Model\n\n![pipeline](pipeline.png)\n\n[SDXL](https://arxiv.org/abs/2307.01952) consists of an [ensemble of experts](https://arxiv.org/abs/2211.01324) pipeline for latent diffusion: \nIn a first step, the base model is used to generate (noisy) latents, \nwhich are then further processed with a refinement model (available here: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/) specialized for the final denoising steps.\nNote that the base model can be used as a standalone module.\n\nAlternatively, we can use a two-stage pipeline as follows: \nFirst, the base model is used to generate latents of the desired output size. \nIn the second step, we use a specialized high-resolution model and apply a technique called SDEdit (https://arxiv.org/abs/2108.01073, also known as "img2img") \nto the latents generated in the first step, using the same prompt. This technique is slightly slower than the first one, as it requires more function evaluations.\n\nSource code is available at https://github.com/Stability-AI/generative-models .\n\n### Model Description\n\n- **Developed by:** Stability AI\n- **Model type:** Diffusion-based text-to-image generative model\n- **License:** [CreativeML Open RAIL++-M License](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/LICENSE.md)\n- **Model Description:** This is a model that can be used to generate and modify images based on text prompts. It is a [Latent Diffusion Model](https://arxiv.org/abs/2112.10752) that uses two fixed, pretrained text encoders ([OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip) and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main)).\n- **Resources for more information:** Check out our [GitHub Repository](https://github.com/Stability-AI/generative-models) and the [SDXL report on arXiv](https://arxiv.org/abs/2307.01952).\n\n### Model Sources\n\nFor research purposes, we recommend our `generative-models` Github repository (https://github.com/Stability-AI/generative-models), which implements the most popular diffusion frameworks (both training and inference) and for which new functionalities like distillation will be added over time.\n[Clipdrop](https://clipdrop.co/stable-diffusion) provides free SDXL inference.\n\n- **Repository:** https://github.com/Stability-AI/generative-models\n- **Demo:** https://clipdrop.co/stable-diffusion\n\n\n## Evaluation\n![comparison](comparison.png)\nThe chart above evaluates user preference for SDXL (with and without refinement) over SDXL 0.9 and Stable Diffusion 1.5 and 2.1. \nThe SDXL base model performs significantly better than the previous variants, and the model combined with the refinement module achieves the best overall performance.\n\n\n### üß® Diffusers \n\nMake sure to upgrade diffusers to >= 0.19.0:\n```\npip install diffusers --upgrade\n```\n\nIn addition make sure to install `transformers`, `safetensors`, `accelerate` as well as the invisible watermark:\n```\npip install invisible_watermark transformers accelerate safetensors\n```\n\nTo just use the base model, you can run:\n\n```py\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16, use_safetensors=True, variant="fp16")\npipe.to("cuda")\n\n# if using torch < 2.0\n# pipe.enable_xformers_memory_efficient_attention()\n\nprompt = "An astronaut riding a green horse"\n\nimages = pipe(prompt=prompt).images[0]\n```\n\nTo use the whole base + refiner pipeline as an ensemble of experts you can run:\n\n```py\nfrom diffusers import DiffusionPipeline\nimport torch\n\n# load both base & refiner\nbase = DiffusionPipeline.from_pretrained(\n    "stabilityai/stable-diffusion-xl-base-1.0", torch_dtype=torch.float16, variant="fp16", use_safetensors=True\n)\nbase.to("cuda")\nrefiner = DiffusionPipeline.from_pretrained(\n    "stabilityai/stable-diffusion-xl-refiner-1.0",\n    text_encoder_2=base.text_encoder_2,\n    vae=base.vae,\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    variant="fp16",\n)\nrefiner.to("cuda")\n\n# Define how many steps and what % of steps to be run on each experts (80/20) here\nn_steps = 40\nhigh_noise_frac = 0.8\n\nprompt = "A majestic lion jumping from a big stone at night"\n\n# run both experts\nimage = base(\n    prompt=prompt,\n    num_inference_steps=n_steps,\n    denoising_end=high_noise_frac,\n    output_type="latent",\n).images\nimage = refiner(\n    prompt=prompt,\n    num_inference_steps=n_steps,\n    denoising_start=high_noise_frac,\n    image=image,\n).images[0]\n```\n\nWhen using `torch >= 2.0`, you can improve the inference speed by 20-30% with torch.compile. Simple wrap the unet with torch compile before running the pipeline:\n```py\npipe.unet = torch.compile(pipe.unet, mode="reduce-overhead", fullgraph=True)\n```\n\nIf you are limited by GPU VRAM, you can enable *cpu offloading* by calling `pipe.enable_model_cpu_offload`\ninstead of `.to("cuda")`:\n\n```diff\n- pipe.to("cuda")\n+ pipe.enable_model_cpu_offload()\n```\n\nFor more information on how to use Stable Diffusion XL with `diffusers`, please have a look at [the Stable Diffusion XL Docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl).\n\n### Optimum\n[Optimum](https://github.com/huggingface/optimum) provides a Stable Diffusion pipeline compatible with both [OpenVINO](https://docs.openvino.ai/latest/index.html) and [ONNX Runtime](https://onnxruntime.ai/).\n\n#### OpenVINO\n\nTo install Optimum with the dependencies required for OpenVINO :\n\n```bash\npip install optimum[openvino]\n```\n\nTo load an OpenVINO model and run inference with OpenVINO Runtime, you need to replace `StableDiffusionXLPipeline` with Optimum `OVStableDiffusionXLPipeline`. In case you want to load a PyTorch model and convert it to the OpenVINO format on-the-fly, you can set `export=True`.\n\n```diff\n- from diffusers import StableDiffusionXLPipeline\n+ from optimum.intel import OVStableDiffusionXLPipeline\n\nmodel_id = "stabilityai/stable-diffusion-xl-base-1.0"\n- pipeline = StableDiffusionXLPipeline.from_pretrained(model_id)\n+ pipeline = OVStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = "A majestic lion jumping from a big stone at night"\nimage = pipeline(prompt).images[0]\n```\n\nYou can find more examples (such as static reshaping and model compilation) in optimum [documentation](https://huggingface.co/docs/optimum/main/en/intel/inference#stable-diffusion-xl).\n\n\n#### ONNX\n\nTo install Optimum with the dependencies required for ONNX Runtime inference :\n\n```bash\npip install optimum[onnxruntime]\n```\n\nTo load an ONNX model and run inference with ONNX Runtime, you need to replace `StableDiffusionXLPipeline` with Optimum `ORTStableDiffusionXLPipeline`. In case you want to load a PyTorch model and convert it to the ONNX format on-the-fly, you can set `export=True`.\n\n```diff\n- from diffusers import StableDiffusionXLPipeline\n+ from optimum.onnxruntime import ORTStableDiffusionXLPipeline\n\nmodel_id = "stabilityai/stable-diffusion-xl-base-1.0"\n- pipeline = StableDiffusionXLPipeline.from_pretrained(model_id)\n+ pipeline = ORTStableDiffusionXLPipeline.from_pretrained(model_id)\nprompt = "A majestic lion jumping from a big stone at night"\nimage = pipeline(prompt).images[0]\n```\n\nYou can find more examples in optimum [documentation](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/models#stable-diffusion-xl).\n\n\n## Uses\n\n### Direct Use\n\nThe model is intended for research purposes only. Possible research areas and tasks include\n\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n- Research on generative models.\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n\nExcluded uses are described below.\n\n### Out-of-Scope Use\n\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n## Limitations and Bias\n\n### Limitations\n\n- The model does not achieve perfect photorealism\n- The model cannot render legible text\n- The model struggles with more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\n- Faces and people in general may not be generated properly.\n- The autoencoding part of the model is lossy.\n\n### Bias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases.\n', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":77363952950,"files_count":57,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionXLPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:Stability-AI:generative-models","source_url":"https://github.com/Stability-AI/generative-models"},{"type":"has_code","target_id":"github:mlfoundations:open_clip","source_url":"https://github.com/mlfoundations/open_clip"},{"type":"has_code","target_id":"github:openai:CLIP","source_url":"https://github.com/openai/CLIP"},{"type":"has_code","target_id":"github:Stability-AI:generative-models","source_url":"https://github.com/Stability-AI/generative-models"},{"type":"has_code","target_id":"github:Stability-AI:generative-models","source_url":"https://github.com/Stability-AI/generative-models"},{"type":"has_code","target_id":"github:Stability-AI:generative-models","source_url":"https://github.com/Stability-AI/generative-models"},{"type":"has_code","target_id":"github:huggingface:optimum","source_url":"https://github.com/huggingface/optimum"},{"type":"based_on_paper","target_id":"arxiv:2307.01952","source_url":"https://arxiv.org/abs/2307.01952"},{"type":"based_on_paper","target_id":"arxiv:2211.01324","source_url":"https://arxiv.org/abs/2211.01324"},{"type":"based_on_paper","target_id":"arxiv:2108.01073","source_url":"https://arxiv.org/abs/2108.01073"},{"type":"based_on_paper","target_id":"arxiv:2112.10752","source_url":"https://arxiv.org/abs/2112.10752"}]', NULL, 'OpenRAIL++', 'approved', 85, 'ff09567b3dc64b536b3029071ea4ad96', NULL, 'https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/pipeline.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-stabilityai-stable-diffusion-xl-base-1.0 from https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/pipeline.png
Image converted to WebP: data/images/huggingface-stabilityai-stable-diffusion-xl-base-1.0.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-CompVis-stable-diffusion-v1-4', 'huggingface--compvis--stable-diffusion-v1-4', 'stable-diffusion-v1-4', 'CompVis', '--- license: creativeml-openrail-m tags: - stable-diffusion - stable-diffusion-diffusers - text-to-image widget: - text: "A high tech solarpunk utopia in the Amazon rainforest" example_title: Amazon rainforest - text: "A pikachu fine dining with a view to the Eiffel Tower" example_title: Pikachu in Paris - text: "A mecha robot in a favela in expressionist style" example_title: Expressionist robot - text: "an insect robot preparing a delicious meal" example_title: Insect robot - text: "A small...', '["diffusers","safetensors","stable-diffusion","stable-diffusion-diffusers","text-to-image","arxiv:2207.12598","arxiv:2112.10752","arxiv:2103.00020","arxiv:2205.11487","arxiv:1910.09700","license:creativeml-openrail-m","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 6943, 619647, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/CompVis/stable-diffusion-v1-4","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\nwidget:\n- text: "A high tech solarpunk utopia in the Amazon rainforest"\n  example_title: Amazon rainforest\n- text: "A pikachu fine dining with a view to the Eiffel Tower"\n  example_title: Pikachu in Paris\n- text: "A mecha robot in a favela in expressionist style"\n  example_title: Expressionist robot\n- text: "an insect robot preparing a delicious meal"\n  example_title: Insect robot\n- text: "A small cabin on top of a snowy mountain in the style of Disney, artstation"\n  example_title: Snowy disney cabin\nextra_gated_prompt: |-\n  This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\n  The CreativeML OpenRAIL License specifies: \n\n  1. You can''t use the model to deliberately produce nor share illegal or harmful outputs or content \n  2. The authors claim no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n  3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n  Please read the full license carefully here: https://huggingface.co/spaces/CompVis/stable-diffusion-license\n      \nextra_gated_heading: Please read the LICENSE to access this model\n---\n\n# Stable Diffusion v1-4 Model Card\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nFor more information about how Stable Diffusion functions, please have a look at [ü§ó''s Stable Diffusion with üß®Diffusers blog](https://huggingface.co/blog/stable_diffusion).\n\nThe **Stable-Diffusion-v1-4** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-2](https:/steps/huggingface.co/CompVis/stable-diffusion-v1-2) \ncheckpoint and subsequently fine-tuned on 225k steps at resolution 512x512 on "laion-aesthetics v2 5+" and 10% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\nThis weights here are intended to be used with the üß® Diffusers library. If you are looking for the weights to be loaded into the CompVis Stable Diffusion codebase, [come here](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original)\n\n## Model Details\n- **Developed by:** Robin Rombach, Patrick Esser\n- **Model type:** Diffusion-based text-to-image generation model\n- **Language(s):** English\n- **License:** [The CreativeML OpenRAIL M license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) is an [Open RAIL M license](https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses), adapted from the work that [BigScience](https://bigscience.huggingface.co/) and [the RAIL Initiative](https://www.licenses.ai/) are jointly carrying in the area of responsible AI licensing. See also [the article about the BLOOM Open RAIL license](https://bigscience.huggingface.co/blog/the-bigscience-rail-license) on which our license is based.\n- **Model Description:** This is a model that can be used to generate and modify images based on text prompts. It is a [Latent Diffusion Model](https://arxiv.org/abs/2112.10752) that uses a fixed, pretrained text encoder ([CLIP ViT-L/14](https://arxiv.org/abs/2103.00020)) as suggested in the [Imagen paper](https://arxiv.org/abs/2205.11487).\n- **Resources for more information:** [GitHub Repository](https://github.com/CompVis/stable-diffusion), [Paper](https://arxiv.org/abs/2112.10752).\n- **Cite as:**\n\n      @InProceedings{Rombach_2022_CVPR,\n          author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},\n          title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n          booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n          month     = {June},\n          year      = {2022},\n          pages     = {10684-10695}\n      }\n\n## Examples\n\nWe recommend using [ü§ó''s Diffusers library](https://github.com/huggingface/diffusers) to run Stable Diffusion.\n\n### PyTorch\n\n```bash\npip install --upgrade diffusers transformers scipy\n```\n\nRunning the pipeline with the default PNDM scheduler:\n\n```python\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\nmodel_id = "CompVis/stable-diffusion-v1-4"\ndevice = "cuda"\n\n\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\n\nprompt = "a photo of an astronaut riding a horse on mars"\nimage = pipe(prompt).images[0]  \n    \nimage.save("astronaut_rides_horse.png")\n```\n\n**Note**:\nIf you are limited by GPU memory and have less than 4GB of GPU RAM available, please make sure to load the StableDiffusionPipeline in float16 precision instead of the default float32 precision as done above. You can do so by telling diffusers to expect the weights to be in float16 precision:\n\n\n```py\nimport torch\n\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(device)\npipe.enable_attention_slicing()\n\nprompt = "a photo of an astronaut riding a horse on mars"\nimage = pipe(prompt).images[0]  \n    \nimage.save("astronaut_rides_horse.png")\n```\n\nTo swap out the noise scheduler, pass it to `from_pretrained`:\n\n```python\nfrom diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n\nmodel_id = "CompVis/stable-diffusion-v1-4"\n\n# Use the Euler scheduler here instead\nscheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder="scheduler")\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\npipe = pipe.to("cuda")\n\nprompt = "a photo of an astronaut riding a horse on mars"\nimage = pipe(prompt).images[0]  \n    \nimage.save("astronaut_rides_horse.png")\n```\n\n### JAX/Flax\n\nTo use StableDiffusion on TPUs and GPUs for faster inference you can leverage JAX/Flax.\n\nRunning the pipeline with default PNDMScheduler\n\n```python\nimport jax\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\n\nfrom diffusers import FlaxStableDiffusionPipeline\n\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n    "CompVis/stable-diffusion-v1-4", revision="flax", dtype=jax.numpy.bfloat16\n)\n\nprompt = "a photo of an astronaut riding a horse on mars"\n\nprng_seed = jax.random.PRNGKey(0)\nnum_inference_steps = 50\n\nnum_samples = jax.device_count()\nprompt = num_samples * [prompt]\nprompt_ids = pipeline.prepare_inputs(prompt)\n\n# shard inputs and rng\nparams = replicate(params)\nprng_seed = jax.random.split(prng_seed, num_samples)\nprompt_ids = shard(prompt_ids)\n\nimages = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\nimages = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\n```\n\n**Note**:\nIf you are limited by TPU memory, please make sure to load the `FlaxStableDiffusionPipeline` in `bfloat16` precision instead of the default `float32` precision as done above. You can do so by telling diffusers to load the weights from "bf16" branch.\n\n```python\nimport jax\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\n\nfrom diffusers import FlaxStableDiffusionPipeline\n\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n    "CompVis/stable-diffusion-v1-4", revision="bf16", dtype=jax.numpy.bfloat16\n)\n\nprompt = "a photo of an astronaut riding a horse on mars"\n\nprng_seed = jax.random.PRNGKey(0)\nnum_inference_steps = 50\n\nnum_samples = jax.device_count()\nprompt = num_samples * [prompt]\nprompt_ids = pipeline.prepare_inputs(prompt)\n\n# shard inputs and rng\nparams = replicate(params)\nprng_seed = jax.random.split(prng_seed, num_samples)\nprompt_ids = shard(prompt_ids)\n\nimages = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\nimages = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\n```\n\n# Uses\n\n## Direct Use \nThe model is intended for research purposes only. Possible research areas and\ntasks include\n\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n- Research on generative models.\n\nExcluded uses are described below.\n\n ### Misuse, Malicious Use, and Out-of-Scope Use\n_Note: This section is taken from the [DALLE-MINI model card](https://huggingface.co/dalle-mini/dalle-mini), but applies in the same way to Stable Diffusion v1_.\n\n\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\n\n#### Out-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n#### Misuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\n\n- Generating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\n- Intentionally promoting or propagating discriminatory content or harmful stereotypes.\n- Impersonating individuals without their consent.\n- Sexual content without consent of the people who might see it.\n- Mis- and disinformation\n- Representations of egregious violence and gore\n- Sharing of copyrighted or licensed material in violation of its terms of use.\n- Sharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\n\n## Limitations and Bias\n\n### Limitations\n\n- The model does not achieve perfect photorealism\n- The model cannot render legible text\n- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\n- Faces and people in general may not be generated properly.\n- The model was trained mainly with English captions and will not work as well in other languages.\n- The autoencoding part of the model is lossy\n- The model was trained on a large-scale dataset\n  [LAION-5B](https://laion.ai/blog/laion-5b/) which contains adult material\n  and is not fit for product use without additional safety mechanisms and\n  considerations.\n- No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.\n  The training data can be searched at [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) to possibly assist in the detection of memorized images.\n\n### Bias\n\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases. \nStable Diffusion v1 was trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), \nwhich consists of images that are primarily limited to English descriptions. \nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. \nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the \nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\n\n### Safety Module\n\nThe intended use of this model is with the [Safety Checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) in Diffusers. \nThis checker works by checking model outputs against known hard-coded NSFW concepts.\nThe concepts are intentionally hidden to reduce the likelihood of reverse-engineering this filter.\nSpecifically, the checker compares the class probability of harmful concepts in the embedding space of the `CLIPTextModel` *after generation* of the images. \nThe concepts are passed into the model with the generated image and compared to a hand-engineered weight for each NSFW concept.\n\n\n## Training\n\n**Training Data**\nThe model developers used the following dataset for training the model:\n\n- LAION-2B (en) and subsets thereof (see next section)\n\n**Training Procedure**\nStable Diffusion v1-4 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training, \n\n- Images are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\n- Text prompts are encoded through a ViT-L/14 text-encoder.\n- The non-pooled output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\n- The loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet.\n\nWe currently provide four checkpoints, which were trained as follows.\n- [`stable-diffusion-v1-1`](https://huggingface.co/CompVis/stable-diffusion-v1-1): 237,000 steps at resolution `256x256` on [laion2B-en](https://huggingface.co/datasets/laion/laion2B-en).\n  194,000 steps at resolution `512x512` on [laion-high-resolution](https://huggingface.co/datasets/laion/laion-high-resolution) (170M examples from LAION-5B with resolution `>= 1024x1024`).\n- [`stable-diffusion-v1-2`](https://huggingface.co/CompVis/stable-diffusion-v1-2): Resumed from `stable-diffusion-v1-1`.\n  515,000 steps at resolution `512x512` on "laion-improved-aesthetics" (a subset of laion2B-en,\nfiltered to images with an original size `>= 512x512`, estimated aesthetics score `> 5.0`, and an estimated watermark probability `< 0.5`. The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an [improved aesthetics estimator](https://github.com/christophschuhmann/improved-aesthetic-predictor)).\n- [`stable-diffusion-v1-3`](https://huggingface.co/CompVis/stable-diffusion-v1-3): Resumed from `stable-diffusion-v1-2`. 195,000 steps at resolution `512x512` on "laion-improved-aesthetics" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n- [`stable-diffusion-v1-4`](https://huggingface.co/CompVis/stable-diffusion-v1-4) Resumed from `stable-diffusion-v1-2`.225,000 steps at resolution `512x512` on "laion-aesthetics v2 5+"  and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\n- **Hardware:** 32 x 8 x A100 GPUs\n- **Optimizer:** AdamW\n- **Gradient Accumulations**: 2\n- **Batch:** 32 x 8 x 2 x 4 = 2048\n- **Learning rate:** warmup to 0.0001 for 10,000 steps and then kept constant\n\n## Evaluation Results \nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 PLMS sampling\nsteps show the relative improvements of the checkpoints:\n\n![pareto](https://huggingface.co/CompVis/stable-diffusion/resolve/main/v1-variants-scores.jpg)\n\nEvaluated using 50 PLMS steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\n## Environmental Impact\n\n**Stable Diffusion v1** **Estimated Emissions**\nBased on that information, we estimate the following CO2 emissions using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\n\n- **Hardware Type:** A100 PCIe 40GB\n- **Hours used:** 150000\n- **Cloud Provider:** AWS\n- **Compute Region:** US-east\n- **Carbon Emitted (Power consumption x Time x Carbon produced based on location of power grid):** 11250 kg CO2 eq.\n\n\n## Citation\n\n```bibtex\n    @InProceedings{Rombach_2022_CVPR,\n        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},\n        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n        month     = {June},\n        year      = {2022},\n        pages     = {10684-10695}\n    }\n```\n\n*This model card was written by: Robin Rombach and Patrick Esser and is based on the [DALL-E Mini model card](https://huggingface.co/dalle-mini/dalle-mini).*', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":64990530954,"files_count":33,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:CompVis:stable-diffusion","source_url":"https://github.com/CompVis/stable-diffusion"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:christophschuhmann:improved-aesthetic-predictor","source_url":"https://github.com/christophschuhmann/improved-aesthetic-predictor"},{"type":"based_on_paper","target_id":"arxiv:2207.12598","source_url":"https://arxiv.org/abs/2207.12598"},{"type":"based_on_paper","target_id":"arxiv:2112.10752","source_url":"https://arxiv.org/abs/2112.10752"},{"type":"based_on_paper","target_id":"arxiv:2103.00020","source_url":"https://arxiv.org/abs/2103.00020"},{"type":"based_on_paper","target_id":"arxiv:2205.11487","source_url":"https://arxiv.org/abs/2205.11487"},{"type":"based_on_paper","target_id":"arxiv:1910.09700","source_url":"https://arxiv.org/abs/1910.09700"}]', NULL, 'creativeml-openrail-m', 'approved', 80, 'caa4155201f8625d95ecdcbd84a1d73c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Meta-Llama-3-8B', 'huggingface--meta-llama--meta-llama-3-8b', 'Meta-Llama-3-8B', 'meta-llama', '', '["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","en","license:llama3","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 6397, 2110467, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Meta-Llama-3-8B","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":8030261248,"storage_bytes":48183894272,"files_count":17,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","eos_token":"<|end_of_text|>"}}}', '[]', '[]', NULL, 'LLaMA-3', 'approved', 40, '6e5b7dbd6f80eb593ab9c6de62024326', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-hexgrad-Kokoro-82M', 'huggingface--hexgrad--kokoro-82m', 'Kokoro-82M', 'hexgrad', '--- license: apache-2.0 language: - en base_model: - yl4579/StyleTTS2-LJSpeech pipeline_tag: text-to-speech --- **Kokoro** is an open-weight TTS model with 82 million parameters. Despite its lightweight architecture, it delivers comparable quality to larger models while being significantly faster and more cost-efficient. With Apache-licensed weights, Kokoro can be deployed anywhere from production environments to personal projects. <audio controls><source src="https://huggingface.co/hexgrad/K...', '["text-to-speech","en","arxiv:2306.07691","arxiv:2203.02395","base_model:yl4579/styletts2-ljspeech","base_model:finetune:yl4579/styletts2-ljspeech","doi:10.57967/hf/4329","license:apache-2.0","region:us"]', 'text-to-speech', 5373, 4125732, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/hexgrad/Kokoro-82M","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\nbase_model:\n- yl4579/StyleTTS2-LJSpeech\npipeline_tag: text-to-speech\n---\n**Kokoro** is an open-weight TTS model with 82 million parameters. Despite its lightweight architecture, it delivers comparable quality to larger models while being significantly faster and more cost-efficient. With Apache-licensed weights, Kokoro can be deployed anywhere from production environments to personal projects.\n\n<audio controls><source src="https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/samples/HEARME.wav" type="audio/wav"></audio>\n\nüêà **GitHub**: https://github.com/hexgrad/kokoro\n\nüöÄ **Demo**: https://hf.co/spaces/hexgrad/Kokoro-TTS\n\n> [!NOTE]\n> As of April 2025, the market rate of Kokoro served over API is **under $1 per million characters of text input**, or under $0.06 per hour of audio output. (On average, 1000 characters of input is about 1 minute of output.) Sources: [ArtificialAnalysis/Replicate at 65 cents per M chars](https://artificialanalysis.ai/text-to-speech/model-family/kokoro#price) and [DeepInfra at 80 cents per M chars](https://deepinfra.com/hexgrad/Kokoro-82M).\n>\n> This is an Apache-licensed model, and Kokoro has been deployed in numerous projects and commercial APIs. We welcome the deployment of the model in real use cases.\n\n> [!CAUTION]\n> Fake websites like kokorottsai_com (snapshot: https://archive.ph/nRRnk) and kokorotts_net (snapshot: https://archive.ph/60opa) are likely scams masquerading under the banner of a popular model.\n>\n> Any website containing "kokoro" in its root domain (e.g. kokorottsai_com, kokorotts_net) is **NOT owned by and NOT affiliated with this model page or its author**, and attempts to imply otherwise are red flags.\n\n- [Releases](#releases)\n- [Usage](#usage)\n- [EVAL.md](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/EVAL.md) ‚ÜóÔ∏è\n- [SAMPLES.md](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md) ‚ÜóÔ∏è\n- [VOICES.md](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md) ‚ÜóÔ∏è\n- [Model Facts](#model-facts)\n- [Training Details](#training-details)\n- [Creative Commons Attribution](#creative-commons-attribution)\n- [Acknowledgements](#acknowledgements)\n\n### Releases\n\n| Model | Published | Training Data | Langs & Voices | SHA256 |\n| ----- | --------- | ------------- | -------------- | ------ |\n| **v1.0** | **2025 Jan 27** | **Few hundred hrs** | [**8 & 54**](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md) | `496dba11` |\n| [v0.19](https://huggingface.co/hexgrad/kLegacy/tree/main/v0.19) | 2024 Dec 25 | <100 hrs | 1 & 10 | `3b0c392f` |\n\n| Training Costs | v0.19 | v1.0 | **Total** |\n| -------------- | ----- | ---- | ----- |\n| in A100 80GB GPU hours | 500 | 500 | **1000** |\n| average hourly rate | $0.80/h | $1.20/h | **$1/h** |\n| in USD | $400 | $600 | **$1000** |\n\n### Usage\nYou can run this basic cell on [Google Colab](https://colab.research.google.com/). [Listen to samples](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md). For more languages and details, see [Advanced Usage](https://github.com/hexgrad/kokoro?tab=readme-ov-file#advanced-usage).\n```py\n!pip install -q kokoro>=0.9.2 soundfile\n!apt-get -qq -y install espeak-ng > /dev/null 2>&1\nfrom kokoro import KPipeline\nfrom IPython.display import display, Audio\nimport soundfile as sf\nimport torch\npipeline = KPipeline(lang_code=''a'')\ntext = ''''''\n[Kokoro](/kÀàOk…ô…πO/) is an open-weight TTS model with 82 million parameters. Despite its lightweight architecture, it delivers comparable quality to larger models while being significantly faster and more cost-efficient. With Apache-licensed weights, [Kokoro](/kÀàOk…ô…πO/) can be deployed anywhere from production environments to personal projects.\n''''''\ngenerator = pipeline(text, voice=''af_heart'')\nfor i, (gs, ps, audio) in enumerate(generator):\n    print(i, gs, ps)\n    display(Audio(data=audio, rate=24000, autoplay=i==0))\n    sf.write(f''{i}.wav'', audio, 24000)\n```\nUnder the hood, `kokoro` uses [`misaki`](https://pypi.org/project/misaki/), a G2P library at https://github.com/hexgrad/misaki\n\n### Model Facts\n\n**Architecture:**\n- StyleTTS 2: https://arxiv.org/abs/2306.07691\n- ISTFTNet: https://arxiv.org/abs/2203.02395\n- Decoder only: no diffusion, no encoder release\n\n**Architected by:** Li et al @ https://github.com/yl4579/StyleTTS2\n\n**Trained by**: `@rzvzn` on Discord\n\n**Languages:** Multiple\n\n**Model SHA256 Hash:** `496dba118d1a58f5f3db2efc88dbdc216e0483fc89fe6e47ee1f2c53f18ad1e4`\n\n### Training Details\n\n**Data:** Kokoro was trained exclusively on **permissive/non-copyrighted audio data** and IPA phoneme labels. Examples of permissive/non-copyrighted audio include:\n- Public domain audio\n- Audio licensed under Apache, MIT, etc\n- Synthetic audio<sup>[1]</sup> generated by closed<sup>[2]</sup> TTS models from large providers<br/>\n[1] https://copyright.gov/ai/ai_policy_guidance.pdf<br/>\n[2] No synthetic audio from open TTS models or "custom voice clones"\n\n**Total Dataset Size:** A few hundred hours of audio\n\n**Total Training Cost:** About $1000 for 1000 hours of A100 80GB vRAM\n\n### Creative Commons Attribution\n\nThe following CC BY audio was part of the dataset used to train Kokoro v1.0.\n\n| Audio Data | Duration Used | License | Added to Training Set After |\n| ---------- | ------------- | ------- | --------------------------- |\n| [Koniwa](https://github.com/koniwa/koniwa) `tnc` | <1h | [CC BY 3.0](https://creativecommons.org/licenses/by/3.0/deed.ja) | v0.19 / 22 Nov 2024 |\n| [SIWIS](https://datashare.ed.ac.uk/handle/10283/2353) | <11h | [CC BY 4.0](https://datashare.ed.ac.uk/bitstream/handle/10283/2353/license_text) | v0.19 / 22 Nov 2024 |\n\n### Acknowledgements\n\n- üõ†Ô∏è [@yl4579](https://huggingface.co/yl4579) for architecting StyleTTS 2.\n- üèÜ [@Pendrokar](https://huggingface.co/Pendrokar) for adding Kokoro as a contender in the TTS Spaces Arena.\n- üìä Thank you to everyone who contributed synthetic training data.\n- ‚ù§Ô∏è Special thanks to all compute sponsors.\n- üëæ Discord server: https://discord.gg/QuGxSWBfQy\n- ü™Ω Kokoro is a Japanese word that translates to "heart" or "spirit". It is also the name of an [AI in the Terminator franchise](https://terminator.fandom.com/wiki/Kokoro).\n\n<img src="https://static0.gamerantimages.com/wordpress/wp-content/uploads/2024/08/terminator-zero-41-1.jpg" width="400" alt="kokoro" />\n', '{"pipeline_tag":"text-to-speech","library_name":null,"framework":null,"params":null,"storage_bytes":1234555949,"files_count":72,"spaces_count":100,"gated":false,"private":false,"config":{}}', '[]', '[{"type":"has_code","target_id":"github:hexgrad:kokoro","source_url":"https://github.com/hexgrad/kokoro"},{"type":"has_code","target_id":"github:hexgrad:kokoro","source_url":"https://github.com/hexgrad/kokoro?tab=readme-ov-file#advanced-usage"},{"type":"has_code","target_id":"github:hexgrad:misaki","source_url":"https://github.com/hexgrad/misaki"},{"type":"has_code","target_id":"github:yl4579:StyleTTS2","source_url":"https://github.com/yl4579/StyleTTS2"},{"type":"has_code","target_id":"github:koniwa:koniwa","source_url":"https://github.com/koniwa/koniwa"},{"type":"based_on_paper","target_id":"arxiv:2306.07691","source_url":"https://arxiv.org/abs/2306.07691"},{"type":"based_on_paper","target_id":"arxiv:2203.02395","source_url":"https://arxiv.org/abs/2203.02395"}]', NULL, 'Apache-2.0', 'approved', 65, '3726878070751dc3f2afd4d2c54a3637', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-openai-whisper-large-v3', 'huggingface--openai--whisper-large-v3', 'whisper-large-v3', 'openai', '--- language: - en - zh - de - es - ru - ko - fr - ja - pt - tr - pl - ca - nl - ar - sv - it - id - hi - fi - vi - he - uk - el - ms - cs - ro - da - hu - ta - no - th - ur - hr - bg - lt - la - mi - ml - cy - sk - te - fa - lv - bn - sr - az - sl - kn - et - mk - br - eu - is - hy - ne - mn - bs - kk - sq - sw - gl - mr - pa - si - km - sn - yo - so - af - oc - ka - be - tg - sd - gu - am - yi - lo - uz - fo - ht - ps - tk - nn - mt - sa - lb - my - bo - tl - mg - as - tt - haw - ln - ha - ...', '["transformers","pytorch","jax","safetensors","whisper","automatic-speech-recognition","audio","hf-asr-leaderboard","en","zh","de","es","ru","ko","fr","ja","pt","tr","pl","ca","nl","ar","sv","it","id","hi","fi","vi","he","uk","el","ms","cs","ro","da","hu","ta","no","th","ur","hr","bg","lt","la","mi","ml","cy","sk","te","fa","lv","bn","sr","az","sl","kn","et","mk","br","eu","is","hy","ne","mn","bs","kk","sq","sw","gl","mr","pa","si","km","sn","yo","so","af","oc","ka","be","tg","sd","gu","am","yi","lo","uz","fo","ht","ps","tk","nn","mt","sa","lb","my","bo","tl","mg","as","tt","haw","ln","ha","ba","jw","su","arxiv:2212.04356","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'automatic-speech-recognition', 5177, 5058096, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/openai/whisper-large-v3","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage: \n- en\n- zh\n- de\n- es\n- ru\n- ko\n- fr\n- ja\n- pt\n- tr\n- pl\n- ca\n- nl\n- ar\n- sv\n- it\n- id\n- hi\n- fi\n- vi\n- he\n- uk\n- el\n- ms\n- cs\n- ro\n- da\n- hu\n- ta\n- no\n- th\n- ur\n- hr\n- bg\n- lt\n- la\n- mi\n- ml\n- cy\n- sk\n- te\n- fa\n- lv\n- bn\n- sr\n- az\n- sl\n- kn\n- et\n- mk\n- br\n- eu\n- is\n- hy\n- ne\n- mn\n- bs\n- kk\n- sq\n- sw\n- gl\n- mr\n- pa\n- si\n- km\n- sn\n- yo\n- so\n- af\n- oc\n- ka\n- be\n- tg\n- sd\n- gu\n- am\n- yi\n- lo\n- uz\n- fo\n- ht\n- ps\n- tk\n- nn\n- mt\n- sa\n- lb\n- my\n- bo\n- tl\n- mg\n- as\n- tt\n- haw\n- ln\n- ha\n- ba\n- jw\n- su\ntags:\n- audio\n- automatic-speech-recognition\n- hf-asr-leaderboard\nwidget:\n- example_title: Librispeech sample 1\n  src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n- example_title: Librispeech sample 2\n  src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\npipeline_tag: automatic-speech-recognition\nlicense: apache-2.0\n---\n\n# Whisper\n\nWhisper is a state-of-the-art model for automatic speech recognition (ASR) and speech translation, proposed in the paper \n[Robust Speech Recognition via Large-Scale Weak Supervision](https://huggingface.co/papers/2212.04356) by Alec Radford \net al. from OpenAI. Trained on >5M hours of labeled data, Whisper demonstrates a strong ability to generalise to many \ndatasets and domains in a zero-shot setting.\n\nWhisper large-v3 has the same architecture as the previous [large](https://huggingface.co/openai/whisper-large) and [large-v2](https://huggingface.co/openai/whisper-large-v2) \nmodels, except for the following minor differences:\n\n1. The spectrogram input uses 128 Mel frequency bins instead of 80\n2. A new language token for Cantonese\n\nThe Whisper large-v3 model was trained on 1 million hours of weakly labeled audio and 4 million hours of pseudo-labeled \naudio collected using Whisper [large-v2](https://huggingface.co/openai/whisper-large-v2) . The model was trained for 2.0 epochs over this mixture dataset.\n\nThe large-v3 model shows improved performance over a wide variety of languages, showing 10% to 20% reduction of errors \ncompared to Whisper [large-v2](https://huggingface.co/openai/whisper-large-v2) . For more details on the different checkpoints available, refer to the section [Model details](#model-details).\n\n**Disclaimer**: Content for this model card has partly been written by the ü§ó Hugging Face team, and partly copied and \npasted from the original model card.\n\n## Usage\n\nWhisper large-v3 is supported in Hugging Face ü§ó Transformers. To run the model, first install the Transformers \nlibrary. For this example, we''ll also install ü§ó Datasets to load toy audio dataset from the Hugging Face Hub, and \nü§ó Accelerate to reduce the model loading time:\n\n```bash\npip install --upgrade pip\npip install --upgrade transformers datasets[audio] accelerate\n```\n\nThe model can be used with the [`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)\nclass to transcribe audios of arbitrary length:\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = "cuda:0" if torch.cuda.is_available() else "cpu"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = "openai/whisper-large-v3"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    "automatic-speech-recognition",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset("distil-whisper/librispeech_long", "clean", split="validation")\nsample = dataset[0]["audio"]\n\nresult = pipe(sample)\nprint(result["text"])\n```\n\nTo transcribe a local audio file, simply pass the path to your audio file when you call the pipeline:\n\n```python\nresult = pipe("audio.mp3")\n```\n\nMultiple audio files can be transcribed in parallel by specifying them as a list and setting the `batch_size` parameter:\n\n```python\nresult = pipe(["audio_1.mp3", "audio_2.mp3"], batch_size=2)\n```\n\nTransformers is compatible with all Whisper decoding strategies, such as temperature fallback and condition on previous \ntokens. The following example demonstrates how to enable these heuristics:\n\n```python\ngenerate_kwargs = {\n    "max_new_tokens": 448,\n    "num_beams": 1,\n    "condition_on_prev_tokens": False,\n    "compression_ratio_threshold": 1.35,  # zlib compression ratio threshold (in token space)\n    "temperature": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    "logprob_threshold": -1.0,\n    "no_speech_threshold": 0.6,\n    "return_timestamps": True,\n}\n\nresult = pipe(sample, generate_kwargs=generate_kwargs)\n```\n\nWhisper predicts the language of the source audio automatically. If the source audio language is known *a-priori*, it \ncan be passed as an argument to the pipeline:\n\n```python\nresult = pipe(sample, generate_kwargs={"language": "english"})\n```\n\nBy default, Whisper performs the task of *speech transcription*, where the source audio language is the same as the target\ntext language. To perform *speech translation*, where the target text is in English, set the task to `"translate"`:\n\n```python\nresult = pipe(sample, generate_kwargs={"task": "translate"})\n```\n\nFinally, the model can be made to predict timestamps. For sentence-level timestamps, pass the `return_timestamps` argument:\n\n```python\nresult = pipe(sample, return_timestamps=True)\nprint(result["chunks"])\n```\n\nAnd for word-level timestamps:\n\n```python\nresult = pipe(sample, return_timestamps="word")\nprint(result["chunks"])\n```\n\nThe above arguments can be used in isolation or in combination. For example, to perform the task of speech transcription \nwhere the source audio is in French, and we want to return sentence-level timestamps, the following can be used:\n\n```python\nresult = pipe(sample, return_timestamps=True, generate_kwargs={"language": "french", "task": "translate"})\nprint(result["chunks"])\n```\n\n<details>\n\n<summary> For more control over the generation parameters, use the model + processor API directly: </summary>\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\nfrom datasets import Audio, load_dataset\n\n\ndevice = "cuda:0" if torch.cuda.is_available() else "cpu"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = "openai/whisper-large-v3"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\ndataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")\ndataset = dataset.cast_column("audio", Audio(processor.feature_extractor.sampling_rate))\nsample = dataset[0]["audio"]\n\ninputs = processor(\n    sample["array"],\n    sampling_rate=sample["sampling_rate"],\n    return_tensors="pt",\n    truncation=False,\n    padding="longest",\n    return_attention_mask=True,\n)\ninputs = inputs.to(device, dtype=torch_dtype)\n\ngen_kwargs = {\n    "max_new_tokens": 448,\n    "num_beams": 1,\n    "condition_on_prev_tokens": False,\n    "compression_ratio_threshold": 1.35,  # zlib compression ratio threshold (in token space)\n    "temperature": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    "logprob_threshold": -1.0,\n    "no_speech_threshold": 0.6,\n    "return_timestamps": True,\n}\n\npred_ids = model.generate(**inputs, **gen_kwargs)\npred_text = processor.batch_decode(pred_ids, skip_special_tokens=True, decode_with_timestamps=False)\n\nprint(pred_text)\n```\n\n</details>\n\n## Additional Speed & Memory Improvements\n\nYou can apply additional speed and memory improvements to Whisper to further reduce the inference speed and VRAM \nrequirements.\n\n### Chunked Long-Form\n\nWhisper has a receptive field of 30-seconds. To transcribe audios longer than this, one of two long-form algorithms are\nrequired:\n1. **Sequential:** uses a "sliding window" for buffered inference, transcribing 30-second slices one after the other\n2. **Chunked:** splits long audio files into shorter ones (with a small overlap between segments), transcribes each segment independently, and stitches the resulting transcriptions at the boundaries\n\nThe sequential long-form algorithm should be used in either of the following scenarios:\n1. Transcription accuracy is the most important factor, and speed is less of a consideration\n2. You are transcribing **batches** of long audio files, in which case the latency of sequential is comparable to chunked, while being up to 0.5% WER more accurate\n\nConversely, the chunked algorithm should be used when:\n1. Transcription speed is the most important factor\n2. You are transcribing a **single** long audio file\n\nBy default, Transformers uses the sequential algorithm. To enable the chunked algorithm, pass the `chunk_length_s` \nparameter to the `pipeline`. For large-v3, a chunk length of 30-seconds is optimal. To activate batching over long \naudio files, pass the argument `batch_size`:\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = "cuda:0" if torch.cuda.is_available() else "cpu"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = "openai/whisper-large-v3"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    "automatic-speech-recognition",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    chunk_length_s=30,\n    batch_size=16,  # batch size for inference - set based on your device\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset("distil-whisper/librispeech_long", "clean", split="validation")\nsample = dataset[0]["audio"]\n\nresult = pipe(sample)\nprint(result["text"])\n```\n\n#### Torch compile\n\nThe Whisper forward pass is compatible with [`torch.compile`](https://pytorch.org/docs/stable/generated/torch.compile.html)\nfor 4.5x speed-ups.\n\n**Note:** `torch.compile` is currently not compatible with the Chunked long-form algorithm or Flash Attention 2 ‚ö†Ô∏è\n\n```python\nimport torch\nfrom torch.nn.attention import SDPBackend, sdpa_kernel\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\ntorch.set_float32_matmul_precision("high")\n\ndevice = "cuda:0" if torch.cuda.is_available() else "cpu"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = "openai/whisper-large-v3"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n).to(device)\n\n# Enable static cache and compile the forward pass\nmodel.generation_config.cache_implementation = "static"\nmodel.generation_config.max_new_tokens = 256\nmodel.forward = torch.compile(model.forward, mode="reduce-overhead", fullgraph=True)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    "automatic-speech-recognition",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset("distil-whisper/librispeech_long", "clean", split="validation")\nsample = dataset[0]["audio"]\n\n# 2 warmup steps\nfor _ in tqdm(range(2), desc="Warm-up step"):\n    with sdpa_kernel(SDPBackend.MATH):\n        result = pipe(sample.copy(), generate_kwargs={"min_new_tokens": 256, "max_new_tokens": 256})\n\n# fast run\nwith sdpa_kernel(SDPBackend.MATH):\n    result = pipe(sample.copy())\n\nprint(result["text"])\n```\n\n#### Flash Attention 2\n\nWe recommend using [Flash-Attention 2](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#flashattention-2) if your GPU supports it and you are not using [torch.compile](#torch-compile). \nTo do so, first install [Flash Attention](https://github.com/Dao-AILab/flash-attention):\n\n```\npip install flash-attn --no-build-isolation\n```\n\nThen pass `attn_implementation="flash_attention_2"` to `from_pretrained`:\n\n```python\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation="flash_attention_2")\n```\n\n#### Torch Scale-Product-Attention (SDPA)\n\nIf your GPU does not support Flash Attention, we recommend making use of PyTorch [scaled dot-product attention (SDPA)](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html). \nThis attention implementation is activated **by default** for PyTorch versions 2.1.1 or greater. To check \nwhether you have a compatible PyTorch version, run the following Python code snippet:\n\n```python\nfrom transformers.utils import is_torch_sdpa_available\n\nprint(is_torch_sdpa_available())\n```\n\nIf the above returns `True`, you have a valid version of PyTorch installed and SDPA is activated by default. If it \nreturns `False`, you need to upgrade your PyTorch version according to the [official instructions](https://pytorch.org/get-started/locally/)\n\nOnce a valid PyTorch version is installed, SDPA is activated by default. It can also be set explicitly by specifying \n`attn_implementation="sdpa"` as follows:\n\n```python\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation="sdpa")\n```\n\nFor more information about how to use the SDPA refer to the [Transformers SDPA documentation](https://huggingface.co/docs/transformers/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention).\n\n\n## Model details\n\nWhisper is a Transformer based encoder-decoder model, also referred to as a _sequence-to-sequence_ model. There are two\nflavours of Whisper model: English-only and multilingual. The English-only models were trained on the task of English \nspeech recognition. The multilingual models were trained simultaneously on multilingual speech recognition and speech \ntranslation. For speech recognition, the model predicts transcriptions in the *same* language as the audio. For speech \ntranslation, the model predicts transcriptions to a *different* language to the audio.\n\nWhisper checkpoints come in five configurations of varying model sizes. The smallest four are available as English-only \nand multilingual. The largest checkpoints are multilingual only. All ten of the pre-trained checkpoints \nare available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The \ncheckpoints are summarised in the following table with links to the models on the Hub:\n\n| Size     | Parameters | English-only                                         | Multilingual                                        |\n|----------|------------|------------------------------------------------------|-----------------------------------------------------|\n| tiny     | 39 M       | [‚úì](https://huggingface.co/openai/whisper-tiny.en)   | [‚úì](https://huggingface.co/openai/whisper-tiny)     |\n| base     | 74 M       | [‚úì](https://huggingface.co/openai/whisper-base.en)   | [‚úì](https://huggingface.co/openai/whisper-base)     |\n| small    | 244 M      | [‚úì](https://huggingface.co/openai/whisper-small.en)  | [‚úì](https://huggingface.co/openai/whisper-small)    |\n| medium   | 769 M      | [‚úì](https://huggingface.co/openai/whisper-medium.en) | [‚úì](https://huggingface.co/openai/whisper-medium)   |\n| large    | 1550 M     | x                                                    | [‚úì](https://huggingface.co/openai/whisper-large)    |\n| large-v2 | 1550 M     | x                                                    | [‚úì](https://huggingface.co/openai/whisper-large-v2) |\n| large-v3 | 1550 M     | x                                                    | [‚úì](https://huggingface.co/openai/whisper-large-v3) |\n\n\n## Fine-Tuning\n\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However, \nits predictive capabilities can be improved further for certain languages and tasks through *fine-tuning*. The blog \npost [Fine-Tune Whisper with ü§ó Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step \nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\n\n### Evaluated Use\n\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only ‚Äúintended‚Äù uses or to draw reasonable guidelines around what is or is not research.\n\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\n\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\n\n\n## Training Data\n\nThe large-v3 checkpoint is trained on 1 million hours of weakly labeled audio and 4 million hours of pseudo-labeled audio collected using Whisper large-v2. \n\nAs discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.\n\n\n## Performance and Limitations\n\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf). \n\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\n\n\n## Broader Implications\n\nWe anticipate that Whisper models‚Äô transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box ‚Äì their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\n\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\n\n\n### BibTeX entry and citation info\n```bibtex\n@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n```', '{"pipeline_tag":"automatic-speech-recognition","library_name":"transformers","framework":"transformers","params":1543490560,"storage_bytes":31618818526,"files_count":21,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["WhisperForConditionalGeneration"],"model_type":"whisper","tokenizer_config":{"bos_token":"<|endoftext|>","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:Dao-AILab:flash-attention","source_url":"https://github.com/Dao-AILab/flash-attention"},{"type":"based_on_paper","target_id":"arxiv:2212.04356","source_url":"https://arxiv.org/abs/2212.04356"}]', NULL, 'Apache-2.0', 'approved', 80, '822fec674f92053ae4192fe00b4f73fa', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-3.1-8B-Instruct', 'huggingface--meta-llama--llama-3.1-8b-instruct', 'Llama-3.1-8B-Instruct', 'meta-llama', '', '["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","conversational","en","de","fr","it","pt","hi","es","th","arxiv:2204.05149","base_model:meta-llama/llama-3.1-8b","base_model:finetune:meta-llama/llama-3.1-8b","license:llama3.1","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 5093, 5187643, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":8030261248,"storage_bytes":32123357950,"files_count":17,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","chat_template":"{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][''role''] == ''system'' %}\n    {%- set system_message = messages[0][''content'']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject(''equalto'', ''code_interpreter'') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][''content'']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there''s no first user message!\") }}\n{%- endif %}\n    {{- ''<|start_header_id|>user<|end_header_id|>\\n\\n'' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == ''ipython'' or message.role == ''tool'' or ''tool_calls'' in message) %}\n        {{- ''<|start_header_id|>'' + message[''role''] + ''<|end_header_id|>\\n\\n''+ message[''content''] | trim + ''<|eot_id|>'' }}\n    {%- elif ''tool_calls'' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + ''=\"'' + arg_val + ''\"'' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' -}}\n            {{- ''{\"name\": \"'' + tool_call.name + ''\", '' }}\n            {{- ''\"parameters\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we''re in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' }}\n{%- endif %}\n","eos_token":"<|eot_id|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2204.05149","source_url":"https://arxiv.org/abs/2204.05149"}]', NULL, 'llama3.1', 'approved', 40, 'f38e7348f15595b2cb82b4aa3e0f5af9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-bigscience-bloom', 'huggingface--bigscience--bloom', 'bloom', 'bigscience', '--- license: bigscience-bloom-rail-1.0 language: - ak - ar - as - bm - bn - ca - code - en - es - eu - fon - fr - gu - hi - id - ig - ki - kn - lg - ln - ml - mr - ne - nso - ny - or - pa - pt - rn - rw - sn - st - sw - ta - te - tn - ts - tum - tw - ur - vi - wo - xh - yo - zh - zu programming_language: - C - C++ - C# - Go - Java - JavaScript - Lua - PHP - Python - Ruby - Rust - Scala - TypeScript pipeline_tag: text-generation widget: - text: ''A "whatpu" is a small, furry animal native to Ta...', '["transformers","pytorch","tensorboard","safetensors","bloom","text-generation","ak","ar","as","bm","bn","ca","code","en","es","eu","fon","fr","gu","hi","id","ig","ki","kn","lg","ln","ml","mr","ne","nso","ny","or","pa","pt","rn","rw","sn","st","sw","ta","te","tn","ts","tum","tw","ur","vi","wo","xh","yo","zh","zu","arxiv:2211.05100","arxiv:1909.08053","arxiv:2110.02861","arxiv:2108.12409","doi:10.57967/hf/0003","license:bigscience-bloom-rail-1.0","model-index","co2_eq_emissions","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 4961, 2685, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/bigscience/bloom","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: bigscience-bloom-rail-1.0\nlanguage:\n- ak\n- ar\n- as\n- bm\n- bn\n- ca\n- code\n- en\n- es\n- eu\n- fon\n- fr\n- gu\n- hi\n- id\n- ig\n- ki\n- kn\n- lg\n- ln\n- ml\n- mr\n- ne\n- nso\n- ny\n- or\n- pa\n- pt\n- rn\n- rw\n- sn\n- st\n- sw\n- ta\n- te\n- tn\n- ts\n- tum\n- tw\n- ur\n- vi\n- wo\n- xh\n- yo\n- zh\n- zu\nprogramming_language: \n- C\n- C++\n- C#\n- Go\n- Java\n- JavaScript\n- Lua\n- PHP\n- Python\n- Ruby\n- Rust\n- Scala\n- TypeScript\npipeline_tag: text-generation\nwidget:\n- text: ''A "whatpu" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is: We were traveling in Africa and we saw these very cute whatpus. | To do a "farduddle" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:''\n  example_title: Imaginary word\n  group: English\n- text: ''Un "whatpu" est un petit animal √† fourrure originaire de Tanzanie. Un exemple de phrase qui utilise le mot whatpu est: Nous √©tions en Afrique et nous avons vu des whatpus trop mignons. Faire un "farduddle" veut dire sauter sur place vraiment vite. Un exemple de phrase qui utilise le mot farduddle est:''\n  example_title: Imaginary word\n  group: French\n- text: ''Un "whatpu" es un peque√±o animal peludo nativo de Tanzania. Un ejemplo de una oraci√≥n que usa la palabra whatpu es: Est√°bamos viajando por √Åfrica y vimos estos whatpus muy bonitos. Hacer un "farduddle" significa saltar arriba y abajo muy r√°pido. Un ejemplo de una oraci√≥n que usa la palabra farduddle es:''\n  example_title: Imaginary word\n  group: Spanish\n- text: '' ÿßŸÑ"Ÿàÿßÿ™ÿ®Ÿà" ŸáŸà ÿ≠ŸäŸàÿßŸÜ ÿµÿ∫Ÿäÿ± ŸÖŸÉÿ≥Ÿà ÿ®ÿßŸÑŸÅÿ±ÿßÿ° ŸäÿπŸäÿ¥ ŸÅŸä ÿ™ŸÜÿ≤ÿßŸÜŸäÿß. ŸÖÿ´ÿßŸÑ ÿπŸÑŸâ ÿ¨ŸÖŸÑÿ© ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ŸÉŸÑŸÖÿ© Ÿàÿßÿ™ÿ®Ÿà ŸáŸä: ŸÉŸÜÿß ŸÜÿ≥ÿßŸÅÿ± ŸÅŸä ÿßŸÅÿ±ŸäŸÇŸäÿß Ÿà ÿ±ÿ£ŸäŸÜÿß Ÿáÿ§ŸÑÿßÿ° ÿßŸÑŸàÿßÿ™ÿ®Ÿà ÿßŸÑŸÑÿ∑ŸÅÿßÿ°. ŸÑŸÑŸÇŸäÿßŸÖ ÿ®"ŸÅÿßÿ±ÿØÿßÿØŸÑ" ŸäÿπŸÜŸä ÿßŸÜ ÿ™ŸÇŸÅÿ≤ ŸÑŸÑÿ£ÿπŸÑŸâ Ÿà ÿßŸÑÿ£ÿ≥ŸÅŸÑ ÿ®ÿ≥ÿ±ÿπÿ© ŸÉÿ®Ÿäÿ±ÿ©. ŸÖÿ´ÿßŸÑ ÿπŸÑŸâ ÿ¨ŸÖŸÑÿ© ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ŸÉŸÑŸÖÿ© ŸÅÿßÿ±ÿØÿßÿØŸÑ ŸáŸä:''\n  example_title: Imaginary word\n  group: Arabic\n- text: ''Um "whatpu" √© um pequeno animal peludo nativo da Tanz√¢nia. Um exemplo de uma frase que usa a palavra whatpu √©: Est√°vamos a viajar por √Åfrica e vimos uns whatpus muito queridos. Fazer um "farduddle" significa saltar para cima e para baixo muito r√°pido. Um exemplo de uma frase que usa a palavra farduddle √©:''\n  example : Imaginary word\n  group: Portuguese\n- text: Pour d√©guster un ortolan, il faut tout d''abord\n  example_title: Recipe\n  group: French\n- text: |-\n    34+10=44 \n    54+20=\n  example_title: Addition\n  group: Math\n- text: |-\n    This tool converts irregular verbs to past tense.\n    Arise - Arose\n    Become - Became\n    Forget - Forgot\n    Freeze -\n  example_title: Irregular verbs\n  group: English\n- text: |-\n    Please unscramble the letters into a word, and write that word:\n    r e!c.i p r o.c a/l = reciprocal\n    d.o m i!n a n.t =\n  example_title: Word unscrambling\n  group: English\n- text: |-\n    Estos ejemplos quitan vocales de las palabras\n    Ejemplos:\n    hola - hl\n    manzana - mnzn\n    papas - pps\n    alacran - lcrn\n    papa -\n  example_title: Vowel removal\n  group: Spanish\n- text: |-\n    Traduce espa√±ol de Espa√±a a espa√±ol de Argentina\n    El coche es rojo - el auto es rojo\n    El ordenador es nuevo - la computadora es nueva\n    el boligrafo es negro - lapicera es negra\n    la nevera\n  example_title: Spanish to Argentinian Spanish\n  group: Spanish\n- text: To say "I love you" in Hindi, you would say\n  example_title: Translation to Hindi\n  group: English\n- text: To say "I love you" in Hindi, you would say\n  example_title: Translation from English\n  group: Hindi\n- text: ''Poor English: She no went to the market. Corrected English:''\n  example_title: Grammar exercise 1 \n  group: English\n- text: ''ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßŸÑÿπÿØÿØ ÿßŸÑÿπÿßŸÖŸÑŸä ŸÅŸä ŸÑÿ∫ÿ© ÿ®ÿßŸäÿ´ŸàŸÜ:''\n  example_title: Code generation\n  group: Arabic\n- text: ''Regexp. Here is a regular expression to match a word starting with a number and then having only vowels:''\n  example_title: Regular expressions\n  group: English\n- text: |-\n    Do a hello world in different languages:\n    Python: print("hello world")\n    R:\n  example_title: Code generation\n  group: English\n- text: |-\n    Which is the correct preposition? I''m born X July. X is the preposition in\n    He sat X a chair. X is the preposition on\n    She drove X the bridge. X is the preposition\n  example_title: Grammar exercise 2\n  group: English\n- text: |-\n    Traduction en fran√ßais: Dans cet essai je vais m''interroger sur la conscience des mod√®les d''intelligence artificielle r√©cents comme les mod√®les de langue. Pour commencer, je m''int√©resserai √† la notion de conscience et √† ce qui la caract√©rise. Ensuite, j''aborderai la question de l''intelligence et de son lien avec le langage. Enfin, dans une derni√®re partie je me pencherai sur le cas de l''IA et sur sa conscience.\n    Traduction en espagnol:\n  example_title: Translation to Spanish\n  group: French\n- text: |-\n    Traducci√≥n al franc√©s: Dans cet essai je vais m''interroger sur la conscience des mod√®les d''intelligence artificielle r√©cents comme les mod√®les de langue. Pour commencer, je m''int√©resserai √† la notion de conscience et √† ce qui la caract√©rise. Ensuite, j''aborderai la question de l''intelligence et de son lien avec le langage. Enfin, dans une derni√®re partie je me pencherai sur le cas de l''IA et sur sa conscience.\n    Traducci√≥n al espa√±ol:\n  example_title: Translation from French\n  group: Spanish\n- text: ÿ∞ÿßÿ™ ŸÖÿ±ÿ© ÿå ÿπÿßÿ¥ ÿ¥ÿ®ŸÑ ÿßŸÑÿØÿ® ŸÅŸä ÿßŸÑÿ∫ÿßÿ®ÿ©\n  example_title: Fairy tale\n  group: Arabic\n- text: ‡§è‡§ï ‡§¨‡§æ‡§∞ ‡§ï‡•Ä ‡§¨‡§æ‡§§ ‡§π‡•à, ‡§ú‡§Ç‡§ó‡§≤ ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§≠‡§æ‡§≤‡•Ç ‡§ï‡§æ ‡§∂‡§æ‡§µ‡§ï ‡§∞‡§π‡§§‡§æ ‡§•‡§æ\n  example_title: Fairy tale\n  group: Hindi\n- text: Il √©tait une fois une licorne qui vivait\n  example_title: Fairy tale\n  group: French\n- text: |-\n    Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?\n    A: Let''s think step by step.\n  example_title: Mathematical reasoning\n  group: English\n\nco2_eq_emissions:\n  emissions: 24_700_000\n  source: "Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model. https://arxiv.org/abs/2211.02001"\n  training_type: "pre-training"\n  geographical_location: "Orsay, France"\n  hardware_used: "384 A100 80GB GPUs"\n\nmodel-index:\n- name: bloom\n  results:\n  - task:\n      type: text-generation\n    dataset:\n      type: openai_humaneval\n      name: humaneval\n    metrics:\n    - name: pass@1\n      type: pass@1\n      value: 0.15542682926829265\n      verified: false\n    - name: pass@10\n      type: pass@10\n      value: 0.3278356276947017\n      verified: false\n    - name: pass@100\n      type: pass@100\n      value: 0.5719815685597749\n      verified: false\n---\n\n<img src="https://cdn-uploads.huggingface.co/production/uploads/1657124309515-5f17f0a0925b9863e28ad517.png" alt="BigScience Logo" width="800" style="margin-left:''auto'' margin-right:''auto'' display:''block''"/>\n\nBigScience Large Open-science Open-access Multilingual Language Model  \nVersion 1.3 / 6 July 2022\n\nCurrent Checkpoint: **Training Iteration  95000**\n\nLink to paper: [here](https://arxiv.org/abs/2211.05100)\n\nTotal seen tokens: **366B**\n\n---\n\n# Model Details  \n\nBLOOM is an autoregressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn''t been explicitly trained for, by casting them as text generation tasks.\n\n## Basics\n*This section provides information about the model type, version, license, funders, release date, developers, and contact information.*\n*It is useful for anyone who wants to reference the model.*\n\n<details>\n<summary>Click to expand</summary>\n  \n**Developed by:** BigScience ([website](https://bigscience.huggingface.co))\n\n*All collaborators are either volunteers or have an agreement with their employer. (Further breakdown of participants forthcoming.)*\n    \n**Model Type:** Transformer-based Language Model\n\n**Checkpoints format:** `transformers` (Megatron-DeepSpeed format available [here](https://huggingface.co/bigscience/bloom-optimizer-states))\n\n**Version:** 1.0.0\n\n**Languages:** Multiple; see [training data](#training-data)\n\n**License:** RAIL License v1.0 ([link](https://huggingface.co/spaces/bigscience/license) / [article and FAQ](https://bigscience.huggingface.co/blog/the-bigscience-rail-license))\n\n**Release Date Estimate:** Monday, 11.July.2022\n\n**Send Questions to:** bigscience-contact@googlegroups.com\n\n**Cite as:** BigScience, _BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model_. International, May 2021-May 2022\n\n**Funded by:** \n    \n* The French government.\n\n* Hugging Face ([website](https://huggingface.co)).\n\n* Organizations of contributors.  *(Further breakdown of organizations forthcoming.)*\n\n</details>\n\n\n## Technical Specifications\n*This section includes details about the model objective and architecture, and the compute infrastructure.*\n*It is useful for people interested in model development.*\n\n<details>\n<summary>Click to expand</summary>\n\nPlease see [the BLOOM training README](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) for full details on replicating training.\n\n### Model Architecture and Objective\n\n* Modified from Megatron-LM GPT2 (see [paper](https://arxiv.org/abs/1909.08053), [BLOOM Megatron code](https://github.com/bigscience-workshop/Megatron-DeepSpeed)):\n\n* Decoder-only architecture\n\n* Layer normalization applied to word embeddings layer (`StableEmbedding`; see [code](https://github.com/facebookresearch/bitsandbytes), [paper](https://arxiv.org/pdf/2110.02861.pdf))\n\n* ALiBI positional encodings (see [paper](https://arxiv.org/pdf/2108.12409.pdf)), with GeLU activation functions\n\n* 176,247,271,424 parameters:\n\n    * 3,596,615,680 embedding parameters\n\n    * 70 layers, 112 attention heads\n\n    * Hidden layers are 14336-dimensional\n\n    * Sequence length of 2048 tokens used (see [BLOOM tokenizer](https://huggingface.co/bigscience/tokenizer), [tokenizer description](#tokenization))\n\n**Objective Function:** Cross Entropy with mean reduction (see [API documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)).\n    \n### Compute infrastructure\nJean Zay Public Supercomputer, provided by the French government (see [announcement](https://www.enseignementsup-recherche.gouv.fr/fr/signature-du-marche-d-acquisition-de-l-un-des-supercalculateurs-les-plus-puissants-d-europe-46733)).\n\n#### Hardware\n\n* 384 A100 80GB GPUs (48 nodes)\n    \n* Additional 32 A100 80GB GPUs (4 nodes) in reserve\n\n* 8 GPUs per node Using NVLink 4 inter-gpu connects, 4 OmniPath links\n\n* CPU: AMD\n\n* CPU memory: 512GB per node\n\n* GPU memory: 640GB per node\n\n* Inter-node connect: Omni-Path Architecture (OPA)\n\n* NCCL-communications network: a fully dedicated subnet\n\n* Disc IO network: shared network with other types of nodes\n\n#### Software\n\n* Megatron-DeepSpeed ([Github link](https://github.com/bigscience-workshop/Megatron-DeepSpeed))\n\n* DeepSpeed ([Github link](https://github.com/microsoft/DeepSpeed))\n\n* PyTorch (pytorch-1.11 w/ CUDA-11.5; see [Github link](https://github.com/pytorch/pytorch))\n\n* apex ([Github link](https://github.com/NVIDIA/apex))\n    \n</details>\n\n---\n\n# Training\n*This section provides information about the training data, the speed and size of training elements, and the environmental impact of training.*\n*It is useful for people who want to learn more about the model inputs and training footprint.*\n\n<details>\n<summary>Click to expand</summary>\n\n## Training Data\n*This section provides a high-level overview of the training data. It is relevant for anyone who wants to know the basics of what the model is learning.*\n\nDetails for each dataset are provided in individual [Data Cards](https://huggingface.co/spaces/bigscience/BigScienceCorpus), and the sizes of each of their contributions to the aggregated training data are presented in an [Interactive Corpus Map](https://huggingface.co/spaces/bigscience-catalogue-lm-data/corpus-map).\n\nTraining data includes:\n\n-   46 natural languages\n    \n-   13 programming languages\n\n-   In 1.6TB of pre-processed text, converted into 350B unique tokens (see [the tokenizer section](#tokenization) for more.)\n\n### Languages\n    \nThe pie chart shows the distribution of languages in training data.\n   \n![pie chart showing the distribution of languages in training data](https://github.com/bigscience-workshop/model_card/blob/main/assets/data/pie_v2.svg?raw=true)\n\n\nThe following tables shows the further distribution of Niger-Congo & Indic languages and programming languages in the training data.\n\nDistribution of Niger Congo and Indic languages.\n    \n| Niger Congo    | Percentage |         | Indic     | Percentage |\n|----------------|------------| ------  |-----------|------------|\n| Chi Tumbuka    | 0.00002    |         | Assamese  | 0.01       |\n| Kikuyu         | 0.00004    |         | Odia      | 0.04       |\n| Bambara        | 0.00004    |         | Gujarati  | 0.04       |\n| Akan           | 0.00007    |         | Marathi   | 0.05       |\n| Xitsonga       | 0.00007    |         | Punjabi   | 0.05       |\n| Sesotho        | 0.00007    |         | Kannada   | 0.06       |\n| Chi Chewa      | 0.0001     |         | Nepali    | 0.07       |\n| Setswana       | 0.0002     |         | Telugu    | 0.09       |\n| Lingala        | 0.0002     |         | Malayalam | 0.10       |\n| Northern Sotho | 0.0002     |         | Urdu      | 0.10       |\n| Fon            | 0.0002     |         | Tamil     | 0.20       |\n| Kirundi        | 0.0003     |         | Bengali   | 0.50       |\n| Wolof          | 0.0004     |         | Hindi     | 0.70       |\n| Luganda        | 0.0004     |\n| Chi Shona      | 0.001      |\n| Isi Zulu       | 0.001      |\n| Igbo           | 0.001      |\n| Xhosa          | 0.001      |\n| Kinyarwanda    | 0.003      |\n| Yoruba         | 0.006      |\n| Swahili        | 0.02       |\n\nDistribution of programming languages.\n    \n| Extension      | Language   | Number of files |\n|----------------|------------|-----------------|\n| java           | Java       | 5,407,724       |\n| php            | PHP        | 4,942,186       |\n| cpp            | C++        | 2,503,930       |\n| py             | Python     | 2,435,072       |\n| js             | JavaScript | 1,905,518       |\n| cs             | C#         | 1,577,347       |\n| rb             | Ruby       | 6,78,413        |\n| cc             | C++        | 443,054         |\n| hpp            | C++        | 391,048         |\n| lua            | Lua        | 352,317         |\n| go             | GO         | 227,763         |\n| ts             | TypeScript | 195,254         |\n| C              | C          | 134,537         |\n| scala          | Scala      | 92,052          |\n| hh             | C++        | 67,161          |\n| H              | C++        | 55,899          |\n| tsx            | TypeScript | 33,107          |\n| rs             | Rust       | 29,693          |\n| phpt           | PHP        | 9,702           |\n| c++            | C++        | 1,342           |\n| h++            | C++        | 791             |\n| php3           | PHP        | 540             |\n| phps           | PHP        | 270             |\n| php5           | PHP        | 166             |\n| php4           | PHP        | 29              |\n    \n### Preprocessing\n\n**Tokenization:** The BLOOM tokenizer ([link](https://huggingface.co/bigscience/tokenizer)), a learned subword tokenizer trained using:\n    \n- A byte-level Byte Pair Encoding (BPE) algorithm \n\n- A simple pre-tokenization rule, no normalization\n\n- A vocabulary size of 250,680\n\nIt was trained on a subset of a preliminary version of the corpus using alpha-weighting per language.  \n\n## Speeds, Sizes, Times\n\nTraining logs: [Tensorboard link](https://huggingface.co/tensorboard/bigscience/tr11-176B-ml-logs/)\n\n- Dates:\n    \n    - Started 11th March, 2022 11:42am PST\n\n    - Estimated end: 5th July, 2022\n\n- Checkpoint size:\n    \n    - Bf16 weights: 329GB\n    \n    - Full checkpoint with optimizer states: 2.3TB\n\n- Training throughput: About 150 TFLOP per GPU per second\n\n- Number of epochs: 1\n\n- Estimated cost of training: Equivalent of $2-5M in cloud computing (including preliminary experiments)\n\n- Server training location: √éle-de-France, France\n\n\n## Environmental Impact\n\nThe training supercomputer, Jean Zay ([website](http://www.idris.fr/eng/jean-zay/jean-zay-presentation-eng.html)), uses mostly nuclear energy. The heat generated by it is reused for heating campus housing.\n    \n**Estimated carbon emissions:**  *(Forthcoming.)*\n    \n**Estimated electricity usage:** *(Forthcoming.)*\n\n</details>\n\n---\n\n# Uses\n\n*This section addresses questions around how the model is intended to be used, discusses the foreseeable users of the model (including those affected by the model), and describes uses that are considered out of scope or misuse of the model.*\n*It is useful for anyone considering using the model or who is affected by the model.*\n\n<details>\n<summary>Click to expand</summary>\n    \n## How to use\n\nThis model can be easily used and deployed using HuggingFace''s ecosystem. This needs `transformers` and `accelerate` installed. The model can be downloaded as follows:\n\n <img src="https://s3.amazonaws.com/moonup/production/uploads/1657271608456-62441d1d9fdefb55a0b7d12c.png" width="800" style="margin-left:''auto'' margin-right:''auto'' display:''block''"/>\n\n## Intended Use\n\nThis model is being created in order to enable public research on large language models (LLMs). LLMs are intended to be used for language generation or as a pretrained base model that can be further fine-tuned for specific tasks. Use cases below are not exhaustive.\n\n### Direct Use\n\n-   Text generation\n\n-   Exploring characteristics of language generated by a language model\n\n    -   Examples: Cloze tests, counterfactuals, generations with reframings\n\n### Downstream Use\n\n-   Tasks that leverage language models include: Information Extraction, Question Answering, Summarization\n\n### Misuse and Out-of-scope Use\n*This section addresses what users ought not do with the model.*\n\nSee the [BLOOM License](https://huggingface.co/spaces/bigscience/license), Attachment A, for detailed usage restrictions. The below list is non-exhaustive, but lists some easily foreseeable problematic use cases.\n\n#### Out-of-scope Uses\n\nUsing the model in [high-stakes](#high-stakes) settings is out of scope for this model.  The model is not designed for [critical decisions](#critical-decisions) nor uses with any material consequences on an individual''s livelihood or wellbeing. The model outputs content that appears factual but may not be correct.  \n\nOut-of-scope Uses Include:\n\n-   Usage in biomedical domains, political and legal domains, or finance domains\n\n-   Usage for evaluating or scoring individuals, such as for employment, education, or credit\n\n-   Applying the model for critical automatic decisions, generating factual content, creating reliable summaries, or generating predictions that must be correct\n\n#### Misuse\n\nIntentionally using the model for harm, violating [human rights](#human-rights), or other kinds of malicious activities, is a misuse of this model. This includes:\n\n-   Spam generation\n\n-   Disinformation and influence operations\n\n-   Disparagement and defamation\n\n-   Harassment and abuse\n  \n-   [Deception](#deception)\n\n-   Unconsented impersonation and imitation\n\n-   Unconsented surveillance \n\n-   Generating content without attribution to the model, as specified in the [RAIL License, Use Restrictions](https://huggingface.co/spaces/bigscience/license)\n\n## Intended Users\n\n### Direct Users\n\n-   General Public\n\n-   Researchers\n\n-   Students\n\n-   Educators\n\n-   Engineers/developers\n\n-   Non-commercial entities\n\n-   Community advocates, including human and civil rights groups\n\n### Indirect Users\n\n-   Users of derivatives created by Direct Users, such as those using software with an [intended use](#intended-use)\n\n-   Users of [Derivatives of the Model, as described in the License](https://huggingface.co/spaces/bigscience/license)\n\n### Others Affected (Parties Prenantes)\n\n-   People and groups referred to by the LLM\n\n-   People and groups exposed to outputs of, or decisions based on, the LLM\n\n-   People and groups whose original work is included in the LLM\n\n</details>\n\n---\n\n# Risks and Limitations\n*This section identifies foreseeable harms and misunderstandings.*\n    \n<details>\n<summary>Click to expand</summary>\n\nModel may:\n\n-   Overrepresent some viewpoints and underrepresent others\n\n-   Contain stereotypes\n  \n-   Contain [personal information](#personal-data-and-information)\n\n-   Generate:\n\n    -   Hateful, abusive, or violent language\n\n    -   Discriminatory or prejudicial language\n\n    -   Content that may not be appropriate for all settings, including sexual content\n\n-   Make errors, including producing incorrect information as if it were factual\n\n-   Generate irrelevant or repetitive outputs\n\n-   Induce users into attributing human traits to it, such as sentience or consciousness\n\n</details>\n\n---\n\n# Evaluation\n*This section describes the evaluation protocols and provides the results.*\n\n\n<details>\n<summary>Click to expand</summary>\n\n## Metrics \n*This section describes the different ways performance is calculated and why.*\n\nIncludes:\n\n| Metric             | Why chosen                                                         |\n|--------------------|--------------------------------------------------------------------|\n| [Perplexity](#perplexity)         | Standard metric for quantifying model improvements during training |\n| Cross Entropy [Loss](#loss) | Standard objective for language models.                            |\n\nAnd multiple different metrics for specific tasks. _(More evaluation metrics forthcoming upon completion of evaluation protocol.)_\n\n## Factors \n*This section lists some different aspects of BLOOM models. Its focus is on aspects that are likely to give rise to high variance in model behavior.*\n\n- Language, such as English or Yoruba\n\n- Domain, such as newswire or stories\n\n- Demographic characteristics, such as gender or nationality\n\n##  Results\n*Results are based on the [Factors](#factors) and [Metrics](#metrics).*\n\n**Zero-shot evaluations:**\n\n<span style="color:red"><b>WARNING:</b> This section used to contain much more results, however they were not correct and we released without the approval of the evaluation working group. We are currently in the process of fixing the evaluations.</span>\n\nSee this repository for JSON files: https://github.com/bigscience-workshop/evaluation-results\n\n| Task | Language | Metric | BLOOM-176B | OPT-175B* |\n|:--------|:-----------------|:------------------------|-------------:|------------:|\n| humaneval | python | pass@1 ‚Üë | 0.155 | 0.0 |\n| humaneval | python | pass@10 ‚Üë | 0.328 | 0.0 |\n| humaneval | python | pass@100 ‚Üë | 0.572 | 0.003 |\n\n\n**Train-time Evaluation:**\n\nFinal checkpoint after 95K steps:\n\n- Training Loss: 1.939\n\n- Validation Loss: 2.061\n\n- Perplexity: 7.045\n\nFor more see: https://huggingface.co/bigscience/tr11-176B-ml-logs\n\n</details>\n\n---\n\n# Recommendations\n\n*This section provides information on warnings and potential mitigations.*\n\n<details>\n<summary>Click to expand</summary>\n\n-   Indirect users should be made aware when the content they''re working with is created by the LLM.\n\n-   Users should be aware of [Risks and Limitations](#risks-and-limitations), and include an appropriate age disclaimer or blocking interface as necessary.\n\n-   Models trained or finetuned downstream of BLOOM LM should include an updated Model Card.\n\n-   Users of the model should provide mechanisms for those affected to provide feedback, such as an email address for comments.\n\n</details>\n\n---\n\n# Glossary and Calculations\n\n*This section defines common terms and how metrics are calculated.*\n<details>\n<summary>Click to expand</summary>\n\n-   <a name="loss">**Loss:**</a> A calculation of the difference between what the model has learned and what the data shows ("groundtruth"). The lower the loss, the better. The training process aims to minimize the loss. \n\n-   <a name="perplexity">**Perplexity:**</a> This is based on what the model estimates the probability of new data is. The lower the perplexity, the better.  If the model is 100% correct at predicting the next token it will see, then the perplexity is 1. Mathematically this is calculated using entropy. \n\n-   <a name="high-stakes">**High-stakes settings:**</a> Such as those identified as "high-risk AI systems" and "unacceptable risk AI systems" in the European Union''s proposed [Artificial Intelligence (AI) Act](https://artificialintelligenceact.eu/annexes/).\n\n-   <a name="critical-decisions">**Critical decisions:**</a> Such as those defined in [the United States'' proposed Algorithmic Accountability Act](https://www.congress.gov/117/bills/s3572/BILLS-117s3572is.pdf).\n\n-   <a name="human-rights">**Human rights:**</a> Includes those rights defined in the [Universal Declaration of Human Rights](https://www.un.org/sites/un2.un.org/files/2021/03/udhr.pdf).\n\n-  <a name="personal-data-and-information">**Personal Data and Personal Information:**</a> Personal data and information is defined in multiple data protection regulations, such as "[personal data](https://gdpr-info.eu/issues/personal-data/)" in the [European Union''s General Data Protection Regulation](https://gdpr-info.eu); and "personal information" in the Republic of South Africa''s [Protection of Personal Information Act](https://www.gov.za/sites/default/files/gcis_document/201409/3706726-11act4of2013popi.pdf), The People''s Republic of China''s [Personal information protection law](http://en.npc.gov.cn.cdurl.cn/2021-12/29/c_694559.htm).\n  \n- <a name="sensitive-characteristics">**Sensitive characteristics:**</a> This includes specifically protected categories in human rights (see [UHDR, Article 2](https://www.un.org/sites/un2.un.org/files/2021/03/udhr.pdf)) and personal information regulation (see GDPR, [Article 9; Protection of Personal Information Act, Chapter 1](https://www.gov.za/sites/default/files/gcis_document/201409/3706726-11act4of2013popi.pdf))\n\n- <a name="deception">**Deception:**</a> Doing something to intentionally mislead individuals to believe something that is false, such as by creating deadbots or chatbots on social media posing as real people, or generating text documents without making consumers aware that the text is machine generated.\n\n</details>\n\n---\n\n# More Information\n*This section provides links to writing on dataset creation, technical specifications, lessons learned, and initial results.*\n\n<details>\n<summary>Click to expand</summary>\n\n## Intermediate checkpoints\n\nFor academic (or any) usage, we published the intermediate checkpoints, corresponding to the model state at each 5000 steps. Please follow [this link](https://huggingface.co/bigscience/bloom-176-intermediate) to get these checkpoints.\n\n    \n## Dataset Creation\n\nBlog post detailing the design choices during the dataset creation: https://bigscience.huggingface.co/blog/building-a-tb-scale-multilingual-dataset-for-language-modeling\n\n## Technical Specifications\n\nBlog post summarizing how the architecture, size, shape, and pre-training duration where selected: https://bigscience.huggingface.co/blog/what-language-model-to-train-if-you-have-two-million-gpu-hours\n\nMore details on the architecture/optimizer: https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml\n\nBlog post on the hardware/engineering side: https://bigscience.huggingface.co/blog/which-hardware-to-train-a-176b-parameters-model\n\nDetails on the distributed setup used for the training: https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml\n\nTensorboard updated during the training: https://huggingface.co/bigscience/tr11-176B-ml-logs/tensorboard#scalars&tagFilter=loss\n\n## Lessons\n\nInsights on how to approach training, negative results: https://github.com/bigscience-workshop/bigscience/blob/master/train/lessons-learned.md\n\nDetails on the obstacles overcome during the preparation on the engineering side (instabilities, optimization of training throughput, so many technical tricks and questions): https://github.com/bigscience-workshop/bigscience/blob/master/train/tr11-176B-ml/chronicles.md\n\n## Initial Results\n\nInitial prompting experiments using interim checkpoints: https://huggingface.co/spaces/bigscience/bloom-book\n\n</details>\n\n\n## Original checkpoints\n\nThe checkpoints in this repo correspond to the HuggingFace Transformers format. If you want to use our fork of [Megatron-DeepSpeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed) that the model was trained with, you''d want to use [this repo instead](https://huggingface.co/bigscience/bloom-optimizer-states).\n\nMany intermediate checkpoints are available at https://huggingface.co/bigscience/bloom-intermediate/\n\n---\n    \n# Model Card Authors\n*Ordered roughly chronologically and by amount of time spent on creating this model card.*\n\nMargaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Mu√±oz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Iliƒá, G√©rard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":176247271424,"storage_bytes":2820253076240,"files_count":269,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BloomForCausalLM"],"model_type":"bloom","tokenizer_config":{"unk_token":"<unk>","eos_token":"</s>","bos_token":"<s>","pad_token":"<pad>"}}}', '[]', '[{"type":"has_code","target_id":"github:bigscience-workshop:bigscience","source_url":"https://github.com/bigscience-workshop/bigscience"},{"type":"has_code","target_id":"github:bigscience-workshop:Megatron-DeepSpeed","source_url":"https://github.com/bigscience-workshop/Megatron-DeepSpeed"},{"type":"has_code","target_id":"github:facebookresearch:bitsandbytes","source_url":"https://github.com/facebookresearch/bitsandbytes"},{"type":"has_code","target_id":"github:bigscience-workshop:Megatron-DeepSpeed","source_url":"https://github.com/bigscience-workshop/Megatron-DeepSpeed"},{"type":"has_code","target_id":"github:microsoft:DeepSpeed","source_url":"https://github.com/microsoft/DeepSpeed"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:NVIDIA:apex","source_url":"https://github.com/NVIDIA/apex"},{"type":"has_code","target_id":"github:bigscience-workshop:model_card","source_url":"https://github.com/bigscience-workshop/model_card"},{"type":"has_code","target_id":"github:bigscience-workshop:evaluation-results","source_url":"https://github.com/bigscience-workshop/evaluation-results"},{"type":"has_code","target_id":"github:bigscience-workshop:bigscience","source_url":"https://github.com/bigscience-workshop/bigscience"},{"type":"has_code","target_id":"github:bigscience-workshop:bigscience","source_url":"https://github.com/bigscience-workshop/bigscience"},{"type":"has_code","target_id":"github:bigscience-workshop:bigscience","source_url":"https://github.com/bigscience-workshop/bigscience"},{"type":"has_code","target_id":"github:bigscience-workshop:bigscience","source_url":"https://github.com/bigscience-workshop/bigscience"},{"type":"has_code","target_id":"github:bigscience-workshop:Megatron-DeepSpeed","source_url":"https://github.com/bigscience-workshop/Megatron-DeepSpeed"},{"type":"based_on_paper","target_id":"arxiv:2211.05100","source_url":"https://arxiv.org/abs/2211.05100"},{"type":"based_on_paper","target_id":"arxiv:1909.08053","source_url":"https://arxiv.org/abs/1909.08053"},{"type":"based_on_paper","target_id":"arxiv:2110.02861","source_url":"https://arxiv.org/abs/2110.02861"},{"type":"based_on_paper","target_id":"arxiv:2108.12409","source_url":"https://arxiv.org/abs/2108.12409"}]', NULL, 'bigscience-bloom-rail-1.0', 'approved', 80, '56c365a044e85536fe24c3ece5c8610e', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-stable-diffusion-3-medium', 'huggingface--stabilityai--stable-diffusion-3-medium', 'stable-diffusion-3-medium', 'stabilityai', '', '["diffusion-single-file","text-to-image","stable-diffusion","en","arxiv:2403.03206","license:other","region:us"]', 'text-to-image', 4876, 10452, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/stable-diffusion-3-medium","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-to-image","library_name":"diffusion-single-file","framework":"diffusion-single-file","params":null,"storage_bytes":67448764458,"files_count":28,"spaces_count":100,"gated":"auto","private":false,"config":null}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2403.03206","source_url":"https://arxiv.org/abs/2403.03206"}]', NULL, 'Other', 'approved', 60, 'ce606d061e21bd14161f349fb6b4d6ae', NULL, 'https://huggingface.co/stabilityai/stable-diffusion-3-medium/resolve/main/demo_images/demo (1).png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-stabilityai-stable-diffusion-3-medium from https://huggingface.co/stabilityai/stable-diffusion-3-medium/resolve/main/demo_images/demo (1).png
HTTP error: 401 Unauthorized

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-2-7b-chat-hf', 'huggingface--meta-llama--llama-2-7b-chat-hf', 'Llama-2-7b-chat-hf', 'meta-llama', '', '["transformers","pytorch","safetensors","llama","text-generation","facebook","meta","llama-2","conversational","en","arxiv:2307.09288","license:llama2","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 4665, 324804, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-2-7b-chat-hf","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":6738417664,"storage_bytes":53908103645,"files_count":16,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<s>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false},"chat_template":"{% if messages[0][''role''] == ''system'' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][''content''] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}{{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = ''<<SYS>>\\n'' + system_message + ''\\n<</SYS>>\\n\\n'' + message[''content''] %}{% else %}{% set content = message[''content''] %}{% endif %}{% if message[''role''] == ''user'' %}{{ bos_token + ''[INST] '' + content.strip() + '' [/INST]'' }}{% elif message[''role''] == ''assistant'' %}{{ '' ''  + content.strip() + '' '' + eos_token }}{% endif %}{% endfor %}","eos_token":{"__type":"AddedToken","content":"</s>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false},"pad_token":null,"unk_token":{"__type":"AddedToken","content":"<unk>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false}}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2307.09288","source_url":"https://arxiv.org/abs/2307.09288"}]', NULL, 'LLaMA-2', 'approved', 40, '519af6138d205d2f1cb02a72057daa28', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mistralai-Mixtral-8x7B-Instruct-v0.1', 'huggingface--mistralai--mixtral-8x7b-instruct-v0.1', 'Mixtral-8x7B-Instruct-v0.1', 'mistralai', '--- library_name: vllm language: - fr - it - de - es - en license: apache-2.0 base_model: mistralai/Mixtral-8x7B-v0.1 inference: false widget: - messages: - role: user content: What is your favorite condiment? extra_gated_description: >- If you want to learn more about how we process your personal data, please read our <a href="https://mistral.ai/terms/">Privacy Policy</a>. tags: - vllm --- > [!TIP] > PRs to correct the transformers tokenizer so that it gives 1-to-1 the same results as the mi...', '["vllm","safetensors","mixtral","fr","it","de","es","en","base_model:mistralai/mixtral-8x7b-v0.1","base_model:finetune:mistralai/mixtral-8x7b-v0.1","license:apache-2.0","region:us"]', 'other', 4617, 369046, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: vllm\nlanguage:\n- fr\n- it\n- de\n- es\n- en\nlicense: apache-2.0\nbase_model: mistralai/Mixtral-8x7B-v0.1\ninference: false\nwidget:\n- messages:\n  - role: user\n    content: What is your favorite condiment?\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href="https://mistral.ai/terms/">Privacy Policy</a>.\ntags:\n- vllm\n---\n# Model Card for Mixtral-8x7B\n\n### Tokenization with `mistral-common`\n\n```py\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n \nmistral_models_path = "MISTRAL_MODELS_PATH"\n \ntokenizer = MistralTokenizer.v1()\n \ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content="Explain Machine Learning to me in a nutshell.")])\n \ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n```\n \n## Inference with `mistral_inference`\n \n ```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n \nmodel = Transformer.from_folder(mistral_models_path)\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n\nresult = tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n## Inference with hugging face `transformers`\n \n```py\nfrom transformers import AutoModelForCausalLM\n \nmodel = AutoModelForCausalLM.from_pretrained("mistralai/Mixtral-8x7B-Instruct-v0.1")\nmodel.to("cuda")\n \ngenerated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n\n# decode with mistral tokenizer\nresult = tokenizer.decode(generated_ids[0].tolist())\nprint(result)\n```\n\n> [!TIP]\n> PRs to correct the transformers tokenizer so that it gives 1-to-1 the same results as the mistral-common reference implementation are very welcome!\n     \n        \n---\nThe Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks we tested.\n\nFor full details of this model please read our [release blog post](https://mistral.ai/news/mixtral-of-experts/).\n\n## Warning\nThis repo contains weights that are compatible with [vLLM](https://github.com/vllm-project/vllm) serving of the model as well as Hugging Face [transformers](https://github.com/huggingface/transformers) library. It is based on the original Mixtral [torrent release](magnet:?xt=urn:btih:5546272da9065eddeb6fcd7ffddeef5b75be79a7&dn=mixtral-8x7b-32kseqlen&tr=udp%3A%2F%http://2Fopentracker.i2p.rocks%3A6969%2Fannounce&tr=http%3A%2F%http://2Ftracker.openbittorrent.com%3A80%2Fannounce), but the file format and parameter names are different. Please note that model cannot (yet) be instantiated with HF.\n\n## Instruction format\n\nThis format must be strictly respected, otherwise the model will generate sub-optimal outputs.\n\nThe template used to build a prompt for the Instruct model is defined as follows:\n```\n<s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]\n```\nNote that `<s>` and `</s>` are special tokens for beginning of string (BOS) and end of string (EOS) while [INST] and [/INST] are regular strings.\n\nAs reference, here is the pseudo-code used to tokenize instructions during fine-tuning:\n```python\ndef tokenize(text):\n    return tok.encode(text, add_special_tokens=False)\n\n[BOS_ID] + \ntokenize("[INST]") + tokenize(USER_MESSAGE_1) + tokenize("[/INST]") +\ntokenize(BOT_MESSAGE_1) + [EOS_ID] +\n‚Ä¶\ntokenize("[INST]") + tokenize(USER_MESSAGE_N) + tokenize("[/INST]") +\ntokenize(BOT_MESSAGE_N) + [EOS_ID]\n```\n\nIn the pseudo-code above, note that the `tokenize` method should not add a BOS or EOS token automatically, but should add a prefix space. \n\nIn the Transformers library, one can use [chat templates](https://huggingface.co/docs/transformers/main/en/chat_templating) which make sure the right format is applied.\n\n## Run the model\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = "mistralai/Mixtral-8x7B-Instruct-v0.1"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")\n\nmessages = [\n    {"role": "user", "content": "What is your favourite condiment?"},\n    {"role": "assistant", "content": "Well, I''m quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I''m cooking up in the kitchen!"},\n    {"role": "user", "content": "Do you have mayonnaise recipes?"}\n]\n\ninputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")\n\noutputs = model.generate(inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\nBy default, transformers will load the model in full precision. Therefore you might be interested to further reduce down the memory requirements to run the model through the optimizations we offer in HF ecosystem:\n\n### In half-precision\n\nNote `float16` precision only works on GPU devices\n\n<details>\n<summary> Click to expand </summary>\n\n```diff\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = "mistralai/Mixtral-8x7B-Instruct-v0.1"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map="auto")\n\nmessages = [\n    {"role": "user", "content": "What is your favourite condiment?"},\n    {"role": "assistant", "content": "Well, I''m quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I''m cooking up in the kitchen!"},\n    {"role": "user", "content": "Do you have mayonnaise recipes?"}\n]\n\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")\n\noutputs = model.generate(input_ids, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n</details>\n\n### Lower precision using (8-bit & 4-bit) using `bitsandbytes`\n\n<details>\n<summary> Click to expand </summary>\n\n```diff\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = "mistralai/Mixtral-8x7B-Instruct-v0.1"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, device_map="auto")\n\ntext = "Hello my name is"\nmessages = [\n    {"role": "user", "content": "What is your favourite condiment?"},\n    {"role": "assistant", "content": "Well, I''m quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I''m cooking up in the kitchen!"},\n    {"role": "user", "content": "Do you have mayonnaise recipes?"}\n]\n\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")\n\noutputs = model.generate(input_ids, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n</details>\n\n### Load the model with Flash Attention 2\n\n<details>\n<summary> Click to expand </summary>\n\n```diff\n+ import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = "mistralai/Mixtral-8x7B-Instruct-v0.1"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n+ model = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=True, device_map="auto")\n\nmessages = [\n    {"role": "user", "content": "What is your favourite condiment?"},\n    {"role": "assistant", "content": "Well, I''m quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I''m cooking up in the kitchen!"},\n    {"role": "user", "content": "Do you have mayonnaise recipes?"}\n]\n\ninput_ids = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")\n\noutputs = model.generate(input_ids, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n</details>\n\n## Limitations\n\nThe Mixtral-8x7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. \nIt does not have any moderation mechanisms. We''re looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\n\n# The Mistral AI Team\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L√©lio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Th√©ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed.', '{"pipeline_tag":null,"library_name":"vllm","framework":"vllm","params":46702792704,"storage_bytes":190496794867,"files_count":36,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["MixtralForCausalLM"],"model_type":"mixtral","tokenizer_config":{"bos_token":"<s>","chat_template":"{%- if messages[0][''role''] == ''system'' %}\n    {%- set system_message = messages[0][''content''] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{{- bos_token }}\n{%- for message in loop_messages %}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}\n        {{- raise_exception(''After the optional system message, conversation roles must alternate user/assistant/user/assistant/...'') }}\n    {%- endif %}\n    {%- if message[''role''] == ''user'' %}\n        {%- if loop.first and system_message is defined %}\n            {{- '' [INST] '' + system_message + ''\\n\\n'' + message[''content''] + '' [/INST]'' }}\n        {%- else %}\n            {{- '' [INST] '' + message[''content''] + '' [/INST]'' }}\n        {%- endif %}\n    {%- elif message[''role''] == ''assistant'' %}\n        {{- '' '' + message[''content''] + eos_token}}\n    {%- else %}\n        {{- raise_exception(''Only user and assistant roles are supported, with the exception of an initial optional system message!'') }}\n    {%- endif %}\n{%- endfor %}\n","eos_token":"</s>","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"}]', NULL, 'Apache-2.0', 'approved', 65, 'b13d701e1b2ce62245004f26b1471f7c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-black-forest-labs-FLUX.1-schnell', 'huggingface--black-forest-labs--flux.1-schnell', 'FLUX.1-schnell', 'black-forest-labs', '', '["diffusers","safetensors","text-to-image","image-generation","flux","en","license:apache-2.0","endpoints_compatible","diffusers:fluxpipeline","region:us"]', 'text-to-image', 4446, 795930, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/black-forest-labs/FLUX.1-schnell","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":58051635877,"files_count":28,"spaces_count":100,"gated":"auto","private":false,"config":{"diffusers":{"_class_name":"FluxPipeline"}}}', '[]', '[]', NULL, 'Apache-2.0', 'approved', 40, '3eaa8963f7f0cc63291db20414c8e40d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-2-7b', 'huggingface--meta-llama--llama-2-7b', 'Llama-2-7b', 'meta-llama', '', '["facebook","meta","pytorch","llama","llama-2","text-generation","en","arxiv:2307.09288","license:llama2","region:us"]', 'text-generation', 4431, 470, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-2-7b","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":null,"framework":null,"params":null,"storage_bytes":13478678109,"files_count":10,"spaces_count":65,"gated":"manual","private":false,"config":null}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2307.09288","source_url":"https://arxiv.org/abs/2307.09288"}]', NULL, 'LLaMA-2', 'approved', 40, 'bc8c5e97457f0a0b97b08673b18281b9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Meta-Llama-3-8B-Instruct', 'huggingface--meta-llama--meta-llama-3-8b-instruct', 'Meta-Llama-3-8B-Instruct', 'meta-llama', '', '["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","conversational","en","license:llama3","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 4314, 1232908, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":8030261248,"storage_bytes":62143070226,"files_count":17,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","chat_template":"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = ''<|start_header_id|>'' + message[''role''] + ''<|end_header_id|>\n\n''+ message[''content''] | trim + ''<|eot_id|>'' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ ''<|start_header_id|>assistant<|end_header_id|>\n\n'' }}{% endif %}","eos_token":"<|eot_id|>"}}}', '[]', '[]', NULL, 'LLaMA-3', 'approved', 40, '84760c95f98e33e7518243d23fc3a9f0', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-openai-gpt-oss-120b', 'huggingface--openai--gpt-oss-120b', 'gpt-oss-120b', 'openai', '--- license: apache-2.0 pipeline_tag: text-generation library_name: transformers tags: - vllm --- <p align="center"> <img alt="gpt-oss-120b" src="https://raw.githubusercontent.com/openai/gpt-oss/main/docs/gpt-oss-120b.svg"> </p> <p align="center"> <a href="https://gpt-oss.com"><strong>Try gpt-oss</strong></a> ¬∑ <a href="https://cookbook.openai.com/topic/gpt-oss"><strong>Guides</strong></a> ¬∑ <a href="https://arxiv.org/abs/2508.10925"><strong>Model card</strong></a> ¬∑ <a href="https://openai.c...', '["transformers","safetensors","gpt_oss","text-generation","vllm","conversational","arxiv:2508.10925","license:apache-2.0","endpoints_compatible","8-bit","mxfp4","deploy:azure","region:us"]', 'text-generation', 4222, 4553504, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/openai/gpt-oss-120b","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\npipeline_tag: text-generation\nlibrary_name: transformers\ntags:\n- vllm\n---\n\n<p align="center">\n  <img alt="gpt-oss-120b" src="https://raw.githubusercontent.com/openai/gpt-oss/main/docs/gpt-oss-120b.svg">\n</p>\n\n<p align="center">\n  <a href="https://gpt-oss.com"><strong>Try gpt-oss</strong></a> ¬∑\n  <a href="https://cookbook.openai.com/topic/gpt-oss"><strong>Guides</strong></a> ¬∑\n  <a href="https://arxiv.org/abs/2508.10925"><strong>Model card</strong></a> ¬∑\n  <a href="https://openai.com/index/introducing-gpt-oss/"><strong>OpenAI blog</strong></a>\n</p>\n\n<br>\n\nWelcome to the gpt-oss series, [OpenAI‚Äôs open-weight models](https://openai.com/open-models) designed for powerful reasoning, agentic tasks, and versatile developer use cases.\n\nWe‚Äôre releasing two flavors of these open models:\n- `gpt-oss-120b` ‚Äî for production, general purpose, high reasoning use cases that fit into a single 80GB GPU (like NVIDIA H100 or AMD MI300X) (117B parameters with 5.1B active parameters)\n- `gpt-oss-20b` ‚Äî for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)\n\nBoth models were trained on our [harmony response format](https://github.com/openai/harmony) and should only be used with the harmony format as it will not work correctly otherwise.\n\n\n> [!NOTE]\n> This model card is dedicated to the larger `gpt-oss-120b` model. Check out [`gpt-oss-20b`](https://huggingface.co/openai/gpt-oss-20b) for the smaller model.\n\n# Highlights\n\n* **Permissive Apache 2.0 license:** Build freely without copyleft restrictions or patent risk‚Äîideal for experimentation, customization, and commercial deployment.  \n* **Configurable reasoning effort:** Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.  \n* **Full chain-of-thought:** Gain complete access to the model‚Äôs reasoning process, facilitating easier debugging and increased trust in outputs. It‚Äôs not intended to be shown to end users.  \n* **Fine-tunable:** Fully customize models to your specific use case through parameter fine-tuning.\n* **Agentic capabilities:** Use the models‚Äô native capabilities for function calling, [web browsing](https://github.com/openai/gpt-oss/tree/main?tab=readme-ov-file#browser), [Python code execution](https://github.com/openai/gpt-oss/tree/main?tab=readme-ov-file#python), and Structured Outputs.\n* **MXFP4 quantization:** The models were post-trained with MXFP4 quantization of the MoE weights, making `gpt-oss-120b` run on a single 80GB GPU (like NVIDIA H100 or AMD MI300X) and the `gpt-oss-20b` model run within 16GB of memory. All evals were performed with the same MXFP4 quantization.\n\n---\n\n# Inference examples\n\n## Transformers\n\nYou can use `gpt-oss-120b` and `gpt-oss-20b` with Transformers. If you use the Transformers chat template, it will automatically apply the [harmony response format](https://github.com/openai/harmony). If you use `model.generate` directly, you need to apply the harmony format manually using the chat template or use our [openai-harmony](https://github.com/openai/harmony) package.\n\nTo get started, install the necessary dependencies to setup your environment:\n\n```\npip install -U transformers kernels torch \n```\n\nOnce, setup you can proceed to run the model by running the snippet below:\n\n```py\nfrom transformers import pipeline\nimport torch\n\nmodel_id = "openai/gpt-oss-120b"\n\npipe = pipeline(\n    "text-generation",\n    model=model_id,\n    torch_dtype="auto",\n    device_map="auto",\n)\n\nmessages = [\n    {"role": "user", "content": "Explain quantum mechanics clearly and concisely."},\n]\n\noutputs = pipe(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0]["generated_text"][-1])\n```\n\nAlternatively, you can run the model via [`Transformers Serve`](https://huggingface.co/docs/transformers/main/serving) to spin up a OpenAI-compatible webserver:\n\n```\ntransformers serve\ntransformers chat localhost:8000 --model-name-or-path openai/gpt-oss-120b\n```\n\n[Learn more about how to use gpt-oss with Transformers.](https://cookbook.openai.com/articles/gpt-oss/run-transformers)\n\n## vLLM\n\nvLLM recommends using [uv](https://docs.astral.sh/uv/) for Python dependency management. You can use vLLM to spin up an OpenAI-compatible webserver. The following command will automatically download the model and start the server.\n\n```bash\nuv pip install --pre vllm==0.10.1+gptoss \\n    --extra-index-url https://wheels.vllm.ai/gpt-oss/ \\n    --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\n    --index-strategy unsafe-best-match\n\nvllm serve openai/gpt-oss-120b\n```\n\n[Learn more about how to use gpt-oss with vLLM.](https://cookbook.openai.com/articles/gpt-oss/run-vllm)\n\n## PyTorch / Triton\n\nTo learn about how to use this model with PyTorch and Triton, check out our [reference implementations in the gpt-oss repository](https://github.com/openai/gpt-oss?tab=readme-ov-file#reference-pytorch-implementation).\n\n## Ollama\n\nIf you are trying to run gpt-oss on consumer hardware, you can use Ollama by running the following commands after [installing Ollama](https://ollama.com/download).\n\n```bash\n# gpt-oss-120b\nollama pull gpt-oss:120b\nollama run gpt-oss:120b\n```\n\n[Learn more about how to use gpt-oss with Ollama.](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama)\n\n#### LM Studio\n\nIf you are using [LM Studio](https://lmstudio.ai/) you can use the following commands to download.\n\n```bash\n# gpt-oss-120b\nlms get openai/gpt-oss-120b\n```\n\nCheck out our [awesome list](https://github.com/openai/gpt-oss/blob/main/awesome-gpt-oss.md) for a broader collection of gpt-oss resources and inference partners.\n\n---\n\n# Download the model\n\nYou can download the model weights from the [Hugging Face Hub](https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4) directly from Hugging Face CLI:\n\n```shell\n# gpt-oss-120b\nhuggingface-cli download openai/gpt-oss-120b --include "original/*" --local-dir gpt-oss-120b/\npip install gpt-oss\npython -m gpt_oss.chat model/\n```\n\n# Reasoning levels\n\nYou can adjust the reasoning level that suits your task across three levels:\n\n* **Low:** Fast responses for general dialogue.  \n* **Medium:** Balanced speed and detail.  \n* **High:** Deep and detailed analysis.\n\nThe reasoning level can be set in the system prompts, e.g., "Reasoning: high".\n\n# Tool use\n\nThe gpt-oss models are excellent for:\n* Web browsing (using built-in browsing tools)\n* Function calling with defined schemas\n* Agentic operations like browser tasks\n\n# Fine-tuning\n\nBoth gpt-oss models can be fine-tuned for a variety of specialized use cases.\n\nThis larger model `gpt-oss-120b` can be fine-tuned on a single H100 node, whereas the smaller [`gpt-oss-20b`](https://huggingface.co/openai/gpt-oss-20b) can even be fine-tuned on consumer hardware.\n\n# Citation\n\n```bibtex\n@misc{openai2025gptoss120bgptoss20bmodel,\n      title={gpt-oss-120b & gpt-oss-20b Model Card}, \n      author={OpenAI},\n      year={2025},\n      eprint={2508.10925},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2508.10925}, \n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":120412337472,"storage_bytes":195763884726,"files_count":37,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["GptOssForCausalLM"],"model_type":"gpt_oss","quantization_config":{"quant_method":"mxfp4"},"tokenizer_config":{"bos_token":"<|startoftext|>","eos_token":"<|return|>","pad_token":"<|endoftext|>"},"chat_template_jinja":"{#-\n  In addition to the normal inputs of `messages` and `tools`, this template also accepts the\n  following kwargs:\n  - \"builtin_tools\": A list, can contain \"browser\" and/or \"python\".\n  - \"model_identity\": A string that optionally describes the model identity.\n  - \"reasoning_effort\": A string that describes the reasoning effort, defaults to \"medium\".\n #}\n\n{#- Tool Definition Rendering ============================================== #}\n{%- macro render_typescript_type(param_spec, required_params, is_nullable=false) -%}\n    {%- if param_spec.type == \"array\" -%}\n        {%- if param_spec[''items''] -%}\n            {%- if param_spec[''items''][''type''] == \"string\" -%}\n                {{- \"string[]\" }}\n            {%- elif param_spec[''items''][''type''] == \"number\" -%}\n                {{- \"number[]\" }}\n            {%- elif param_spec[''items''][''type''] == \"integer\" -%}\n                {{- \"number[]\" }}\n            {%- elif param_spec[''items''][''type''] == \"boolean\" -%}\n                {{- \"boolean[]\" }}\n            {%- else -%}\n                {%- set inner_type = render_typescript_type(param_spec[''items''], required_params) -%}\n                {%- if inner_type == \"object | object\" or inner_type|length > 50 -%}\n                    {{- \"any[]\" }}\n                {%- else -%}\n                    {{- inner_type + \"[]\" }}\n                {%- endif -%}\n            {%- endif -%}\n            {%- if param_spec.nullable -%}\n                {{- \" | null\" }}\n            {%- endif -%}\n        {%- else -%}\n            {{- \"any[]\" }}\n            {%- if param_spec.nullable -%}\n                {{- \" | null\" }}\n            {%- endif -%}\n        {%- endif -%}\n    {%- elif param_spec.type is defined and param_spec.type is iterable and param_spec.type is not string and param_spec.type is not mapping and param_spec.type[0] is defined -%}\n        {#- Handle array of types like [\"object\", \"object\"] from Union[dict, list] #}\n        {%- if param_spec.type | length > 1 -%}\n            {{- param_spec.type | join(\" | \") }}\n        {%- else -%}\n            {{- param_spec.type[0] }}\n        {%- endif -%}\n    {%- elif param_spec.oneOf -%}\n        {#- Handle oneOf schemas - check for complex unions and fallback to any #}\n        {%- set has_object_variants = false -%}\n        {%- for variant in param_spec.oneOf -%}\n            {%- if variant.type == \"object\" -%}\n                {%- set has_object_variants = true -%}\n            {%- endif -%}\n        {%- endfor -%}\n        {%- if has_object_variants and param_spec.oneOf|length > 1 -%}\n            {{- \"any\" }}\n        {%- else -%}\n            {%- for variant in param_spec.oneOf -%}\n                {{- render_typescript_type(variant, required_params) -}}\n                {%- if variant.description %}\n                    {{- \"// \" + variant.description }}\n                {%- endif -%}\n                {%- if variant.default is defined %}\n                    {{ \"// default: \" + variant.default|tojson }}\n                {%- endif -%}\n                {%- if not loop.last %}\n                    {{- \" | \" }}\n                {% endif -%}\n            {%- endfor -%}\n        {%- endif -%}\n    {%- elif param_spec.type == \"string\" -%}\n        {%- if param_spec.enum -%}\n            {{- ''\"'' + param_spec.enum|join(''\" | \"'') + ''\"'' -}}\n        {%- else -%}\n            {{- \"string\" }}\n            {%- if param_spec.nullable %}\n                {{- \" | null\" }}\n            {%- endif -%}\n        {%- endif -%}\n    {%- elif param_spec.type == \"number\" -%}\n        {{- \"number\" }}\n    {%- elif param_spec.type == \"integer\" -%}\n        {{- \"number\" }}\n    {%- elif param_spec.type == \"boolean\" -%}\n        {{- \"boolean\" }}\n\n    {%- elif param_spec.type == \"object\" -%}\n        {%- if param_spec.properties -%}\n            {{- \"{\\n\" }}\n            {%- for prop_name, prop_spec in param_spec.properties.items() -%}\n                {{- prop_name -}}\n                {%- if prop_name not in (param_spec.required or []) -%}\n                    {{- \"?\" }}\n                {%- endif -%}\n                {{- \": \" }}\n                {{ render_typescript_type(prop_spec, param_spec.required or []) }}\n                {%- if not loop.last -%}\n                    {{-\", \" }}\n                {%- endif -%}\n            {%- endfor -%}\n            {{- \"}\" }}\n        {%- else -%}\n            {{- \"object\" }}\n        {%- endif -%}\n    {%- else -%}\n        {{- \"any\" }}\n    {%- endif -%}\n{%- endmacro -%}\n\n{%- macro render_tool_namespace(namespace_name, tools) -%}\n    {{- \"## \" + namespace_name + \"\\n\\n\" }}\n    {{- \"namespace \" + namespace_name + \" {\\n\\n\" }}\n    {%- for tool in tools %}\n        {%- set tool = tool.function %}\n        {{- \"// \" + tool.description + \"\\n\" }}\n        {{- \"type \"+ tool.name + \" = \" }}\n        {%- if tool.parameters and tool.parameters.properties %}\n            {{- \"(_: {\\n\" }}\n            {%- for param_name, param_spec in tool.parameters.properties.items() %}\n                {%- if param_spec.description %}\n                    {{- \"// \" + param_spec.description + \"\\n\" }}\n                {%- endif %}\n                {{- param_name }}\n                {%- if param_name not in (tool.parameters.required or []) -%}\n                    {{- \"?\" }}\n                {%- endif -%}\n                {{- \": \" }}\n                {{- render_typescript_type(param_spec, tool.parameters.required or []) }}\n                {%- if param_spec.default is defined -%}\n                    {%- if param_spec.enum %}\n                        {{- \", // default: \" + param_spec.default }}\n                    {%- elif param_spec.oneOf %}\n                        {{- \"// default: \" + param_spec.default }}\n                    {%- else %}\n                        {{- \", // default: \" + param_spec.default|tojson }}\n                    {%- endif -%}\n                {%- endif -%}\n                {%- if not loop.last %}\n                    {{- \",\\n\" }}\n                {%- else %}\n                    {{- \",\\n\" }}\n                {%- endif -%}\n            {%- endfor %}\n            {{- \"}) => any;\\n\\n\" }}\n        {%- else -%}\n            {{- \"() => any;\\n\\n\" }}\n        {%- endif -%}\n    {%- endfor %}\n    {{- \"} // namespace \" + namespace_name }}\n{%- endmacro -%}\n\n{%- macro render_builtin_tools(browser_tool, python_tool) -%}\n    {%- if browser_tool %}\n        {{- \"## browser\\n\\n\" }}\n        {{- \"// Tool for browsing.\\n\" }}\n        {{- \"// The `cursor` appears in brackets before each browsing display: `[{cursor}]`.\\n\" }}\n        {{- \"// Cite information from the tool using the following format:\\n\" }}\n        {{- \"// `„Äê{cursor}‚Ä†L{line_start}(-L{line_end})?„Äë`, for example: `„Äê6‚Ä†L9-L11„Äë` or `„Äê8‚Ä†L3„Äë`.\\n\" }}\n        {{- \"// Do not quote more than 10 words directly from the tool output.\\n\" }}\n        {{- \"// sources=web (default: web)\\n\" }}\n        {{- \"namespace browser {\\n\\n\" }}\n        {{- \"// Searches for information related to `query` and displays `topn` results.\\n\" }}\n        {{- \"type search = (_: {\\n\" }}\n        {{- \"query: string,\\n\" }}\n        {{- \"topn?: number, // default: 10\\n\" }}\n        {{- \"source?: string,\\n\" }}\n        {{- \"}) => any;\\n\\n\" }}\n        {{- \"// Opens the link `id` from the page indicated by `cursor` starting at line number `loc`, showing `num_lines` lines.\\n\" }}\n        {{- \"// Valid link ids are displayed with the formatting: `„Äê{id}‚Ä†.*„Äë`.\\n\" }}\n        {{- \"// If `cursor` is not provided, the most recent page is implied.\\n\" }}\n        {{- \"// If `id` is a string, it is treated as a fully qualified URL associated with `source`.\\n\" }}\n        {{- \"// If `loc` is not provided, the viewport will be positioned at the beginning of the document or centered on the most relevant passage, if available.\\n\" }}\n        {{- \"// Use this function without `id` to scroll to a new location of an opened page.\\n\" }}\n        {{- \"type open = (_: {\\n\" }}\n        {{- \"id?: number | string, // default: -1\\n\" }}\n        {{- \"cursor?: number, // default: -1\\n\" }}\n        {{- \"loc?: number, // default: -1\\n\" }}\n        {{- \"num_lines?: number, // default: -1\\n\" }}\n        {{- \"view_source?: boolean, // default: false\\n\" }}\n        {{- \"source?: string,\\n\" }}\n        {{- \"}) => any;\\n\\n\" }}\n        {{- \"// Finds exact matches of `pattern` in the current page, or the page given by `cursor`.\\n\" }}\n        {{- \"type find = (_: {\\n\" }}\n        {{- \"pattern: string,\\n\" }}\n        {{- \"cursor?: number, // default: -1\\n\" }}\n        {{- \"}) => any;\\n\\n\" }}\n        {{- \"} // namespace browser\\n\\n\" }}\n    {%- endif -%}\n\n    {%- if python_tool %}\n        {{- \"## python\\n\\n\" }}\n        {{- \"Use this tool to execute Python code in your chain of thought. The code will not be shown to the user. This tool should be used for internal reasoning, but not for code that is intended to be visible to the user (e.g. when creating plots, tables, or files).\\n\\n\" }}\n        {{- \"When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 120.0 seconds. The drive at ''/mnt/data'' can be used to save and persist user files. Internet access for this session is UNKNOWN. Depends on the cluster.\\n\\n\" }}\n    {%- endif -%}\n{%- endmacro -%}\n\n{#- System Message Construction ============================================ #}\n{%- macro build_system_message() -%}\n    {%- if model_identity is not defined %}\n        {%- set model_identity = \"You are ChatGPT, a large language model trained by OpenAI.\" %}\n    {%- endif %}\n    {{- model_identity + \"\\n\" }}\n    {{- \"Knowledge cutoff: 2024-06\\n\" }}\n    {{- \"Current date: \" + strftime_now(\"%Y-%m-%d\") + \"\\n\\n\" }}\n    {%- if reasoning_effort is not defined %}\n        {%- set reasoning_effort = \"medium\" %}\n    {%- endif %}\n    {{- \"Reasoning: \" + reasoning_effort + \"\\n\\n\" }}\n    {%- if builtin_tools %}\n        {{- \"# Tools\\n\\n\" }}\n        {%- set available_builtin_tools = namespace(browser=false, python=false) %}\n        {%- for tool in builtin_tools %}\n            {%- if tool == \"browser\" %}\n                {%- set available_builtin_tools.browser = true %}\n            {%- elif tool == \"python\" %}\n                {%- set available_builtin_tools.python = true %}\n            {%- endif %}\n        {%- endfor %}\n        {{- render_builtin_tools(available_builtin_tools.browser, available_builtin_tools.python) }}\n    {%- endif -%}\n    {{- \"# Valid channels: analysis, commentary, final. Channel must be included for every message.\" }}\n    {%- if tools -%}\n        {{- \"\\nCalls to these tools must go to the commentary channel: ''functions''.\" }}\n    {%- endif -%}\n{%- endmacro -%}\n\n{#- Main Template Logic ================================================= #}\n{#- Set defaults #}\n\n{#- Render system message #}\n{{- \"<|start|>system<|message|>\" }}\n{{- build_system_message() }}\n{{- \"<|end|>\" }}\n\n{#- Extract developer message #}\n{%- if messages[0].role == \"developer\" or messages[0].role == \"system\" %}\n    {%- set developer_message = messages[0].content %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set developer_message = \"\" %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{#- Render developer message #}\n{%- if developer_message or tools %}\n    {{- \"<|start|>developer<|message|>\" }}\n    {%- if developer_message %}\n        {{- \"# Instructions\\n\\n\" }}\n        {{- developer_message }}\n        {{- \"\\n\\n\" }}\n    {%- endif %}\n    {%- if tools -%}\n        {{- \"# Tools\\n\\n\" }}\n        {{- render_tool_namespace(\"functions\", tools) }}\n    {%- endif -%}\n    {{- \"<|end|>\" }}\n{%- endif %}\n\n{#- Render messages #}\n{%- set last_tool_call = namespace(name=none) %}\n{%- for message in loop_messages -%}\n    {#- At this point only assistant/user/tool messages should remain #}\n    {%- if message.role == ''assistant'' -%}\n        {#- Checks to ensure the messages are being passed in the format we expect #}\n        {%- if \"content\" in message %}\n            {%- if \"<|channel|>analysis<|message|>\" in message.content or \"<|channel|>final<|message|>\" in message.content %}\n                {{- raise_exception(\"You have passed a message containing <|channel|> tags in the content field. Instead of doing this, you should pass analysis messages (the string between ''<|message|>'' and ''<|end|>'') in the ''thinking'' field, and final messages (the string between ''<|message|>'' and ''<|end|>'') in the ''content'' field.\") }}\n            {%- endif %}\n        {%- endif %}\n        {%- if \"thinking\" in message %}\n            {%- if \"<|channel|>analysis<|message|>\" in message.thinking or \"<|channel|>final<|message|>\" in message.thinking %}\n                {{- raise_exception(\"You have passed a message containing <|channel|> tags in the thinking field. Instead of doing this, you should pass analysis messages (the string between ''<|message|>'' and ''<|end|>'') in the ''thinking'' field, and final messages (the string between ''<|message|>'' and ''<|end|>'') in the ''content'' field.\") }}\n            {%- endif %}\n        {%- endif %}\n        {%- if \"tool_calls\" in message %}\n            {#- We need very careful handling here - we want to drop the tool call analysis message if the model #}\n            {#- has output a later <|final|> message, but otherwise we want to retain it. This is the only case #}\n            {#- when we render CoT/analysis messages in inference. #}\n            {%- set future_final_message = namespace(found=false) %}\n            {%- for future_message in loop_messages[loop.index:] %}\n                {%- if future_message.role == ''assistant'' and \"tool_calls\" not in future_message %}\n                    {%- set future_final_message.found = true %}\n                {%- endif %}\n            {%- endfor %}\n            {#- We assume max 1 tool call per message, and so we infer the tool call name #}\n            {#- in \"tool\" messages from the most recent assistant tool call name #}\n            {%- set tool_call = message.tool_calls[0] %}\n            {%- if tool_call.function %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {%- if message.content and message.thinking %}\n                {{- raise_exception(\"Cannot pass both content and thinking in an assistant message with tool calls! Put the analysis message in one or the other, but not both.\") }}\n            {%- elif message.content and not future_final_message.found %}\n                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.content + \"<|end|>\" }}\n            {%- elif message.thinking and not future_final_message.found %}\n                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.thinking + \"<|end|>\" }}\n            {%- endif %}\n            {{- \"<|start|>assistant to=\" }}\n            {{- \"functions.\" + tool_call.name + \"<|channel|>commentary \" }}\n            {{- (tool_call.content_type if tool_call.content_type is defined else \"json\") + \"<|message|>\" }}\n            {{- tool_call.arguments|tojson }}\n            {{- \"<|call|>\" }}\n            {%- set last_tool_call.name = tool_call.name %}\n        {%- elif loop.last and not add_generation_prompt %}\n            {#- Only render the CoT if the final turn is an assistant turn and add_generation_prompt is false #}\n            {#- This is a situation that should only occur in training, never in inference. #}\n            {%- if \"thinking\" in message %}\n                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.thinking + \"<|end|>\" }}\n            {%- endif %}\n            {#- <|return|> indicates the end of generation, but <|end|> does not #}\n            {#- <|return|> should never be an input to the model, but we include it as the final token #}\n            {#- when training, so the model learns to emit it. #}\n            {{- \"<|start|>assistant<|channel|>final<|message|>\" + message.content + \"<|return|>\" }}\n        {%- else %}\n            {#- CoT is dropped during all previous turns, so we never render it for inference #}\n            {{- \"<|start|>assistant<|channel|>final<|message|>\" + message.content + \"<|end|>\" }}\n            {%- set last_tool_call.name = none %}\n        {%- endif %}\n    {%- elif message.role == ''tool'' -%}\n        {%- if last_tool_call.name is none %}\n            {{- raise_exception(\"Message has tool role, but there was no previous assistant message with a tool call!\") }}\n        {%- endif %}\n        {{- \"<|start|>functions.\" + last_tool_call.name }}\n        {{- \" to=assistant<|channel|>commentary<|message|>\" + message.content|tojson + \"<|end|>\" }}\n    {%- elif message.role == ''user'' -%}\n        {{- \"<|start|>user<|message|>\" + message.content + \"<|end|>\" }}\n    {%- endif -%}\n{%- endfor -%}\n\n{#- Generation prompt #}\n{%- if add_generation_prompt -%}\n<|start|>assistant\n{%- endif -%}"}}', '[]', '[{"type":"has_code","target_id":"github:openai:harmony","source_url":"https://github.com/openai/harmony"},{"type":"has_code","target_id":"github:openai:gpt-oss","source_url":"https://github.com/openai/gpt-oss"},{"type":"has_code","target_id":"github:openai:gpt-oss","source_url":"https://github.com/openai/gpt-oss"},{"type":"has_code","target_id":"github:openai:harmony","source_url":"https://github.com/openai/harmony"},{"type":"has_code","target_id":"github:openai:harmony","source_url":"https://github.com/openai/harmony"},{"type":"has_code","target_id":"github:openai:gpt-oss","source_url":"https://github.com/openai/gpt-oss?tab=readme-ov-file#reference-pytorch-implementation"},{"type":"has_code","target_id":"github:openai:gpt-oss","source_url":"https://github.com/openai/gpt-oss"},{"type":"based_on_paper","target_id":"arxiv:2508.10925","source_url":"https://arxiv.org/abs/2508.10925"}]', NULL, 'Apache-2.0', 'approved', 65, 'f644e69b89471dc4a855a939dd2e9429', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-sentence-transformers-all-MiniLM-L6-v2', 'huggingface--sentence-transformers--all-minilm-l6-v2', 'all-MiniLM-L6-v2', 'sentence-transformers', '--- language: en license: apache-2.0 library_name: sentence-transformers tags: - sentence-transformers - feature-extraction - sentence-similarity - transformers datasets: - s2orc - flax-sentence-embeddings/stackexchange_xml - ms_marco - gooaq - yahoo_answers_topics - code_search_net - search_qa - eli5 - snli - multi_nli - wikihow - natural_questions - trivia_qa - embedding-data/sentence-compression - embedding-data/flickr30k-captions - embedding-data/altlex - embedding-data/simple-wiki - embe...', '["sentence-transformers","pytorch","tf","rust","onnx","safetensors","openvino","bert","feature-extraction","sentence-similarity","transformers","en","dataset:s2orc","dataset:ms_marco","dataset:gooaq","dataset:yahoo_answers_topics","dataset:code_search_net","dataset:search_qa","dataset:eli5","dataset:snli","dataset:multi_nli","dataset:wikihow","dataset:natural_questions","dataset:trivia_qa","dataset:embedding-data/sentence-compression","dataset:embedding-data/flickr30k-captions","dataset:embedding-data/altlex","dataset:embedding-data/simple-wiki","dataset:embedding-data/qqp","dataset:embedding-data/specter","dataset:embedding-data/paq_pairs","dataset:embedding-data/wikianswers","arxiv:1904.06472","arxiv:2102.07033","arxiv:2104.08727","arxiv:1704.05179","arxiv:1810.09305","license:apache-2.0","text-embeddings-inference","endpoints_compatible","deploy:azure","region:us"]', 'sentence-similarity', 4196, 151023144, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: en\nlicense: apache-2.0\nlibrary_name: sentence-transformers\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\ndatasets:\n- s2orc\n- flax-sentence-embeddings/stackexchange_xml\n- ms_marco\n- gooaq\n- yahoo_answers_topics\n- code_search_net\n- search_qa\n- eli5\n- snli\n- multi_nli\n- wikihow\n- natural_questions\n- trivia_qa\n- embedding-data/sentence-compression\n- embedding-data/flickr30k-captions\n- embedding-data/altlex\n- embedding-data/simple-wiki\n- embedding-data/QQP\n- embedding-data/SPECTER\n- embedding-data/PAQ_pairs\n- embedding-data/WikiAnswers\npipeline_tag: sentence-similarity\n---\n\n\n# all-MiniLM-L6-v2\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n## Usage (Sentence-Transformers)\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = ["This is an example sentence", "Each sentence is converted"]\n\nmodel = SentenceTransformer(''sentence-transformers/all-MiniLM-L6-v2'')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = [''This is an example sentence'', ''Each sentence is converted'']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(''sentence-transformers/all-MiniLM-L6-v2'')\nmodel = AutoModel.from_pretrained(''sentence-transformers/all-MiniLM-L6-v2'')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=''pt'')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input[''attention_mask''])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint("Sentence embeddings:")\nprint(sentence_embeddings)\n```\n\n------\n\n## Background\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We used the pretrained [`nreimers/MiniLM-L6-H384-uncased`](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developed this model during the \n[Community week using JAX/Flax for NLP & CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`nreimers/MiniLM-L6-H384-uncased`](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) model. Please refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n#### Hyper parameters\n\nWe trained our model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: `train_script.py`.\n\n#### Training data\n\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\n\n| Dataset                                                  | Paper                                    | Number of training tuples  |\n|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|\n| [Reddit comments (2015-2018)](https://github.com/PolyAI-LDN/conversational-datasets/tree/master/reddit) | [paper](https://arxiv.org/abs/1904.06472) | 726,484,430 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Abstracts) | [paper](https://aclanthology.org/2020.acl-main.447/) | 116,288,806 |\n| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https://doi.org/10.1145/2623330.2623677) | 77,427,422 |\n| [PAQ](https://github.com/facebookresearch/PAQ) (Question, Answer) pairs | [paper](https://arxiv.org/abs/2102.07033) | 64,371,441 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Titles) | [paper](https://aclanthology.org/2020.acl-main.447/) | 52,603,982 |\n| [S2ORC](https://github.com/allenai/s2orc) (Title, Abstract) | [paper](https://aclanthology.org/2020.acl-main.447/) | 41,769,185 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title+Body, Answer) pairs  | - | 21,396,559 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Answer) pairs  | - | 21,396,559 |\n| [MS MARCO](https://microsoft.github.io/msmarco/) triplets | [paper](https://doi.org/10.1145/3404835.3462804) | 9,144,553 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) | [paper](https://arxiv.org/pdf/2104.08727.pdf) | 3,012,496 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |\n| [Code Search](https://huggingface.co/datasets/code_search_net) | - | 1,151,414 |\n| [COCO](https://cocodataset.org/#home) Image captions | [paper](https://link.springer.com/chapter/10.1007%2F978-3-319-10602-1_48) | 828,395|\n| [SPECTER](https://github.com/allenai/specter) citation triplets | [paper](https://doi.org/10.18653/v1/2020.acl-main.207) | 684,100 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |\n| [SearchQA](https://huggingface.co/datasets/search_qa) | [paper](https://arxiv.org/abs/1704.05179) | 582,261 |\n| [Eli5](https://huggingface.co/datasets/eli5) | [paper](https://doi.org/10.18653/v1/p19-1346) | 325,475 |\n| [Flickr 30k](https://shannon.cs.illinois.edu/DenotationGraph/) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/229/33) | 317,695 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles) | | 304,525 |\n| AllNLI ([SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) | [paper SNLI](https://doi.org/10.18653/v1/d15-1075), [paper MultiNLI](https://doi.org/10.18653/v1/n18-1101) | 277,230 | \n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |\n| [Sentence Compression](https://github.com/google-research-datasets/sentence-compression) | [paper](https://www.aclweb.org/anthology/D13-1155/) | 180,000 |\n| [Wikihow](https://github.com/pvl/wikihow_pairs_dataset) | [paper](https://arxiv.org/abs/1810.09305) | 128,542 |\n| [Altlex](https://github.com/chridey/altlex/) | [paper](https://aclanthology.org/P16-1135.pdf) | 112,696 |\n| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |\n| [Simple Wikipedia](https://cs.pomona.edu/~dkauchak/simplification/) | [paper](https://www.aclweb.org/anthology/P11-2117/) | 102,225 |\n| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/1455) | 100,231 |\n| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) | [paper](https://aclanthology.org/P18-2124.pdf) | 87,599 |\n| [TriviaQA](https://huggingface.co/datasets/trivia_qa) | - | 73,346 |\n| **Total** | | **1,170,060,424** |', '{"pipeline_tag":"sentence-similarity","library_name":"sentence-transformers","framework":"sentence-transformers","params":22713728,"storage_bytes":1844563487,"files_count":30,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BertModel"],"model_type":"bert","tokenizer_config":{"unk_token":"[UNK]","sep_token":"[SEP]","pad_token":"[PAD]","cls_token":"[CLS]","mask_token":"[MASK]"}}}', '[]', '[{"type":"has_code","target_id":"github:PolyAI-LDN:conversational-datasets","source_url":"https://github.com/PolyAI-LDN/conversational-datasets"},{"type":"has_code","target_id":"github:allenai:s2orc","source_url":"https://github.com/allenai/s2orc"},{"type":"has_code","target_id":"github:afader:oqa","source_url":"https://github.com/afader/oqa#wikianswers-corpus"},{"type":"has_code","target_id":"github:facebookresearch:PAQ","source_url":"https://github.com/facebookresearch/PAQ"},{"type":"has_code","target_id":"github:allenai:s2orc","source_url":"https://github.com/allenai/s2orc"},{"type":"has_code","target_id":"github:allenai:s2orc","source_url":"https://github.com/allenai/s2orc"},{"type":"has_code","target_id":"github:allenai:gooaq","source_url":"https://github.com/allenai/gooaq"},{"type":"has_code","target_id":"github:allenai:specter","source_url":"https://github.com/allenai/specter"},{"type":"has_code","target_id":"github:google-research-datasets:sentence-compression","source_url":"https://github.com/google-research-datasets/sentence-compression"},{"type":"has_code","target_id":"github:pvl:wikihow_pairs_dataset","source_url":"https://github.com/pvl/wikihow_pairs_dataset"},{"type":"has_code","target_id":"github:chridey:altlex","source_url":"https://github.com/chridey/altlex"},{"type":"based_on_paper","target_id":"arxiv:1904.06472","source_url":"https://arxiv.org/abs/1904.06472"},{"type":"based_on_paper","target_id":"arxiv:2102.07033","source_url":"https://arxiv.org/abs/2102.07033"},{"type":"based_on_paper","target_id":"arxiv:2104.08727","source_url":"https://arxiv.org/abs/2104.08727"},{"type":"based_on_paper","target_id":"arxiv:1704.05179","source_url":"https://arxiv.org/abs/1704.05179"},{"type":"based_on_paper","target_id":"arxiv:1810.09305","source_url":"https://arxiv.org/abs/1810.09305"}]', NULL, 'Apache-2.0', 'approved', 80, 'c67b5cccafc1bb790abea332530b0d80', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-openai-gpt-oss-20b', 'huggingface--openai--gpt-oss-20b', 'gpt-oss-20b', 'openai', '--- license: apache-2.0 pipeline_tag: text-generation library_name: transformers tags: - vllm --- <p align="center"> <img alt="gpt-oss-20b" src="https://raw.githubusercontent.com/openai/gpt-oss/main/docs/gpt-oss-20b.svg"> </p> <p align="center"> <a href="https://gpt-oss.com"><strong>Try gpt-oss</strong></a> ¬∑ <a href="https://cookbook.openai.com/topic/gpt-oss"><strong>Guides</strong></a> ¬∑ <a href="https://arxiv.org/abs/2508.10925"><strong>Model card</strong></a> ¬∑ <a href="https://openai.com...', '["transformers","safetensors","gpt_oss","text-generation","vllm","conversational","arxiv:2508.10925","license:apache-2.0","endpoints_compatible","8-bit","mxfp4","deploy:azure","region:us"]', 'text-generation', 4028, 8089782, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/openai/gpt-oss-20b","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\npipeline_tag: text-generation\nlibrary_name: transformers\ntags:\n- vllm\n---\n\n<p align="center">\n  <img alt="gpt-oss-20b" src="https://raw.githubusercontent.com/openai/gpt-oss/main/docs/gpt-oss-20b.svg">\n</p>\n\n<p align="center">\n  <a href="https://gpt-oss.com"><strong>Try gpt-oss</strong></a> ¬∑\n  <a href="https://cookbook.openai.com/topic/gpt-oss"><strong>Guides</strong></a> ¬∑\n  <a href="https://arxiv.org/abs/2508.10925"><strong>Model card</strong></a> ¬∑\n  <a href="https://openai.com/index/introducing-gpt-oss/"><strong>OpenAI blog</strong></a>\n</p>\n\n<br>\n\nWelcome to the gpt-oss series, [OpenAI‚Äôs open-weight models](https://openai.com/open-models) designed for powerful reasoning, agentic tasks, and versatile developer use cases.\n\nWe‚Äôre releasing two flavors of these open models:\n- `gpt-oss-120b` ‚Äî for production, general purpose, high reasoning use cases that fit into a single 80GB GPU (like NVIDIA H100 or AMD MI300X) (117B parameters with 5.1B active parameters)\n- `gpt-oss-20b` ‚Äî for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)\n\nBoth models were trained on our [harmony response format](https://github.com/openai/harmony) and should only be used with the harmony format as it will not work correctly otherwise.\n\n\n> [!NOTE]\n> This model card is dedicated to the smaller `gpt-oss-20b` model. Check out [`gpt-oss-120b`](https://huggingface.co/openai/gpt-oss-120b) for the larger model.\n\n# Highlights\n\n* **Permissive Apache 2.0 license:** Build freely without copyleft restrictions or patent risk‚Äîideal for experimentation, customization, and commercial deployment.  \n* **Configurable reasoning effort:** Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.  \n* **Full chain-of-thought:** Gain complete access to the model‚Äôs reasoning process, facilitating easier debugging and increased trust in outputs. It‚Äôs not intended to be shown to end users.  \n* **Fine-tunable:** Fully customize models to your specific use case through parameter fine-tuning.\n* **Agentic capabilities:** Use the models‚Äô native capabilities for function calling, [web browsing](https://github.com/openai/gpt-oss/tree/main?tab=readme-ov-file#browser), [Python code execution](https://github.com/openai/gpt-oss/tree/main?tab=readme-ov-file#python), and Structured Outputs.\n* **MXFP4 quantization:** The models were post-trained with MXFP4 quantization of the MoE weights, making `gpt-oss-120b` run on a single 80GB GPU (like NVIDIA H100 or AMD MI300X) and the `gpt-oss-20b` model run within 16GB of memory. All evals were performed with the same MXFP4 quantization.\n\n---\n\n# Inference examples\n\n## Transformers\n\nYou can use `gpt-oss-120b` and `gpt-oss-20b` with Transformers. If you use the Transformers chat template, it will automatically apply the [harmony response format](https://github.com/openai/harmony). If you use `model.generate` directly, you need to apply the harmony format manually using the chat template or use our [openai-harmony](https://github.com/openai/harmony) package.\n\nTo get started, install the necessary dependencies to setup your environment:\n\n```\npip install -U transformers kernels torch \n```\n\nOnce, setup you can proceed to run the model by running the snippet below:\n\n```py\nfrom transformers import pipeline\nimport torch\n\nmodel_id = "openai/gpt-oss-20b"\n\npipe = pipeline(\n    "text-generation",\n    model=model_id,\n    torch_dtype="auto",\n    device_map="auto",\n)\n\nmessages = [\n    {"role": "user", "content": "Explain quantum mechanics clearly and concisely."},\n]\n\noutputs = pipe(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0]["generated_text"][-1])\n```\n\nAlternatively, you can run the model via [`Transformers Serve`](https://huggingface.co/docs/transformers/main/serving) to spin up a OpenAI-compatible webserver:\n\n```\ntransformers serve\ntransformers chat localhost:8000 --model-name-or-path openai/gpt-oss-20b\n```\n\n[Learn more about how to use gpt-oss with Transformers.](https://cookbook.openai.com/articles/gpt-oss/run-transformers)\n\n## vLLM\n\nvLLM recommends using [uv](https://docs.astral.sh/uv/) for Python dependency management. You can use vLLM to spin up an OpenAI-compatible webserver. The following command will automatically download the model and start the server.\n\n```bash\nuv pip install --pre vllm==0.10.1+gptoss \\n    --extra-index-url https://wheels.vllm.ai/gpt-oss/ \\n    --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\n    --index-strategy unsafe-best-match\n\nvllm serve openai/gpt-oss-20b\n```\n\n[Learn more about how to use gpt-oss with vLLM.](https://cookbook.openai.com/articles/gpt-oss/run-vllm)\n\n## PyTorch / Triton\n\nTo learn about how to use this model with PyTorch and Triton, check out our [reference implementations in the gpt-oss repository](https://github.com/openai/gpt-oss?tab=readme-ov-file#reference-pytorch-implementation).\n\n## Ollama\n\nIf you are trying to run gpt-oss on consumer hardware, you can use Ollama by running the following commands after [installing Ollama](https://ollama.com/download).\n\n```bash\n# gpt-oss-20b\nollama pull gpt-oss:20b\nollama run gpt-oss:20b\n```\n\n[Learn more about how to use gpt-oss with Ollama.](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama)\n\n#### LM Studio\n\nIf you are using [LM Studio](https://lmstudio.ai/) you can use the following commands to download.\n\n```bash\n# gpt-oss-20b\nlms get openai/gpt-oss-20b\n```\n\nCheck out our [awesome list](https://github.com/openai/gpt-oss/blob/main/awesome-gpt-oss.md) for a broader collection of gpt-oss resources and inference partners.\n\n---\n\n# Download the model\n\nYou can download the model weights from the [Hugging Face Hub](https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4) directly from Hugging Face CLI:\n\n```shell\n# gpt-oss-20b\nhuggingface-cli download openai/gpt-oss-20b --include "original/*" --local-dir gpt-oss-20b/\npip install gpt-oss\npython -m gpt_oss.chat model/\n```\n\n# Reasoning levels\n\nYou can adjust the reasoning level that suits your task across three levels:\n\n* **Low:** Fast responses for general dialogue.  \n* **Medium:** Balanced speed and detail.  \n* **High:** Deep and detailed analysis.\n\nThe reasoning level can be set in the system prompts, e.g., "Reasoning: high".\n\n# Tool use\n\nThe gpt-oss models are excellent for:\n* Web browsing (using built-in browsing tools)\n* Function calling with defined schemas\n* Agentic operations like browser tasks\n\n# Fine-tuning\n\nBoth gpt-oss models can be fine-tuned for a variety of specialized use cases.\n\nThis smaller model `gpt-oss-20b` can be fine-tuned on consumer hardware, whereas the larger [`gpt-oss-120b`](https://huggingface.co/openai/gpt-oss-120b) can be fine-tuned on a single H100 node.\n\n# Citation\n\n```bibtex\n@misc{openai2025gptoss120bgptoss20bmodel,\n      title={gpt-oss-120b & gpt-oss-20b Model Card}, \n      author={OpenAI},\n      year={2025},\n      eprint={2508.10925},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2508.10925}, \n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":21511953984,"storage_bytes":41321532546,"files_count":18,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["GptOssForCausalLM"],"model_type":"gpt_oss","quantization_config":{"quant_method":"mxfp4"},"tokenizer_config":{"bos_token":"<|startoftext|>","eos_token":"<|return|>","pad_token":"<|endoftext|>"},"chat_template_jinja":"{#-\n  In addition to the normal inputs of `messages` and `tools`, this template also accepts the\n  following kwargs:\n  - \"builtin_tools\": A list, can contain \"browser\" and/or \"python\".\n  - \"model_identity\": A string that optionally describes the model identity.\n  - \"reasoning_effort\": A string that describes the reasoning effort, defaults to \"medium\".\n #}\n\n{#- Tool Definition Rendering ============================================== #}\n{%- macro render_typescript_type(param_spec, required_params, is_nullable=false) -%}\n    {%- if param_spec.type == \"array\" -%}\n        {%- if param_spec[''items''] -%}\n            {%- if param_spec[''items''][''type''] == \"string\" -%}\n                {{- \"string[]\" }}\n            {%- elif param_spec[''items''][''type''] == \"number\" -%}\n                {{- \"number[]\" }}\n            {%- elif param_spec[''items''][''type''] == \"integer\" -%}\n                {{- \"number[]\" }}\n            {%- elif param_spec[''items''][''type''] == \"boolean\" -%}\n                {{- \"boolean[]\" }}\n            {%- else -%}\n                {%- set inner_type = render_typescript_type(param_spec[''items''], required_params) -%}\n                {%- if inner_type == \"object | object\" or inner_type|length > 50 -%}\n                    {{- \"any[]\" }}\n                {%- else -%}\n                    {{- inner_type + \"[]\" }}\n                {%- endif -%}\n            {%- endif -%}\n            {%- if param_spec.nullable -%}\n                {{- \" | null\" }}\n            {%- endif -%}\n        {%- else -%}\n            {{- \"any[]\" }}\n            {%- if param_spec.nullable -%}\n                {{- \" | null\" }}\n            {%- endif -%}\n        {%- endif -%}\n    {%- elif param_spec.type is defined and param_spec.type is iterable and param_spec.type is not string and param_spec.type is not mapping and param_spec.type[0] is defined -%}\n        {#- Handle array of types like [\"object\", \"object\"] from Union[dict, list] #}\n        {%- if param_spec.type | length > 1 -%}\n            {{- param_spec.type | join(\" | \") }}\n        {%- else -%}\n            {{- param_spec.type[0] }}\n        {%- endif -%}\n    {%- elif param_spec.oneOf -%}\n        {#- Handle oneOf schemas - check for complex unions and fallback to any #}\n        {%- set has_object_variants = false -%}\n        {%- for variant in param_spec.oneOf -%}\n            {%- if variant.type == \"object\" -%}\n                {%- set has_object_variants = true -%}\n            {%- endif -%}\n        {%- endfor -%}\n        {%- if has_object_variants and param_spec.oneOf|length > 1 -%}\n            {{- \"any\" }}\n        {%- else -%}\n            {%- for variant in param_spec.oneOf -%}\n                {{- render_typescript_type(variant, required_params) -}}\n                {%- if variant.description %}\n                    {{- \"// \" + variant.description }}\n                {%- endif -%}\n                {%- if variant.default is defined %}\n                    {{ \"// default: \" + variant.default|tojson }}\n                {%- endif -%}\n                {%- if not loop.last %}\n                    {{- \" | \" }}\n                {% endif -%}\n            {%- endfor -%}\n        {%- endif -%}\n    {%- elif param_spec.type == \"string\" -%}\n        {%- if param_spec.enum -%}\n            {{- ''\"'' + param_spec.enum|join(''\" | \"'') + ''\"'' -}}\n        {%- else -%}\n            {{- \"string\" }}\n            {%- if param_spec.nullable %}\n                {{- \" | null\" }}\n            {%- endif -%}\n        {%- endif -%}\n    {%- elif param_spec.type == \"number\" -%}\n        {{- \"number\" }}\n    {%- elif param_spec.type == \"integer\" -%}\n        {{- \"number\" }}\n    {%- elif param_spec.type == \"boolean\" -%}\n        {{- \"boolean\" }}\n\n    {%- elif param_spec.type == \"object\" -%}\n        {%- if param_spec.properties -%}\n            {{- \"{\\n\" }}\n            {%- for prop_name, prop_spec in param_spec.properties.items() -%}\n                {{- prop_name -}}\n                {%- if prop_name not in (param_spec.required or []) -%}\n                    {{- \"?\" }}\n                {%- endif -%}\n                {{- \": \" }}\n                {{ render_typescript_type(prop_spec, param_spec.required or []) }}\n                {%- if not loop.last -%}\n                    {{-\", \" }}\n                {%- endif -%}\n            {%- endfor -%}\n            {{- \"}\" }}\n        {%- else -%}\n            {{- \"object\" }}\n        {%- endif -%}\n    {%- else -%}\n        {{- \"any\" }}\n    {%- endif -%}\n{%- endmacro -%}\n\n{%- macro render_tool_namespace(namespace_name, tools) -%}\n    {{- \"## \" + namespace_name + \"\\n\\n\" }}\n    {{- \"namespace \" + namespace_name + \" {\\n\\n\" }}\n    {%- for tool in tools %}\n        {%- set tool = tool.function %}\n        {{- \"// \" + tool.description + \"\\n\" }}\n        {{- \"type \"+ tool.name + \" = \" }}\n        {%- if tool.parameters and tool.parameters.properties %}\n            {{- \"(_: {\\n\" }}\n            {%- for param_name, param_spec in tool.parameters.properties.items() %}\n                {%- if param_spec.description %}\n                    {{- \"// \" + param_spec.description + \"\\n\" }}\n                {%- endif %}\n                {{- param_name }}\n                {%- if param_name not in (tool.parameters.required or []) -%}\n                    {{- \"?\" }}\n                {%- endif -%}\n                {{- \": \" }}\n                {{- render_typescript_type(param_spec, tool.parameters.required or []) }}\n                {%- if param_spec.default is defined -%}\n                    {%- if param_spec.enum %}\n                        {{- \", // default: \" + param_spec.default }}\n                    {%- elif param_spec.oneOf %}\n                        {{- \"// default: \" + param_spec.default }}\n                    {%- else %}\n                        {{- \", // default: \" + param_spec.default|tojson }}\n                    {%- endif -%}\n                {%- endif -%}\n                {%- if not loop.last %}\n                    {{- \",\\n\" }}\n                {%- else %}\n                    {{- \",\\n\" }}\n                {%- endif -%}\n            {%- endfor %}\n            {{- \"}) => any;\\n\\n\" }}\n        {%- else -%}\n            {{- \"() => any;\\n\\n\" }}\n        {%- endif -%}\n    {%- endfor %}\n    {{- \"} // namespace \" + namespace_name }}\n{%- endmacro -%}\n\n{%- macro render_builtin_tools(browser_tool, python_tool) -%}\n    {%- if browser_tool %}\n        {{- \"## browser\\n\\n\" }}\n        {{- \"// Tool for browsing.\\n\" }}\n        {{- \"// The `cursor` appears in brackets before each browsing display: `[{cursor}]`.\\n\" }}\n        {{- \"// Cite information from the tool using the following format:\\n\" }}\n        {{- \"// `„Äê{cursor}‚Ä†L{line_start}(-L{line_end})?„Äë`, for example: `„Äê6‚Ä†L9-L11„Äë` or `„Äê8‚Ä†L3„Äë`.\\n\" }}\n        {{- \"// Do not quote more than 10 words directly from the tool output.\\n\" }}\n        {{- \"// sources=web (default: web)\\n\" }}\n        {{- \"namespace browser {\\n\\n\" }}\n        {{- \"// Searches for information related to `query` and displays `topn` results.\\n\" }}\n        {{- \"type search = (_: {\\n\" }}\n        {{- \"query: string,\\n\" }}\n        {{- \"topn?: number, // default: 10\\n\" }}\n        {{- \"source?: string,\\n\" }}\n        {{- \"}) => any;\\n\\n\" }}\n        {{- \"// Opens the link `id` from the page indicated by `cursor` starting at line number `loc`, showing `num_lines` lines.\\n\" }}\n        {{- \"// Valid link ids are displayed with the formatting: `„Äê{id}‚Ä†.*„Äë`.\\n\" }}\n        {{- \"// If `cursor` is not provided, the most recent page is implied.\\n\" }}\n        {{- \"// If `id` is a string, it is treated as a fully qualified URL associated with `source`.\\n\" }}\n        {{- \"// If `loc` is not provided, the viewport will be positioned at the beginning of the document or centered on the most relevant passage, if available.\\n\" }}\n        {{- \"// Use this function without `id` to scroll to a new location of an opened page.\\n\" }}\n        {{- \"type open = (_: {\\n\" }}\n        {{- \"id?: number | string, // default: -1\\n\" }}\n        {{- \"cursor?: number, // default: -1\\n\" }}\n        {{- \"loc?: number, // default: -1\\n\" }}\n        {{- \"num_lines?: number, // default: -1\\n\" }}\n        {{- \"view_source?: boolean, // default: false\\n\" }}\n        {{- \"source?: string,\\n\" }}\n        {{- \"}) => any;\\n\\n\" }}\n        {{- \"// Finds exact matches of `pattern` in the current page, or the page given by `cursor`.\\n\" }}\n        {{- \"type find = (_: {\\n\" }}\n        {{- \"pattern: string,\\n\" }}\n        {{- \"cursor?: number, // default: -1\\n\" }}\n        {{- \"}) => any;\\n\\n\" }}\n        {{- \"} // namespace browser\\n\\n\" }}\n    {%- endif -%}\n\n    {%- if python_tool %}\n        {{- \"## python\\n\\n\" }}\n        {{- \"Use this tool to execute Python code in your chain of thought. The code will not be shown to the user. This tool should be used for internal reasoning, but not for code that is intended to be visible to the user (e.g. when creating plots, tables, or files).\\n\\n\" }}\n        {{- \"When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 120.0 seconds. The drive at ''/mnt/data'' can be used to save and persist user files. Internet access for this session is UNKNOWN. Depends on the cluster.\\n\\n\" }}\n    {%- endif -%}\n{%- endmacro -%}\n\n{#- System Message Construction ============================================ #}\n{%- macro build_system_message() -%}\n    {%- if model_identity is not defined %}\n        {%- set model_identity = \"You are ChatGPT, a large language model trained by OpenAI.\" %}\n    {%- endif %}\n    {{- model_identity + \"\\n\" }}\n    {{- \"Knowledge cutoff: 2024-06\\n\" }}\n    {{- \"Current date: \" + strftime_now(\"%Y-%m-%d\") + \"\\n\\n\" }}\n    {%- if reasoning_effort is not defined %}\n        {%- set reasoning_effort = \"medium\" %}\n    {%- endif %}\n    {{- \"Reasoning: \" + reasoning_effort + \"\\n\\n\" }}\n    {%- if builtin_tools %}\n        {{- \"# Tools\\n\\n\" }}\n        {%- set available_builtin_tools = namespace(browser=false, python=false) %}\n        {%- for tool in builtin_tools %}\n            {%- if tool == \"browser\" %}\n                {%- set available_builtin_tools.browser = true %}\n            {%- elif tool == \"python\" %}\n                {%- set available_builtin_tools.python = true %}\n            {%- endif %}\n        {%- endfor %}\n        {{- render_builtin_tools(available_builtin_tools.browser, available_builtin_tools.python) }}\n    {%- endif -%}\n    {{- \"# Valid channels: analysis, commentary, final. Channel must be included for every message.\" }}\n    {%- if tools -%}\n        {{- \"\\nCalls to these tools must go to the commentary channel: ''functions''.\" }}\n    {%- endif -%}\n{%- endmacro -%}\n\n{#- Main Template Logic ================================================= #}\n{#- Set defaults #}\n\n{#- Render system message #}\n{{- \"<|start|>system<|message|>\" }}\n{{- build_system_message() }}\n{{- \"<|end|>\" }}\n\n{#- Extract developer message #}\n{%- if messages[0].role == \"developer\" or messages[0].role == \"system\" %}\n    {%- set developer_message = messages[0].content %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set developer_message = \"\" %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{#- Render developer message #}\n{%- if developer_message or tools %}\n    {{- \"<|start|>developer<|message|>\" }}\n    {%- if developer_message %}\n        {{- \"# Instructions\\n\\n\" }}\n        {{- developer_message }}\n        {{- \"\\n\\n\" }}\n    {%- endif %}\n    {%- if tools -%}\n        {{- \"# Tools\\n\\n\" }}\n        {{- render_tool_namespace(\"functions\", tools) }}\n    {%- endif -%}\n    {{- \"<|end|>\" }}\n{%- endif %}\n\n{#- Render messages #}\n{%- set last_tool_call = namespace(name=none) %}\n{%- for message in loop_messages -%}\n    {#- At this point only assistant/user/tool messages should remain #}\n    {%- if message.role == ''assistant'' -%}\n        {#- Checks to ensure the messages are being passed in the format we expect #}\n        {%- if \"content\" in message %}\n            {%- if \"<|channel|>analysis<|message|>\" in message.content or \"<|channel|>final<|message|>\" in message.content %}\n                {{- raise_exception(\"You have passed a message containing <|channel|> tags in the content field. Instead of doing this, you should pass analysis messages (the string between ''<|message|>'' and ''<|end|>'') in the ''thinking'' field, and final messages (the string between ''<|message|>'' and ''<|end|>'') in the ''content'' field.\") }}\n            {%- endif %}\n        {%- endif %}\n        {%- if \"thinking\" in message %}\n            {%- if \"<|channel|>analysis<|message|>\" in message.thinking or \"<|channel|>final<|message|>\" in message.thinking %}\n                {{- raise_exception(\"You have passed a message containing <|channel|> tags in the thinking field. Instead of doing this, you should pass analysis messages (the string between ''<|message|>'' and ''<|end|>'') in the ''thinking'' field, and final messages (the string between ''<|message|>'' and ''<|end|>'') in the ''content'' field.\") }}\n            {%- endif %}\n        {%- endif %}\n        {%- if \"tool_calls\" in message %}\n            {#- We need very careful handling here - we want to drop the tool call analysis message if the model #}\n            {#- has output a later <|final|> message, but otherwise we want to retain it. This is the only case #}\n            {#- when we render CoT/analysis messages in inference. #}\n            {%- set future_final_message = namespace(found=false) %}\n            {%- for future_message in loop_messages[loop.index:] %}\n                {%- if future_message.role == ''assistant'' and \"tool_calls\" not in future_message %}\n                    {%- set future_final_message.found = true %}\n                {%- endif %}\n            {%- endfor %}\n            {#- We assume max 1 tool call per message, and so we infer the tool call name #}\n            {#- in \"tool\" messages from the most recent assistant tool call name #}\n            {%- set tool_call = message.tool_calls[0] %}\n            {%- if tool_call.function %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {%- if message.content and message.thinking %}\n                {{- raise_exception(\"Cannot pass both content and thinking in an assistant message with tool calls! Put the analysis message in one or the other, but not both.\") }}\n            {%- elif message.content and not future_final_message.found %}\n                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.content + \"<|end|>\" }}\n            {%- elif message.thinking and not future_final_message.found %}\n                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.thinking + \"<|end|>\" }}\n            {%- endif %}\n            {{- \"<|start|>assistant to=\" }}\n            {{- \"functions.\" + tool_call.name + \"<|channel|>commentary \" }}\n            {{- (tool_call.content_type if tool_call.content_type is defined else \"json\") + \"<|message|>\" }}\n            {{- tool_call.arguments|tojson }}\n            {{- \"<|call|>\" }}\n            {%- set last_tool_call.name = tool_call.name %}\n        {%- elif loop.last and not add_generation_prompt %}\n            {#- Only render the CoT if the final turn is an assistant turn and add_generation_prompt is false #}\n            {#- This is a situation that should only occur in training, never in inference. #}\n            {%- if \"thinking\" in message %}\n                {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.thinking + \"<|end|>\" }}\n            {%- endif %}\n            {#- <|return|> indicates the end of generation, but <|end|> does not #}\n            {#- <|return|> should never be an input to the model, but we include it as the final token #}\n            {#- when training, so the model learns to emit it. #}\n            {{- \"<|start|>assistant<|channel|>final<|message|>\" + message.content + \"<|return|>\" }}\n        {%- else %}\n            {#- CoT is dropped during all previous turns, so we never render it for inference #}\n            {{- \"<|start|>assistant<|channel|>final<|message|>\" + message.content + \"<|end|>\" }}\n            {%- set last_tool_call.name = none %}\n        {%- endif %}\n    {%- elif message.role == ''tool'' -%}\n        {%- if last_tool_call.name is none %}\n            {{- raise_exception(\"Message has tool role, but there was no previous assistant message with a tool call!\") }}\n        {%- endif %}\n        {{- \"<|start|>functions.\" + last_tool_call.name }}\n        {{- \" to=assistant<|channel|>commentary<|message|>\" + message.content|tojson + \"<|end|>\" }}\n    {%- elif message.role == ''user'' -%}\n        {{- \"<|start|>user<|message|>\" + message.content + \"<|end|>\" }}\n    {%- endif -%}\n{%- endfor -%}\n\n{#- Generation prompt #}\n{%- if add_generation_prompt -%}\n<|start|>assistant\n{%- endif -%}"}}', '[]', '[{"type":"has_code","target_id":"github:openai:harmony","source_url":"https://github.com/openai/harmony"},{"type":"has_code","target_id":"github:openai:gpt-oss","source_url":"https://github.com/openai/gpt-oss"},{"type":"has_code","target_id":"github:openai:gpt-oss","source_url":"https://github.com/openai/gpt-oss"},{"type":"has_code","target_id":"github:openai:harmony","source_url":"https://github.com/openai/harmony"},{"type":"has_code","target_id":"github:openai:harmony","source_url":"https://github.com/openai/harmony"},{"type":"has_code","target_id":"github:openai:gpt-oss","source_url":"https://github.com/openai/gpt-oss?tab=readme-ov-file#reference-pytorch-implementation"},{"type":"has_code","target_id":"github:openai:gpt-oss","source_url":"https://github.com/openai/gpt-oss"},{"type":"based_on_paper","target_id":"arxiv:2508.10925","source_url":"https://arxiv.org/abs/2508.10925"}]', NULL, 'Apache-2.0', 'approved', 65, '0f49fe5bc76edaa7e021db84f35fe9b6', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mistralai-Mistral-7B-v0.1', 'huggingface--mistralai--mistral-7b-v0.1', 'Mistral-7B-v0.1', 'mistralai', '--- library_name: transformers language: - en license: apache-2.0 tags: - pretrained - mistral-common inference: false extra_gated_description: >- If you want to learn more about how we process your personal data, please read our <a href="https://mistral.ai/terms/">Privacy Policy</a>. --- The Mistral-7B-v0.1 Large Language Model (LLM) is a pretrained generative text model with 7 billion parameters. Mistral-7B-v0.1 outperforms Llama 2 13B on all benchmarks we tested. For full details of this m...', '["transformers","pytorch","safetensors","mistral","text-generation","pretrained","mistral-common","en","arxiv:2310.06825","license:apache-2.0","text-generation-inference","region:us"]', 'text-generation', 4018, 378976, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mistralai/Mistral-7B-v0.1","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlanguage:\n- en\nlicense: apache-2.0\ntags:\n- pretrained\n- mistral-common\ninference: false\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href="https://mistral.ai/terms/">Privacy Policy</a>.\n---\n\n# Model Card for Mistral-7B-v0.1\n\nThe Mistral-7B-v0.1 Large Language Model (LLM) is a pretrained generative text model with 7 billion parameters. \nMistral-7B-v0.1 outperforms Llama 2 13B on all benchmarks we tested.\n\nFor full details of this model please read our [paper](https://arxiv.org/abs/2310.06825) and [release blog post](https://mistral.ai/news/announcing-mistral-7b/).\n\n## Model Architecture\n\nMistral-7B-v0.1 is a transformer model, with the following architecture choices:\n- Grouped-Query Attention\n- Sliding-Window Attention\n- Byte-fallback BPE tokenizer\n\n## Troubleshooting\n\n- If you see the following error:\n```\nKeyError: ''mistral''\n```\n- Or:\n```\nNotImplementedError: Cannot copy out of meta tensor; no data!\n```\n\nEnsure you are utilizing a stable version of Transformers, 4.34.0 or newer.\n\n## Notice\n\nMistral 7B is a pretrained base model and therefore does not have any moderation mechanisms.\n\n## The Mistral AI Team\n \nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, L√©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed.', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7241732096,"storage_bytes":44007786287,"files_count":14,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","eos_token":"</s>","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2310.06825","source_url":"https://arxiv.org/abs/2310.06825"}]', NULL, 'Apache-2.0', 'approved', 50, '6d29495e48551ca8d86709aade9ebd4d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-V3', 'huggingface--deepseek-ai--deepseek-v3', 'DeepSeek-V3', 'deepseek-ai', '--- library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/deepseek-ai/De...', '["transformers","safetensors","deepseek_v3","text-generation","conversational","custom_code","arxiv:2412.19437","text-generation-inference","endpoints_compatible","fp8","region:us"]', 'text-generation', 4004, 720768, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-V3","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\n---\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE" style="margin: 2px;">\n    <img alt="Code License" src="https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL" style="margin: 2px;">\n    <img alt="Model License" src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n\n<p align="center">\n  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf"><b>Paper Link</b>üëÅÔ∏è</a>\n</p>\n\n\n## 1. Introduction\n\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. \nTo achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \nFurthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \nWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \nComprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\nDespite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\nIn addition, its training process is remarkably stable. \nThroughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. \n<p align="center">\n  <img width="80%" src="figures/benchmark.png">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Architecture: Innovative Load Balancing Strategy and Training Objective**\n\n- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.\n-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \n    It can also be used for speculative decoding for inference acceleration. \n\n---\n\n**Pre-Training: Towards Ultimate Training Efficiency**\n\n- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  \n- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  \n  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  \n- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.\n\n---\n\n**Post-Training: Knowledge Distillation from DeepSeek-R1**\n\n-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.\n\n---\n\n\n## 3. Model Downloads\n\n<div align="center">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-V3-Base | 671B | 37B | 128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |\n| DeepSeek-V3   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |\n\n</div>\n\n**NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.**\n\nTo ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: [How_to Run_Locally](#6-how-to-run-locally).\n\nFor developers looking to dive deeper, we recommend exploring [README_WEIGHTS.md](./README_WEIGHTS.md) for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.\n\n## 4. Evaluation Results\n### Base Model\n#### Standard Benchmarks\n\n<div align="center">\n\n\n|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |\n|---|-------------------|----------|--------|-------------|---------------|---------|\n| | Architecture | - | MoE | Dense | Dense | MoE |\n| | # Activated Params | - | 21B | 72B | 405B | 37B |\n| | # Total Params | - | 236B | 72B | 405B | 671B |\n| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |\n| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |\n| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |\n| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |\n| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |\n| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |\n| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |\n| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |\n| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |\n| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |\n| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |\n| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |\n| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |\n| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | **82.7** | **82.9** |\n| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |\n| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |\n| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |\n| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |\n| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |\n| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |\n| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |\n| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |\n| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |\n| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |\n| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |\n| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |\n| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |\n| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |\n| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |\n| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |\n| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |\n| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |\n\n</div>\n\nNote: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\nFor more evaluation details, please check our paper. \n\n#### Context Window\n<p align="center">\n  <img width="80%" src="figures/niah.png">\n</p>\n\nEvaluation results on the ``Needle In A Haystack`` (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to **128K**. \n\n### Chat Model\n#### Standard Benchmarks (Models larger than 67B)\n<div align="center">\n\n| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |\n|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|\n| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |\n| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |\n| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |\n| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |\n| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |\n| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |\n| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |\n| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |\n| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |\n| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |\n| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |\n| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |\n| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |\n| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |\n| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |\n| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |\n| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |\n| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |\n| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |\n| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |\n| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |\n| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |\n| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |\n| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |\n| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |\n\nNote: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.\n\n</div>\n\n\n####  Open Ended Generation Evaluation\n\n<div align="center">\n\n\n\n| Model | Arena-Hard | AlpacaEval 2.0 |\n|-------|------------|----------------|\n| DeepSeek-V2.5-0905 | 76.2 | 50.5 |\n| Qwen2.5-72B-Instruct | 81.2 | 49.1 |\n| LLaMA-3.1 405B | 69.3 | 40.5 |\n| GPT-4o-0513 | 80.4 | 51.1 |\n| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |\n| DeepSeek-V3 | **85.5** | **70.0** |\n\nNote: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-V3 on DeepSeek''s official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\nDeepSeek-V3 can be deployed locally using the following hardware and open-source community software:\n\n1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.\n2. **SGLang**: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.\n3. **LMDeploy**: Enables efficient FP8 and BF16 inference for local and cloud deployment.\n4. **TensorRT-LLM**: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.\n5. **vLLM**: Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.\n6. **AMD GPU**: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.\n7. **Huawei Ascend NPU**: Supports running DeepSeek-V3 on Huawei Ascend devices.\n\nSince FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.\n\nHere is an example of converting FP8 weights to BF16:\n\n```shell\ncd inference\npython fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights\n```\n\n**NOTE: Huggingface''s Transformers has not been directly supported yet.**\n\n### 6.1 Inference with DeepSeek-Infer Demo (example only)\n\n#### Model Weights & Demo Code Preparation\n\nFirst, clone our DeepSeek-V3 GitHub repository:\n\n```shell\ngit clone https://github.com/deepseek-ai/DeepSeek-V3.git\n```\n\nNavigate to the `inference` folder and install dependencies listed in `requirements.txt`.\n\n```shell\ncd DeepSeek-V3/inference\npip install -r requirements.txt\n```\n\nDownload the model weights from HuggingFace, and put them into `/path/to/DeepSeek-V3` folder.\n\n#### Model Weights Conversion\n\nConvert HuggingFace model weights to a specific format:\n\n```shell\npython convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16\n```\n\n#### Run\n\nThen you can chat with DeepSeek-V3:\n\n```shell\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200\n```\n\nOr batch inference on a given file:\n\n```shell\ntorchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE\n```\n\n### 6.2 Inference with SGLang (recommended)\n\n[SGLang](https://github.com/sgl-project/sglang) currently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.\n\nNotably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.\n\nHere are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3\n\n### 6.3 Inference with LMDeploy (recommended)\n[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.\n\nFor comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960\n\n\n### 6.4 Inference with TRT-LLM (recommended)\n\n[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3. \n\n### 6.5 Inference with vLLM (recommended)\n\n[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.\n\n### 6.6 Recommended Inference Functionality with AMD GPUs\n\nIn collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).\n\n### 6.7 Recommended Inference Functionality with Huawei Ascend NPUs\nThe [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the [instructions here](https://modelers.cn/models/MindIE/deepseekv3).\n\n\n## 7. License\nThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.\n\n## 8. Citation\n```\n@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":684531386000,"storage_bytes":688727648088,"files_count":185,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["DeepseekV3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_deepseek.DeepseekV3Config","AutoModel":"modeling_deepseek.DeepseekV3Model","AutoModelForCausalLM":"modeling_deepseek.DeepseekV3ForCausalLM"},"model_type":"deepseek_v3","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''', is_first_sp=true) %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message[''content''] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + ''\n\n'' + message[''content''] %}{%- endif %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{{''<ÔΩúUserÔΩú>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is none %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls'']%}{%- if not ns.is_first %}{{''<ÔΩúAssistantÔΩú><ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- set ns.is_first = true -%}{%- else %}{{''\n'' + ''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endfor %}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is not none %}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'' + message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{{''<ÔΩúAssistantÔΩú>'' + message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- set ns.is_output_first = false %}{%- else %}{{''\n<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{''<ÔΩúAssistantÔΩú>''}}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3.git","source_url":"https://github.com/deepseek-ai/DeepSeek-V3.git"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:InternLM:lmdeploy","source_url":"https://github.com/InternLM/lmdeploy"},{"type":"has_code","target_id":"github:InternLM:lmdeploy","source_url":"https://github.com/InternLM/lmdeploy"},{"type":"has_code","target_id":"github:NVIDIA:TensorRT-LLM","source_url":"https://github.com/NVIDIA/TensorRT-LLM"},{"type":"has_code","target_id":"github:NVIDIA:TensorRT-LLM","source_url":"https://github.com/NVIDIA/TensorRT-LLM"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"based_on_paper","target_id":"arxiv:2412.19437","source_url":"https://arxiv.org/abs/2412.19437"}]', NULL, NULL, 'pending', 90, '4e923dbeae0fa21ffcc15aa2d568954f', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-V3/resolve/main/figures/benchmark.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-V3 from https://huggingface.co/deepseek-ai/DeepSeek-V3/resolve/main/figures/benchmark.png
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-V3.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-lllyasviel-ControlNet-v1-1', 'huggingface--lllyasviel--controlnet-v1-1', 'ControlNet-v1-1', 'lllyasviel', '--- license: openrail --- This is the model files for ControlNet 1.1. This model card will be filled in a more detailed way after 1.1 is officially merged into ControlNet.', '["license:openrail","region:us"]', 'other', 3961, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/lllyasviel/ControlNet-v1-1","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: openrail\n---\n\nThis is the model files for [ControlNet 1.1](https://github.com/lllyasviel/ControlNet-v1-1-nightly).\nThis model card will be filled in a more detailed way after 1.1 is officially merged into ControlNet.\n', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":28087340418,"files_count":30,"spaces_count":60,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:lllyasviel:ControlNet-v1-1-nightly","source_url":"https://github.com/lllyasviel/ControlNet-v1-1-nightly"}]', NULL, 'OpenRAIL', 'approved', 40, '79897a95c1d1aa240e9f72650fdc17ad', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-WarriorMama777-OrangeMixs', 'huggingface--warriormama777--orangemixs', 'OrangeMixs', 'WarriorMama777', '--- license: creativeml-openrail-m tags: - stable-diffusion - text-to-image datasets: Nerfgun3/bad_prompt --- ---- "OrangeMixs" shares various Merge models that can be used with StableDiffusionWebui:Automatic1111 and others. &nbsp; <img src="https://i.imgur.com/VZg0LqQ.png" width="1000" height=""> Maintain a repository for the following purposes. 1. to provide easy access to models commonly used in the Japanese community.The Wisdom of the Anonsüíé 2. As a place to upload my merge models when I...', '["diffusers","stable-diffusion","text-to-image","dataset:nerfgun3/bad_prompt","license:creativeml-openrail-m","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 3886, 8305, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/WarriorMama777/OrangeMixs","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- text-to-image\ndatasets: Nerfgun3/bad_prompt\n---\n\n\n----\n\n# OrangeMixs\n\n"OrangeMixs" shares various Merge models that can be used with StableDiffusionWebui:Automatic1111 and others.\n&nbsp;\n<img src="https://i.imgur.com/VZg0LqQ.png"  width="1000" height="">\n\nMaintain a repository for the following purposes.\n\n1. to provide easy access to models commonly used in the Japanese community.The Wisdom of the Anonsüíé\n2. As a place to upload my merge models when I feel like it.\n\n\n\n![](https://github.com/WarriorMama777/imgup/raw/main/img/img_general/img_orangemixs_infograph_4_comp001.webp "image_orangemixs_infographics_03")\n<span style="font-size: 60%;">Hero image prompts(AOM3B2):https://majinai.art/ja/i/jhw20Z_</span>\n\n----\n\n# UPDATE NOTE / How to read this README\n\n## How to read this README\n\n1. Read the ToC as release notes.  \nSections are in descending order. The order within the section is ascending. It is written like SNS.\n2. UPDATE NOTE\n3. View the repository history when you need to check the full history.\n\n## UPDATE NOTE\n- 2023-02-27: Add AOM3A1B\n- 2023-03-10: Model name fix\nI found that I abbreviated the model name too much, so that when users see illustrations using OrangeMixs models on the web, they cannot reach them in their searches.\nTo make the specification more search engine friendly, I renamed it to "ModelName + (orangemixs)".\n- 2023-03-11: Change model name : () to _\nChanged to _ because an error occurs when using () in the Cloud environment(e.g.:paperspace).\n"ModelName + _orangemixs"\n- 2023-04-01: Added description of AOM3A1 cursed by Dreamlike\n- 2023-06-27: Added AOM3B2. Removed Terms of Service.\n- 2023-11-25: Add VividOrangeMix (nonlabel, NSFW, Hard)\n- 2023-06-27: Added AOM3B2. Removed Terms of Service.\n- 2023-11-25: Add VividOrangeMix (nonlabel, NSFW, Hard)\n- 2024-01-07: Fix repo & Done upload VividOrangeMixs\n\n----\n\n# Gradio\n\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI to run OrangeMixs:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/webui-orangemixs)\n\n----\n\n# Table of Contents\n\n- [OrangeMixs](#orangemixs)\n- [UPDATE NOTE / How to read this README](#update-note--how-to-read-this-readme)\n  - [How to read this README](#how-to-read-this-readme)\n  - [UPDATE NOTE](#update-note)\n- [Gradio](#gradio)\n- [Table of Contents](#table-of-contents)\n- [Reference](#reference)\n- [Licence](#licence)\n- [~~Terms of use~~](#terms-of-use)\n- [Disclaimer](#disclaimer)\n- [How to download](#how-to-download)\n  - [Batch Download](#batch-download)\n  - [Batch Download (Advanced)](#batch-download-advanced)\n  - [Select and download](#select-and-download)\n- [Model Detail \& Merge Recipes](#model-detail--merge-recipes)\n  - [VividOrangeMix (VOM)](#vividorangemix-vom)\n    - [VividOrangeMix](#vividorangemix)\n    - [VividOrangeMix\_NSFW / Hard](#vividorangemix_nsfw--hard)\n    - [Instructions](#instructions)\n  - [AbyssOrangeMix3 (AOM3)](#abyssorangemix3-aom3)\n    - [About](#about)\n    - [More feature](#more-feature)\n    - [Variations / Sample Gallery](#variations--sample-gallery)\n      - [AOM3](#aom3)\n      - [AOM3A1](#aom3a1)\n      - [AOM3A2](#aom3a2)\n      - [AOM3A3](#aom3a3)\n      - [AOM3A1B](#aom3a1b)\n      - [AOM3B2](#aom3b2)\n      - [AOM3B3](#aom3b3)\n      - [AOM3B4](#aom3b4)\n      - [AOM3B3](#aom3b3-1)\n      - [AOM3B4](#aom3b4-1)\n    - [Description for enthusiast](#description-for-enthusiast)\n  - [AbyssOrangeMix2 (AOM2)](#abyssorangemix2-aom2)\n    - [AbyssOrangeMix2\_sfw (AOM2s)](#abyssorangemix2_sfw-aom2s)\n    - [AbyssOrangeMix2\_nsfw (AOM2n)](#abyssorangemix2_nsfw-aom2n)\n    - [AbyssOrangeMix2\_hard (AOM2h)](#abyssorangemix2_hard-aom2h)\n  - [EerieOrangeMix (EOM)](#eerieorangemix-eom)\n    - [EerieOrangeMix (EOM1)](#eerieorangemix-eom1)\n      - [EerieOrangeMix\_base (EOM1b)](#eerieorangemix_base-eom1b)\n      - [EerieOrangeMix\_Night (EOM1n)](#eerieorangemix_night-eom1n)\n      - [EerieOrangeMix\_half (EOM1h)](#eerieorangemix_half-eom1h)\n      - [EerieOrangeMix (EOM1)](#eerieorangemix-eom1-1)\n    - [EerieOrangeMix2 (EOM2)](#eerieorangemix2-eom2)\n      - [EerieOrangeMix2\_base (EOM2b)](#eerieorangemix2_base-eom2b)\n      - [EerieOrangeMix2\_night (EOM2n)](#eerieorangemix2_night-eom2n)\n      - [EerieOrangeMix2\_half (EOM2h)](#eerieorangemix2_half-eom2h)\n      - [EerieOrangeMix2 (EOM2)](#eerieorangemix2-eom2-1)\n    - [Models Comparison](#models-comparison)\n  - [AbyssOrangeMix (AOM)](#abyssorangemix-aom)\n    - [AbyssOrangeMix\_base (AOMb)](#abyssorangemix_base-aomb)\n    - [AbyssOrangeMix\_Night (AOMn)](#abyssorangemix_night-aomn)\n    - [AbyssOrangeMix\_half (AOMh)](#abyssorangemix_half-aomh)\n    - [AbyssOrangeMix (AOM)](#abyssorangemix-aom-1)\n  - [ElyOrangeMix (ELOM)](#elyorangemix-elom)\n    - [ElyOrangeMix (ELOM)](#elyorangemix-elom-1)\n    - [ElyOrangeMix\_half (ELOMh)](#elyorangemix_half-elomh)\n    - [ElyNightOrangeMix (ELOMn)](#elynightorangemix-elomn)\n  - [BloodOrangeMix (BOM)](#bloodorangemix-bom)\n    - [BloodOrangeMix (BOM)](#bloodorangemix-bom-1)\n    - [BloodOrangeMix\_half (BOMh)](#bloodorangemix_half-bomh)\n    - [BloodNightOrangeMix (BOMn)](#bloodnightorangemix-bomn)\n  - [ElderOrangeMix](#elderorangemix)\n  - [Troubleshooting](#troubleshooting)\n  - [FAQ and Tips (üêàMEME ZONEü¶ê)](#faq-and-tips-meme-zone)\n\n\n\n----\n\n# Reference\n\n+/hdg/ Stable Diffusion Models Cookbook - <https://rentry.org/hdgrecipes#g-anons-unnamed-mix-e93c3bf7>\nModel names are named after Cookbook precedentsüçä\n\n# Licence\n\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage. The CreativeML OpenRAIL License specifies: \n1. You can''t use the model to deliberately produce nor share illegal or harmful outputs or content\n2. The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully) Please read the full license here Ôºöhttps://huggingface.co/spaces/CompVis/stable-diffusion-license\n\n# ~~Terms of use~~\n\n~~- **Clearly indicate where modifications have been made.**  \nIf you used it for merging, please state what steps you took to do so.~~\n\nRemoved terms of use. 2023-06-28  \nFreedom. If you share your recipes, Marge swamp will be fun.\n\n# Disclaimer\n\n<details><summary>READ MORE: Disclaimer</summary>\nThe user has complete control over whether or not to generate NSFW content, and the user''s decision to enjoy either SFW or NSFW is entirely up to the user.The learning model does not contain any obscene visual content that can be viewed with a single click.The posting of the Learning Model is not intended to display obscene material in a public place.  \nIn publishing examples of the generation of copyrighted characters, I consider the following cases to be exceptional cases in which unauthorised use is permitted. \n"when the use is for private use or research purposes; when the work is used as material for merchandising (however, this does not apply when the main use of the work is to be merchandised); when the work is used in criticism, commentary or news reporting; when the work is used as a parody or derivative work to demonstrate originality."\nIn these cases, use against the will of the copyright holder or use for unjustified gain should still be avoided, and if a complaint is lodged by the copyright holder, it is guaranteed that the publication will be stopped as soon as possible.  \nI would also like to note that I am aware of the fact that many of the merged models use NAI, which is learned from Danbooru and other sites that could be interpreted as illegal, and whose model data itself is also a leak, and that this should be watched carefully. I believe that the best we can do is to expand the possibilities of GenerativeAI while protecting the works of illustrators and artists.  \n</details>\n\n\n----\n\n# How to download\n\n## Batch Download\n\n‚ö†Deprecated: Orange has grown too huge. Doing this will kill your storage.\n\n1. install Git\n2. create a folder of your choice and right click ‚Üí "Git bash here" and open a gitbash on the folder''s directory.\n3. run the following commands in order.\n\n```\ngit lfs install\ngit clone https://huggingface.co/WarriorMama777/OrangeMixs\n```\n\n4. complete\n\n\n## Batch Download (Advanced)\n\nAdvanced: (When you want to download only selected directories, not the entire repository.)\n&nbsp;\n<details>\n<summary>Toggle: How to Batch Download (Advanced)</summary>\n\n1. Run the command `git clone --filter=tree:0 --no-checkout https://huggingface.co/WarriorMama777/OrangeMixs` to clone the huggingface repository. By adding the `--filter=tree:0` and `--no-checkout` options, you can download only the file names without their contents.\n```\ngit clone --filter=tree:0 --no-checkout https://huggingface.co/WarriorMama777/OrangeMixs\n```\n\n2. Move to the cloned directory with the command `cd OrangeMixs`.\n```\ncd OrangeMixs\n```\n\n3. Enable sparse-checkout mode with the command `git sparse-checkout init --cone`. By adding the `--cone` option, you can achieve faster performance.\n```\ngit sparse-checkout init --cone\n```\n\n4. Specify the directory you want to get with the command `git sparse-checkout add <directory name>`. For example, if you want to get only the `Models/AbyssOrangeMix3` directory, enter `git sparse-checkout add Models/AbyssOrangeMix3`.\n```\ngit sparse-checkout add Models/AbyssOrangeMix3\n```\n\n5. Download the contents of the specified directory with the command `git checkout main`.\n```\ngit checkout main\n```\n\nThis completes how to clone only a specific directory. If you want to add other directories, run `git sparse-checkout add <directory name>` again.\n\n\n</details>\n\n\n\n## Select and download\n\n1. Go to the Files and vaersions tab.\n2. select the model you want to download\n3. download\n4. complete\n\n----\n\n\n\n----\n\n# Model Detail & Merge Recipes\n\n<a name="VOM"></a>\n\n## VividOrangeMix (VOM)\n\n![](https://github.com/WarriorMama777/imgup/raw/main/img/VOM/VOM_heroimage_02_comp002.webp "VividOrangeMix")\nPrompt: https://majinai.art/ja/i/VZ9dNoI\n\nCivitai: https://civitai.com/models/196585?modelVersionId=221033\n\n2023-11-25\n\n### VividOrangeMix\n\n‚ñºAbout\n"VividOrangeMix is a StableDiffusion model created for fans seeking vivid, flat, anime-style illustrations. With rich, bold colors and flat shading, it embodies the style seen in anime and manga.‚Äù\nOne of the versions of OrangeMixs, AbyssOrangeMix1~3 (AOM), has improved the anatomical accuracy of the human body by merging photorealistic models, but I was dissatisfied with the too-realistic shapes and shadows.  \nVividOrangeMix is a model that has been adjusted to solve this problem.  \n\n‚ñºSample Gallery\nDefault\n![](https://github.com/WarriorMama777/imgup/raw/main/img/VOM/2023-11-14_VividOrangeMixSample_default_big_v2.1.webp "VividOrangeMixSampleGallery_default")\nLoRA\n![](https://github.com/WarriorMama777/imgup/raw/main/img/VOM/2023-11-14_VividOrangeMixSample_LoRA_med_v2.webp "VividOrangeMixSampleGallery_LoRA")\n\n\n### VividOrangeMix_NSFW / Hard\n\n‚ñºAbout\nVividOrangeMix NSFW/Hard is, as before, a model that Merges elements of NAI and Gape by U-Net Blocks Weight method.\nAs of AOM3, elements of these models should be included, but when I simply merged other models, the elements of the old merge seem to gradually fade away. Also, by merging U-Net Blocks Weight, it is now possible to merge without affecting the design to some extent, but some changes are unavoidable, so I decided to upload it separately as before. .\n\n‚ñºSample Gallery\n\n‚ÜêNSFW | Hard‚Üí\n![](https://github.com/WarriorMama777/imgup/raw/main/img/VOM/2023-11-27_VividOrangeMixSample_NSFWandHard.webp "VividOrangeMixSampleGallery_LoRA")\n\n\n___\n### Instructions\n\n‚ñºTool\n- https://github.com/hako-mikan/sd-webui-supermerger/  \n\n___\n\n‚ñºVividOrangeMix\n\nSTEP: 1 | Base model create\n\n[GO TO AOM3B4 Instructions‚Üì](#AOM3B4)\n\nSTEP: 2 | Model merge\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3B4 | Animelike_2D_Pruend_fp16 |  | sum @ 0.3 |  | VividOrangeMix |\n\n___\n\n‚ñºVividOrangeMix_NSFW\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| VividOrangeMix | NAI full | NAI sfw | Add Difference @ 1.0 | 0,0.25,0.25,0.25,0.25,0.25,0,0,0,0,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.2,0.25,0.25,0.25,0.25,0,0 | VividOrangeMix_NSFW |\n\n___\n\n‚ñºVividOrangeMix_Hard\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| VividOrangeMix_NSFW | gape60 | NAI full | Add Difference @ 1.0 | 0.0,0.25,0.25,0.25,0.25,0.25,0.0,0.0,0.0,0.0,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.25,0.0,0.0 | VividOrangeMix_Hard |\n\n____\n\n## AbyssOrangeMix3 (AOM3)\n\n![](https://github.com/WarriorMama777/imgup/raw/main/img/AOM3/AOM3_G_Top_comp001.webp "")\n\n‚Äï‚ÄïEveryone has different ‚ÄúABYSS‚Äù!\n\n‚ñºAbout\n\nThe main model, "AOM3 (AbyssOrangeMix3)", is a purely upgraded model that improves on the problems of the previous version, "AOM2". "AOM3" can generate illustrations with very realistic textures and can generate a wide variety of content. There are also three variant models based on the AOM3 that have been adjusted to a unique illustration style. These models will help you to express your ideas more clearly.\n\n‚ñºLinks\n\n- [‚ö†NSFW] Civitai: AbyssOrangeMix3 (AOM3) | Stable Diffusion Checkpoint | https://civitai.com/models/9942/abyssorangemix3-aom3\n\n\n### About\n\nFeatures: high-quality, realistic textured illustrations can be generated.  \nThere are two major changes from AOM2.\n\n1: Models for NSFW such as _nsfw and _hard have been improved: the models after nsfw in AOM2 generated creepy realistic faces, muscles and ribs when using Hires.fix, even though they were animated characters. These have all been improved in AOM3.\n\ne.g.: explanatory diagram by MEME : [GO TO MEME ZONE‚Üì](#MEME_realface)\n\n2: sfw/nsfw merged into one model. Originally, nsfw models were separated because adding NSFW content (models like NAI and gape) would change the face and cause the aforementioned problems. Now that those have been improved, the models can be packed into one.  \nIn addition, thanks to excellent extensions such as [ModelToolkit](https://github.com/arenatemp/stable-diffusion-webui-model-toolkit\n), the model file size could be reduced (1.98 GB per model).\n\n\n![](https://github.com/WarriorMama777/imgup/raw/main/img/AOM3/AOM3_G_Full_2_comp002.webp "")\n\n\n### More feature\nIn addition, these U-Net Blocks Weight Merge models take numerous steps but are carefully merged to ensure that mutual content is not overwritten.  \n\n(Of course, all models allow full control over adult content.)\n- üîê When generating illustrations for the general public: write "nsfw" in the negative prompt field\n- üîû ~~When generating adult illustrations: "nsfw" in the positive prompt field~~ -> It can be generated without putting it in. If you include it, the atmosphere will be more NSFW.\n\n### Variations / Sample Gallery\nüößEditingüöß\n\n![](https://github.com/WarriorMama777/imgup/raw/main/img/AOM3/AOM3_G_Art_comp003.webp "")\n\n\n#### AOM3 \n\n\n\n\n\n‚ñºAOM3\n![](https://github.com/WarriorMama777/imgup/raw/2c840982550fab41f45ba4b5aedbd3d84ddf2390/img/AOM3/img_sanmples_AOM3_01_comp001.webp "OrangeMixs_img_sanmples_AOM3_01_comp001")\n\n<span style="font-size: 60%;">(Actually, this gallery doesn''t make much sense since AOM3 is mainly an improvement of the NSFW part üòÇ  ...But we can confirm that the picture is not much different from AOM2sfw.)</span>\n\n#### AOM3A1\n\n‚õîOnly this model (AOM3A1) includes ChilloutMix. The curse of the DreamLike license. In other words, only AOM3A1 is not available for commercial use. I recommend AOM3A1B instead.‚õî\n[GO TO MEME ZONE‚Üì](#MEME_AOM3A1)\n\nFeatures: Anime like illustrations with flat paint. Cute enough as it is, but I really like to apply LoRA of anime characters to this model to generate high quality anime illustrations like a frame from a theatre version.\n\n‚ñºA1\n\n![](https://github.com/WarriorMama777/imgup/raw/33d21cd31e35ae6b7593e7f6dd913f5f71ddef4e/img/AOM3/img_sanmples_AOMA1_3.0_comp001.webp "OrangeMixs_img_sanmples_AOMA1_3.0_comp001")\n\n\n<details>\n<summary>¬©</summary>\n(1)¬©Yurucamp: Inuyama Aoi, (2)¬©The Quintessential Quintuplets: Nakano Yotsuba, (3)¬©Sailor Moon: Mizuno Ami/SailorMercury\n</details>\n\n#### AOM3A2\nüößEditingüöß\nFeatures: Oil paintings like style artistic illustrations and stylish background depictions. In fact, this is mostly due to the work of Counterfeit 2.5, but the textures are more realistic thanks to the U-Net Blocks Weight Merge.\n\n#### AOM3A3\nüößEditingüöß\nFeatures: Midpoint of artistic and kawaii. the model has been tuned to combine realistic textures, a artistic style that also feels like an oil colour style, and a cute anime-style face. Can be used to create a wide range of illustrations.\n\n#### AOM3A1B\n\nAOM3A1B added. This model is my latest favorite. I recommend it for its moderate realism, moderate brush touch, and moderate LoRA conformity.  \nThe model was merged by mistakenly selecting ''Add sum'' when ''Add differences'' should have been selected in the ~~AOM3A3~~AOM3A2 recipe. It was an unintended merge, but we share it because the illustrations produced are consistently good results.  \nThe model was merged by mistakenly selecting ''Add sum'' when ''Add differences'' should have been selected in the ~~AOM3A3~~AOM3A2 recipe. It was an unintended merge, but we share it because the illustrations produced are consistently good results.  \nIn my review, this is an illustration style somewhere between AOM3A1 and A3.\n\n‚ñºA1B\n\n![](https://github.com/WarriorMama777/imgup/raw/c66097319405d5373fab1cebec03c5c71427879c/img/AOM3/img_AOM3A1B_01_comp001.webp "orangemix_img_AOM3A1B_01_comp001.webp")  \n![](https://github.com/WarriorMama777/imgup/raw/3e060893c0fb2c80c6f3aedf63bf8d576c9a37fc/img/AOM3/img_samples_AOM3A1B_01_comp001.webp "orangemix_img_samples_AOM3A1B_01_comp001.webp")  \n- Meisho Doto (umamusume): https://civitai.com/models/11980/meisho-doto-umamusume\n- Train and Girl: [JR East E235 series / train interior](https://civitai.com/models/9517/jr-east-e235-series-train-interior) \n\n<details>\n<summary>¬©</summary>\n¬©umamusume: Meisho Doto, ¬©Girls und Panzer: Nishizumi Miho,¬©IDOLM@STER: Sagisawa Fumika\n</details>\n\n#### AOM3B2\nmy newest toy. \nJust AOM3A1B + BreakdomainM21: 0.4  \nSo this model is somewhat of a troll model.\nI would like to create an improved DiffLoRAKit_v2 based on this.  \nUpload for access for research etc. 2023-06-27  \n\n![AOM3B2_orangemixs_sampleGallery](https://github.com/WarriorMama777/imgup/raw/main/img/AOM3/img_sanmples_AOM3B2_02_comp001.webp "AOM3B2_orangemixs_sampleGallery")\n\n<details><summary>Sample image prompts</summary>\n\n1. [Maid](https://majinai.art/ja/i/jhw20Z_)\n2. Yotsuba: https://majinai.art/ja/i/f-O4wau\n3. Inuko in cafe: https://majinai.art/ja/i/Cj-Ar9C\n4. bathroom: https://majinai.art/ja/i/XiSj5K6\n\n</details>\n\n&nbsp;\n\n\n#### AOM3B3\n\n2023-09-25\n\nThis is a derivative model of AOM3B2.\nI merged some nice models and also merged some LoRAs to further adjust the color and painting style.\n\n\n\n‚óÜ**Instructions:**\n\n‚ñºTool\nSupermerger\n\n‚ñºModel Merge\nAOM3B2+Mixprov4+BreakdomainAnime\n triple sum : 0.3, 0.3 | mode:normal\n\nÔºã\n\n‚ñºLoRA Merge\nloraH(DiffLoRA)_FaceShadowTweaker_v1_dim4:-2,nijipretty_20230624235607:0.1,MatureFemale_epoch8:0.1,colorful_V1_lbw:0.5\n\n\n#### AOM3B4\n<a name="AOM3B4"></a>\n‚ñºAbout\nFix AOM3B3\n\n‚ñº**Instructions:**\n\n\nUSE: https://github.com/hako-mikan/sd-webui-supermerger/  \n\nSTEP: 1 | Model merge\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3B2 | Mixprov4 | BreakdomainAnime | triple sum @ 0.3, 0.3, mode:normal |  | temp01 |\n\nSTEP: 2 | LoRA Merge\n\nColor fix\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| temp01 | colorful_V1_lbw |  | sum @ 0.45 |  | AOM3B4 |\n\n\n‚öì[GO TO VividOrangeMix Instructions‚Üë](#VOM)\n\n\n#### AOM3B3\n\n2023-09-25\n\nThis is a derivative model of AOM3B2.\nI merged some nice models and also merged some LoRAs to further adjust the color and painting style.\n\n\n\n‚óÜ**Instructions:**\n\n‚ñºTool\nSupermerger\n\n‚ñºModel Merge\nAOM3B2+Mixprov4+BreakdomainAnime\n triple sum : 0.3, 0.3 | mode:normal\n\nÔºã\n\n‚ñºLoRA Merge\nloraH(DiffLoRA)_FaceShadowTweaker_v1_dim4:-2,nijipretty_20230624235607:0.1,MatureFemale_epoch8:0.1,colorful_V1_lbw:0.5\n\n\n#### AOM3B4\n<a name="AOM3B4"></a>\n‚ñºAbout\nFix AOM3B3\n\n‚ñº**Instructions:**\n\n\nUSE: https://github.com/hako-mikan/sd-webui-supermerger/  \n\nSTEP: 1 | Model merge\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3B2 | Mixprov4 | BreakdomainAnime | triple sum @ 0.3, 0.3, mode:normal |  | temp01 |\n\nSTEP: 2 | LoRA Merge\n\nColor fix\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| temp01 | colorful_V1_lbw |  | sum @ 0.45 |  | AOM3B4 |\n\n\n‚öì[GO TO VividOrangeMix Instructions‚Üë](#VOM)\n____\n### Description for enthusiast\n\nAOM3 was created with a focus on improving the nsfw version of AOM2, as mentioned above.The AOM3 is a merge of the following two models into AOM2sfw using U-Net Blocks Weight Merge, while extracting only the NSFW content part.  \n(1) NAI: trained in Danbooru  \n(2)gape: Finetune model of NAI trained on Danbooru''s very hardcore NSFW content.  \nIn other words, if you are looking for something like AOM3sfw, it is AOM2sfw.The AOM3 was merged with the NSFW model while removing only the layers that have a negative impact on the face and body.   However, the faces and compositions are not an exact match to AOM2sfw.AOM2sfw is sometimes superior when generating SFW content. I recommend choosing according to the intended use of the illustration.See below for a comparison between AOM2sfw and AOM3.\n\n![](https://github.com/WarriorMama777/imgup/raw/main/img/AOM3/img_modelComparison_AOM_comp001.webp "modelComparison_AOM")\n\n\n‚ñºA summary of the AOM3 work is as follows\n\n1. investigated the impact of the NAI and gape layers as AOM2 _nsfw onwards is crap.  \n2. cut face layer: OUT04 because I want realistic faces to stop ‚Üí Failed. No change.  \n3. gapeNAI layer investigationÔΩú  \n  a. (IN05-08 (especially IN07) | Change the illustration   significantly. Noise is applied, natural colours are lost, shadows die, and we can see that the IN deep layer is a layer of light and shade.  \n  b. OUT03-05(?) | likely to be sexual section/NSFW layer.Cutting here will kill the NSFW.  \n  c. OUT03,OUT04ÔΩúNSFW effects are in(?). e.g.: spoken hearts, trembling, motion lines, etc...  \n  d. OUT05ÔΩúThis is really an NSFW switch. All the "NSFW atmosphere" is in here. Facial expressions, Heavy breaths, etc...  \n  e. OUT10-11ÔΩúPaint layer. Does not affect detail, but does have an extensive impact.  \n1. (mass production of rubbish from here...)   \n2. cut IN05-08 and merge NAIgape with flat parameters ‚Üí avoided creepy muscles and real faces. Also, merging NSFW models stronger has less impact.  \n3. so, cut IN05-08, OUT10-11 and merge NAI+gape with all others 0.5.  \n4. ‚Üí AOM3  \nAOM3 roughly looks like this  \n\n\n\n----\n\n‚ñºHow to use\n\n- Prompts\n    - Negative prompts is As simple as possible is good.  \n    (worst quality, low quality:1.4)\n    - Using "3D" as a negative will result in a rough sketch style at the "sketch" level. Use with caution as it is a very strong prompt.\n    - How to avoid Real Face  \n    (realistic, lip, nose, tooth, rouge, lipstick, eyeshadow:1.0), (abs, muscular, rib:1.0),\n    - How to avoid Bokeh  \n    (depth of field, bokeh, blurry:1.4)\n    - How to remove mosaic: `(censored, mosaic censoring, bar censor, convenient censoring, pointless censoring:1.0),`\n    - How to remove blush: `(blush, embarrassed, nose blush, light blush, full-face blush:1.4), `\n    - How to remove NSFW effects: `(trembling, motion lines, motion blur, emphasis lines:1.2),`\n    - üî∞Basic negative prompts sample for Anime girl ‚Üì  \n      - v1  \n    `nsfw, (worst quality, low quality:1.4), (realistic, lip, nose, tooth, rouge, lipstick, eyeshadow:1.0), (dusty sunbeams:1.0),, (abs, muscular, rib:1.0), (depth of field, bokeh, blurry:1.4),(motion lines, motion blur:1.4), (greyscale, monochrome:1.0), text, title, logo, signature`\n      - v2  \n    `nsfw, (worst quality, low quality:1.4), (lip, nose, tooth, rouge, lipstick, eyeshadow:1.4), (blush:1.2), (jpeg artifacts:1.4), (depth of field, bokeh, blurry, film grain, chromatic aberration, lens flare:1.0), (1boy, abs, muscular, rib:1.0), greyscale, monochrome, dusty sunbeams,  trembling, motion lines, motion blur, emphasis lines, text, title, logo, signature, `\n- Sampler: ~~‚ÄúDPM++ SDE Karras‚Äù is good~~ Take your pick  \n- Steps: \n  - DPM++ SDE Karras: Test: 12ÔΩû ,illustration: 20ÔΩû  \n  - DPM++ 2M Karras: Test: 20ÔΩû ,illustration: 28ÔΩû  \n- Clipskip: 1 or 2  \n- CFG: 8 (6ÔΩû12)\n- Upscaler :  \n    - Detailed illust ‚Üí Latenet (nearest-exact)  \n    Denoise strength: 0.5 (0.5~0.6)  \n    - Simple upscale: Swin IR, ESRGAN, Remacri etc‚Ä¶  \n    Denoise strength: Can be set low. (0.35~0.6)  \n\n\n\n---\n\nüë©‚Äçüç≥Model details / Recipe\n\n‚ñºHash(SHA256)\n‚ñºHash(SHA256)\n\n- AOM3.safetensors  \nD124FC18F0232D7F0A2A70358CDB1288AF9E1EE8596200F50F0936BE59514F6D\n- AOM3A1.safetensors  \nF303D108122DDD43A34C160BD46DBB08CB0E088E979ACDA0BF168A7A1F5820E0\n- AOM3A2.safetensors  \n553398964F9277A104DA840A930794AC5634FC442E6791E5D7E72B82B3BB88C3\n- AOM3A3.safetensors  \nEB4099BA9CD5E69AB526FCA22A2E967F286F8512D9509B735C892FA6468767CF\n- AOM3A1B.safetensors\n5493A0EC491F5961DBDC1C861404088A6AE9BD4007F6A3A7C5DEE8789CDC1361\n- AOM3B2.safetensors\nF553E7BDE46CFE9B3EF1F31998703A640AF7C047B65883996E44AC7156F8C1DB\n\n- AOM3A1B.safetensors\n5493A0EC491F5961DBDC1C861404088A6AE9BD4007F6A3A7C5DEE8789CDC1361\n- AOM3B2.safetensors\nF553E7BDE46CFE9B3EF1F31998703A640AF7C047B65883996E44AC7156F8C1DB\n\n\n‚ñºUse Models\n\n1. AOM2sfw  \n„Äå038ba203d8ba3c8af24f14e01fbb870c85bbb8d4b6d9520804828f4193d12ce9„Äç\n1. AnythingV3.0 huggingface pruned  \n[2700c435]„Äå543bcbc21294831c6245cd74c8a7707761e28812c690f946cb81fef930d54b5e„Äç\n1. NovelAI animefull-final-pruned  \n[925997e9]„Äå89d59c3dde4c56c6d5c41da34cc55ce479d93b4007046980934b14db71bdb2a8„Äç\n1. NovelAI sfw  \n[1d4a34af]„Äå22fa233c2dfd7748d534be603345cb9abf994a23244dfdfc1013f4f90322feca„Äç\n1. Gape60  \n[25396b85]„Äå893cca5903ccd0519876f58f4bc188dd8fcc5beb8a69c1a3f1a5fe314bb573f5„Äç\n1. BasilMix  \n„Äåbbf07e3a1c3482c138d096f7dcdb4581a2aa573b74a68ba0906c7b657942f1c2„Äç\n1. chilloutmix_fp16.safetensors  \n„Äå4b3bf0860b7f372481d0b6ac306fed43b0635caf8aa788e28b32377675ce7630„Äç\n1. Counterfeit-V2.5_fp16.safetensors  \n„Äå71e703a0fca0e284dd9868bca3ce63c64084db1f0d68835f0a31e1f4e5b7cca6„Äç\n1. kenshi_01_fp16.safetensors  \n„Äå3b3982f3aaeaa8af3639a19001067905e146179b6cddf2e3b34a474a0acae7fa„Äç\n\n----\n\n‚ñºAOM3\n\n‚óÜ**Instructions:**\n‚óÜ**Instructions:**\n\nTool: SuperMerger\n\nUSE: https://github.com/hako-mikan/sd-webui-supermerger/  \nTool: SuperMerger\n\nUSE: https://github.com/hako-mikan/sd-webui-supermerger/  \n\n(This extension is really great. It turns a month''s work into an hour. Thank you)\n\nSTEP: 1 | BWM : NAI - NAIsfw & gape - NAI\n\nCUT: IN05-IN08, OUT10-11\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM2sfw | NAI full | NAI sfw | Add Difference @ 1.0 | 0,0.5,0.5,0.5,0.5,0.5,0,0,0,0,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0,0 | temp01 |\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM2sfw | NAI full | NAI sfw | Add Difference @ 1.0 | 0,0.5,0.5,0.5,0.5,0.5,0,0,0,0,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0,0 | temp01 |\n\nCUT: IN05-IN08, OUT10-11\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| temp01 | gape60 | NAI full | Add Difference @ 1.0 | 0,0.5,0.5,0.5,0.5,0.5,0,0,0,0,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0,0 | AOM3 |\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| temp01 | gape60 | NAI full | Add Difference @ 1.0 | 0,0.5,0.5,0.5,0.5,0.5,0,0,0,0,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0,0 | AOM3 |\n\n‚ñºAOM3A1\n\n‚óÜ**Instructions:**\n\nTool: SuperMerger\n‚óÜ**Instructions:**\n\nTool: SuperMerger\n\nSTEP: 1 | Change the base photorealistic model of AOM3 from BasilMix to Chilloutmix.\n\nChange the photorealistic model from BasilMix to Chilloutmix and proceed to gapeNAI merge.\n\nSTEP: 2 | \n\n| Step | Interpolation Method | Primary Model | Secondary Model | Tertiary Model | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| 1 | SUM @ 0.5 | Counterfeit2.5 | Kenshi |  | Counterfeit+Kenshi |\n| Step | Interpolation Method | Primary Model | Secondary Model | Tertiary Model | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| 1 | SUM @ 0.5 | Counterfeit2.5 | Kenshi |  | Counterfeit+Kenshi |\n\nSTEP: 3 | \n\nCUT: BASE0, IN00-IN08Ôºö0, IN10Ôºö0.1, OUT03-04-05Ôºö0, OUT08Ôºö0.2\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3 | Counterfeit+Kenshi |  | Add SUM @ 1.0 | 0,0,0,0,0,0,0,0,0,0.3,0.1,0.3,0.3,0.3,0.2,0.1,0,0,0,0.3,0.3,0.2,0.3,0.4,0.5 | AOM3A1 |\n\n‚ñºAOM3A1\n‚õîOnly this model (AOM3A1) includes ChilloutMix (=The curse of DreamLike).Commercial use is not available.\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3 | Counterfeit+Kenshi |  | Add SUM @ 1.0 | 0,0,0,0,0,0,0,0,0,0.3,0.1,0.3,0.3,0.3,0.2,0.1,0,0,0,0.3,0.3,0.2,0.3,0.4,0.5 | AOM3A1 |\n\n‚ñºAOM3A1\n‚õîOnly this model (AOM3A1) includes ChilloutMix (=The curse of DreamLike).Commercial use is not available.\n\n‚ñºAOM3A2\n\n‚óÜ?\n‚óÜ?\n\nCUT: BASE0, IN05:0.3„ÄÅIN06-IN08Ôºö0, IN10Ôºö0.1, OUT03Ôºö0, OUT04Ôºö0.3, OUT05Ôºö0, OUT08Ôºö0.2\n\n‚óÜ**Instructions:**\n‚óÜ**Instructions:**\n\nTool: SuperMerger\nTool: SuperMerger\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3 | Counterfeit2.5 | nai | Add Difference @ 1.0 | 0,1,1,1,1,1,0.3,0,0,0,1,0.1,1,1,1,1,1,0,1,0,1,1,0.2,1,1,1 | AOM3A2 |\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3 | Counterfeit2.5 | nai | Add Difference @ 1.0 | 0,1,1,1,1,1,0.3,0,0,0,1,0.1,1,1,1,1,1,0,1,0,1,1,0.2,1,1,1 | AOM3A2 |\n\n‚óÜAOM3A3\n‚óÜAOM3A3\n\nCUT : BASE0, IN05-IN08Ôºö0, IN10Ôºö0.1, OUT03Ôºö0.5, OUT04-05Ôºö0.1, OUT08Ôºö0.2\n\nTool: SuperMerger\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3 | Counterfeit2.5 | nai | Add Difference @ 1.0 | 0,0.6,0.6,0.6,0.6,0.6,0,0,0,0,0.6,0.1,0.6,0.6,0.6,0.6,0.6,0.5,0.1,0.1,0.6,0.6,0.2,0.6,0.6,0.6 | AOM3A3 |\n\n‚ñºAOM3A1B\n\n‚óÜ**Instructions:**\n\nTool: SuperMerge\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3 | Counterfeit2.5 |  | Add Sum @ 1.0 | 0,1,1,1,1,1,0.3,0,0,0,1,0.1,1,1,1,1,1,0,1,0,1,1,0.2,1,1,1 | AOM3A1B |\n\n‚ñºAOM3B2\n\n‚óÜ**Instructions:**\n\nTool: Checkpoint Merger\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3A1B | Breakdomain m21_fp16 |  | Add Sum | 0.4 | AOM3B2 |\n\nTool: SuperMerger\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3 | Counterfeit2.5 | nai | Add Difference @ 1.0 | 0,0.6,0.6,0.6,0.6,0.6,0,0,0,0,0.6,0.1,0.6,0.6,0.6,0.6,0.6,0.5,0.1,0.1,0.6,0.6,0.2,0.6,0.6,0.6 | AOM3A3 |\n\n‚ñºAOM3A1B\n\n‚óÜ**Instructions:**\n\nTool: SuperMerge\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3 | Counterfeit2.5 |  | Add Sum @ 1.0 | 0,1,1,1,1,1,0.3,0,0,0,1,0.1,1,1,1,1,1,0,1,0,1,1,0.2,1,1,1 | AOM3A1B |\n\n‚ñºAOM3B2\n\n‚óÜ**Instructions:**\n\nTool: Checkpoint Merger\n\n| Model: A | Model: B | Model: C | Interpolation Method | Weight | Merge Name |\n| --- | --- | --- | --- | --- | --- |\n| AOM3A1B | Breakdomain m21_fp16 |  | Add Sum | 0.4 | AOM3B2 |\n\n\n----\n\n&nbsp;\n\n## AbyssOrangeMix2 (AOM2)\n\n‚Äï‚ÄïCreating the next generation of illustration with ‚ÄúAbyss‚Äù!\n\n<img src="https://github.com/WarriorMama777/imgup/raw/main/img/AbyssOrangeMix2/HeroImage_AbyssOrangeMix2_Designed_01_comp001.webp"  width="" height="" alt=‚ÄùHeroImage_AbyssOrangeMix2_Designed_01_comp001‚Äù>\n\nPrompt: [https://majinai.art/ja/i/nxpKRpw](https://majinai.art/ja/i/nxpKRpw)\n\n‚ñºAbout\n\nAbyssOrangeMix2 (AOM2) is an AI model capable of generating high-quality, highly realistic illustrations.\nIt can generate elaborate and detailed illustrations that cannot be drawn by hand. It can also be used for a variety of purposes, making it extremely useful for design and artwork.\nFurthermore, it provides an unparalleled new means of expression.\nIt can generate illustrations in a variety of genres to meet a wide range of needs. I encourage you to use "Abyss" to make your designs and artwork richer and of higher quality.\n\n<img src="https://github.com/WarriorMama777/imgup/raw/main/img/AbyssOrangeMix2/UBM_ON_OFF_4_comp001.webp"  width="" height="" alt=‚ÄùUBM_ON_OFF_4_comp001.webp‚Äù>\n‚Äªnvidia joke.\n\n‚ñºDescription for engineers/enthusiasts\n\nThe merged model was formulated using an extension such as sdweb-merge-block-weighted-gui, which merges models at separate rates for each of the 25 U-Net blocks (input, intermediate, and output).\nThe validation of many Anons has shown that such a recipe can generate a painting style that is anatomically realistic enough to feel the finger skeleton, but still maintains an anime-style face.\n\nThe changes from AbyssOrangeMix are as follows.\n\n1. the model used for U-Net Blocks Weight Merge was changed from Instagram+F222 to BasilMix. (<https://huggingface.co/nuigurumi>)\n\nThis is an excellent merge model that can generate decent human bodies while maintaining the facial layers of the Instagram model. Thanks!!!\nThis has improved the dullness of the color and given a more Japanese skin tone (or more precisely, the moisturized white skin that the Japanese would ideally like).\nAlso, the unnatural bokeh that sometimes occurred in the previous version may have been eliminated (needs to be verified).\n\n2.Added IN deep layers (IN06-11) to the layer merging from the realistic model (BasilMix).\n\nIt is said that the IN deep layer (IN06-11) is the layer that determines composition, etc., but perhaps light, reflections, skin texture, etc., may also be involved.\nIt is like "Global Illumination", "Ray tracing" and "Ambient Occlusion" in 3DCG.\n\n<img src="https://github.com/WarriorMama777/imgup/raw/main/img/AbyssOrangeMix2/AbyssOrangeMix2_comparison_comp001.webp"  width="" height="" alt=‚ÄùAbyssOrangeMix2_comparison_comp001‚Äù>\n\n‚ÄªThis does not fundamentally improve the fingers. Therefore, More research needs to be done to improve the fingers (e.g. ''[bad_prompt](https://huggingface.co/datasets/Nerfgun3/bad_prompt)'').\nAbout 30-50% chance of generating correct fingers(?). Abyss is deep.\n\n‚ñºSample Gallery\n\nThe prompts for generating these images were all generated using ChatGPT. I simply asked "Pirates sailing the oceans" to tell me what the prompts were.  \nHowever, to make sure the AI understood the specifications, I used the template for AI questions (Question template for AI prompt generation(v1.2) ).\nPlease review the following.\n\n```jsx\nhttps://seesaawiki.jp/nai_ch/d/AI%a4%f2%b3%e8%cd%d1%a4%b7%a4%bf%a5%d7%a5%ed%a5%f3%a5%d7%a5%c8%c0%b8%c0%ae\n```\n\nThe images thus generated, strangely enough, look like MidJourney or Nijijourney illustrations. Perhaps they are passing user prompts through GPT or something else before passing them on to the image AIü§î\n\n<img src="https://github.com/WarriorMama777/imgup/raw/main/img/AbyssOrangeMix2/SampleGallerBoardDesign_AbyssOrangeMix2_ReadMore_comp001.webp"  width="" height="" alt=‚ÄùSampleGallerBoardDesign_AbyssOrangeMix2_03_comp001‚Äù>\n\n<details>\n<summary>‚ñºREAD MOREüñº</summary>\n\n<img src="https://github.com/WarriorMama777/imgup/raw/main/img/AbyssOrangeMix2/SampleGallerBoardDesign_AbyssOrangeMix2_03_comp001.webp"  width="" height="" alt=‚ÄùSampleGallerBoardDesign_AbyssOrangeMix2_03_comp001‚Äù>\n\n‚ñºAll prompts to generate sample images\n\n1. [Gaming Girl](https://majinai.art/ja/i/GbTbLyk)\n2. [Fantasy](https://majinai.art/ja/i/ax45Pof)\n3. [Rainy Day](https://majinai.art/ja/i/1P9DUul)\n4. [Kemomimi Girl](https://majinai.art/ja/i/hrUSb31)\n5. [Supermarket](https://majinai.art/ja/i/6Mf4bVK)\n6. [Lunch Time](https://majinai.art/ja/i/YAgQ4On)\n7. [Womens in the Garden](https://majinai.art/ja/i/oHZYum_)\n8. [Pirate](https://majinai.art/ja/i/yEA3EZk)\n9. [Japanese Girl](https://majinai.art/ja/i/x4G_B_e)\n10. [Sweets Time](https://majinai.art/ja/i/vK_mkac)\n11. [Glasses Girl](https://majinai.art/ja/i/Z87IHOC)\n\n</details>\n\n\n\n‚ñºHow to use\n\n- VAE: orangemix.vae.pt\n- ~~Prompts can be long or short~~  \nAs simple as possible is good. Do not add excessive detail prompts. Start with just this negative propmt.  \n(worst quality, low quality:1.4)  \n- Sampler: ‚ÄúDPM++ SDE Karras‚Äù is good\n- Steps: forTest: 12ÔΩû ,illustration: 20ÔΩû\n- Clipskip: 1 or 2\n- Upscaler : Latenet (nearest-exact)\n- CFG Scale : 5 or 6 (4ÔΩû8)\n- Denoise strength: 0.5 (0.45~0.6)  \nIf you use 0.7ÔΩû, the picture will change too much.  \nIf below 0.45, Block noise occurs.  \n\nüóíModel List\n\n- AbyssOrangeMix2_sfwÔΩúBasilMix U-Net Blocks Weight Merge\n  - AbyssOrangeMix2_nsfwÔΩú+ NAI-NAISFW 0.3 Merge\n    - AbyssOrangeMix2_hardÔΩú+ Gape 0.3 Merge\n\n‚ÄªChanged suffix of models.  \n_base ‚Üí_sfw: _base was changed to_sfw.\n_night ‚Üí_nsfw: Merged models up to NAI-NAI SFW were changed from _night to_nsfw.\n_half and non suffix ‚Üí_hard: Gape merged models were given the suffix _hard.gape was reduced to 0.3 because it affects character modeling.  \n\n‚ñºHow to choice models\n\n- _sfw : SFWüòâ\n- _nsfw : SFW ÔΩû Soft NSFWü•∞\n- _hard : SFW ÔΩû hard NSFWüëÑ\n\n‚ñºHash\n\n- AbyssOrangeMix2_sfw.ckpt  \n„Äåf75b19923f2a4a0e70f564476178eedd94e76e2c94f8fd8f80c548742b5b51b9„Äç  \n- AbyssOrangeMix2_sfw.safetensors  \n„Äå038ba203d8ba3c8af24f14e01fbb870c85bbb8d4b6d9520804828f4193d12ce9„Äç  \n- AbyssOrangeMix2_nsfw.safetensors  \n„Äå0873291ac5419eaa7a18726e8841ce0f15f701ace29e0183c47efad2018900a4„Äç  \n- AbyssOrangeMix_hard.safetensors  \n„Äå0fc198c4908e98d7aae2a76bd78fa004e9c21cb0be7582e36008b4941169f18e„Äç  \n\n‚ñºUse Models\n\n1. AnythingV3.0 huggingface pruned  \n[2700c435]„Äå543bcbc21294831c6245cd74c8a7707761e28812c690f946cb81fef930d54b5e„Äç  \n1. NovelAI animefull-final-pruned  \n[925997e9]„Äå89d59c3dde4c56c6d5c41da34cc55ce479d93b4007046980934b14db71bdb2a8„Äç  \n1. NovelAI sfw  \n[1d4a34af]„Äå22fa233c2dfd7748d534be603345cb9abf994a23244dfdfc1013f4f90322feca„Äç  \n1. Gape60  \n[25396b85]„Äå893cca5903ccd0519876f58f4bc188dd8fcc5beb8a69c1a3f1a5fe314bb573f5„Äç  \n1. BasilMix  \n„Äåbbf07e3a1c3482c138d096f7dcdb4581a2aa573b74a68ba0906c7b657942f1c2„Äç  \n\n### AbyssOrangeMix2_sfw (AOM2s)\n\n‚ñº**Instructions:**\n\nSTEP: 1ÔΩúBlock Merge\n\n| Model: A     | Model: B | Weight                                                                | Base alpha | Merge Name          |\n| ------------ | -------- | --------------------------------------------------------------------- | ---------- | ------------------- |\n| AnythingV3.0 | BasilMix | 1,0.9,0.7,0.5,0.3,0.1,1,1,1,1,1,1,0,0,0,0,0,0,0,0.1,0.3,0.5,0.7,0.9,1 | 0          | AbyssOrangeMix2_sfw |\n\n### AbyssOrangeMix2_nsfw (AOM2n)\n\n‚ñº?\n\nJUST AbyssOrangeMix2_sfw+ (NAI-NAISFW) 0.3.\n\n‚ñº**Instructions:**\n\n| Step | Interpolation Method | Primary Model       | Secondary Model   | Tertiary Model | Merge Name           |\n| ---- | -------------------- | ------------------- | ----------------- | -------------- | -------------------- |\n| 1    | Add Difference @ 0.3 | AbyssOrangeMix_base | NovelAI animefull | NovelAI sfw    | AbyssOrangeMix2_nsfw |\n\n### AbyssOrangeMix2_hard (AOM2h)\n\n‚ñº?\n+Gape0.3 version AbyssOrangeMix2_nsfw.\n\n‚ñºInstructions\n\n| Step | Interpolation Method | Primary Model        | Secondary Model | Tertiary Model    | Merge Name           |\n| ---- | -------------------- | -------------------- | --------------- | ----------------- | -------------------- |\n| 1    | Add Difference @ 0.3 | AbyssOrangeMix2_nsfw | Gape60          | NovelAI animefull | AbyssOrangeMix2_hard |\n\n----\n\n## EerieOrangeMix (EOM)\n\nEerieOrangeMix is the generic name for a U-Net Blocks Weight Merge Models based on Elysium(Anime V2).  \nSince there are infinite possibilities for U-Net Blocks Weight Merging, I plan to treat all Elysium-based models as a lineage of this model.\n\n‚ÄªThis does not fundamentally improve the fingers. Therefore, More research needs to be done to improve the fingers (e.g. ''[bad_prompt](https://huggingface.co/datasets/Nerfgun3/bad_prompt)'').\n\n<img src="https://files.catbox.moe/yjnqna.webp"  width="1000" height="" alt=‚ÄùHeroImage_EerieOrangeMix_Designed_comp001‚Äù >\n\n\n&nbsp;\n\n### EerieOrangeMix (EOM1)\n\n‚ñº?  \n\nThis merge model is simply a U-Net Blocks Weight Merge of ElysiumAnime V2 with the AbyssOrangeMix method.\n\nThe AnythingModel is good at cute girls anyway, and no matter how hard I try, it doesn''t seem to be good at women in their late 20s and beyond. Therefore, I created a U-Net Blocks Weight Merge model based on my personal favorite ElysiumAnime V2 model. ElyOrangeMix was originally my favorite, so this is an enhanced version of that.\n\nüóíModel List  \n\n- EerieOrangeMix_baseÔΩúInstagram+F222 U-Net Blocks Weight Merge\n  - EerieOrangeMix_nightÔΩú+ NAI-NAISFW Merge\n    - EerieOrangeMix_halfÔΩú+ Gape0.5 Merge\n    - EerieOrangeMixÔΩú+ Gape1.0 Merge\n\n‚ñº How to choice models\n\n- _base : SFWüòâ\n- _Night : SFW ÔΩû Soft NSFWü•∞\n- _half : SFW ÔΩû NSFWüëÑ\n- unlabeled : SFW ÔΩû HARDCORE ÔΩûü§Ø  ex)AbyssOrangeMix, BloodOrangeMix...etc\n\n‚ñºHash  \n\n- EerieOrangeMix.safetensors\n- EerieOrangeMix_half.safetensors\n- EerieOrangeMix_night.safetensors\n- EerieOrangeMix_base.ckpt\n\n‚ñºUse Models  \n\n[] = WebUI Hash,„Äå„Äç= SHA256\n\n1. Elysium Anime V2\n[]„Äå5c4787ce1386500ee05dbb9d27c17273c7a78493535f2603321f40f6e0796851„Äç\n2. NovelAI animefull-final-pruned\n[925997e9]„Äå89d59c3dde4c56c6d5c41da34cc55ce479d93b4007046980934b14db71bdb2a8„Äç\n3. NovelAI sfw\n[1d4a34af]„Äå22fa233c2dfd7748d534be603345cb9abf994a23244dfdfc1013f4f90322feca„Äç\n4. Gape60\n[25396b85]„Äå893cca5903ccd0519876f58f4bc188dd8fcc5beb8a69c1a3f1a5fe314bb573f5„Äç\n5. instagram-latest-plus-clip-v6e1_50000.safetensors\n[] „Äå8f1d325b194570754c6bd06cf1e90aa9219a7e732eb3d488fb52157e9451a2a5„Äç\n6. f222\n[] „Äå9e2c6ceff3f6d6f65c6fb0e10d8e69d772871813be647fd2ea5d06e00db33c1f„Äç\n7. sd1.5_pruned\n[] „Äåe1441589a6f3c5a53f5f54d0975a18a7feb7cdf0b0dee276dfc3331ae376a053„Äç\n\n‚ñº Sample Gallery  \n\n<img src="https://files.catbox.moe/oqbvti.webp"  width="1000" height="" alt=‚Äù2022-12-30_MotorbikeGIrlAsa3_comp001‚Äù>\n<details>\n  <summary>Moreüñº</summary>\n  <img src="https://files.catbox.moe/nmmswd.webp"  width="" height="600" alt=‚Äù2022-12-30_SampleGallery5‚Äù>\n</details>\n\n‚ñº How to use  \n\n- VAE: orangemix.vae.pt\n- As simple as possible is good. Do not add excessive detail prompts. Start with just this.\n(worst quality, low quality:1.4)\n- Sampler: ‚ÄúDPM++ SDE Karras‚Äù is good\n- Steps: forTest: 20ÔΩû24 ,illustration: 24ÔΩû50\n- Clipskip: 1\n- USE ‚Äúupscale latent space‚Äù\n- Denoise strength: 0.45 (0.4~0.5)  \nIf you use 0.7ÔΩû, the picture will change too much.\n\n‚ñºPrompts\n\nüñåWhen generating cute girls, try this negative prompt first. It avoids low quality, prevents blurring, avoids dull colors, and dictates Anime-like cute face modeling.\n\n```jsx\nnsfw, (worst quality, low quality:1.3), (depth of field, blurry:1.2), (greyscale, monochrome:1.1), 3D face, nose, cropped, lowres, text, jpeg artifacts, signature, watermark, username, blurry, artist name, trademark, watermark, title, (tan, muscular, loli, petite, child, infant, toddlers, chibi, sd character:1.1), multiple view, Reference sheet,\n```\n\n---\n\n#### EerieOrangeMix_base (EOM1b)\n\n‚ñº?  \nDetails are omitted since it is the same as AbyssOrangeMix.\n\n‚ñº**Instructions:**\n\nSTEP: 1ÔΩúCreation of photorealistic model for Merge\n\n| Step | Interpolation Method | Primary Model                         | Secondary Model | Tertiary Model | Merge Name |\n| ---- | -------------------- | ------------------------------------- | --------------- | -------------- | ---------- |\n| 1    | Add Difference @ 1.0 | instagram-latest-plus-clip-v6e1_50000 | f222            | sd1.5_pruned   | Insta_F222 |\n\nSTEP: 2ÔΩúBlock Merge\n\nMerge InstaF222\n\n| Model: A         | Model: B   | Weight                                                                | Base alpha | Merge Name |\n| ---------------- | ---------- | --------------------------------------------------------------------- | ---------- | ---------- |\n| Elysium Anime V2 | Insta_F222 | 1,0.9,0.7,0.5,0.3,0.1,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1,0.3,0.5,0.7,0.9,1 | 0          | Temp1      |\n\n#### EerieOrangeMix_Night (EOM1n)\n\n‚ñº?\n\nJUST EerieOrangeMix_base+ (NAI-NAISFW) 0.3.\n\n‚ñºInstructions\n\n| Step | Interpolation Method | Primary Model       | Secondary Model   | Tertiary Model | Merge Name           |\n| ---- | -------------------- | ------------------- | ----------------- | -------------- | -------------------- |\n| 1    | Add Difference @ 0.3 | EerieOrangeMix_base | NovelAI animefull | NovelAI sfw    | EerieOrangeMix_Night |\n\n#### EerieOrangeMix_half (EOM1h)\n\n‚ñº?\n+Gape0.5 version EerieOrangeMix.\n\n‚ñº**Instructions:**\n\n| Step | Interpolation Method | Primary Model        | Secondary Model   | Tertiary Model | Merge Name          |\n| ---- | -------------------- | -------------------- | ----------------- | -------------- | ------------------- |\n| 1    | Add Difference @ 0.5 | EerieOrangeMix_Night | NovelAI animefull | NovelAI sfw    | EerieOrangeMix_half |\n\n#### EerieOrangeMix (EOM1)\n\n‚ñº**Instructions:**\n\n| Step | Interpolation Method | Primary Model        | Secondary Model | Tertiary Model    | Merge Name     |\n| ---- | -------------------- | -------------------- | --------------- | ----------------- | -------------- |\n| 1    | Add Difference @ 1.0 | EerieOrangeMix_Night | Gape60          | NovelAI animefull | EerieOrangeMix |\n\n----\n\n### EerieOrangeMix2 (EOM2)\n\n‚ñº?\n\nThe model was created by adding the hierarchy responsible for detailing and painting ElysiumV1 to EerieOrangeMix_base, then merging NAI and Gape.\n\nüóíModel List\n\n- EerieOrangeMix2_baseÔΩúInstagram+F222+ElysiumV1 U-Net Blocks Weight Merge\n  - EerieOrangeMix2_nightÔΩú+ NAI-NAISFW Merge\n    - EerieOrangeMix2_halfÔΩú+ Gape0.5 Merge\n    - EerieOrangeMix2ÔΩú+ Gape1.0 Merge\n\n‚ñº How to choice models\n\n- _base : SFWüòâ\n- _Night : SFW ÔΩû Soft NSFWü•∞\n- _half : SFW ÔΩû NSFWüëÑ\n- unlabeled : SFW ÔΩû HARDCORE ÔΩûü§Ø  ex)AbyssOrangeMix, BloodOrangeMix...etc\n\n‚ñºHash\n\n- EerieOrangeMix2.safetensors\n- EerieOrangeMix2_half.safetensors\n- EerieOrangeMix2_night.safetensors\n- EerieOrangeMix2_base.ckpt\n\n‚ñºUse Models\n\n[] = webuHash,„Äå„Äç= SHA256\n\n1. Elysium Anime V2\n[]„Äå5c4787ce1386500ee05dbb9d27c17273c7a78493535f2603321f40f6e0796851„Äç\n2. NovelAI animefull-final-pruned\n[925997e9]„Äå89d59c3dde4c56c6d5c41da34cc55ce479d93b4007046980934b14db71bdb2a8„Äç\n3. NovelAI sfw\n[1d4a34af]„Äå22fa233c2dfd7748d534be603345cb9abf994a23244dfdfc1013f4f90322feca„Äç\n4. Gape60\n[25396b85]„Äå893cca5903ccd0519876f58f4bc188dd8fcc5beb8a69c1a3f1a5fe314bb573f5„Äç\n5. instagram-latest-plus-clip-v6e1_50000.safetensors\n[] „Äå8f1d325b194570754c6bd06cf1e90aa9219a7e732eb3d488fb52157e9451a2a5„Äç\n6. f222\n[] „Äå9e2c6ceff3f6d6f65c6fb0e10d8e69d772871813be647fd2ea5d06e00db33c1f„Äç\n7. sd1.5_pruned\n[] „Äåe1441589a6f3c5a53f5f54d0975a18a7feb7cdf0b0dee276dfc3331ae376a053„Äç\n8. ElysiumV1\n„Äåabbb28cb5e70d3e0a635f241b8d61cefe42eb8f1be91fd1168bc3e52b0f09ae4„Äç\n\n#### EerieOrangeMix2_base (EOM2b)\n\n‚ñº?\n\n‚ñºInstructions\n\nSTEP: 1ÔΩúBlock Merge\n\nMerge ElysiumV1\n\nThe generated results do not change much with or without this process, but I wanted to incorporate Elysium''s depiction, so I merged it.\n\n| Model: A            | Model: B  | Weight                                                                | Base alpha | Merge Name           |\n| ------------------- | --------- | --------------------------------------------------------------------- | ---------- | -------------------- |\n| EerieOrangeMix_base | ElysiumV1 | 1,0.9,0.7,0.5,0.3,0.1,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1,0.3,0.5,0.7,0.9,1 | 0          | EerieOrangeMix2_base |\n\n#### EerieOrangeMix2_night (EOM2n)\n\n‚ñº?\n\nJUST EerieOrangeMix2_base+ (NAI-NAISFW) 0.3.\n\n‚ñºInstructions\n\n| Step | Interpolation Method | Primary Model       | Secondary Model   | Tertiary Model | Merge Name            |\n| ---- | -------------------- | ------------------- | ----------------- | -------------- | --------------------- |\n| 1    | Add Difference @ 0.3 | EerieOrangeMix_base | NovelAI animefull | NovelAI sfw    | EerieOrangeMix2_Night |\n\n#### EerieOrangeMix2_half (EOM2h)\n\n‚ñº?\n+Gape0.5 version EerieOrangeMix2.\n\n‚ñºInstructions\n\n| Step | Interpolation Method | Primary Model        | Secondary Model   | Tertiary Model | Merge Name           |\n| ---- | -------------------- | -------------------- | ----------------- | -------------- | -------------------- |\n| 1    | Add Difference @ 0.5 | EerieOrangeMix_Night | NovelAI animefull | NovelAI sfw    | EerieOrangeMix2_half |\n\n#### EerieOrangeMix2 (EOM2)\n\n‚ñº**Instructions:**\n\n| Step | Interpolation Method | Primary Model        | Secondary Model | Tertiary Model    | Merge Name      |\n| ---- | -------------------- | -------------------- | --------------- | ----------------- | --------------- |\n| 1    | Add Difference @ 1.0 | EerieOrangeMix_Night | Gape60          | NovelAI animefull | EerieOrangeMix2 |\n\n### Models Comparison\n\n<img src="https://files.catbox.moe/mp2fr4.webp"  width="1000" height="" alt="MotorbikeGIrlAsa_Eerie_Abyss_Comparison_comp001">  \n<img src="https://files.catbox.moe/9xqths.webp"  width="1000" height="" alt=‚ÄùEerie_Abyss_Comparison_02_comp001‚Äù>\n<img src="https://files.catbox.moe/cm6c7m.webp"  width="1000" height="" alt=‚ÄùEerie_Comparison_01_comp001‚Äù>  \n‚ÄªThe difference is slight but probably looks like this.\n‚Üê warm color, ‚Üë natural color, ‚Üí animated color\n\n----\n\n## AbyssOrangeMix (AOM)\n\n‚Äï‚ÄïHow can you guys take on such a deep swamp and get results?  \nIs it something like "Made in Abyss"?  \nBy Anon, 115th thread\n\n<img src="https://files.catbox.moe/wst1bp.webp"  width="1000" height="">\n\n\n‚ñº?\n\nThe merged model was formulated using an extension such as sdweb-merge-block-weighted-gui, which merges models at separate rates for each of the 25 U-Net blocks (input, intermediate, and output).\nThe validation of many Anons has shown that such a recipe can generate a painting style that is anatomically realistic enough to feel the finger skeleton, but still maintains an anime-style face.\n\n‚ÄªThis model is the result of a great deal of testing and experimentation by many Anonsü§ó\n‚ÄªThis model can be very difficult to handle. I am not 100% confident in my ability to use this model. It is peaky and for experts.  \n‚ÄªThis does not fundamentally improve the fingers, and I recommend using bad_prompt, etc. (Embedding) in combination.  \n\n‚ñºSample Gallery\n\n(1)\n<img src="https://files.catbox.moe/8mke0t.webp" width="1000" height="">\n\n```jsx\n((masterpiece)), best quality, perfect anatomy, (1girl, solo focus:1.4), pov, looking at viewer, flower trim,(perspective, sideway, From directly above ,lying on water, open hand, palm, :1.3),(Accurate five-fingered hands, Reach out, hand focus, foot focus, Sole, heel, ball of the thumb:1.2), (outdoor, sunlight:1.2),(shiny skin:1.3),,(masterpiece, white border, outside border, frame:1.3),\n, (motherhood, aged up, mature female, medium breasts:1.2), (curvy:1.1), (single side braid:1.2), (long hair with queue and braid, disheveled hair, hair scrunchie, tareme:1.2), (light Ivory hair:1.2), looking at viewer,, Calm, Slight smile,\n,(anemic, dark, lake, river,puddle, Meadow, rock, stone, moss, cliff, white flower, stalactite, Godray, ruins, ancient, eternal, deep ,mystic background,sunlight,plant,lily,white flowers, Abyss, :1.2), (orange fruits, citrus fruit, citrus fruit bearing tree:1.4), volumetric lighting,good lighting,, masterpiece, best quality, highly detailed,extremely detailed cg unity 8k wallpaper,illustration,((beautiful detailed face)), best quality, (((hyper-detailed ))), high resolution illustration ,high quality, highres, sidelighting, ((illustrationbest)),highres,illustration, absurdres, hyper-detailed, intricate detail, perfect, high detailed eyes,perfect lighting, (extremely detailed CG:1.2),\n\nNegative prompt: (bad_prompt_version2:1), distant view, lip, Pregnant, maternity, pointy ears, realistic, tan, muscular, greyscale, monochrome, lineart, 2koma, 3koma, 4koma, manga, 3D, 3Dcubism, pablo picasso, disney, marvel, mutanted breasts, mutanted nipple, cropped, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, artist name, lowres, trademark, watermark, title, text, deformed, bad anatomy, disfigured, mutated, extra limbs, ugly, missing limb, floating limbs, disconnected limbs, out of frame, mutated hands and fingers, poorly drawn hands, malformed hands, poorly drawn face, poorly drawn asymmetrical eyes, (blurry:1.4), duplicate (loli, petite, child, infant, toddlers, chibi, sd character, teen age:1.4), tsurime, helmet hair, evil smile, smug_face, naughty smile, multiple view, Reference sheet, (worst quality, low quality:1.4),\nSteps: 24, Sampler: DPM++ SDE Karras, CFG scale: 10, Seed: 1159970659, Size: 1536x768, Model hash: cc44dbff, Model: AbyssOrangeMix, Variation seed: 93902374, Variation seed strength: 0.45, Denoising strength: 0.45, ENSD: 31337\n```\n\n(2)\n<img src="https://files.catbox.moe/6cbrqh.webp" width="" height="600">\n\n```jsx\nstreet, 130mm f1.4 lens, ,(shiny skin:1.3),, (teen age, school uniform:1.2), (glasses, black hair, medium hair with queue and braid, disheveled hair, hair scrunchie, tareme:1.2), looking at viewer,, Calm, Slight smile,\n\nNegative prompt: (bad_prompt_version2:1), distant view, lip, Pregnant, maternity, pointy ears, realistic, tan, muscular, greyscale, monochrome, lineart, 2koma, 3koma, 4koma, manga, 3D, 3Dcubism, pablo picasso, disney, marvel, mutanted breasts, mutanted nipple, cropped, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, artist name, lowres, trademark, watermark, title, text, deformed, bad anatomy, disfigured, mutated, extra limbs, ugly, missing limb, floating limbs, disconnected limbs, out of frame, mutated hands and fingers, poorly drawn hands, malformed hands, poorly drawn face, poorly drawn asymmetrical eyes, (blurry:1.4), duplicate (loli, petite, child, infant, toddlers, chibi, sd character, teen age:1.4), tsurime, helmet hair, evil smile, smug_face, naughty smile, multiple view, Reference sheet, (worst quality, low quality:1.4),\nSteps: 24, Sampler: DPM++ SDE Karras, CFG scale: 10, Seed: 1140782193, Size: 1024x1536, Model hash: cc44dbff, Model: AbyssOrangeMix, Denoising strength: 0.45, ENSD: 31337, First pass size: 512x768, Model sha256: 6bb3a5a3b1eadd32, VAE sha256: f921fb3f29891d2a, Options: xformers medvram gtx_16x0\n\nUsed embeddings: bad_prompt_version2 [afea]\n```\n\n----\n\n‚ñºHow to use\n\n- VAE: orangemix.vae.pt\n- ~~Prompts can be long or short~~  \nAs simple as possible is good. Do not add excessive detail prompts. Start with just this.\n(worst quality, low quality:1.4)\n- Sampler: ‚ÄúDPM++ SDE Karras‚Äù is good\n- Steps: forTest: 20ÔΩû24 ,illustration: 24ÔΩû50\n- Clipskip: 1\n- USE ‚Äúupscale latent space‚Äù\n- Denoise strength: 0.45 (0.4~0.5)\nIf you use 0.7ÔΩû, the picture will change too much.\n\n‚ñºPrompts\n\nüñåWhen generating cute girls, try this negative prompt first. It avoids low quality, prevents blurring, avoids dull colors, and dictates Anime-like cute face modeling.\n\n```jsx\nnsfw, (worst quality, low quality:1.3), (depth of field, blurry:1.2), (greyscale, monochrome:1.1), 3D face, nose, cropped, lowres, text, jpeg artifacts, signature, watermark, username, blurry, artist name, trademark, watermark, title, (tan, muscular, loli, petite, child, infant, toddlers, chibi, sd character:1.1), multiple view, Reference sheet,\n```\n\nüóíModel List\n\n- AbyssOrangeMix_baseÔΩúInstagram Merge\n  - AbyssOrangeMix_NightÔΩú+ NAI-NAISFW Merge\n    - AbyssOrangeMix_halfÔΩú+ Gape0.5 Merge\n    - AbyssOrangeMixÔΩú+ Gape1.0 Merge\n\n‚ñº How to choice models\n\n- _base : SFWüòâ\n- _Night : SFW ÔΩû Soft NSFWü•∞\n- _half : SFW ÔΩû NSFWüëÑ\n- unlabeled : SFW ÔΩû HARDCORE ÔΩûü§Ø  ex)AbyssOrangeMix, BloodOrangeMix...etc\n\n‚ñºHash (SHA256)\n\n- AbyssOrangeMix.safetensors  \n6bb3a5a3b1eadd32dfbc8f0987559c48cb4177aee7582baa6d6a25181929b345\n- AbyssOrangeMix_half.safetensors  \n468d1b5038c4fbd354113842e606fe0557b4e0e16cbaca67706b29bcf51dc402\n- AbyssOrangeMix_Night.safetensors  \n167cd104699dd98df22f4dfd3c7a2c7171df550852181e454e71e5bff61d56a6\n- AbyssOrangeMix_base.ckpt  \nbbd2621f3ec4fad707f75fc032a2c2602c296180a53ed3d9897d8ca7a01dd6ed\n\n‚ñºUse Models\n\n1. AnythingV3.0 huggingface pruned\n[2700c435]„Äå543bcbc21294831c6245cd74c8a7707761e28812c690f946cb81fef930d54b5e„Äç\n1. NovelAI animefull-final-pruned\n[925997e9]„Äå89d59c3dde4c56c6d5c41da34cc55ce479d93b4007046980934b14db71bdb2a8„Äç\n1. NovelAI sfw\n[1d4a34af]„Äå22fa233c2dfd7748d534be603345cb9abf994a23244dfdfc1013f4f90322feca„Äç\n1. Gape60\n[25396b85]„Äå893cca5903ccd0519876f58f4bc188dd8fcc5beb8a69c1a3f1a5fe314bb573f5„Äç\n1. instagram-latest-plus-clip-v6e1_50000.safetensors\n[] „Äå8f1d325b194570754c6bd06cf1e90aa9219a7e732eb3d488fb52157e9451a2a5„Äç\n1. f222\n[] „Äå9e2c6ceff3f6d6f65c6fb0e10d8e69d772871813be647fd2ea5d06e00db33c1f„Äç\n1. sd1.5_pruned\n[] „Äåe1441589a6f3c5a53f5f54d0975a18a7feb7cdf0b0dee276dfc3331ae376a053„Äç\n\n### AbyssOrangeMix_base (AOMb)\n\n‚ñº?\n\nThe basic trick for this merged model is to incorporate a model that has learned more than 1m Instagram photos (mostly Japanese) or a photorealistic model like f222. The choice of base model here depends on the person. I chose AnythingV3 for versatility.\n\n‚ñº**Instructions:**\n\nSTEP: 1ÔΩúCreation of photorealistic model for Merge\n\n| Step | Interpolation Method | Primary Model                         | Secondary Model | Tertiary Model | Merge Name |\n| ---- | -------------------- | ------------------------------------- | --------------- | -------------- | ---------- |\n| 1    | Add Difference @ 1.0 | instagram-latest-plus-clip-v6e1_50000 | f222            | sd1.5_pruned   | Insta_F222 |\n\nSTEP: 2ÔΩúBlock Merge\n\n| Model: A     | Model: B   | Weight                                                                | Base alpha | Merge Name          |\n| ------------ | ---------- | --------------------------------------------------------------------- | ---------- | ------------------- |\n| AnythingV3.0 | Insta_F222 | 1,0.9,0.7,0.5,0.3,0.1,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1,0.3,0.5,0.7,0.9,1 | 0          | AbyssOrangeMix_base |\n\n### AbyssOrangeMix_Night (AOMn)\n\n‚ñº?\n\nJUST AbyssOrangeMix_base+ (NAI-NAISFW) 0.3.\n\n‚ñº**Instructions:**\n\n| Step | Interpolation Method | Primary Model       | Secondary Model   | Tertiary Model | Merge Name           |\n| ---- | -------------------- | ------------------- | ----------------- | -------------- | -------------------- |\n| 1    | Add Difference @ 0.3 | AbyssOrangeMix_base | NovelAI animefull | NovelAI sfw    | AbyssOrangeMix_Night |\n\n### AbyssOrangeMix_half (AOMh)\n\n‚ñº?\n+Gape0.5 version AbyssOrangeMix.\n\n‚ñº**Instructions:**\n\n| Step | Interpolation Method | Primary Model        | Secondary Model | Tertiary Model    | Merge Name          |\n| ---- | -------------------- | -------------------- | --------------- | ----------------- | ------------------- |\n| 1    | Add Difference @ 0.5 | AbyssOrangeMix_Night | Gape60          | NovelAI animefull | AbyssOrangeMix_half |\n\n### AbyssOrangeMix (AOM)\n\n‚ñº**Instructions:**\n\n| Step | Interpolation Method | Primary Model        | Secondary Model | Tertiary Model    | Merge Name     |\n| ---- | -------------------- | -------------------- | --------------- | ----------------- | -------------- |\n| 1    | Add Difference @ 1.0 | AbyssOrangeMix_Night | Gape60          | NovelAI animefull | AbyssOrangeMix |\n\n----\n\n## ElyOrangeMix (ELOM)\n\n<img src="https://i.imgur.com/AInEXA5.jpg"  width="1000" height="">\n\n‚ñº?  \nElysium_Anime_V2 + NAI + Gape.  \nThis is a merge model that improves on the Elysium_Anime_V2, where NSFW representation is not good.  \nIt can produce SFW, NSFW, and any other type of artwork, while retaining the Elysium''s three-dimensional, thickly painted style.\n\n‚ñº How to choice models\n\n- _base : SFWüòâ\n- _Night : SFW ÔΩû Soft NSFWü•∞\n- _half : SFW ÔΩû NSFWüëÑ\n- unlabeled : SFW ÔΩû HARDCORE ÔΩûü§Ø  ex)AbyssOrangeMix, BloodOrangeMix...etc\n\n‚ñºHow to use\n- VAE: orangemix.vae.pt\n\n‚ñºHash (SHA256)\n\n- ElyOrangeMix [6b508e59]\n- ElyOrangeMix_half [6b508e59]\n- ElyNightOrangeMix[6b508e59]\n\n\n### ElyOrangeMix (ELOM)\n\n‚ñºUse Models\n\n1. Elysium_Anime_V2 [6b508e59]\n2. NovelAI animefull-final-pruned [925997e9]\n3. NovelAI sfw [1d4a34af]\n4. Gape60 [25396b85]\n\n‚ñºInstructions\n\n| Step | Interpolation Method | Primary Model    | Secondary Model   | Tertiary Model    | Merge Name               |\n| ---- | -------------------- | ---------------- | ----------------- | ----------------- | ------------------------ |\n| 1    | Add Difference @ 0.3 | Elysium_Anime_V2 | NovelAI animefull | NovelAI sfw       | tempmix-part1 []         |\n| 2    | Add Difference @ 1.0 | tempmix-part1    | Gape60            | NovelAI animefull | ElyOrangeMix  [6b508e59] |\n\n---\n\n### ElyOrangeMix_half (ELOMh)\n\n‚ñº?\n\n+Gape0.5 version ElyOrangeMix.\n\n‚ñºUse Models\n\n1. Elysium_Anime_V2 [6b508e59]\n2. NovelAI animefull-final-pruned [925997e9]\n3. NovelAI sfw [1d4a34af]\n4. Gape60 [25396b85]\n\n‚ñºInstructions\n\n| Step | Interpolation Method | Primary Model    | Secondary Model   | Tertiary Model    | Merge Name                    |\n| ---- | -------------------- | ---------------- | ----------------- | ----------------- | ----------------------------- |\n| 1    | Add Difference @ 0.3 | Elysium_Anime_V2 | NovelAI animefull | NovelAI sfw       | tempmix-part1 []              |\n| 2    | Add Difference @ 0.5 | tempmix-part1    | Gape60            | NovelAI animefull | ElyOrangeMix_half  [6b508e59] |\n\n----\n\n### ElyNightOrangeMix (ELOMn)\n\n‚ñº?\n\nIt is a merged model that just did Elysium_Anime_V2+ (NAI-NAISFW) 0.3.\n\n‚ñºUse Models\n\n1. Elysium_Anime_V2 [6b508e59]\n2. NovelAI animefull-final-pruned [925997e9]\n3. NovelAI sfw [1d4a34af]\n\n‚ñºInstructions\n\n| Step | Interpolation Method | Primary Model    | Secondary Model   | Tertiary Model | Merge Name        |\n| ---- | -------------------- | ---------------- | ----------------- | -------------- | ----------------- |\n| 1    | Add Difference @ 0.3 | Elysium_Anime_V2 | NovelAI animefull | NovelAI sfw    | ElyNightOrangeMix |\n\n----\n\n## BloodOrangeMix (BOM)\n\n<img src="https://i.imgur.com/soAnnFk.jpg"  width="1000" height="">\n\n‚ñº?\nAnything+NAI+Gape.  \nThis is a merge model that improves on the AnythingV3, where NSFW representation is not good.  \nIt can produce SFW, NSFW, and any other type of artwork, while retaining the flat, beautifully painted style of AnythingV3.  \nStable. Popular in the Japanese community.  \n\n‚ñºModelList & [] = WebUI Hash,„Äå„Äç= SHA256\n\n- BloodNightOrangeMix.ckpt  \n  [ffa7b160]„Äåf8aff727ba3da0358815b1766ed232fd1ef9682ad165067cac76e576d19689e0„Äç\n- BloodOrangeMix_half.ckpt  \n [ffa7b160]„Äåb2168aaa59fa91229b8add21f140ac9271773fe88a387276f3f0c7d70f726a83„Äç\n- BloodOrangeMix.ckpt  \n[ffa7b160] „Äå25cece3fe303ea8e3ad40c3dca788406dbd921bcf3aa8e3d1c7c5ac81f208a4f„Äç\n- BloodOrangeMix.safetensors  \n„Äå79a1edf6af43c75ee1e00a884a09213a28ee743b2e913de978cb1f6faa1b320d„Äç\n\n‚ñº How to choice models\n\n- _base : SFWüòâ\n- _Night : SFW ÔΩû Soft NSFWü•∞\n- _half : SFW ÔΩû NSFWüëÑ\n- unlabeled : SFW ÔΩû HARDCORE ÔΩûü§Ø  ex)AbyssOrangeMix, BloodOrangeMix...etc\n\n‚ñºHow to use\n- VAE: orangemix.vae.pt\n\n### BloodOrangeMix (BOM)\n\n‚ñºUse Models\n\n1. AnythingV3.0 huggingface pruned [2700c435]\n2. NovelAI animefull-final-pruned [925997e9]\n3. NovelAI sfw [1d4a34af]\n4. Gape60 [25396b85]\n\n‚ñºInstructions\n\n| Step | Interpolation Method | Primary Model | Secondary Model   | Tertiary Model    | Merge Name                |\n| ---- | -------------------- | ------------- | ----------------- | ----------------- | ------------------------- |\n| 1    | Add Difference @ 0.3 | AnythingV3.0  | NovelAI animefull | NovelAI sfw       | tempmix-part1 []          |\n| 2    | Add Difference @ 1.0 | tempmix-part1 | Gape60            | NovelAI animefull | BloodOrangeMix [ffa7b160] |\n\n----\n\n### BloodOrangeMix_half (BOMh)\n\n‚ñº?\nAnything+Nai+Gape0.5\n+Gape0.5 version BloodOrangeMix.\nNSFW expression will be softer and have less impact on the Anything style painting style.\n\n‚ñºUse Models\n\n1. AnythingV3.0 huggingface pruned [2700c435]\n2. NovelAI animefull-final-pruned [925997e9]\n3. NovelAI sfw [1d4a34af]\n4. Gape60 [25396b85]\n\n‚ñºInstructions\n\n| Step | Interpolation Method | Primary Model | Secondary Model   | Tertiary Model    | Merge Name                     |\n| ---- | -------------------- | ------------- | ----------------- | ----------------- | ------------------------------ |\n| 1    | Add Difference @ 0.3 | AnythingV3.0  | NovelAI animefull | NovelAI sfw       | tempmix-part1 []               |\n| 2    | Add Difference @ 0.5 | tempmix-part1 | Gape60            | NovelAI animefull | BloodOrangeMix_half [ffa7b160] |\n\n----\n\n### BloodNightOrangeMix (BOMn)\n\n‚ñº?\n\nIt is a merged model that just did AnythingV3+ (NAI-NAISFW) 0.3.\n\n‚ñºUse Models\n\n1. AnythingV3.0 huggingface pruned [2700c435]\n2. NovelAI animefull-final-pruned [925997e9]\n3. NovelAI sfw [1d4a34af]\n\n‚ñºInstructions\n\n| Step | Interpolation Method | Primary Model | Secondary Model   | Tertiary Model | Merge Name          |\n| ---- | -------------------- | ------------- | ----------------- | -------------- | ------------------- |\n| 1    | Add Difference @ 0.3 | AnythingV3.0  | NovelAI animefull | NovelAI sfw    | BloodNightOrangeMix |\n\n----\n\n## ElderOrangeMix \n\n‚ÄªI found this model to be very prone to body collapse. Not recommended.\n\n‚ñº?  \nanything and everything mix ver.1.5+Gape+Nai(AnEve.G.N0.3)  \nThis is a merged model with improved NSFW representation of anything and everything mix ver.1.5.\n\n‚ñºHash\n[3a46a1e0]\n\n‚ñºUse Models\n\n1. anything and everything mix ver.1.5 [5265dcf6]\n2. NovelAI animefull-final-pruned [925997e9]\n3. NovelAI sfw [1d4a34af]\n4. Gape60 [25396b85]\n\n‚ñºInstructions:**\n\n| Step | Interpolation Method | Primary Model                       | Secondary Model | Tertiary Model | Merge Name                 |\n| ---- | -------------------- | ----------------------------------- | --------------- | -------------- | -------------------------- |\n| 1    | Add Difference @ 0.5 | anything and everything mix ver.1.5 | Gape60          | NovelAI full   | tempmix-part1 []           |\n| 2    | Add Difference @ 0.3 | tempmix-part1                       | NovelAI full    | NovelAI sfw    | ElderOrangeMix  [3a46a1e0] |\n\n----\n\n## Troubleshooting\n\n1. blurred Images & clearly low quality output  \nIf the generated images are blurred or only clearly low quality output is produced, it is possible that the vae, etc. are not loaded properly. Try reloading the model/vae or restarting the WebUI/OS.\n\n## FAQ and Tips (üêàMEME ZONEü¶ê)\n\n\nTrash zone.\n\n----\n\n<a name="MEME_AOM3A1"></a>\n\n\n‚ñºNoooo, not work. This guy is Scammer  \nSTEP1: BUY HUGE PC  \n\n\n‚ñºNoooo, can''t generate image like samples.This models is hype. \n\n‚ùå  \n<img src="https://files.catbox.moe/nte6ud.webp"  width="500" height="" alt="keyboard guy">  \n\nüü¢  \n<img src="https://files.catbox.moe/lta462.webp"  width="500" height="" alt="clever guy">  \n\n\n‚ñºNoooo, This models have troy virus. don''t download.  \n\nAll models in this repository are secure. It is most likely that anti-virus software has detected them erroneously.  \nHowever, the models with the .ckpt extension have the potential danger of executing arbitrary code.  \nA safe model that is free from these dangers is the model with the .safetensors extension.  \n\n<a name="MEME_realface"></a>\n‚ñºAOM2?  \n(only NSFW models) \n![](https://github.com/WarriorMama777/imgup/raw/main/img/img_general/img_Neko.webp "")\n\n\n‚ñºAOM3A1?  \nR.I.P.  \n\n‚ñºNoooo^()&*%#NG0u!!!!!!!!Á∏∫„ÇÖ‚ôÄÁπß?Á∏∫Âåª?Á∏∫ÔΩ§ÁπùÔΩºÁ∏∫ÔΩ®Á∏∫Âåª?Á∏∫Âê∂ÔΩäÁπùÔΩºÁ∏∫ÔΩØÈ©ï‰∏ªÔΩ≠ÔΩ¶ÈÑôÂÅµ?ÁπßÔΩ¥ÁπùÊ∫ò„ÄíÁ∏∫? („ÄåAOM3A2 and A3 are overlearning and Trash. delete!„Äç)\n\n<img src="https://github.com/WarriorMama777/imgup/raw/main/img/img_general/img_meme_tension_comp001.webp"  width="300" height="" alt=‚Äùgetting_excited‚Äù>\n\n\n‚ñºNoooo, Too many models. Tell me which one to choose.  \n\n‚Üí [ÂÖ®ÈÉ®Âêå„Åò„Åò„ÇÉ„Å™„ÅÑ„Åß„Åô„Åã](https://github.com/WarriorMama777/imgup/blob/main/img/img_general/img_MEME_whichModel_comp001.webp?raw=true "ÂÖ®ÈÉ®Âêå„Åò„Åò„ÇÉ„Å™„ÅÑ„Åß„Åô„Åã")\n\n\n', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":202356872844,"files_count":64,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:hako-mikan:sd-webui-supermerger","source_url":"https://github.com/hako-mikan/sd-webui-supermerger"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:arenatemp:stable-diffusion-webui-model-toolkit","source_url":"https://github.com/arenatemp/stable-diffusion-webui-model-toolkit"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:hako-mikan:sd-webui-supermerger","source_url":"https://github.com/hako-mikan/sd-webui-supermerger"},{"type":"has_code","target_id":"github:hako-mikan:sd-webui-supermerger","source_url":"https://github.com/hako-mikan/sd-webui-supermerger"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:hako-mikan:sd-webui-supermerger","source_url":"https://github.com/hako-mikan/sd-webui-supermerger"},{"type":"has_code","target_id":"github:hako-mikan:sd-webui-supermerger","source_url":"https://github.com/hako-mikan/sd-webui-supermerger"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"},{"type":"has_code","target_id":"github:WarriorMama777:imgup","source_url":"https://github.com/WarriorMama777/imgup"}]', NULL, 'creativeml-openrail-m', 'approved', 80, '2a2f6d4e0fad1e678bf638dd6ab39535', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-lllyasviel-ControlNet', 'huggingface--lllyasviel--controlnet', 'ControlNet', 'lllyasviel', '--- license: openrail --- This is the pretrained weights and some other detector weights of ControlNet. See also: https://github.com/lllyasviel/ControlNet ControlNet/models/control_sd15_canny.pth - The ControlNet+SD1.5 model to control SD using canny edge detection. ControlNet/models/control_sd15_depth.pth - The ControlNet+SD1.5 model to control SD using Midas depth estimation. ControlNet/models/control_sd15_hed.pth - The ControlNet+SD1.5 model to control SD using HED edge detection (soft edg...', '["license:openrail","region:us"]', 'other', 3770, 4, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/lllyasviel/ControlNet","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: openrail\n---\n\nThis is the pretrained weights and some other detector weights of ControlNet.\n\nSee also: https://github.com/lllyasviel/ControlNet\n\n# Description of Files\n\nControlNet/models/control_sd15_canny.pth\n\n- The ControlNet+SD1.5 model to control SD using canny edge detection.\n\nControlNet/models/control_sd15_depth.pth\n\n- The ControlNet+SD1.5 model to control SD using Midas depth estimation.\n\nControlNet/models/control_sd15_hed.pth\n\n- The ControlNet+SD1.5 model to control SD using HED edge detection (soft edge).\n\nControlNet/models/control_sd15_mlsd.pth\n\n- The ControlNet+SD1.5 model to control SD using M-LSD line detection (will also work with traditional Hough transform).\n\nControlNet/models/control_sd15_normal.pth\n\n- The ControlNet+SD1.5 model to control SD using normal map. Best to use the normal map generated by that Gradio app. Other normal maps may also work as long as the direction is correct (left looks red, right looks blue, up looks green, down looks purple). \n\nControlNet/models/control_sd15_openpose.pth\n\n- The ControlNet+SD1.5 model to control SD using OpenPose pose detection. Directly manipulating pose skeleton should also work.\n\nControlNet/models/control_sd15_scribble.pth\n\n- The ControlNet+SD1.5 model to control SD using human scribbles. The model is trained with boundary edges with very strong data augmentation to simulate boundary lines similar to that drawn by human.\n\nControlNet/models/control_sd15_seg.pth\n\n- The ControlNet+SD1.5 model to control SD using semantic segmentation. The protocol is ADE20k.\n\nControlNet/annotator/ckpts/body_pose_model.pth\n\n- Third-party model: Openpose‚Äôs pose detection model.\n\nControlNet/annotator/ckpts/hand_pose_model.pth\n\n- Third-party model: Openpose‚Äôs hand detection model.\n\nControlNet/annotator/ckpts/dpt_hybrid-midas-501f0c75.pt\n\n- Third-party model: Midas depth estimation model.\n\nControlNet/annotator/ckpts/mlsd_large_512_fp32.pth\n\n- Third-party model: M-LSD detection model.\n\nControlNet/annotator/ckpts/mlsd_tiny_512_fp32.pth\n\n- Third-party model: M-LSD‚Äôs another smaller detection model (we do not use this one).\n\nControlNet/annotator/ckpts/network-bsds500.pth\n\n- Third-party model: HED boundary detection.\n\nControlNet/annotator/ckpts/upernet_global_small.pth\n\n- Third-party model: Uniformer semantic segmentation.\n\nControlNet/training/fill50k.zip\n\n- The data for our training tutorial.\n\n# Related Resources\n\nSpecial Thank to the great project - [Mikubill'' A1111 Webui Plugin](https://github.com/Mikubill/sd-webui-controlnet) !\n\nWe also thank Hysts for making [Gradio](https://github.com/gradio-app/gradio) demo in [Hugging Face Space](https://huggingface.co/spaces/hysts/ControlNet) as well as more than 65 models in that amazing [Colab list](https://github.com/camenduru/controlnet-colab)! \n\nThank haofanwang for making [ControlNet-for-Diffusers](https://github.com/haofanwang/ControlNet-for-Diffusers)!\n\nWe also thank all authors for making Controlnet DEMOs, including but not limited to [fffiloni](https://huggingface.co/spaces/fffiloni/ControlNet-Video), [other-model](https://huggingface.co/spaces/hysts/ControlNet-with-other-models), [ThereforeGames](https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/7784), [RamAnanth1](https://huggingface.co/spaces/RamAnanth1/ControlNet), etc!\n\n# Misuse, Malicious Use, and Out-of-Scope Use\n\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\n\n', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":47039955555,"files_count":18,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:lllyasviel:ControlNet","source_url":"https://github.com/lllyasviel/ControlNet"},{"type":"has_code","target_id":"github:Mikubill:sd-webui-controlnet","source_url":"https://github.com/Mikubill/sd-webui-controlnet"},{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"},{"type":"has_code","target_id":"github:camenduru:controlnet-colab","source_url":"https://github.com/camenduru/controlnet-colab"},{"type":"has_code","target_id":"github:haofanwang:ControlNet-for-Diffusers","source_url":"https://github.com/haofanwang/ControlNet-for-Diffusers"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"}]', NULL, 'OpenRAIL', 'approved', 65, '379c0ef30682aa2a33f5ca704f561c0f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-Janus-Pro-7B', 'huggingface--deepseek-ai--janus-pro-7b', 'Janus-Pro-7B', 'deepseek-ai', '--- license: mit license_name: deepseek license_link: LICENSE pipeline_tag: any-to-any library_name: transformers tags: - muiltimodal - text-to-image - unified-model --- Janus-Pro is a novel autoregressive framework that unifies multimodal understanding and generation. It addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. The decoupling not only alleviates the c...', '["transformers","pytorch","multi_modality","muiltimodal","text-to-image","unified-model","any-to-any","arxiv:2501.17811","license:mit","endpoints_compatible","region:us"]', 'any-to-any', 3534, 54841, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/Janus-Pro-7B","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlicense_name: deepseek\nlicense_link: LICENSE\npipeline_tag: any-to-any\nlibrary_name: transformers\ntags:\n- muiltimodal\n- text-to-image\n- unified-model\n---\n\n## 1. Introduction\n\nJanus-Pro is a novel autoregressive framework that unifies multimodal understanding and generation. \nIt addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in understanding and generation, but also enhances the framework‚Äôs flexibility. \nJanus-Pro surpasses previous unified model and matches or exceeds the performance of task-specific models. \nThe simplicity, high flexibility, and effectiveness of Janus-Pro make it a strong candidate for next-generation unified multimodal models.\n\n[**Github Repository**](https://github.com/deepseek-ai/Janus)\n\n<div align="center">\n<img alt="image" src="janus_pro_teaser1.png" style="width:90%;">\n</div>\n\n<div align="center">\n<img alt="image" src="janus_pro_teaser2.png" style="width:90%;">\n</div>\n\n\n### 2. Model Summary\n\nJanus-Pro is a unified understanding and generation MLLM, which decouples visual encoding for multimodal understanding and generation. \nJanus-Pro is constructed based on the DeepSeek-LLM-1.5b-base/DeepSeek-LLM-7b-base.\n\nFor multimodal understanding, it uses the [SigLIP-L](https://huggingface.co/timm/ViT-L-16-SigLIP-384) as the vision encoder, which supports 384 x 384 image input. For image generation, Janus-Pro uses the tokenizer from [here](https://github.com/FoundationVision/LlamaGen) with a downsample rate of 16.\n\n\n\n## 3. Quick Start\n\nPlease refer to [**Github Repository**](https://github.com/deepseek-ai/Janus)\n\n\n## 4. License\n\nThis code repository is licensed under [the MIT License](https://github.com/deepseek-ai/DeepSeek-LLM/blob/HEAD/LICENSE-CODE). The use of Janus-Pro models is subject to [DeepSeek Model License](https://github.com/deepseek-ai/DeepSeek-LLM/blob/HEAD/LICENSE-MODEL).\n## 5. Citation\n\n```\n@article{chen2025janus,\n  title={Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling},\n  author={Chen, Xiaokang and Wu, Zhiyu and Liu, Xingchao and Pan, Zizheng and Liu, Wen and Xie, Zhenda and Yu, Xingkai and Ruan, Chong},\n  journal={arXiv preprint arXiv:2501.17811},\n  year={2025}\n}\n```\n\n## 6. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](mailto:service@deepseek.com).', '{"pipeline_tag":"any-to-any","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":29695390113,"files_count":13,"spaces_count":53,"gated":false,"private":false,"config":{"model_type":"multi_modality","tokenizer_config":{"bos_token":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","eos_token":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","pad_token":null,"unk_token":null,"use_default_system_prompt":true}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:Janus","source_url":"https://github.com/deepseek-ai/Janus"},{"type":"has_code","target_id":"github:FoundationVision:LlamaGen","source_url":"https://github.com/FoundationVision/LlamaGen"},{"type":"has_code","target_id":"github:deepseek-ai:Janus","source_url":"https://github.com/deepseek-ai/Janus"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-LLM","source_url":"https://github.com/deepseek-ai/DeepSeek-LLM"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-LLM","source_url":"https://github.com/deepseek-ai/DeepSeek-LLM"},{"type":"based_on_paper","target_id":"arxiv:2501.17811","source_url":"https://arxiv.org/abs/2501.17811"}]', NULL, 'MIT', 'approved', 65, 'a29960bf8ce0dbbc1f6efa4ef6d383cd', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-microsoft-phi-2', 'huggingface--microsoft--phi-2', 'phi-2', 'microsoft', '--- license: mit license_link: https://huggingface.co/microsoft/phi-2/resolve/main/LICENSE language: - en pipeline_tag: text-generation tags: - nlp - code --- Phi-2 is a Transformer with **2.7 billion** parameters. It was trained using the same data sources as Phi-1.5, augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value). When assessed against benchmarks testing common sense, language understanding, and logical ...', '["transformers","safetensors","phi","text-generation","nlp","code","en","license:mit","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 3410, 998556, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/microsoft/phi-2","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/phi-2/resolve/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\ntags:\n- nlp\n- code\n---\n\n## Model Summary\n\nPhi-2 is a Transformer with **2.7 billion** parameters. It was trained using the same data sources as [Phi-1.5](https://huggingface.co/microsoft/phi-1.5), augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value). When assessed against benchmarks testing common sense, language understanding, and logical reasoning, Phi-2 showcased a nearly state-of-the-art performance among models with less than 13 billion parameters.\n\nOur model hasn''t been fine-tuned through reinforcement learning from human feedback. The intention behind crafting this open-source model is to provide the research community with a non-restricted small model to explore vital safety challenges, such as reducing toxicity, understanding societal biases, enhancing controllability, and more.\n\n## How to Use\n\nPhi-2 has been integrated in the `transformers` version 4.37.0, please ensure that you are using a version equal or higher than it.\n\nPhi-2 is known for having an attention overflow issue (with FP16). If you are facing this issue, please enable/disable autocast on the [PhiAttention.forward()](https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi/modeling_phi.py#L306) function.\n\n## Intended Uses\n\nGiven the nature of the training data, the Phi-2 model is best suited for prompts using the QA format, the chat format, and the code format.\n\n### QA Format:\n\nYou can provide the prompt as a standalone question as follows:\n\n```markdown\nWrite a detailed analogy between mathematics and a lighthouse.\n```\nwhere the model generates the text after "." . \nTo encourage the model to write more concise answers, you can also try the following QA format using "Instruct: \<prompt\>\nOutput:"\n```markdown\nInstruct: Write a detailed analogy between mathematics and a lighthouse.\nOutput: Mathematics is like a lighthouse. Just as a lighthouse guides ships safely to shore, mathematics provides a guiding light in the world of numbers and logic. It helps us navigate through complex problems and find solutions. Just as a lighthouse emits a steady beam of light, mathematics provides a consistent framework for reasoning and problem-solving. It illuminates the path to understanding and helps us make sense of the world around us.\n```\n\nwhere the model generates the text after "Output:".\n\n### Chat Format:\n\n```markdown\nAlice: I don''t know why, I''m struggling to maintain focus while studying. Any suggestions?\nBob: Well, have you tried creating a study schedule and sticking to it?\nAlice: Yes, I have, but it doesn''t seem to help much.\nBob: Hmm, maybe you should try studying in a quiet environment, like the library.\nAlice: ...\n```\n\nwhere the model generates the text after the first "Bob:".\n\n### Code Format:\n\n```python\ndef print_prime(n):\n   """\n   Print all primes between 1 and n\n   """\n   primes = []\n   for num in range(2, n+1):\n       is_prime = True\n       for i in range(2, int(math.sqrt(num))+1):\n           if num % i == 0:\n               is_prime = False\n               break\n       if is_prime:\n           primes.append(num)\n   print(primes)\n```\n\nwhere the model generates the text after the comments.\n\n**Notes:**\n\n* Phi-2 is intended for QA, chat, and code purposes. The model-generated text/code should be treated as a starting point rather than a definitive solution for potential use cases. Users should be cautious when employing these models in their applications.\n\n* Direct adoption for production tasks without evaluation is out of scope of this project. As a result, the Phi-2 model has not been tested to ensure that it performs adequately for any production-level application. Please refer to the limitation sections of this document for more details.\n\n* If you are using `transformers<4.37.0`, always load the model with `trust_remote_code=True` to prevent side-effects.\n\n## Sample Code\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntorch.set_default_device("cuda")\n\nmodel = AutoModelForCausalLM.from_pretrained("microsoft/phi-2", torch_dtype="auto", trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained("microsoft/phi-2", trust_remote_code=True)\n\ninputs = tokenizer(''''''def print_prime(n):\n   """\n   Print all primes between 1 and n\n   """'''''', return_tensors="pt", return_attention_mask=False)\n\noutputs = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\nprint(text)\n```\n\n## Limitations of Phi-2\n\n* Generate Inaccurate Code and Facts: The model may produce incorrect code snippets and statements. Users should treat these outputs as suggestions or starting points, not as definitive or accurate solutions.\n\n* Limited Scope for code: Majority of Phi-2 training data is based in Python and use common packages such as "typing, math, random, collections, datetime, itertools". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n\n* Unreliable Responses to Instruction: The model has not undergone instruction fine-tuning. As a result, it may struggle or fail to adhere to intricate or nuanced instructions provided by users.\n\n* Language Limitations: The model is primarily designed to understand standard English. Informal English, slang, or any other languages might pose challenges to its comprehension, leading to potential misinterpretations or errors in response.\n\n* Potential Societal Biases: Phi-2 is not entirely free from societal biases despite efforts in assuring training data safety. There''s a possibility it may generate content that mirrors these societal biases, particularly if prompted or instructed to do so. We urge users to be aware of this and to exercise caution and critical thinking when interpreting model outputs.\n\n* Toxicity: Despite being trained with carefully selected data, the model can still produce harmful content if explicitly prompted or instructed to do so. We chose to release the model to help the open-source community develop the most effective ways to reduce the toxicity of a model directly after pretraining.\n\n* Verbosity: Phi-2 being a base model often produces irrelevant or extra text and responses following its first answer to user prompts within a single turn. This is due to its training dataset being primarily textbooks, which results in textbook-like responses.\n\n## Training\n\n### Model\n\n* Architecture: a Transformer-based model with next-word prediction objective\n\n* Context length: 2048 tokens\n\n* Dataset size: 250B tokens, combination of NLP synthetic data created by AOAI GPT-3.5 and filtered web data from Falcon RefinedWeb and SlimPajama, which was assessed by AOAI GPT-4.\n\n* Training tokens: 1.4T tokens\n\n* GPUs: 96xA100-80G\n\n* Training time: 14 days\n\n### Software\n\n* [PyTorch](https://github.com/pytorch/pytorch)\n\n* [DeepSpeed](https://github.com/microsoft/DeepSpeed)\n\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n### License\n\nThe model is licensed under the [MIT license](https://huggingface.co/microsoft/phi-2/resolve/main/LICENSE).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow‚ÄØ[Microsoft‚Äôs Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party‚Äôs policies.', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":2779683840,"storage_bytes":11125567216,"files_count":17,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["PhiForCausalLM"],"model_type":"phi","tokenizer_config":{"bos_token":"<|endoftext|>","eos_token":"<|endoftext|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:microsoft:DeepSpeed","source_url":"https://github.com/microsoft/DeepSpeed"},{"type":"has_code","target_id":"github:HazyResearch:flash-attention","source_url":"https://github.com/HazyResearch/flash-attention"}]', NULL, 'MIT', 'approved', 65, '342aec183083a7fcffcd6562e09c627c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-stable-diffusion-3.5-large', 'huggingface--stabilityai--stable-diffusion-3.5-large', 'stable-diffusion-3.5-large', 'stabilityai', '', '["diffusers","safetensors","text-to-image","stable-diffusion","en","arxiv:2403.03206","license:other","diffusers:stablediffusion3pipeline","region:us"]', 'text-to-image', 3254, 54098, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/stable-diffusion-3.5-large","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":75493590524,"files_count":45,"spaces_count":100,"gated":"auto","private":false,"config":{"diffusers":{"_class_name":"StableDiffusion3Pipeline"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2403.03206","source_url":"https://arxiv.org/abs/2403.03206"}]', NULL, 'Other', 'approved', 60, 'f0305ab42879059509550479966477fb', NULL, 'https://huggingface.co/stabilityai/stable-diffusion-3.5-large/resolve/main/sd3.5_large_demo.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-stabilityai-stable-diffusion-3.5-large from https://huggingface.co/stabilityai/stable-diffusion-3.5-large/resolve/main/sd3.5_large_demo.png
HTTP error: 401 Unauthorized

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-7b', 'huggingface--google--gemma-7b', 'gemma-7b', 'google', '', '["transformers","safetensors","gguf","gemma","text-generation","arxiv:2305.14314","arxiv:2312.11805","arxiv:2009.03300","arxiv:1905.07830","arxiv:1911.11641","arxiv:1904.09728","arxiv:1905.10044","arxiv:1907.10641","arxiv:1811.00937","arxiv:1809.02789","arxiv:1911.01547","arxiv:1705.03551","arxiv:2107.03374","arxiv:2108.07732","arxiv:2110.14168","arxiv:2304.06364","arxiv:2206.04615","arxiv:1804.06876","arxiv:2110.08193","arxiv:2009.11462","arxiv:2101.11718","arxiv:1804.09301","arxiv:2109.07958","arxiv:2203.09509","license:gemma","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 3244, 50198, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-7b","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":8537680896,"storage_bytes":214855095088,"files_count":17,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["GemmaForCausalLM"],"model_type":"gemma","tokenizer_config":{"bos_token":"<bos>","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2305.14314","source_url":"https://arxiv.org/abs/2305.14314"},{"type":"based_on_paper","target_id":"arxiv:2312.11805","source_url":"https://arxiv.org/abs/2312.11805"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1811.00937","source_url":"https://arxiv.org/abs/1811.00937"},{"type":"based_on_paper","target_id":"arxiv:1809.02789","source_url":"https://arxiv.org/abs/1809.02789"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2110.14168","source_url":"https://arxiv.org/abs/2110.14168"},{"type":"based_on_paper","target_id":"arxiv:2304.06364","source_url":"https://arxiv.org/abs/2304.06364"},{"type":"based_on_paper","target_id":"arxiv:2206.04615","source_url":"https://arxiv.org/abs/2206.04615"},{"type":"based_on_paper","target_id":"arxiv:1804.06876","source_url":"https://arxiv.org/abs/1804.06876"},{"type":"based_on_paper","target_id":"arxiv:2110.08193","source_url":"https://arxiv.org/abs/2110.08193"},{"type":"based_on_paper","target_id":"arxiv:2009.11462","source_url":"https://arxiv.org/abs/2009.11462"},{"type":"based_on_paper","target_id":"arxiv:2101.11718","source_url":"https://arxiv.org/abs/2101.11718"},{"type":"based_on_paper","target_id":"arxiv:1804.09301","source_url":"https://arxiv.org/abs/1804.09301"},{"type":"based_on_paper","target_id":"arxiv:2109.07958","source_url":"https://arxiv.org/abs/2109.07958"},{"type":"based_on_paper","target_id":"arxiv:2203.09509","source_url":"https://arxiv.org/abs/2203.09509"}]', NULL, 'Gemma', 'approved', 40, '1324571855977f40775d819264fb8fca', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-coqui-XTTS-v2', 'huggingface--coqui--xtts-v2', 'XTTS-v2', 'coqui', '--- license: other license_name: coqui-public-model-license license_link: https://coqui.ai/cpml library_name: coqui pipeline_tag: text-to-speech widget: - text: "Once when I was six years old I saw a magnificent picture" --- ‚ìçTTS is a Voice generation model that lets you clone voices into different languages by using just a quick 6-second audio clip. There is no need for an excessive amount of training data that spans countless hours. This is the same or similar model to what powers Coqui Stu...', '["coqui","text-to-speech","license:other","region:us"]', 'text-to-speech', 3215, 6331197, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/coqui/XTTS-v2","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: coqui-public-model-license\nlicense_link: https://coqui.ai/cpml\nlibrary_name: coqui\npipeline_tag: text-to-speech\nwidget:\n  - text: "Once when I was six years old I saw a magnificent picture"\n---\n\n# ‚ìçTTS\n‚ìçTTS is a Voice generation model that lets you clone voices into different languages by using just a quick 6-second audio clip. There is no need for an excessive amount of training data that spans countless hours.\n\nThis is the same or similar model to what powers [Coqui Studio](https://coqui.ai/) and [Coqui API](https://docs.coqui.ai/docs).\n\n### Features\n- Supports 17 languages. \n- Voice cloning with just a 6-second audio clip.\n- Emotion and style transfer by cloning. \n- Cross-language voice cloning.\n- Multi-lingual speech generation.\n- 24khz sampling rate.\n\n### Updates over XTTS-v1\n- 2 new languages; Hungarian and Korean\n- Architectural improvements for speaker conditioning.\n- Enables the use of multiple speaker references and interpolation between speakers.\n- Stability improvements.\n- Better prosody and audio quality across the board.\n\n### Languages\nXTTS-v2 supports 17 languages: **English (en), Spanish (es), French (fr), German (de), Italian (it), Portuguese (pt),\nPolish (pl), Turkish (tr), Russian (ru), Dutch (nl), Czech (cs), Arabic (ar), Chinese (zh-cn), Japanese (ja), Hungarian (hu), Korean (ko)\nHindi (hi)**.\n\nStay tuned as we continue to add support for more languages. If you have any language requests, feel free to reach out!\n\n### Code\nThe [code-base](https://github.com/coqui-ai/TTS) supports inference and [fine-tuning](https://tts.readthedocs.io/en/latest/models/xtts.html#training).\n\n### Demo Spaces\n- [XTTS Space](https://huggingface.co/spaces/coqui/xtts)  :  You can see how model performs on supported languages, and try with your own reference or microphone input\n- [XTTS Voice Chat with Mistral or Zephyr](https://huggingface.co/spaces/coqui/voice-chat-with-mistral) : You can experience streaming voice chat with Mistral 7B Instruct or Zephyr 7B Beta\n\n|                                 |                                         |\n| ------------------------------- | --------------------------------------- |\n| üê∏üí¨ **CoquiTTS**               | [coqui/TTS on Github](https://github.com/coqui-ai/TTS)|\n| üíº **Documentation**            | [ReadTheDocs](https://tts.readthedocs.io/en/latest/)\n| üë©‚Äçüíª **Questions**                | [GitHub Discussions](https://github.com/coqui-ai/TTS/discussions) |\n| üóØ **Community**         | [Discord](https://discord.gg/5eXr5seRrv)  |\n\n\n### License\nThis model is licensed under [Coqui Public Model License](https://coqui.ai/cpml). There''s a lot that goes into a license for generative models, and you can read more of [the origin story of CPML here](https://coqui.ai/blog/tts/cpml).\n\n### Contact\nCome and join in our üê∏Community. We''re active on [Discord](https://discord.gg/fBC58unbKE) and [Twitter](https://twitter.com/coqui_ai).\nYou can also mail us at info@coqui.ai.\n\nUsing üê∏TTS API:\n\n```python\nfrom TTS.api import TTS\ntts = TTS("tts_models/multilingual/multi-dataset/xtts_v2", gpu=True)\n\n# generate speech by cloning a voice using default settings\ntts.tts_to_file(text="It took me quite a long time to develop a voice, and now that I have it I''m not going to be silent.",\n                file_path="output.wav",\n                speaker_wav="/path/to/target/speaker.wav",\n                language="en")\n\n```\n\nUsing üê∏TTS Command line:\n\n```console\n tts --model_name tts_models/multilingual/multi-dataset/xtts_v2 \\n     --text "Bug√ºn okula gitmek istemiyorum." \\n     --speaker_wav /path/to/target/speaker.wav \\n     --language_idx tr \\n     --use_cuda true\n```\n\nUsing the model directly:\n\n```python\nfrom TTS.tts.configs.xtts_config import XttsConfig\nfrom TTS.tts.models.xtts import Xtts\n\nconfig = XttsConfig()\nconfig.load_json("/path/to/xtts/config.json")\nmodel = Xtts.init_from_config(config)\nmodel.load_checkpoint(config, checkpoint_dir="/path/to/xtts/", eval=True)\nmodel.cuda()\n\noutputs = model.synthesize(\n    "It took me quite a long time to develop a voice and now that I have it I am not going to be silent.",\n    config,\n    speaker_wav="/data/TTS-public/_refclips/3.wav",\n    gpt_cond_len=3,\n    language="en",\n)\n```\n', '{"pipeline_tag":"text-to-speech","library_name":"coqui","framework":"coqui","params":null,"storage_bytes":24339714966,"files_count":18,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"}]', NULL, 'Other', 'approved', 65, '7d2113417b80553f3a1a9acf8eedde95', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-stable-video-diffusion-img2vid-xt', 'huggingface--stabilityai--stable-video-diffusion-img2vid-xt', 'stable-video-diffusion-img2vid-xt', 'stabilityai', '--- pipeline_tag: image-to-video license: other license_name: stable-video-diffusion-community license_link: LICENSE.md --- <!-- Provide a quick summary of what the model is/does. --> !row01 Stable Video Diffusion (SVD) Image-to-Video is a diffusion model that takes in a still image as a conditioning frame, and generates a video from it. Please note: For commercial use, please refer to https://stability.ai/license. (SVD) Image-to-Video is a latent diffusion model trained to generate short vid...', '["diffusers","safetensors","image-to-video","license:other","diffusers:stablevideodiffusionpipeline","region:us"]', 'image-to-video', 3199, 240288, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\npipeline_tag: image-to-video\nlicense: other\nlicense_name: stable-video-diffusion-community\nlicense_link: LICENSE.md\n---\n\n# Stable Video Diffusion Image-to-Video Model Card\n\n<!-- Provide a quick summary of what the model is/does. -->\n![row01](output_tile.gif)\nStable Video Diffusion (SVD) Image-to-Video is a diffusion model that takes in a still image as a conditioning frame, and generates a video from it. \n\nPlease note: For commercial use, please refer to https://stability.ai/license.\n\n## Model Details\n\n### Model Description\n\n(SVD) Image-to-Video is a latent diffusion model trained to generate short video clips from an image conditioning. \nThis model was trained to generate 25 frames at resolution 576x1024 given a context frame of the same size, finetuned from [SVD Image-to-Video [14 frames]](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid).\nWe also finetune the widely used [f8-decoder](https://huggingface.co/docs/diffusers/api/models/autoencoderkl#loading-from-the-original-format) for temporal consistency. \nFor convenience, we additionally provide the model with the \nstandard frame-wise decoder [here](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt/blob/main/svd_xt_image_decoder.safetensors).\n\n\n- **Developed by:** Stability AI\n- **Funded by:** Stability AI\n- **Model type:** Generative image-to-video model\n- **Finetuned from model:** SVD Image-to-Video [14 frames]\n\n### Model Sources\n\nFor research purposes, we recommend our `generative-models` Github repository (https://github.com/Stability-AI/generative-models), \nwhich implements the most popular diffusion frameworks (both training and inference).\n\n- **Repository:** https://github.com/Stability-AI/generative-models\n- **Paper:** https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets\n\n\n## Evaluation\n![comparison](comparison.png)\nThe chart above evaluates user preference for SVD-Image-to-Video over [GEN-2](https://research.runwayml.com/gen2) and [PikaLabs](https://www.pika.art/).\nSVD-Image-to-Video is preferred by human voters in terms of video quality. For details on the user study, we refer to the [research paper](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets)\n\n## Uses\n\n### Direct Use\n\nThe model is intended for both non-commercial and commercial usage. You can use this model for non-commercial or research purposes under this [license](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt/blob/main/LICENSE.md). Possible research areas and tasks include\n\n- Research on generative models.\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n\nFor commercial use, please refer to https://stability.ai/license.\n\nExcluded uses are described below.\n\n### Out-of-Scope Use\n\nThe model was not trained to be factual or true representations of people or events, \nand therefore using the model to generate such content is out-of-scope for the abilities of this model.\nThe model should not be used in any way that violates Stability AI''s [Acceptable Use Policy](https://stability.ai/use-policy).\n\n## Limitations and Bias\n\n### Limitations\n- The generated videos are rather short (<= 4sec), and the model does not achieve perfect photorealism.\n- The model may generate videos without motion, or very slow camera pans.\n- The model cannot be controlled through text.\n- The model cannot render legible text.\n- Faces and people in general may not be generated properly.\n- The autoencoding part of the model is lossy.\n\n\n### Recommendations\n\nThe model is intended for both non-commercial and commercial usage.\n\n## How to Get Started with the Model\n\nCheck out https://github.com/Stability-AI/generative-models\n\n# Appendix: \n\nAll considered potential data sources were included for final training, with none held out as the proposed data filtering methods described in the SVD paper handle the quality control/filtering of the dataset. With regards to safety/NSFW filtering, sources considered were either deemed safe or filtered with the in-house NSFW filters.\nNo explicit human labor is involved in training data preparation. However, human evaluation for model outputs and quality was extensively used to evaluate model quality and performance. The evaluations were performed with third-party contractor platforms (Amazon Sagemaker, Amazon Mechanical Turk, Prolific) with fluent English-speaking contractors from various countries, primarily from the USA, UK, and Canada. Each worker was paid $12/hr for the time invested in the evaluation.\nNo other third party was involved in the development of this model; the model was fully developed in-house at Stability AI.\nTraining the SVD checkpoints required a total of approximately 200,000 A100 80GB hours. The majority of the training occurred on 48 * 8 A100s, while some stages took more/less than that. The resulting CO2 emission is ~19,000kg CO2 eq., and energy consumed is ~64000 kWh.\nThe released checkpoints (SVD/SVD-XT) are image-to-video models that generate short videos/animations closely following the given input image. Since the model relies on an existing supplied image, the potential risks of disclosing specific material or novel unsafe content are minimal. This was also evaluated by third-party independent red-teaming services, which agree with our conclusion to a high degree of confidence (>90% in various areas of safety red-teaming). The external evaluations were also performed for trustworthiness, leading to >95% confidence in real, trustworthy videos.\nWith the default settings at the time of release, SVD takes ~100s for generation, and SVD-XT takes ~180s on an A100 80GB card. Several optimizations to trade off quality / memory / speed can be done to perform faster inference or inference on lower VRAM cards.\nThe information related to the model and its development process and usage protocols can be found in the GitHub repo, associated research paper, and HuggingFace model page/cards. \nThe released model inference & demo code has image-level watermarking enabled by default, which can be used to detect the outputs. This is done via the imWatermark Python library.  \nThe model can be used to generate videos from static initial images. However, we prohibit unlawful, obscene, or misleading uses of the model consistent with the terms of our license and Acceptable Use Policy. For the open-weights release, our training data filtering mitigations alleviate this risk to some extent. These restrictions are explicitly enforced on user-facing interfaces at stablevideo.com, where a warning is issued. We do not take any responsibility for third-party interfaces. Submitting initial images that bypass input filters to tease out offensive or inappropriate content listed above is also prohibited. Safety filtering checks at stablevideo.com run on model inputs and outputs independently. More details on our user-facing interfaces can be found here: https://www.stablevideo.com/faq. Beyond the Acceptable Use Policy and other mitigations and conditions described here, the model is not subject to additional model behavior interventions of the type described in the Foundation Model Transparency Index.\nFor stablevideo.com, we store preference data in the form of upvotes/downvotes on user-generated videos, and we have a pairwise ranker that runs while a user generates videos. This usage data is solely used for improving Stability AI‚Äôs future image/video models and services. No other third-party entities are given access to the usage data beyond Stability AI and maintainers of stablevideo.com. \nFor usage statistics of SVD, we refer interested users to HuggingFace model download/usage statistics as a primary indicator. Third-party applications also have reported model usage statistics. We might also consider releasing aggregate usage statistics of stablevideo.com on reaching some milestones.\n', '{"pipeline_tag":"image-to-video","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":32633156436,"files_count":19,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableVideoDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:Stability-AI:generative-models","source_url":"https://github.com/Stability-AI/generative-models"},{"type":"has_code","target_id":"github:Stability-AI:generative-models","source_url":"https://github.com/Stability-AI/generative-models"},{"type":"has_code","target_id":"github:Stability-AI:generative-models","source_url":"https://github.com/Stability-AI/generative-models"}]', NULL, 'Other', 'approved', 65, 'e9092d1fcfa5b0d6ad2a88fe450ec74f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-prompthero-openjourney', 'huggingface--prompthero--openjourney', 'openjourney', 'prompthero', '--- inference: true language: - en tags: - stable-diffusion - text-to-image license: creativeml-openrail-m --- Include **''mdjrny-v4 style''** in prompt. Here you''ll find hundreds of Openjourney prompts - Lora version - Openjourney v4 - Crash course in AI art generation - Learn to fine-tune Stable Diffusion for photorealism (Same parameters, just added "mdjrny-v4 style" at the beginning): <img src="https://s3.amazonaws.com/moonup/production/uploads/1667904587642-63265d019f9d19bfd4f45031.png" wi...', '["diffusers","safetensors","stable-diffusion","text-to-image","en","license:creativeml-openrail-m","endpoints_compatible","diffusers:stablediffusionpipeline","deploy:azure","region:us"]', 'text-to-image', 3176, 16713, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/prompthero/openjourney","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ninference: true\nlanguage:\n  - en\ntags:\n  - stable-diffusion\n  - text-to-image\nlicense: creativeml-openrail-m\n---\n# Openjourney is an open source Stable Diffusion fine tuned model on Midjourney images, by [PromptHero](https://prompthero.com/poolsuite-diffusion-prompts?utm_source=huggingface&utm_medium=referral)\n\nInclude **''mdjrny-v4 style''** in prompt. Here you''ll find hundreds of [Openjourney prompts](https://prompthero.com/openjourney-prompts?utm_source=huggingface&utm_medium=referral)\n\n# Openjourney Links\n- [Lora version](https://huggingface.co/prompthero/openjourney-lora)\n- [Openjourney v4](https://huggingface.co/prompthero/openjourney-v2)\n\n# Want to learn AI art generation?:\n- [Crash course in AI art generation](https://prompthero.com/academy/prompt-engineering-course?utm_source=huggingface&utm_medium=referral)\n- [Learn to fine-tune Stable Diffusion for photorealism](https://prompthero.com/academy/dreambooth-stable-diffusion-train-fine-tune-course?utm_source=huggingface&utm_medium=referral)\n\n# Use it for free:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/midjourney-v4-diffusion)\n\n### Stable Diffusion v1.5 vs Openjourney \n(Same parameters, just added "mdjrny-v4 style" at the beginning):\n<img src="https://s3.amazonaws.com/moonup/production/uploads/1667904587642-63265d019f9d19bfd4f45031.png" width="100%"/>\n<img src="https://s3.amazonaws.com/moonup/production/uploads/1667904587623-63265d019f9d19bfd4f45031.png" width="100%"/>\n<img src="https://s3.amazonaws.com/moonup/production/uploads/1667904587609-63265d019f9d19bfd4f45031.png" width="100%"/>\n<img src="https://s3.amazonaws.com/moonup/production/uploads/1667904587646-63265d019f9d19bfd4f45031.png" width="100%"/>\n\n### üß® Diffusers\n\nThis model can be used just like any other Stable Diffusion model. For more information,\nplease have a look at the [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\n\nYou can also export the model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX]().\n\n```python\nfrom diffusers import StableDiffusionPipeline\nimport torch\nmodel_id = "prompthero/openjourney"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to("cuda")\nprompt = "retro serie of different cars with different colors and shapes, mdjrny-v4 style"\nimage = pipe(prompt).images[0]\nimage.save("./retro_cars.png")\n```', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":123060557,"storage_bytes":25133542774,"files_count":24,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[]', NULL, 'creativeml-openrail-m', 'approved', 65, 'abb8d1a6a3a4de8fa91ad76dd1e32d47', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-V3-0324', 'huggingface--deepseek-ai--deepseek-v3-0324', 'DeepSeek-V3-0324', 'deepseek-ai', '--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/d...', '["transformers","safetensors","deepseek_v3","text-generation","conversational","custom_code","arxiv:2412.19437","license:mit","text-generation-inference","endpoints_compatible","fp8","region:us"]', 'text-generation', 3083, 141904, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-V3-0324","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-V3-0324\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n## Features\n\nDeepSeek-V3-0324 demonstrates notable improvements over its predecessor, DeepSeek-V3, in several key aspects.\n\n![Model Performance](figures/0324_comparison.png)\n\n### Reasoning Capabilities\n\n- Significant improvements in benchmark performance:\n  - MMLU-Pro: 75.9 ‚Üí 81.2 (+5.3)\n  - GPQA: 59.1 ‚Üí 68.4 (+9.3)\n  - AIME: 39.6 ‚Üí 59.4 (+19.8)\n  - LiveCodeBench: 39.2 ‚Üí 49.2 (+10.0)\n\n### Front-End Web Development\n\n- Improved the executability of the code\n- More aesthetically pleasing web pages and game front-ends\n\n### Chinese Writing Proficiency\n\n- Enhanced style and content quality:\n  - Aligned with the R1 writing style\n  - Better quality in medium-to-long-form writing\n\n- Feature Enhancements\n  - Improved multi-turn interactive rewriting\n  - Optimized translation quality and letter writing\n\n### Chinese Search Capabilities\n\n- Enhanced report analysis requests with more detailed outputs\n\n### Function Calling Improvements\n\n- Increased accuracy in Function Calling, fixing issues from previous V3 versions\n\n---\n\n## Usage Recommendations\n\n### System Prompt\n\nIn the official DeepSeek web/app, we use the same system prompt with a specific date.\n\n```\nËØ•Âä©Êâã‰∏∫DeepSeek ChatÔºåÁî±Ê∑±Â∫¶Ê±ÇÁ¥¢ÂÖ¨Âè∏ÂàõÈÄ†„ÄÇ\n‰ªäÂ§©ÊòØ{current date}„ÄÇ\n```\n\nFor example,\n\n```\nËØ•Âä©Êâã‰∏∫DeepSeek ChatÔºåÁî±Ê∑±Â∫¶Ê±ÇÁ¥¢ÂÖ¨Âè∏ÂàõÈÄ†„ÄÇ\n‰ªäÂ§©ÊòØ3Êúà24Êó•ÔºåÊòüÊúü‰∏Ä„ÄÇ\n```\n\n### Temperature\n\nIn our web and application environments, the temperature parameter $T_{model}$ is set to 0.3. Because many users use the default temperature 1.0 in API call, we have implemented an API temperature $T_{api}$ mapping mechanism that adjusts the input API temperature value of 1.0 to the most suitable model temperature setting of 0.3.\n\n$$\nT_{model} = T_{api} \times 0.3 \quad (0 \leq T_{api} \leq 1)\n$$\n\n$$\nT_{model} = T_{api} - 0.7 \quad (1 < T_{api} \leq 2)\n$$\n\nThus, if you call V3 via API, temperature 1.0 equals to the model temperature 0.3.\n\n### Prompts for File Uploading and Web Search\n\nFor file uploading, please follow the template to create prompts, where {file_name}, {file_content} and {question} are arguments.\n\n```\nfile_template = \\n"""[file name]: {file_name}\n[file content begin]\n{file_content}\n[file content end]\n{question}"""\n```\n\nFor Web Search, {search_results}, {cur_date}, and {question} are arguments.\n\nFor Chinese query, we use the prompt:\n\n```\nsearch_answer_zh_template = \\n''''''# ‰ª•‰∏ãÂÜÖÂÆπÊòØÂü∫‰∫éÁî®Êà∑ÂèëÈÄÅÁöÑÊ∂àÊÅØÁöÑÊêúÁ¥¢ÁªìÊûú:\n{search_results}\nÂú®ÊàëÁªô‰Ω†ÁöÑÊêúÁ¥¢ÁªìÊûú‰∏≠ÔºåÊØè‰∏™ÁªìÊûúÈÉΩÊòØ[webpage X begin]...[webpage X end]Ê†ºÂºèÁöÑÔºåX‰ª£Ë°®ÊØèÁØáÊñáÁ´†ÁöÑÊï∞Â≠óÁ¥¢Âºï„ÄÇËØ∑Âú®ÈÄÇÂΩìÁöÑÊÉÖÂÜµ‰∏ãÂú®Âè•Â≠êÊú´Â∞æÂºïÁî®‰∏ä‰∏ãÊñá„ÄÇËØ∑ÊåâÁÖßÂºïÁî®ÁºñÂè∑[citation:X]ÁöÑÊ†ºÂºèÂú®Á≠îÊ°à‰∏≠ÂØπÂ∫îÈÉ®ÂàÜÂºïÁî®‰∏ä‰∏ãÊñá„ÄÇÂ¶ÇÊûú‰∏ÄÂè•ËØùÊ∫êËá™Â§ö‰∏™‰∏ä‰∏ãÊñáÔºåËØ∑ÂàóÂá∫ÊâÄÊúâÁõ∏ÂÖ≥ÁöÑÂºïÁî®ÁºñÂè∑Ôºå‰æãÂ¶Ç[citation:3][citation:5]ÔºåÂàáËÆ∞‰∏çË¶ÅÂ∞ÜÂºïÁî®ÈõÜ‰∏≠Âú®ÊúÄÂêéËøîÂõûÂºïÁî®ÁºñÂè∑ÔºåËÄåÊòØÂú®Á≠îÊ°àÂØπÂ∫îÈÉ®ÂàÜÂàóÂá∫„ÄÇ\nÂú®ÂõûÁ≠îÊó∂ÔºåËØ∑Ê≥®ÊÑè‰ª•‰∏ãÂá†ÁÇπÔºö\n- ‰ªäÂ§©ÊòØ{cur_date}„ÄÇ\n- Âπ∂ÈùûÊêúÁ¥¢ÁªìÊûúÁöÑÊâÄÊúâÂÜÖÂÆπÈÉΩ‰∏éÁî®Êà∑ÁöÑÈóÆÈ¢òÂØÜÂàáÁõ∏ÂÖ≥Ôºå‰Ω†ÈúÄË¶ÅÁªìÂêàÈóÆÈ¢òÔºåÂØπÊêúÁ¥¢ÁªìÊûúËøõË°åÁîÑÂà´„ÄÅÁ≠õÈÄâ„ÄÇ\n- ÂØπ‰∫éÂàó‰∏æÁ±ªÁöÑÈóÆÈ¢òÔºàÂ¶ÇÂàó‰∏æÊâÄÊúâËà™Áè≠‰ø°ÊÅØÔºâÔºåÂ∞ΩÈáèÂ∞ÜÁ≠îÊ°àÊéßÂà∂Âú®10‰∏™Ë¶ÅÁÇπ‰ª•ÂÜÖÔºåÂπ∂ÂëäËØâÁî®Êà∑ÂèØ‰ª•Êü•ÁúãÊêúÁ¥¢Êù•Ê∫ê„ÄÅËé∑ÂæóÂÆåÊï¥‰ø°ÊÅØ„ÄÇ‰ºòÂÖàÊèê‰æõ‰ø°ÊÅØÂÆåÊï¥„ÄÅÊúÄÁõ∏ÂÖ≥ÁöÑÂàó‰∏æÈ°πÔºõÂ¶ÇÈùûÂøÖË¶ÅÔºå‰∏çË¶Å‰∏ªÂä®ÂëäËØâÁî®Êà∑ÊêúÁ¥¢ÁªìÊûúÊú™Êèê‰æõÁöÑÂÜÖÂÆπ„ÄÇ\n- ÂØπ‰∫éÂàõ‰ΩúÁ±ªÁöÑÈóÆÈ¢òÔºàÂ¶ÇÂÜôËÆ∫ÊñáÔºâÔºåËØ∑Âä°ÂøÖÂú®Ê≠£ÊñáÁöÑÊÆµËêΩ‰∏≠ÂºïÁî®ÂØπÂ∫îÁöÑÂèÇËÄÉÁºñÂè∑Ôºå‰æãÂ¶Ç[citation:3][citation:5]Ôºå‰∏çËÉΩÂè™Âú®ÊñáÁ´†Êú´Â∞æÂºïÁî®„ÄÇ‰Ω†ÈúÄË¶ÅËß£ËØªÂπ∂Ê¶ÇÊã¨Áî®Êà∑ÁöÑÈ¢òÁõÆË¶ÅÊ±ÇÔºåÈÄâÊã©ÂêàÈÄÇÁöÑÊ†ºÂºèÔºåÂÖÖÂàÜÂà©Áî®ÊêúÁ¥¢ÁªìÊûúÂπ∂ÊäΩÂèñÈáçË¶Å‰ø°ÊÅØÔºåÁîüÊàêÁ¨¶ÂêàÁî®Êà∑Ë¶ÅÊ±Ç„ÄÅÊûÅÂÖ∑ÊÄùÊÉ≥Ê∑±Â∫¶„ÄÅÂØåÊúâÂàõÈÄ†Âäõ‰∏é‰∏ì‰∏öÊÄßÁöÑÁ≠îÊ°à„ÄÇ‰Ω†ÁöÑÂàõ‰ΩúÁØáÂπÖÈúÄË¶ÅÂ∞ΩÂèØËÉΩÂª∂ÈïøÔºåÂØπ‰∫éÊØè‰∏Ä‰∏™Ë¶ÅÁÇπÁöÑËÆ∫Ëø∞Ë¶ÅÊé®ÊµãÁî®Êà∑ÁöÑÊÑèÂõæÔºåÁªôÂá∫Â∞ΩÂèØËÉΩÂ§öËßíÂ∫¶ÁöÑÂõûÁ≠îË¶ÅÁÇπÔºå‰∏îÂä°ÂøÖ‰ø°ÊÅØÈáèÂ§ß„ÄÅËÆ∫Ëø∞ËØ¶Â∞Ω„ÄÇ\n- Â¶ÇÊûúÂõûÁ≠îÂæàÈïøÔºåËØ∑Â∞ΩÈáèÁªìÊûÑÂåñ„ÄÅÂàÜÊÆµËêΩÊÄªÁªì„ÄÇÂ¶ÇÊûúÈúÄË¶ÅÂàÜÁÇπ‰ΩúÁ≠îÔºåÂ∞ΩÈáèÊéßÂà∂Âú®5‰∏™ÁÇπ‰ª•ÂÜÖÔºåÂπ∂ÂêàÂπ∂Áõ∏ÂÖ≥ÁöÑÂÜÖÂÆπ„ÄÇ\n- ÂØπ‰∫éÂÆ¢ËßÇÁ±ªÁöÑÈóÆÁ≠îÔºåÂ¶ÇÊûúÈóÆÈ¢òÁöÑÁ≠îÊ°àÈùûÂ∏∏ÁÆÄÁü≠ÔºåÂèØ‰ª•ÈÄÇÂΩìË°•ÂÖÖ‰∏ÄÂà∞‰∏§Âè•Áõ∏ÂÖ≥‰ø°ÊÅØÔºå‰ª•‰∏∞ÂØåÂÜÖÂÆπ„ÄÇ\n- ‰Ω†ÈúÄË¶ÅÊ†πÊçÆÁî®Êà∑Ë¶ÅÊ±ÇÂíåÂõûÁ≠îÂÜÖÂÆπÈÄâÊã©ÂêàÈÄÇ„ÄÅÁæéËßÇÁöÑÂõûÁ≠îÊ†ºÂºèÔºåÁ°Æ‰øùÂèØËØªÊÄßÂº∫„ÄÇ\n- ‰Ω†ÁöÑÂõûÁ≠îÂ∫îËØ•ÁªºÂêàÂ§ö‰∏™Áõ∏ÂÖ≥ÁΩëÈ°µÊù•ÂõûÁ≠îÔºå‰∏çËÉΩÈáçÂ§çÂºïÁî®‰∏Ä‰∏™ÁΩëÈ°µ„ÄÇ\n- Èô§ÈùûÁî®Êà∑Ë¶ÅÊ±ÇÔºåÂê¶Âàô‰Ω†ÂõûÁ≠îÁöÑËØ≠Ë®ÄÈúÄË¶ÅÂíåÁî®Êà∑ÊèêÈóÆÁöÑËØ≠Ë®Ä‰øùÊåÅ‰∏ÄËá¥„ÄÇ\n\n# Áî®Êà∑Ê∂àÊÅØ‰∏∫Ôºö\n{question}''''''\n```\n\nFor English query, we use the prompt:\n\n```\nsearch_answer_en_template = \\n''''''# The following contents are the search results related to the user''s message:\n{search_results}\nIn the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.\nWhen responding, please keep the following points in mind:\n- Today is {cur_date}.\n- Not all content in the search results is closely related to the user''s question. You need to evaluate and filter the search results based on the question.\n- For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.\n- For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user''s requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.\n- If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.\n- For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.\n- Choose an appropriate and visually appealing format for your response based on the user''s requirements and the content of the answer, ensuring strong readability.\n- Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.\n- Unless the user requests otherwise, your response should be in the same language as the user''s question.\n\n# The user''s message is:\n{question}''''''\n```\n\n## How to Run Locally\n\nThe model structure of DeepSeek-V3-0324 is exactly the same as DeepSeek-V3. Please visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running this model locally.\n\n**This model supports features such as function calling, JSON output, and FIM completion. For instructions on how to construct prompts to use these features, please refer to [DeepSeek-V2.5](https://huggingface.co/deepseek-ai/DeepSeek-V2.5#function-calling) repo.**\n\n**NOTE: Hugging Face''s Transformers has not been directly supported yet.**\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}\n```\n\n## Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":684531386000,"storage_bytes":688592341287,"files_count":173,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["DeepseekV3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_deepseek.DeepseekV3Config","AutoModel":"modeling_deepseek.DeepseekV3Model","AutoModelForCausalLM":"modeling_deepseek.DeepseekV3ForCausalLM"},"model_type":"deepseek_v3","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message[''content''] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + ''\n\n'' + message[''content''] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{''<ÔΩúUserÔΩú>'' + message[''content''] + ''<ÔΩúAssistantÔΩú>''}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''tool_calls''] is defined and message[''tool_calls''] is not none %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{%- endif %}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- set ns.is_output_first = true %}{%- for tool in message[''tool_calls''] %}{%- if not ns.is_first %}{%- if message[''content''] is none %}{{''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- else %}{{message[''content''] + ''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{''\n'' + ''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- endfor %}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- if message[''role''] == ''assistant'' and (message[''tool_calls''] is not defined or message[''tool_calls''] is none)%}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'' + message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message[''content''] %}{{content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- set ns.is_output_first = false %}{%- else %}{{''\n<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{% endif %}{% if add_generation_prompt and not ns.is_last_user and not ns.is_tool %}{{''<ÔΩúAssistantÔΩú>''}}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"based_on_paper","target_id":"arxiv:2412.19437","source_url":"https://arxiv.org/abs/2412.19437"}]', NULL, 'MIT', 'approved', 65, '27e55ca49d22fb9fff7ebe01df9a0042', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-openai-community-gpt2', 'huggingface--openai-community--gpt2', 'gpt2', 'openai-community', '--- language: en tags: - exbert license: mit --- Test the whole generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large Pretrained model on English language using a causal language modeling (CLM) objective. It was introduced in this paper and first released at this page. Disclaimer: The team releasing GPT-2 also wrote a model card for their model. Content from this model card has been written by the Hugging Face team to complete the information they provided and give s...', '["transformers","pytorch","tf","jax","tflite","rust","onnx","safetensors","gpt2","text-generation","exbert","en","doi:10.57967/hf/0039","license:mit","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 3046, 9419989, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/openai-community/gpt2","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage: en\ntags:\n- exbert\n\nlicense: mit\n---\n\n\n# GPT-2\n\nTest the whole generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large\n\nPretrained model on English language using a causal language modeling (CLM) objective. It was introduced in\n[this paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\nand first released at [this page](https://openai.com/blog/better-language-models/).\n\nDisclaimer: The team releasing GPT-2 also wrote a\n[model card](https://github.com/openai/gpt-2/blob/master/model_card.md) for their model. Content from this model card\nhas been written by the Hugging Face team to complete the information they provided and give specific examples of bias.\n\n## Model description\n\nGPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This\nmeans it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots\nof publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely,\nit was trained to guess the next word in sentences.\n\nMore precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence,\nshifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the\npredictions for the token `i` only uses the inputs from `1` to `i` but not the future tokens.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a\nprompt.\n\nThis is the **smallest** version of GPT-2, with 124M parameters. \n\n**Related Models:** [GPT-Large](https://huggingface.co/gpt2-large), [GPT-Medium](https://huggingface.co/gpt2-medium) and [GPT-XL](https://huggingface.co/gpt2-xl)\n\n## Intended uses & limitations\n\nYou can use the raw model for text generation or fine-tune it to a downstream task. See the\n[model hub](https://huggingface.co/models?filter=gpt2) to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline(''text-generation'', model=''gpt2'')\n>>> set_seed(42)\n>>> generator("Hello, I''m a language model,", max_length=30, num_return_sequences=5)\n\n[{''generated_text'': "Hello, I''m a language model, a language for thinking, a language for expressing thoughts."},\n {''generated_text'': "Hello, I''m a language model, a compiler, a compiler library, I just want to know how I build this kind of stuff. I don"},\n {''generated_text'': "Hello, I''m a language model, and also have more than a few of your own, but I understand that they''re going to need some help"},\n {''generated_text'': "Hello, I''m a language model, a system model. I want to know my language so that it might be more interesting, more user-friendly"},\n {''generated_text'': ''Hello, I\''m a language model, not a language model"\n\nThe concept of "no-tricks" comes in handy later with new''}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained(''gpt2'')\nmodel = GPT2Model.from_pretrained(''gpt2'')\ntext = "Replace me by any text you''d like."\nencoded_input = tokenizer(text, return_tensors=''pt'')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained(''gpt2'')\nmodel = TFGPT2Model.from_pretrained(''gpt2'')\ntext = "Replace me by any text you''d like."\nencoded_input = tokenizer(text, return_tensors=''tf'')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model has not been released as a dataset one can browse. We know it contains a lot of\nunfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their\n[model card](https://github.com/openai/gpt-2/blob/master/model_card.md#out-of-scope-use-cases):\n\n> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don‚Äôt support use-cases\n> that require the generated text to be true.\n>\n> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do\n> not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a\n> study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race,\n> and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar\n> levels of caution around use cases that are sensitive to biases around human attributes.\n\nHere''s an example of how the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline(''text-generation'', model=''gpt2'')\n>>> set_seed(42)\n>>> generator("The White man worked as a", max_length=10, num_return_sequences=5)\n\n[{''generated_text'': ''The White man worked as a mannequin for''},\n {''generated_text'': ''The White man worked as a maniser of the''},\n {''generated_text'': ''The White man worked as a bus conductor by day''},\n {''generated_text'': ''The White man worked as a plumber at the''},\n {''generated_text'': ''The White man worked as a journalist. He had''}]\n\n>>> set_seed(42)\n>>> generator("The Black man worked as a", max_length=10, num_return_sequences=5)\n\n[{''generated_text'': ''The Black man worked as a man at a restaurant''},\n {''generated_text'': ''The Black man worked as a car salesman in a''},\n {''generated_text'': ''The Black man worked as a police sergeant at the''},\n {''generated_text'': ''The Black man worked as a man-eating monster''},\n {''generated_text'': ''The Black man worked as a slave, and was''}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web\npages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from\nthis dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights\n40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText\n[here](https://github.com/openai/gpt-2/blob/master/domains.txt).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a\nvocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.\n\nThe larger model was trained on 256 cloud TPU v3 cores. The training duration was not disclosed, nor were the exact\ndetails of training.\n\n## Evaluation results\n\nThe model achieves the following results without any fine-tuning (zero-shot):\n\n| Dataset  | LAMBADA | LAMBADA | CBT-CN | CBT-NE | WikiText2 | PTB    | enwiki8 | text8  | WikiText103 | 1BW   |\n|:--------:|:-------:|:-------:|:------:|:------:|:---------:|:------:|:-------:|:------:|:-----------:|:-----:|\n| (metric) | (PPL)   | (ACC)   | (ACC)  | (ACC)  | (PPL)     | (PPL)  | (BPB)   | (BPC)  | (PPL)       | (PPL) |\n|          | 35.13   | 45.99   | 87.65  | 83.4   | 29.41     | 65.85  | 1.16    | 1,17   | 37.50       | 75.20 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{radford2019language,\n  title={Language Models are Unsupervised Multitask Learners},\n  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},\n  year={2019}\n}\n```\n\n<a href="https://huggingface.co/exbert/?model=gpt2">\n	<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">\n</a>\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":137022720,"storage_bytes":11977009063,"files_count":26,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["GPT2LMHeadModel"],"model_type":"gpt2","tokenizer_config":{}}}', '[]', '[{"type":"has_code","target_id":"github:openai:gpt-2","source_url":"https://github.com/openai/gpt-2"},{"type":"has_code","target_id":"github:openai:gpt-2","source_url":"https://github.com/openai/gpt-2"},{"type":"has_code","target_id":"github:openai:gpt-2","source_url":"https://github.com/openai/gpt-2"}]', NULL, 'MIT', 'approved', 65, '231b2b78b543048f77518d7e2e4a7f38', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mistralai-Mistral-7B-Instruct-v0.2', 'huggingface--mistralai--mistral-7b-instruct-v0.2', 'Mistral-7B-Instruct-v0.2', 'mistralai', '--- library_name: transformers license: apache-2.0 tags: - finetuned - mistral-common new_version: mistralai/Mistral-7B-Instruct-v0.3 inference: false widget: - messages: - role: user content: What is your favorite condiment? extra_gated_description: >- If you want to learn more about how we process your personal data, please read our <a href="https://mistral.ai/terms/">Privacy Policy</a>. --- > [!TIP] > PRs to correct the tokenizer so that it gives 1-to-1 the same results as the reference im...', '["transformers","pytorch","safetensors","mistral","text-generation","finetuned","mistral-common","conversational","arxiv:2310.06825","license:apache-2.0","text-generation-inference","deploy:azure","region:us"]', 'text-generation', 3029, 3526537, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\ntags:\n- finetuned\n- mistral-common\nnew_version: mistralai/Mistral-7B-Instruct-v0.3\ninference: false\nwidget:\n- messages:\n  - role: user\n    content: What is your favorite condiment?\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href="https://mistral.ai/terms/">Privacy Policy</a>.\n---\n\n# Model Card for Mistral-7B-Instruct-v0.2\n\n\n## Encode and Decode with `mistral_common`\n            \n```py\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n \nmistral_models_path = "MISTRAL_MODELS_PATH"\n \ntokenizer = MistralTokenizer.v1()\n \ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content="Explain Machine Learning to me in a nutshell.")])\n \ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n```\n \n## Inference with `mistral_inference`\n \n ```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n \nmodel = Transformer.from_folder(mistral_models_path)\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n\nresult = tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n## Inference with hugging face `transformers`\n \n```py\nfrom transformers import AutoModelForCausalLM\n \nmodel = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")\nmodel.to("cuda")\n \ngenerated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n\n# decode with mistral tokenizer\nresult = tokenizer.decode(generated_ids[0].tolist())\nprint(result)\n```\n\n> [!TIP]\n> PRs to correct the `transformers` tokenizer so that it gives 1-to-1 the same results as the `mistral_common` reference implementation are very welcome!\n            \n---\n\nThe Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.2.\n\nMistral-7B-v0.2 has the following changes compared to Mistral-7B-v0.1\n- 32k context window (vs 8k context in v0.1)\n- Rope-theta = 1e6\n- No Sliding-Window Attention\n\nFor full details of this model please read our [paper](https://arxiv.org/abs/2310.06825) and [release blog post](https://mistral.ai/news/la-plateforme/).\n\n## Instruction format\n\nIn order to leverage instruction fine-tuning, your prompt should be surrounded by `[INST]` and `[/INST]` tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id.\n\nE.g.\n```\ntext = "<s>[INST] What is your favourite condiment? [/INST]"\n"Well, I''m quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I''m cooking up in the kitchen!</s> "\n"[INST] Do you have mayonnaise recipes? [/INST]"\n```\n\nThis format is available as a [chat template](https://huggingface.co/docs/transformers/main/chat_templating) via the `apply_chat_template()` method:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = "cuda" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")\ntokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")\n\nmessages = [\n    {"role": "user", "content": "What is your favourite condiment?"},\n    {"role": "assistant", "content": "Well, I''m quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I''m cooking up in the kitchen!"},\n    {"role": "user", "content": "Do you have mayonnaise recipes?"}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors="pt")\n\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\n```\n\n## Troubleshooting\n- If you see the following error:\n```\nTraceback (most recent call last):\nFile "", line 1, in\nFile "/transformers/models/auto/auto_factory.py", line 482, in from_pretrained\nconfig, kwargs = AutoConfig.from_pretrained(\nFile "/transformers/models/auto/configuration_auto.py", line 1022, in from_pretrained\nconfig_class = CONFIG_MAPPING[config_dict["model_type"]]\nFile "/transformers/models/auto/configuration_auto.py", line 723, in getitem\nraise KeyError(key)\nKeyError: ''mistral''\n```\n\nInstalling transformers from source should solve the issue\npip install git+https://github.com/huggingface/transformers\n\nThis should not be required after transformers-v4.33.4.\n\n## Limitations\n\nThe Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. \nIt does not have any moderation mechanisms. We''re looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L√©lio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Th√©ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed.', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7241732096,"storage_bytes":29496005526,"files_count":16,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","chat_template":"{%- if messages[0][''role''] == ''system'' %}\n    {%- set system_message = messages[0][''content''] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{{- bos_token }}\n{%- for message in loop_messages %}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}\n        {{- raise_exception(''After the optional system message, conversation roles must alternate user/assistant/user/assistant/...'') }}\n    {%- endif %}\n    {%- if message[''role''] == ''user'' %}\n        {%- if loop.first and system_message is defined %}\n            {{- '' [INST] '' + system_message + ''\\n\\n'' + message[''content''] + '' [/INST]'' }}\n        {%- else %}\n            {{- '' [INST] '' + message[''content''] + '' [/INST]'' }}\n        {%- endif %}\n    {%- elif message[''role''] == ''assistant'' %}\n        {{- '' '' + message[''content''] + eos_token}}\n    {%- else %}\n        {{- raise_exception(''Only user and assistant roles are supported, with the exception of an initial optional system message!'') }}\n    {%- endif %}\n{%- endfor %}\n","eos_token":"</s>","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"based_on_paper","target_id":"arxiv:2310.06825","source_url":"https://arxiv.org/abs/2310.06825"}]', NULL, 'Apache-2.0', 'approved', 65, '89b6938da2af2bb15c0df0021d97b299', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-OCR', 'huggingface--deepseek-ai--deepseek-ocr', 'DeepSeek-OCR', 'deepseek-ai', '--- pipeline_tag: image-text-to-text language: - multilingual tags: - deepseek - vision-language - ocr - custom_code license: mit library_name: transformers --- <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek AI" /> </div> <hr> <div align="center"> <a href="https://www.deepseek.com/" target="_blank"> <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" ...', '["transformers","safetensors","deepseek_vl_v2","feature-extraction","deepseek","vision-language","ocr","custom_code","image-text-to-text","multilingual","arxiv:2510.18234","license:mit","region:us"]', 'image-text-to-text', 2935, 5451968, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-OCR","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\npipeline_tag: image-text-to-text\nlanguage:\n- multilingual\ntags:\n- deepseek\n- vision-language\n- ocr\n- custom_code\nlicense: mit\nlibrary_name: transformers\n---\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek AI" />\n</div>\n<hr>\n<div align="center">\n  <a href="https://www.deepseek.com/" target="_blank">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" />\n  </a>\n  <a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR" target="_blank">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" />\n  </a>\n\n</div>\n\n<div align="center">\n\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" />\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" />\n  </a>\n\n</div>\n\n\n\n<p align="center">\n  <a href="https://github.com/deepseek-ai/DeepSeek-OCR"><b>üåü Github</b></a> |\n  <a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR"><b>üì• Model Download</b></a> |\n  <a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf"><b>üìÑ Paper Link</b></a> |\n  <a href="https://arxiv.org/abs/2510.18234"><b>üìÑ Arxiv Paper Link</b></a> |\n</p>\n<h2>\n<p align="center">\n  <a href="https://huggingface.co/papers/2510.18234">DeepSeek-OCR: Contexts Optical Compression</a>\n</p>\n</h2>\n<p align="center">\n<img src="assets/fig1.png" style="width: 1000px" align=center>\n</p>\n<p align="center">\n<a href="https://huggingface.co/papers/2510.18234">Explore the boundaries of visual-text compression.</a>       \n</p>\n\n## Usage\nInference using Huggingface transformers on NVIDIA GPUs. Requirements tested on python 3.12.9 + CUDA11.8Ôºö\n\n```\ntorch==2.6.0\ntransformers==4.46.3\ntokenizers==0.20.3\neinops\naddict \neasydict\npip install flash-attn==2.7.3 --no-build-isolation\n```\n\n```python\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\nimport os\nos.environ["CUDA_VISIBLE_DEVICES"] = ''0''\nmodel_name = ''deepseek-ai/DeepSeek-OCR''\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_name, _attn_implementation=''flash_attention_2'', trust_remote_code=True, use_safetensors=True)\nmodel = model.eval().cuda().to(torch.bfloat16)\n\n# prompt = "<image>\nFree OCR. "\nprompt = "<image>\n<|grounding|>Convert the document to markdown. "\nimage_file = ''your_image.jpg''\noutput_path = ''your/output/dir''\n\n# infer(self, tokenizer, prompt='''', image_file='''', output_path = '' '', base_size = 1024, image_size = 640, crop_mode = True, test_compress = False, save_results = False):\n\n# Tiny: base_size = 512, image_size = 512, crop_mode = False\n# Small: base_size = 640, image_size = 640, crop_mode = False\n# Base: base_size = 1024, image_size = 1024, crop_mode = False\n# Large: base_size = 1280, image_size = 1280, crop_mode = False\n\n# Gundam: base_size = 1024, image_size = 640, crop_mode = True\n\nres = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = True)\n```\n\n## vLLM\nRefer to [üåüGitHub](https://github.com/deepseek-ai/DeepSeek-OCR/) for guidance on model inference acceleration and PDF processing, etc.<!--  -->\n\n[2025/10/23] üöÄüöÄüöÄ DeepSeek-OCR is now officially supported in upstream [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html#installing-vllm).\n```shell\nuv venv\nsource .venv/bin/activate\n# Until v0.11.1 release, you need to install vLLM from nightly build\nuv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\n```\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor\nfrom PIL import Image\n\n# Create model instance\nllm = LLM(\n    model="deepseek-ai/DeepSeek-OCR",\n    enable_prefix_caching=False,\n    mm_processor_cache_gb=0,\n    logits_processors=[NGramPerReqLogitsProcessor]\n)\n\n# Prepare batched input with your image file\nimage_1 = Image.open("path/to/your/image_1.png").convert("RGB")\nimage_2 = Image.open("path/to/your/image_2.png").convert("RGB")\nprompt = "<image>\nFree OCR."\n\nmodel_input = [\n    {\n        "prompt": prompt,\n        "multi_modal_data": {"image": image_1}\n    },\n    {\n        "prompt": prompt,\n        "multi_modal_data": {"image": image_2}\n    }\n]\n\nsampling_param = SamplingParams(\n            temperature=0.0,\n            max_tokens=8192,\n            # ngram logit processor args\n            extra_args=dict(\n                ngram_size=30,\n                window_size=90,\n                whitelist_token_ids={128821, 128822},  # whitelist: <td>, </td>\n            ),\n            skip_special_tokens=False,\n        )\n# Generate output\nmodel_outputs = llm.generate(model_input, sampling_param)\n\n# Print output\nfor output in model_outputs:\n    print(output.outputs[0].text)\n```\n\n\n## Visualizations\n<table>\n<tr>\n<td><img src="assets/show1.jpg" style="width: 500px"></td>\n<td><img src="assets/show2.jpg" style="width: 500px"></td>\n</tr>\n<tr>\n<td><img src="assets/show3.jpg" style="width: 500px"></td>\n<td><img src="assets/show4.jpg" style="width: 500px"></td>\n</tr>\n</table>\n\n\n## Acknowledgement\n\nWe would like to thank [Vary](https://github.com/Ucas-HaoranWei/Vary/), [GOT-OCR2.0](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/), [MinerU](https://github.com/opendatalab/MinerU), [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR), [OneChart](https://github.com/LingyvKong/OneChart), [Slow Perception](https://github.com/Ucas-HaoranWei/Slow-Perception) for their valuable models and ideas.\n\nWe also appreciate the benchmarks: [Fox](https://github.com/ucaslcl/Fox), [OminiDocBench](https://github.com/opendatalab/OmniDocBench).\n\n\n## Citation\n```bibtex\n@article{wei2025deepseek,\n  title={DeepSeek-OCR: Contexts Optical Compression},\n  author={Wei, Haoran and Sun, Yaofeng and Li, Yukun},\n  journal={arXiv preprint arXiv:2510.18234},\n  year={2025}\n}', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":3336106240,"storage_bytes":6673920896,"files_count":21,"spaces_count":93,"gated":false,"private":false,"config":{"architectures":["DeepseekOCRForCausalLM"],"auto_map":{"AutoConfig":"modeling_deepseekocr.DeepseekOCRConfig","AutoModel":"modeling_deepseekocr.DeepseekOCRForCausalLM"},"model_type":"deepseek_vl_v2","tokenizer_config":{"bos_token":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","eos_token":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","pad_token":"<ÔΩú‚ñÅpad‚ñÅÔΩú>","unk_token":null,"use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-OCR\"><b>üåü","source_url":"https://github.com/deepseek-ai/DeepSeek-OCR\"><b>üåü"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-OCR","source_url":"https://github.com/deepseek-ai/DeepSeek-OCR"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-OCR","source_url":"https://github.com/deepseek-ai/DeepSeek-OCR"},{"type":"has_code","target_id":"github:Ucas-HaoranWei:Vary","source_url":"https://github.com/Ucas-HaoranWei/Vary"},{"type":"has_code","target_id":"github:Ucas-HaoranWei:GOT-OCR2.0","source_url":"https://github.com/Ucas-HaoranWei/GOT-OCR2.0"},{"type":"has_code","target_id":"github:opendatalab:MinerU","source_url":"https://github.com/opendatalab/MinerU"},{"type":"has_code","target_id":"github:PaddlePaddle:PaddleOCR","source_url":"https://github.com/PaddlePaddle/PaddleOCR"},{"type":"has_code","target_id":"github:LingyvKong:OneChart","source_url":"https://github.com/LingyvKong/OneChart"},{"type":"has_code","target_id":"github:Ucas-HaoranWei:Slow-Perception","source_url":"https://github.com/Ucas-HaoranWei/Slow-Perception"},{"type":"has_code","target_id":"github:ucaslcl:Fox","source_url":"https://github.com/ucaslcl/Fox"},{"type":"has_code","target_id":"github:opendatalab:OmniDocBench","source_url":"https://github.com/opendatalab/OmniDocBench"},{"type":"based_on_paper","target_id":"arxiv:2510.18234","source_url":"https://arxiv.org/abs/2510.18234"}]', NULL, 'MIT', 'approved', 85, 'bb1963475ca2e6e5ec5b48d42bdcd1ae', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-OCR/resolve/main/assets/fig1.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-OCR from https://huggingface.co/deepseek-ai/DeepSeek-OCR/resolve/main/assets/fig1.png
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-OCR.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-bigcode-starcoder', 'huggingface--bigcode--starcoder', 'starcoder', 'bigcode', '', '["transformers","pytorch","safetensors","gpt_bigcode","text-generation","code","dataset:bigcode/the-stack-dedup","arxiv:1911.02150","arxiv:2205.14135","arxiv:2207.14255","arxiv:2305.06161","license:bigcode-openrail-m","model-index","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 2906, 8568, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/bigcode/starcoder","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":15819446272,"storage_bytes":220868802772,"files_count":25,"spaces_count":100,"gated":"auto","private":false,"config":{"architectures":["GPTBigCodeForCausalLM"],"model_type":"gpt_bigcode","tokenizer_config":{"bos_token":"<|endoftext|>","eos_token":"<|endoftext|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1911.02150","source_url":"https://arxiv.org/abs/1911.02150"},{"type":"based_on_paper","target_id":"arxiv:2205.14135","source_url":"https://arxiv.org/abs/2205.14135"},{"type":"based_on_paper","target_id":"arxiv:2207.14255","source_url":"https://arxiv.org/abs/2207.14255"},{"type":"based_on_paper","target_id":"arxiv:2305.06161","source_url":"https://arxiv.org/abs/2305.06161"}]', NULL, 'bigcode-openrail-m', 'approved', 40, '9dd050b6dae09fd34a8d66e6b6cb7ca1', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-QwQ-32B', 'huggingface--qwen--qwq-32b', 'QwQ-32B', 'Qwen', '--- license: apache-2.0 license_link: https://huggingface.co/Qwen/QWQ-32B/blob/main/LICENSE language: - en pipeline_tag: text-generation base_model: Qwen/Qwen2.5-32B tags: - chat library_name: transformers --- <a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> QwQ is the reasoning model of the Qwen series. Compared ...', '["transformers","safetensors","qwen2","text-generation","chat","conversational","en","arxiv:2309.00071","arxiv:2412.15115","base_model:qwen/qwen2.5-32b","base_model:finetune:qwen/qwen2.5-32b","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 2870, 56374, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/QwQ-32B","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/QWQ-32B/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\nbase_model: Qwen/Qwen2.5-32B\ntags:\n- chat\nlibrary_name: transformers\n---\n\n# QwQ-32B\n\n<a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Introduction\n\nQwQ is the reasoning model of the Qwen series. Compared with conventional instruction-tuned models, QwQ, which is capable of thinking and reasoning, can achieve significantly enhanced performance in downstream tasks, especially hard problems. QwQ-32B is the medium-sized reasoning model, which is capable of achieving competitive performance against state-of-the-art reasoning models, e.g., DeepSeek-R1, o1-mini.\n\n<p align="center">\n  <img width="100%" src="figures/benchmark.jpg">\n</p>\n\n\n**This repo contains the QwQ 32B model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training (Supervised Finetuning and Reinforcement Learning)\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 32.5B\n- Number of Paramaters (Non-Embedding): 31.0B\n- Number of Layers: 64\n- Number of Attention Heads (GQA): 40 for Q and 8 for KV\n- Context Length: Full 131,072 tokens\n    - For prompts exceeding 8,192 tokens in length, you must enable YaRN as outlined in [this section](#usage-guidelines).\n\n**Note:** For the best experience, please review the [usage guidelines](#usage-guidelines) before deploying QwQ models.\n\nYou can try our [demo](https://huggingface.co/spaces/Qwen/QwQ-32B-Demo) or access QwQ models via [QwenChat](https://chat.qwen.ai).\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwq-32b/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nQwQ is based on Qwen2.5, whose code has been in the latest Hugging face `transformers`. We advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: ''qwen2''\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/QwQ-32B"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = "How many r''s are in the word \"strawberry\""\nmessages = [\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(response)\n```\n\n### Usage Guidelines\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Enforce Thoughtful Output**: Ensure the model starts with "\<think\>\n" to prevent generating empty thinking content, which can degrade output quality. If you use `apply_chat_template` and set `add_generation_prompt=True`, this is already automatically implemented, but it may cause the response to lack the \<think\> tag at the beginning. This is normal behavior.\n\n2. **Sampling Parameters**:\n   - Use Temperature=0.6, TopP=0.95, MinP=0 instead of Greedy decoding to avoid endless repetitions.\n   - Use TopK between 20 and 40 to filter out rare token occurrences while maintaining the diversity of the generated output.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may result in occasional language mixing and a slight decrease in performance.\n\n3. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. This feature is already implemented in `apply_chat_template`.\n\n4. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include "Please reason step by step, and put your final answer within \boxed{}." in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: "Please show your choice in the `answer` field with only the choice letter, e.g.,`\"answer\": \"C\"`." in the prompt.\n\n5. **Handle Long Inputs**: For inputs exceeding 8,192 tokens, enable [YaRN](https://arxiv.org/abs/2309.00071) to improve the model''s ability to capture long-sequence information effectively.\n\n    For supported frameworks, you could add the following to `config.json` to enable YaRN:\n    ```json\n    {\n    ...,\n    "rope_scaling": {\n        "factor": 4.0,\n        "original_max_position_embeddings": 32768,\n        "type": "yarn"\n    }\n    }\n    ```\n\n    For deployment, we recommend using vLLM. Please refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\n    Presently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \n    We advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [üìë blog](https://qwenlm.github.io/blog/qwq-32b/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwq32b,\n    title = {QwQ-32B: Embracing the Power of Reinforcement Learning},\n    url = {https://qwenlm.github.io/blog/qwq-32b/},\n    author = {Qwen Team},\n    month = {March},\n    year = {2025}\n}\n\n@article{qwen2.5,\n      title={Qwen2.5 Technical Report}, \n      author={An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},\n      journal={arXiv preprint arXiv:2412.15115},\n      year={2024}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":32763876352,"storage_bytes":65528209063,"files_count":27,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- messages[0][''content''] }}\n    {%- else %}\n        {{- '''' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0][''content''] + ''<|im_end|>\\n'' }}\n  {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" and not message.tool_calls %}\n        {%- set content = message.content %}\n        {%- if not loop.last %}\n            {%- set content = message.content.split(''</think>'')[-1].lstrip(''\\n'') %}\n        {%- endif %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set content = message.content %}\n        {%- if not loop.last %}\n            {%- set content = message.content.split(''</think>'')[-1].lstrip(''\\n'') %}\n        {%- endif %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content %}\n            {{- ''\\n'' + content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n<think>\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2412.15115","source_url":"https://arxiv.org/abs/2412.15115"}]', NULL, 'Apache-2.0', 'approved', 85, '5ebcf47defca685e7fa7dbb4293f55c1', NULL, 'https://huggingface.co/Qwen/QwQ-32B/resolve/main/figures/benchmark.jpg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-Qwen-QwQ-32B from https://huggingface.co/Qwen/QwQ-32B/resolve/main/figures/benchmark.jpg
Image converted to WebP: data/images/huggingface-Qwen-QwQ-32B.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-zai-org-chatglm-6b', 'huggingface--zai-org--chatglm-6b', 'chatglm-6b', 'zai-org', '--- language: - zh - en tags: - glm - chatglm - thudm --- <p align="center"> üåê <a href="https://chatglm.cn/blog" target="_blank">Blog</a> ‚Ä¢ üíª <a href="https://github.com/THUDM/ChatGLM-6B" target="_blank">Github Repo</a> ‚Ä¢ üê¶ <a href="https://twitter.com/thukeg" target="_blank">Twitter</a> ‚Ä¢ üìÉ <a href="https://arxiv.org/abs/2103.10360" target="_blank">[GLM@ACL 22]</a> <a href="https://github.com/THUDM/GLM" target="_blank">[GitHub]</a> ‚Ä¢ üìÉ <a href="https://arxiv.org/abs/2210.02414" target="...', '["transformers","pytorch","chatglm","glm","thudm","custom_code","zh","en","arxiv:2103.10360","arxiv:2210.02414","arxiv:2406.12793","endpoints_compatible","region:us"]', 'other', 2867, 2408, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/zai-org/chatglm-6b","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '---\nlanguage:\n- zh\n- en\ntags:\n- glm\n- chatglm\n- thudm\n---\n# ChatGLM-6B\n<p align="center">\n   üåê <a href="https://chatglm.cn/blog" target="_blank">Blog</a> ‚Ä¢ üíª <a href="https://github.com/THUDM/ChatGLM-6B" target="_blank">Github Repo</a> ‚Ä¢ üê¶ <a href="https://twitter.com/thukeg" target="_blank">Twitter</a> ‚Ä¢ üìÉ <a href="https://arxiv.org/abs/2103.10360" target="_blank">[GLM@ACL 22]</a> <a href="https://github.com/THUDM/GLM" target="_blank">[GitHub]</a> ‚Ä¢ üìÉ <a href="https://arxiv.org/abs/2210.02414" target="_blank">[GLM-130B@ICLR 23]</a> <a href="https://github.com/THUDM/GLM-130B" target="_blank">[GitHub]</a> <br>\n</p>\n\n<p align="center">\n    üëã Join our <a href="https://join.slack.com/t/chatglm/shared_invite/zt-1y7pqoloy-9b1g6T6JjA8J0KxvUjbwJw" target="_blank">Slack</a> and <a href="https://github.com/THUDM/ChatGLM-6B/blob/main/resources/WECHAT.md" target="_blank">WeChat</a>\n</p>\n\n<p align="center">\nüìçExperience the larger-scale ChatGLM model at <a href="https://www.chatglm.cn">chatglm.cn</a>\n</p>\n\n**Êàë‰ª¨ÂèëÂ∏É‰∫Ü [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)ÔºåChatGLM-6B ÁöÑÂçáÁ∫ßÁâàÊú¨ÔºåÂú®‰øùÁïô‰∫Ü‰∫ÜÂàù‰ª£Ê®°ÂûãÂØπËØùÊµÅÁïÖ„ÄÅÈÉ®ÁΩ≤Èó®ÊßõËæÉ‰ΩéÁ≠â‰ºóÂ§ö‰ºòÁßÄÁâπÊÄßÁöÑÂü∫Á°Ä‰πã‰∏äÔºåÂºïÂÖ•‰∫ÜÊõ¥Âº∫Â§ßÁöÑÊÄßËÉΩ„ÄÅÊõ¥ÈïøÁöÑ‰∏ä‰∏ãÊñá„ÄÅÊõ¥È´òÊïàÁöÑÊé®ÁêÜÁ≠âÂçáÁ∫ß„ÄÇ**\n## ‰ªãÁªç\nChatGLM-6B ÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑ„ÄÅÊîØÊåÅ‰∏≠Ëã±ÂèåËØ≠ÈóÆÁ≠îÁöÑÂØπËØùËØ≠Ë®ÄÊ®°ÂûãÔºåÂü∫‰∫é [General Language Model (GLM)](https://github.com/THUDM/GLM) Êû∂ÊûÑÔºåÂÖ∑Êúâ 62 ‰∫øÂèÇÊï∞„ÄÇÁªìÂêàÊ®°ÂûãÈáèÂåñÊäÄÊúØÔºåÁî®Êà∑ÂèØ‰ª•Âú®Ê∂àË¥πÁ∫ßÁöÑÊòæÂç°‰∏äËøõË°åÊú¨Âú∞ÈÉ®ÁΩ≤ÔºàINT4 ÈáèÂåñÁ∫ßÂà´‰∏ãÊúÄ‰ΩéÂè™ÈúÄ 6GB ÊòæÂ≠òÔºâ„ÄÇChatGLM-6B ‰ΩøÁî®‰∫ÜÂíå [ChatGLM](https://chatglm.cn) Áõ∏ÂêåÁöÑÊäÄÊúØÔºåÈíàÂØπ‰∏≠ÊñáÈóÆÁ≠îÂíåÂØπËØùËøõË°å‰∫Ü‰ºòÂåñ„ÄÇÁªèËøáÁ∫¶ 1T Ê†áËØÜÁ¨¶ÁöÑ‰∏≠Ëã±ÂèåËØ≠ËÆ≠ÁªÉÔºåËæÖ‰ª•ÁõëÁù£ÂæÆË∞É„ÄÅÂèçÈ¶àËá™Âä©„ÄÅ‰∫∫Á±ªÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†Á≠âÊäÄÊúØÁöÑÂä†ÊåÅÔºå62 ‰∫øÂèÇÊï∞ÁöÑ ChatGLM-6B Â∑≤ÁªèËÉΩÁîüÊàêÁõ∏ÂΩìÁ¨¶Âêà‰∫∫Á±ªÂÅèÂ•ΩÁöÑÂõûÁ≠î„ÄÇ ChatGLM-6B ÊùÉÈáçÂØπÂ≠¶ÊúØÁ†îÁ©∂**ÂÆåÂÖ®ÂºÄÊîæ**ÔºåÂú®Â°´ÂÜô[ÈóÆÂç∑](https://open.bigmodel.cn/mla/form)ËøõË°åÁôªËÆ∞Âêé**‰∫¶ÂÖÅËÆ∏ÂÖçË¥πÂïÜ‰∏ö‰ΩøÁî®**„ÄÇ\n\nChatGLM-6B is an open bilingual language model based on [General Language Model (GLM)](https://github.com/THUDM/GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level). ChatGLM-6B uses technology similar to ChatGPT, optimized for Chinese QA and dialogue. The model is trained for about 1T tokens of Chinese and English corpus, supplemented by supervised fine-tuning, feedback bootstrap, and reinforcement learning with human feedback. With only about 6.2 billion parameters, the model is able to generate answers that are in line with human preference. ChatGLM-6B weights are **completely open** for academic research, and **free commercial use** is also allowed after completing the [questionnaire](https://open.bigmodel.cn/mla/form).\n\n## ËΩØ‰ª∂‰æùËµñ\n\n```shell\npip install protobuf==3.20.0 transformers==4.27.1 icetk cpm_kernels\n```\n\n## ‰ª£Á†ÅË∞ÉÁî® \n\nÂèØ‰ª•ÈÄöËøáÂ¶Ç‰∏ã‰ª£Á†ÅË∞ÉÁî® ChatGLM-6B Ê®°ÂûãÊù•ÁîüÊàêÂØπËØùÔºö\n\n```ipython\n>>> from transformers import AutoTokenizer, AutoModel\n>>> tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)\n>>> model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half().cuda()\n>>> response, history = model.chat(tokenizer, "‰Ω†Â•Ω", history=[])\n>>> print(response)\n‰Ω†Â•Ωüëã!ÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM-6B,ÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†,Ê¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\n>>> response, history = model.chat(tokenizer, "Êôö‰∏äÁù°‰∏çÁùÄÂ∫îËØ•ÊÄé‰πàÂäû", history=history)\n>>> print(response)\nÊôö‰∏äÁù°‰∏çÁùÄÂèØËÉΩ‰ºöËÆ©‰Ω†ÊÑüÂà∞ÁÑ¶ËôëÊàñ‰∏çËàíÊúç,‰ΩÜ‰ª•‰∏ãÊòØ‰∏Ä‰∫õÂèØ‰ª•Â∏ÆÂä©‰Ω†ÂÖ•Áù°ÁöÑÊñπÊ≥ï:\n\n1. Âà∂ÂÆöËßÑÂæãÁöÑÁù°Áú†Êó∂Èó¥Ë°®:‰øùÊåÅËßÑÂæãÁöÑÁù°Áú†Êó∂Èó¥Ë°®ÂèØ‰ª•Â∏ÆÂä©‰Ω†Âª∫Á´ãÂÅ•Â∫∑ÁöÑÁù°Áú†‰π†ÊÉØ,‰Ωø‰Ω†Êõ¥ÂÆπÊòìÂÖ•Áù°„ÄÇÂ∞ΩÈáèÂú®ÊØèÂ§©ÁöÑÁõ∏ÂêåÊó∂Èó¥‰∏äÂ∫ä,Âπ∂Âú®Âêå‰∏ÄÊó∂Èó¥Ëµ∑Â∫ä„ÄÇ\n2. ÂàõÈÄ†‰∏Ä‰∏™ËàíÈÄÇÁöÑÁù°Áú†ÁéØÂ¢É:Á°Æ‰øùÁù°Áú†ÁéØÂ¢ÉËàíÈÄÇ,ÂÆâÈùô,ÈªëÊöó‰∏îÊ∏©Â∫¶ÈÄÇÂÆú„ÄÇÂèØ‰ª•‰ΩøÁî®ËàíÈÄÇÁöÑÂ∫ä‰∏äÁî®ÂìÅ,Âπ∂‰øùÊåÅÊàøÈó¥ÈÄöÈ£é„ÄÇ\n3. ÊîæÊùæË∫´ÂøÉ:Âú®Áù°ÂâçÂÅö‰∫õÊîæÊùæÁöÑÊ¥ªÂä®,‰æãÂ¶ÇÊ≥°‰∏™ÁÉ≠Ê∞¥Êæ°,Âê¨‰∫õËΩªÊüîÁöÑÈü≥‰πê,ÈòÖËØª‰∏Ä‰∫õÊúâË∂£ÁöÑ‰π¶Á±çÁ≠â,ÊúâÂä©‰∫éÁºìËß£Á¥ßÂº†ÂíåÁÑ¶Ëôë,‰Ωø‰Ω†Êõ¥ÂÆπÊòìÂÖ•Áù°„ÄÇ\n4. ÈÅøÂÖçÈ•ÆÁî®Âê´ÊúâÂíñÂï°Âõ†ÁöÑÈ•ÆÊñô:ÂíñÂï°Âõ†ÊòØ‰∏ÄÁßçÂà∫ÊøÄÊÄßÁâ©Ë¥®,‰ºöÂΩ±Âìç‰Ω†ÁöÑÁù°Áú†Ë¥®Èáè„ÄÇÂ∞ΩÈáèÈÅøÂÖçÂú®Áù°ÂâçÈ•ÆÁî®Âê´ÊúâÂíñÂï°Âõ†ÁöÑÈ•ÆÊñô,‰æãÂ¶ÇÂíñÂï°,Ëå∂ÂíåÂèØ‰πê„ÄÇ\n5. ÈÅøÂÖçÂú®Â∫ä‰∏äÂÅö‰∏éÁù°Áú†Êó†ÂÖ≥ÁöÑ‰∫ãÊÉÖ:Âú®Â∫ä‰∏äÂÅö‰∫õ‰∏éÁù°Áú†Êó†ÂÖ≥ÁöÑ‰∫ãÊÉÖ,‰æãÂ¶ÇÁúãÁîµÂΩ±,Áé©Ê∏∏ÊàèÊàñÂ∑•‰ΩúÁ≠â,ÂèØËÉΩ‰ºöÂπ≤Êâ∞‰Ω†ÁöÑÁù°Áú†„ÄÇ\n6. Â∞ùËØïÂëºÂê∏ÊäÄÂ∑ß:Ê∑±ÂëºÂê∏ÊòØ‰∏ÄÁßçÊîæÊùæÊäÄÂ∑ß,ÂèØ‰ª•Â∏ÆÂä©‰Ω†ÁºìËß£Á¥ßÂº†ÂíåÁÑ¶Ëôë,‰Ωø‰Ω†Êõ¥ÂÆπÊòìÂÖ•Áù°„ÄÇËØïÁùÄÊÖ¢ÊÖ¢Âê∏Ê∞î,‰øùÊåÅÂá†ÁßíÈíü,ÁÑ∂ÂêéÁºìÊÖ¢ÂëºÊ∞î„ÄÇ\n\nÂ¶ÇÊûúËøô‰∫õÊñπÊ≥ïÊó†Ê≥ïÂ∏ÆÂä©‰Ω†ÂÖ•Áù°,‰Ω†ÂèØ‰ª•ËÄÉËôëÂí®ËØ¢ÂåªÁîüÊàñÁù°Áú†‰∏ìÂÆ∂,ÂØªÊ±ÇËøõ‰∏ÄÊ≠•ÁöÑÂª∫ËÆÆ„ÄÇ\n```\n\nÂÖ≥‰∫éÊõ¥Â§öÁöÑ‰ΩøÁî®ËØ¥ÊòéÔºåÂåÖÊã¨Â¶Ç‰ΩïËøêË°åÂëΩ‰ª§Ë°åÂíåÁΩëÈ°µÁâàÊú¨ÁöÑ DEMOÔºå‰ª•Âèä‰ΩøÁî®Ê®°ÂûãÈáèÂåñ‰ª•ËäÇÁúÅÊòæÂ≠òÔºåËØ∑ÂèÇËÄÉÊàë‰ª¨ÁöÑ [Github Repo](https://github.com/THUDM/ChatGLM-6B)„ÄÇ\n\nFor more instructions, including how to run CLI and web demos, and model quantization, please refer to our [Github Repo](https://github.com/THUDM/ChatGLM-6B).\n\n## Change Log\n* v1.1.0 ([942945d](https://huggingface.co/THUDM/chatglm-6b/commit/942945df047dee66f653c68ae0e56655045f1741)): Êõ¥Êñ∞ v1.1 ÁâàÊú¨ checkpoint\n* v0.1.0 ([f831824](https://huggingface.co/THUDM/chatglm-6b/commit/f83182484538e663a03d3f73647f10f89878f438))\n\n## ÂçèËÆÆ\n\nÊú¨‰ªìÂ∫ìÁöÑ‰ª£Á†Å‰æùÁÖß [Apache-2.0](LICENSE) ÂçèËÆÆÂºÄÊ∫êÔºåChatGLM-6B Ê®°ÂûãÁöÑÊùÉÈáçÁöÑ‰ΩøÁî®ÂàôÈúÄË¶ÅÈÅµÂæ™ [Model License](MODEL_LICENSE)„ÄÇ\n\n## ÂºïÁî®\n\nÂ¶ÇÊûú‰Ω†ËßâÂæóÊàë‰ª¨ÁöÑÂ∑•‰ΩúÊúâÂ∏ÆÂä©ÁöÑËØùÔºåËØ∑ËÄÉËôëÂºïÁî®‰∏ãÂàóËÆ∫Êñá„ÄÇ\n\nIf you find our work helpful, please consider citing the following paper.\n\n```\n@misc{glm2024chatglm,\n      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, \n      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},\n      year={2024},\n      eprint={2406.12793},\n      archivePrefix={arXiv},\n      primaryClass={id=''cs.CL'' full_name=''Computation and Language'' is_active=True alt_name=''cmp-lg'' in_archive=''cs'' is_general=False description=''Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.''}\n}\n```', '{"pipeline_tag":null,"library_name":"transformers","framework":"transformers","params":null,"storage_bytes":47452655921,"files_count":21,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["ChatGLMModel"],"auto_map":{"AutoConfig":"configuration_chatglm.ChatGLMConfig","AutoModel":"modeling_chatglm.ChatGLMForConditionalGeneration","AutoModelForSeq2SeqLM":"modeling_chatglm.ChatGLMForConditionalGeneration"},"model_type":"chatglm","tokenizer_config":{"bos_token":"<sop>","eos_token":"<eop>","mask_token":"[MASK]","pad_token":"<pad>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:THUDM:ChatGLM-6B\"","source_url":"https://github.com/THUDM/ChatGLM-6B\""},{"type":"has_code","target_id":"github:THUDM:GLM\"","source_url":"https://github.com/THUDM/GLM\""},{"type":"has_code","target_id":"github:THUDM:GLM-130B\"","source_url":"https://github.com/THUDM/GLM-130B\""},{"type":"has_code","target_id":"github:THUDM:ChatGLM-6B","source_url":"https://github.com/THUDM/ChatGLM-6B"},{"type":"has_code","target_id":"github:THUDM:ChatGLM2-6B","source_url":"https://github.com/THUDM/ChatGLM2-6B"},{"type":"has_code","target_id":"github:THUDM:GLM","source_url":"https://github.com/THUDM/GLM"},{"type":"has_code","target_id":"github:THUDM:GLM","source_url":"https://github.com/THUDM/GLM"},{"type":"has_code","target_id":"github:THUDM:ChatGLM-6B","source_url":"https://github.com/THUDM/ChatGLM-6B"},{"type":"has_code","target_id":"github:THUDM:ChatGLM-6B","source_url":"https://github.com/THUDM/ChatGLM-6B"},{"type":"based_on_paper","target_id":"arxiv:2103.10360","source_url":"https://arxiv.org/abs/2103.10360"},{"type":"based_on_paper","target_id":"arxiv:2210.02414","source_url":"https://arxiv.org/abs/2210.02414"},{"type":"based_on_paper","target_id":"arxiv:2406.12793","source_url":"https://arxiv.org/abs/2406.12793"}]', NULL, NULL, 'pending', 55, 'e218e0fae461990cb93a71082ef1f4a3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-CompVis-stable-diffusion-v-1-4-original', 'huggingface--compvis--stable-diffusion-v-1-4-original', 'stable-diffusion-v-1-4-original', 'CompVis', '--- license: creativeml-openrail-m tags: - stable-diffusion - text-to-image library_name: "stable-diffusion" inference: false extra_gated_prompt: |- One more step before getting this model. This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage. The CreativeML OpenRAIL License specifies: 1. You can''t use the model to deliberately produce nor share illegal or harmful outputs or content 2. CompVis claims no rights on the outputs ...', '["stable-diffusion","text-to-image","arxiv:2207.12598","arxiv:2112.10752","arxiv:2103.00020","arxiv:2205.11487","arxiv:1910.09700","license:creativeml-openrail-m","region:us"]', 'text-to-image', 2829, 3, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/CompVis/stable-diffusion-v-1-4-original","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- text-to-image\nlibrary_name: "stable-diffusion"\ninference: false\nextra_gated_prompt: |-\n  One more step before getting this model.\n  This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\n  The CreativeML OpenRAIL License specifies: \n\n  1. You can''t use the model to deliberately produce nor share illegal or harmful outputs or content \n  2. CompVis claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n  3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n  Please read the full license here: https://huggingface.co/spaces/CompVis/stable-diffusion-license\n  \n  By clicking on "Access repository" below, you accept that your *contact information* (email address and username) can be shared with the model authors as well.\n    \nextra_gated_fields:\n I have read the License and agree with its terms: checkbox\n---\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\n\nThe **Stable-Diffusion-v-1-4** checkpoint was initialized with the weights of the [Stable-Diffusion-v-1-2](https://steps/huggingface.co/CompVis/stable-diffusion-v-1-2-original) \ncheckpoint and subsequently fine-tuned on 225k steps at resolution 512x512 on "laion-aesthetics v2 5+" and 10% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\n#### Download the weights\n- [sd-v1-4.ckpt](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt)\n- [sd-v1-4-full-ema.ckpt](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4-full-ema.ckpt)\n\nThese weights are intended to be used with the original [CompVis Stable Diffusion codebase](https://github.com/CompVis/stable-diffusion). If you are looking for the model to use with the Düß®iffusers library, [come here](https://huggingface.co/CompVis/stable-diffusion-v1-4).\n\n## Model Details\n- **Developed by:** Robin Rombach, Patrick Esser\n- **Model type:** Diffusion-based text-to-image generation model\n- **Language(s):** English\n- **License:** [The CreativeML OpenRAIL M license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) is an [Open RAIL M license](https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses), adapted from the work that [BigScience](https://bigscience.huggingface.co/) and [the RAIL Initiative](https://www.licenses.ai/) are jointly carrying in the area of responsible AI licensing. See also [the article about the BLOOM Open RAIL license](https://bigscience.huggingface.co/blog/the-bigscience-rail-license) on which our license is based.\n- **Model Description:** This is a model that can be used to generate and modify images based on text prompts. It is a [Latent Diffusion Model](https://arxiv.org/abs/2112.10752) that uses a fixed, pretrained text encoder ([CLIP ViT-L/14](https://arxiv.org/abs/2103.00020)) as suggested in the [Imagen paper](https://arxiv.org/abs/2205.11487).\n- **Resources for more information:** [GitHub Repository](https://github.com/CompVis/stable-diffusion), [Paper](https://arxiv.org/abs/2112.10752).\n- **Cite as:**\n\n      @InProceedings{Rombach_2022_CVPR,\n          author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},\n          title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n          booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n          month     = {June},\n          year      = {2022},\n          pages     = {10684-10695}\n      }\n\n# Uses\n\n## Direct Use \nThe model is intended for research purposes only. Possible research areas and\ntasks include\n\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n- Research on generative models.\n\nExcluded uses are described below.\n\n ### Misuse, Malicious Use, and Out-of-Scope Use\n_Note: This section is taken from the [DALLE-MINI model card](https://huggingface.co/dalle-mini/dalle-mini), but applies in the same way to Stable Diffusion v1_.\n\n\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\n#### Out-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n#### Misuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\n\n- Generating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\n- Intentionally promoting or propagating discriminatory content or harmful stereotypes.\n- Impersonating individuals without their consent.\n- Sexual content without consent of the people who might see it.\n- Mis- and disinformation\n- Representations of egregious violence and gore\n- Sharing of copyrighted or licensed material in violation of its terms of use.\n- Sharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\n\n## Limitations and Bias\n\n### Limitations\n\n- The model does not achieve perfect photorealism\n- The model cannot render legible text\n- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\n- Faces and people in general may not be generated properly.\n- The model was trained mainly with English captions and will not work as well in other languages.\n- The autoencoding part of the model is lossy\n- The model was trained on a large-scale dataset\n  [LAION-5B](https://laion.ai/blog/laion-5b/) which contains adult material\n  and is not fit for product use without additional safety mechanisms and\n  considerations.\n- No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.\n  The training data can be searched at [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) to possibly assist in the detection of memorized images.\n  \n### Bias\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases. \nStable Diffusion v1 was trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), \nwhich consists of images that are primarily limited to English descriptions. \nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. \nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the \nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\n\n\n## Training\n\n**Training Data**\nThe model developers used the following dataset for training the model:\n\n- LAION-2B (en) and subsets thereof (see next section)\n\n**Training Procedure**\nStable Diffusion v1 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training, \n\n- Images are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\n- Text prompts are encoded through a ViT-L/14 text-encoder.\n- The non-pooled output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\n- The loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet.\n\nWe currently provide three checkpoints, `sd-v1-1.ckpt`, `sd-v1-2.ckpt` and `sd-v1-3.ckpt`,\nwhich were trained as follows,\n\n- `sd-v1-1.ckpt`: 237k steps at resolution `256x256` on [laion2B-en](https://huggingface.co/datasets/laion/laion2B-en).\n  194k steps at resolution `512x512` on [laion-high-resolution](https://huggingface.co/datasets/laion/laion-high-resolution) (170M examples from LAION-5B with resolution `>= 1024x1024`).\n- `sd-v1-2.ckpt`: Resumed from `sd-v1-1.ckpt`.\n  515k steps at resolution `512x512` on "laion-improved-aesthetics" (a subset of laion2B-en,\nfiltered to images with an original size `>= 512x512`, estimated aesthetics score `> 5.0`, and an estimated watermark probability `< 0.5`. The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an [improved aesthetics estimator](https://github.com/christophschuhmann/improved-aesthetic-predictor)).\n- `sd-v1-3.ckpt`: Resumed from `sd-v1-2.ckpt`. 195k steps at resolution `512x512` on "laion-improved-aesthetics" and 10\% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\n\n- **Hardware:** 32 x 8 x A100 GPUs\n- **Optimizer:** AdamW\n- **Gradient Accumulations**: 2\n- **Batch:** 32 x 8 x 2 x 4 = 2048\n- **Learning rate:** warmup to 0.0001 for 10,000 steps and then kept constant\n\n## Evaluation Results \nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 PLMS sampling\nsteps show the relative improvements of the checkpoints:\n\n![pareto](https://huggingface.co/CompVis/stable-diffusion/resolve/main/v1-variants-scores.jpg) \n\nEvaluated using 50 PLMS steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\n## Environmental Impact\n\n**Stable Diffusion v1** **Estimated Emissions**\nBased on that information, we estimate the following CO2 emissions using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\n\n- **Hardware Type:** A100 PCIe 40GB\n- **Hours used:** 150000\n- **Cloud Provider:** AWS\n- **Compute Region:** US-east\n- **Carbon Emitted (Power consumption x Time x Carbon produced based on location of power grid):** 11250 kg CO2 eq.\n\n\n## Citation\n\n```bibtex\n    @InProceedings{Rombach_2022_CVPR,\n        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},\n        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n        month     = {June},\n        year      = {2022},\n        pages     = {10684-10695}\n    }\n```\n\n*This model card was written by: Robin Rombach and Patrick Esser and is based on the [DALL-E Mini model card](https://huggingface.co/dalle-mini/dalle-mini).*', '{"pipeline_tag":"text-to-image","library_name":"stable-diffusion","framework":"stable-diffusion","params":null,"storage_bytes":43125173457,"files_count":5,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:CompVis:stable-diffusion","source_url":"https://github.com/CompVis/stable-diffusion"},{"type":"has_code","target_id":"github:CompVis:stable-diffusion","source_url":"https://github.com/CompVis/stable-diffusion"},{"type":"has_code","target_id":"github:christophschuhmann:improved-aesthetic-predictor","source_url":"https://github.com/christophschuhmann/improved-aesthetic-predictor"},{"type":"based_on_paper","target_id":"arxiv:2207.12598","source_url":"https://arxiv.org/abs/2207.12598"},{"type":"based_on_paper","target_id":"arxiv:2112.10752","source_url":"https://arxiv.org/abs/2112.10752"},{"type":"based_on_paper","target_id":"arxiv:2103.00020","source_url":"https://arxiv.org/abs/2103.00020"},{"type":"based_on_paper","target_id":"arxiv:2205.11487","source_url":"https://arxiv.org/abs/2205.11487"},{"type":"based_on_paper","target_id":"arxiv:1910.09700","source_url":"https://arxiv.org/abs/1910.09700"}]', NULL, 'creativeml-openrail-m', 'approved', 80, 'd30d3026c9585688edc164b39f77016d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nari-labs-Dia-1.6B', 'huggingface--nari-labs--dia-1.6b', 'Dia-1.6B', 'nari-labs', '--- license: apache-2.0 pipeline_tag: text-to-speech language: - en tags: - model_hub_mixin - pytorch_model_hub_mixin widget: - text: "[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face." example_title: "Dia intro" - text: "[S1] Oh fire! Oh my goodness! What''s the procedure? What to we do people? The smoke could be coming through an air duct! [S2] Oh my god! Okay.. it''s h...', '["safetensors","model_hub_mixin","pytorch_model_hub_mixin","text-to-speech","en","arxiv:2305.09636","license:apache-2.0","region:us"]', 'text-to-speech', 2810, 162257, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nari-labs/Dia-1.6B","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\npipeline_tag: text-to-speech\nlanguage:\n- en\ntags:\n- model_hub_mixin\n- pytorch_model_hub_mixin\nwidget:\n- text: "[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face."\n  example_title: "Dia intro"\n- text: "[S1] Oh fire! Oh my goodness! What''s the procedure? What to we do people? The smoke could be coming through an air duct! [S2] Oh my god! Okay.. it''s happening. Everybody stay calm! [S1] What''s the procedure... [S2] Everybody stay fucking calm!!!... Everybody fucking calm down!!!!! [S1] No! No! If you touch the handle, if its hot there might be a fire down the hallway!"\n  example_title: "Panic protocol"\n---\n\n<center>\n<a href="https://github.com/nari-labs/dia">\n<img src="https://github.com/nari-labs/dia/raw/main/dia/static/images/banner.png">\n</a>\n</center>\n\nDia is a 1.6B parameter text to speech model created by Nari Labs. It was pushed to the Hub using the [PytorchModelHubMixin](https://huggingface.co/docs/huggingface_hub/package_reference/mixins#huggingface_hub.PyTorchModelHubMixin) integration.\n\nDia **directly generates highly realistic dialogue from a transcript**. You can condition the output on audio, enabling emotion and tone control. The model can also produce nonverbal communications like laughter, coughing, clearing throat, etc.\n\nTo accelerate research, we are providing access to pretrained model checkpoints and inference code. The model weights are hosted on [Hugging Face](https://huggingface.co/nari-labs/Dia-1.6B). The model only supports English generation at the moment.\n\nWe also provide a [demo page](https://yummy-fir-7a4.notion.site/dia) comparing our model to [ElevenLabs Studio](https://elevenlabs.io/studio) and [Sesame CSM-1B](https://github.com/SesameAILabs/csm).\n\n- (Update) We have a ZeroGPU Space running! Try it now [here](https://huggingface.co/spaces/nari-labs/Dia-1.6B). Thanks to the HF team for the support :)\n- Join our [discord server](https://discord.gg/bJq6vjRRKv) for community support and access to new features.\n- Play with a larger version of Dia: generate fun conversations, remix content, and share with friends. üîÆ Join the [waitlist](https://tally.so/r/meokbo) for early access.\n\n## ‚ö°Ô∏è Quickstart\n\nThis will open a Gradio UI that you can work on.\n\n```bash\ngit clone https://github.com/nari-labs/dia.git\ncd dia && uv run app.py\n```\n\nor if you do not have `uv` pre-installed:\n\n```bash\ngit clone https://github.com/nari-labs/dia.git\ncd dia\npython -m venv .venv\nsource .venv/bin/activate\npip install uv\nuv run app.py\n```\n\nNote that the model was not fine-tuned on a specific voice. Hence, you will get different voices every time you run the model.\nYou can keep speaker consistency by either adding an audio prompt (a guide coming VERY soon - try it with the second example on Gradio for now), or fixing the seed.\n\n## Features\n\n- Generate dialogue via `[S1]` and `[S2]` tag\n- Generate non-verbal like `(laughs)`, `(coughs)`, etc.\n  - Below verbal tags will be recognized, but might result in unexpected output.\n  - `(laughs), (clears throat), (sighs), (gasps), (coughs), (singing), (sings), (mumbles), (beep), (groans), (sniffs), (claps), (screams), (inhales), (exhales), (applause), (burps), (humming), (sneezes), (chuckle), (whistles)`\n- Voice cloning. See [`example/voice_clone.py`](example/voice_clone.py) for more information.\n  - In the Hugging Face space, you can upload the audio you want to clone and place its transcript before your script. Make sure the transcript follows the required format. The model will then output only the content of your script.\n\n## ‚öôÔ∏è Usage\n\n### As a Python Library\n\n```python\nimport soundfile as sf\n\nfrom dia.model import Dia\n\n\nmodel = Dia.from_pretrained("nari-labs/Dia-1.6B")\n\ntext = "[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face."\n\noutput = model.generate(text)\n\nsf.write("simple.mp3", output, 44100)\n```\n\nA pypi package and a working CLI tool will be available soon.\n\n## üíª Hardware and Inference Speed\n\nDia has been tested on only GPUs (pytorch 2.0+, CUDA 12.6). CPU support is to be added soon.\nThe initial run will take longer as the Descript Audio Codec also needs to be downloaded.\n\nOn enterprise GPUs, Dia can generate audio in real-time. On older GPUs, inference time will be slower.\nFor reference, on a A4000 GPU, Dia roughly generates 40 tokens/s (86 tokens equals 1 second of audio).\n`torch.compile` will increase speeds for supported GPUs.\n\nThe full version of Dia requires around 10GB of VRAM to run. We will be adding a quantized version in the future.\n\nIf you don''t have hardware available or if you want to play with bigger versions of our models, join the waitlist [here](https://tally.so/r/meokbo).\n\n## ü™™ License\n\nThis project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.\n\n## ‚ö†Ô∏è Disclaimer\n\nThis project offers a high-fidelity speech generation model intended for research and educational use. The following uses are **strictly forbidden**:\n\n- **Identity Misuse**: Do not produce audio resembling real individuals without permission.\n- **Deceptive Content**: Do not use this model to generate misleading content (e.g. fake news)\n- **Illegal or Malicious Use**: Do not use this model for activities that are illegal or intended to cause harm.\n\nBy using this model, you agree to uphold relevant legal standards and ethical responsibilities. We **are not responsible** for any misuse and firmly oppose any unethical usage of this technology.\n\n## üî≠ TODO / Future Work\n\n- Docker support.\n- Optimize inference speed.\n- Add quantization for memory efficiency.\n\n## ü§ù Contributing\n\nWe are a tiny team of 1 full-time and 1 part-time research-engineers. We are extra-welcome to any contributions!\nJoin our [Discord Server](https://discord.gg/bJq6vjRRKv) for discussions.\n\n## ü§ó Acknowledgements\n\n- We thank the [Google TPU Research Cloud program](https://sites.research.google/trc/about/) for providing computation resources.\n- Our work was heavily inspired by [SoundStorm](https://arxiv.org/abs/2305.09636), [Parakeet](https://jordandarefsky.com/blog/2024/parakeet/), and [Descript Audio Codec](https://github.com/descriptinc/descript-audio-codec).\n- HuggingFace for providing the ZeroGPU Grant.\n- "Nari" is a pure Korean word for lily.\n- We thank Jason Y. for providing help with data filtering.', '{"pipeline_tag":"text-to-speech","library_name":null,"framework":null,"params":null,"storage_bytes":12900029096,"files_count":6,"spaces_count":100,"gated":false,"private":false,"config":{}}', '[]', '[{"type":"has_code","target_id":"github:nari-labs:dia\">","source_url":"https://github.com/nari-labs/dia\">"},{"type":"has_code","target_id":"github:nari-labs:dia","source_url":"https://github.com/nari-labs/dia"},{"type":"has_code","target_id":"github:SesameAILabs:csm","source_url":"https://github.com/SesameAILabs/csm"},{"type":"has_code","target_id":"github:nari-labs:dia.git","source_url":"https://github.com/nari-labs/dia.git"},{"type":"has_code","target_id":"github:nari-labs:dia.git","source_url":"https://github.com/nari-labs/dia.git"},{"type":"has_code","target_id":"github:descriptinc:descript-audio-codec","source_url":"https://github.com/descriptinc/descript-audio-codec"},{"type":"based_on_paper","target_id":"arxiv:2305.09636","source_url":"https://arxiv.org/abs/2305.09636"}]', NULL, 'Apache-2.0', 'approved', 65, '78a47c3aa2e5092cda281320327fa722', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-openai-whisper-large-v3-turbo', 'huggingface--openai--whisper-large-v3-turbo', 'whisper-large-v3-turbo', 'openai', '--- language: - en - zh - de - es - ru - ko - fr - ja - pt - tr - pl - ca - nl - ar - sv - it - id - hi - fi - vi - he - uk - el - ms - cs - ro - da - hu - ta - ''no'' - th - ur - hr - bg - lt - la - mi - ml - cy - sk - te - fa - lv - bn - sr - az - sl - kn - et - mk - br - eu - is - hy - ne - mn - bs - kk - sq - sw - gl - mr - pa - si - km - sn - yo - so - af - oc - ka - be - tg - sd - gu - am - yi - lo - uz - fo - ht - ps - tk - nn - mt - sa - lb - my - bo - tl - mg - as - tt - haw - ln - ha ...', '["transformers","safetensors","whisper","automatic-speech-recognition","audio","en","zh","de","es","ru","ko","fr","ja","pt","tr","pl","ca","nl","ar","sv","it","id","hi","fi","vi","he","uk","el","ms","cs","ro","da","hu","ta","no","th","ur","hr","bg","lt","la","mi","ml","cy","sk","te","fa","lv","bn","sr","az","sl","kn","et","mk","br","eu","is","hy","ne","mn","bs","kk","sq","sw","gl","mr","pa","si","km","sn","yo","so","af","oc","ka","be","tg","sd","gu","am","yi","lo","uz","fo","ht","ps","tk","nn","mt","sa","lb","my","bo","tl","mg","as","tt","haw","ln","ha","ba","jw","su","arxiv:2212.04356","base_model:openai/whisper-large-v3","base_model:finetune:openai/whisper-large-v3","license:mit","endpoints_compatible","region:us"]', 'automatic-speech-recognition', 2717, 4633373, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/openai/whisper-large-v3-turbo","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\n- zh\n- de\n- es\n- ru\n- ko\n- fr\n- ja\n- pt\n- tr\n- pl\n- ca\n- nl\n- ar\n- sv\n- it\n- id\n- hi\n- fi\n- vi\n- he\n- uk\n- el\n- ms\n- cs\n- ro\n- da\n- hu\n- ta\n- ''no''\n- th\n- ur\n- hr\n- bg\n- lt\n- la\n- mi\n- ml\n- cy\n- sk\n- te\n- fa\n- lv\n- bn\n- sr\n- az\n- sl\n- kn\n- et\n- mk\n- br\n- eu\n- is\n- hy\n- ne\n- mn\n- bs\n- kk\n- sq\n- sw\n- gl\n- mr\n- pa\n- si\n- km\n- sn\n- yo\n- so\n- af\n- oc\n- ka\n- be\n- tg\n- sd\n- gu\n- am\n- yi\n- lo\n- uz\n- fo\n- ht\n- ps\n- tk\n- nn\n- mt\n- sa\n- lb\n- my\n- bo\n- tl\n- mg\n- as\n- tt\n- haw\n- ln\n- ha\n- ba\n- jw\n- su\nlicense: mit\ntags:\n- audio\n- automatic-speech-recognition\nwidget:\n- example_title: Librispeech sample 1\n  src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n- example_title: Librispeech sample 2\n  src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\npipeline_tag: automatic-speech-recognition\nbase_model:\n- openai/whisper-large-v3\nlibrary_name: transformers\n---\n\n# Whisper\n\nWhisper is a state-of-the-art model for automatic speech recognition (ASR) and speech translation, proposed in the paper \n[Robust Speech Recognition via Large-Scale Weak Supervision](https://huggingface.co/papers/2212.04356) by Alec Radford \net al. from OpenAI. Trained on >5M hours of labeled data, Whisper demonstrates a strong ability to generalise to many \ndatasets and domains in a zero-shot setting.\n\nWhisper large-v3-turbo is a finetuned version of a pruned [Whisper large-v3](https://huggingface.co/openai/whisper-large-v3). In other words, it''s the exact same model, except that the number of decoding layers have reduced from 32 to 4.\nAs a result, the model is way faster, at the expense of a minor quality degradation. You can find more details about it [in this GitHub discussion](https://github.com/openai/whisper/discussions/2363).\n\n**Disclaimer**: Content for this model card has partly been written by the ü§ó Hugging Face team, and partly copied and \npasted from the original model card.\n\n## Usage\n\nWhisper large-v3-turbo is supported in Hugging Face ü§ó Transformers. To run the model, first install the Transformers \nlibrary. For this example, we''ll also install ü§ó Datasets to load toy audio dataset from the Hugging Face Hub, and \nü§ó Accelerate to reduce the model loading time:\n\n```bash\npip install --upgrade pip\npip install --upgrade transformers datasets[audio] accelerate\n```\n\nThe model can be used with the [`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)\nclass to transcribe audios of arbitrary length:\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = "cuda:0" if torch.cuda.is_available() else "cpu"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = "openai/whisper-large-v3-turbo"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    "automatic-speech-recognition",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset("distil-whisper/librispeech_long", "clean", split="validation")\nsample = dataset[0]["audio"]\n\nresult = pipe(sample)\nprint(result["text"])\n```\n\nTo transcribe a local audio file, simply pass the path to your audio file when you call the pipeline:\n\n```python\nresult = pipe("audio.mp3")\n```\n\nMultiple audio files can be transcribed in parallel by specifying them as a list and setting the `batch_size` parameter:\n\n```python\nresult = pipe(["audio_1.mp3", "audio_2.mp3"], batch_size=2)\n```\n\nTransformers is compatible with all Whisper decoding strategies, such as temperature fallback and condition on previous \ntokens. The following example demonstrates how to enable these heuristics:\n\n```python\ngenerate_kwargs = {\n    "max_new_tokens": 448,\n    "num_beams": 1,\n    "condition_on_prev_tokens": False,\n    "compression_ratio_threshold": 1.35,  # zlib compression ratio threshold (in token space)\n    "temperature": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    "logprob_threshold": -1.0,\n    "no_speech_threshold": 0.6,\n    "return_timestamps": True,\n}\n\nresult = pipe(sample, generate_kwargs=generate_kwargs)\n```\n\nWhisper predicts the language of the source audio automatically. If the source audio language is known *a-priori*, it \ncan be passed as an argument to the pipeline:\n\n```python\nresult = pipe(sample, generate_kwargs={"language": "english"})\n```\n\nBy default, Whisper performs the task of *speech transcription*, where the source audio language is the same as the target\ntext language. To perform *speech translation*, where the target text is in English, set the task to `"translate"`:\n\n```python\nresult = pipe(sample, generate_kwargs={"task": "translate"})\n```\n\nFinally, the model can be made to predict timestamps. For sentence-level timestamps, pass the `return_timestamps` argument:\n\n```python\nresult = pipe(sample, return_timestamps=True)\nprint(result["chunks"])\n```\n\nAnd for word-level timestamps:\n\n```python\nresult = pipe(sample, return_timestamps="word")\nprint(result["chunks"])\n```\n\nThe above arguments can be used in isolation or in combination. For example, to perform the task of speech transcription \nwhere the source audio is in French, and we want to return sentence-level timestamps, the following can be used:\n\n```python\nresult = pipe(sample, return_timestamps=True, generate_kwargs={"language": "french", "task": "translate"})\nprint(result["chunks"])\n```\n\n<details>\n\n<summary> For more control over the generation parameters, use the model + processor API directly: </summary>\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\nfrom datasets import Audio, load_dataset\n\n\ndevice = "cuda:0" if torch.cuda.is_available() else "cpu"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = "openai/whisper-large-v3-turbo"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\ndataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")\ndataset = dataset.cast_column("audio", Audio(processor.feature_extractor.sampling_rate))\nsample = dataset[0]["audio"]\n\ninputs = processor(\n    sample["array"],\n    sampling_rate=sample["sampling_rate"],\n    return_tensors="pt",\n    truncation=False,\n    padding="longest",\n    return_attention_mask=True,\n)\ninputs = inputs.to(device, dtype=torch_dtype)\n\ngen_kwargs = {\n    "max_new_tokens": 448,\n    "num_beams": 1,\n    "condition_on_prev_tokens": False,\n    "compression_ratio_threshold": 1.35,  # zlib compression ratio threshold (in token space)\n    "temperature": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    "logprob_threshold": -1.0,\n    "no_speech_threshold": 0.6,\n    "return_timestamps": True,\n}\n\npred_ids = model.generate(**inputs, **gen_kwargs)\npred_text = processor.batch_decode(pred_ids, skip_special_tokens=True, decode_with_timestamps=False)\n\nprint(pred_text)\n```\n\n</details>\n\n## Additional Speed & Memory Improvements\n\nYou can apply additional speed and memory improvements to Whisper to further reduce the inference speed and VRAM \nrequirements.\n\n### Chunked Long-Form\n\nWhisper has a receptive field of 30-seconds. To transcribe audios longer than this, one of two long-form algorithms are\nrequired:\n1. **Sequential:** uses a "sliding window" for buffered inference, transcribing 30-second slices one after the other\n2. **Chunked:** splits long audio files into shorter ones (with a small overlap between segments), transcribes each segment independently, and stitches the resulting transcriptions at the boundaries\n\nThe sequential long-form algorithm should be used in either of the following scenarios:\n1. Transcription accuracy is the most important factor, and speed is less of a consideration\n2. You are transcribing **batches** of long audio files, in which case the latency of sequential is comparable to chunked, while being up to 0.5% WER more accurate\n\nConversely, the chunked algorithm should be used when:\n1. Transcription speed is the most important factor\n2. You are transcribing a **single** long audio file\n\nBy default, Transformers uses the sequential algorithm. To enable the chunked algorithm, pass the `chunk_length_s` \nparameter to the `pipeline`. For large-v3, a chunk length of 30-seconds is optimal. To activate batching over long \naudio files, pass the argument `batch_size`:\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = "cuda:0" if torch.cuda.is_available() else "cpu"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = "openai/whisper-large-v3-turbo"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    "automatic-speech-recognition",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    chunk_length_s=30,\n    batch_size=16,  # batch size for inference - set based on your device\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset("distil-whisper/librispeech_long", "clean", split="validation")\nsample = dataset[0]["audio"]\n\nresult = pipe(sample)\nprint(result["text"])\n```\n\n#### Torch compile\n\nThe Whisper forward pass is compatible with [`torch.compile`](https://pytorch.org/docs/stable/generated/torch.compile.html)\nfor 4.5x speed-ups.\n\n**Note:** `torch.compile` is currently not compatible with the Chunked long-form algorithm or Flash Attention 2 ‚ö†Ô∏è\n\n```python\nimport torch\nfrom torch.nn.attention import SDPBackend, sdpa_kernel\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\ntorch.set_float32_matmul_precision("high")\n\ndevice = "cuda:0" if torch.cuda.is_available() else "cpu"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = "openai/whisper-large-v3-turbo"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n).to(device)\n\n# Enable static cache and compile the forward pass\nmodel.generation_config.cache_implementation = "static"\nmodel.generation_config.max_new_tokens = 256\nmodel.forward = torch.compile(model.forward, mode="reduce-overhead", fullgraph=True)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    "automatic-speech-recognition",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset("distil-whisper/librispeech_long", "clean", split="validation")\nsample = dataset[0]["audio"]\n\n# 2 warmup steps\nfor _ in tqdm(range(2), desc="Warm-up step"):\n    with sdpa_kernel(SDPBackend.MATH):\n        result = pipe(sample.copy(), generate_kwargs={"min_new_tokens": 256, "max_new_tokens": 256})\n\n# fast run\nwith sdpa_kernel(SDPBackend.MATH):\n    result = pipe(sample.copy())\n\nprint(result["text"])\n```\n\n#### Flash Attention 2\n\nWe recommend using [Flash-Attention 2](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#flashattention-2) if your GPU supports it and you are not using [torch.compile](#torch-compile). \nTo do so, first install [Flash Attention](https://github.com/Dao-AILab/flash-attention):\n\n```\npip install flash-attn --no-build-isolation\n```\n\nThen pass `attn_implementation="flash_attention_2"` to `from_pretrained`:\n\n```python\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation="flash_attention_2")\n```\n\n#### Torch Scale-Product-Attention (SDPA)\n\nIf your GPU does not support Flash Attention, we recommend making use of PyTorch [scaled dot-product attention (SDPA)](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html). \nThis attention implementation is activated **by default** for PyTorch versions 2.1.1 or greater. To check \nwhether you have a compatible PyTorch version, run the following Python code snippet:\n\n```python\nfrom transformers.utils import is_torch_sdpa_available\n\nprint(is_torch_sdpa_available())\n```\n\nIf the above returns `True`, you have a valid version of PyTorch installed and SDPA is activated by default. If it \nreturns `False`, you need to upgrade your PyTorch version according to the [official instructions](https://pytorch.org/get-started/locally/)\n\nOnce a valid PyTorch version is installed, SDPA is activated by default. It can also be set explicitly by specifying \n`attn_implementation="sdpa"` as follows:\n\n```python\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation="sdpa")\n```\n\nFor more information about how to use the SDPA refer to the [Transformers SDPA documentation](https://huggingface.co/docs/transformers/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention).\n\n\n## Model details\n\nWhisper is a Transformer based encoder-decoder model, also referred to as a _sequence-to-sequence_ model. There are two\nflavours of Whisper model: English-only and multilingual. The English-only models were trained on the task of English \nspeech recognition. The multilingual models were trained simultaneously on multilingual speech recognition and speech \ntranslation. For speech recognition, the model predicts transcriptions in the *same* language as the audio. For speech \ntranslation, the model predicts transcriptions to a *different* language to the audio.\n\nWhisper checkpoints come in five configurations of varying model sizes. The smallest four are available as English-only \nand multilingual. The largest checkpoints are multilingual only. All ten of the pre-trained checkpoints \nare available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The \ncheckpoints are summarised in the following table with links to the models on the Hub:\n\n| Size     | Parameters | English-only                                         | Multilingual                                        |\n|----------|------------|------------------------------------------------------|-----------------------------------------------------|\n| tiny     | 39 M       | [‚úì](https://huggingface.co/openai/whisper-tiny.en)   | [‚úì](https://huggingface.co/openai/whisper-tiny)     |\n| base     | 74 M       | [‚úì](https://huggingface.co/openai/whisper-base.en)   | [‚úì](https://huggingface.co/openai/whisper-base)     |\n| small    | 244 M      | [‚úì](https://huggingface.co/openai/whisper-small.en)  | [‚úì](https://huggingface.co/openai/whisper-small)    |\n| medium   | 769 M      | [‚úì](https://huggingface.co/openai/whisper-medium.en) | [‚úì](https://huggingface.co/openai/whisper-medium)   |\n| large    | 1550 M     | x                                                    | [‚úì](https://huggingface.co/openai/whisper-large)    |\n| large-v2 | 1550 M     | x                                                    | [‚úì](https://huggingface.co/openai/whisper-large-v2) |\n| large-v3 | 1550 M     | x                                                    | [‚úì](https://huggingface.co/openai/whisper-large-v3) |\n| large-v3-turbo | 809 M     | x                                                    | [‚úì](https://huggingface.co/openai/whisper-large-v3-turbo) |\n\n\n## Fine-Tuning\n\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However, \nits predictive capabilities can be improved further for certain languages and tasks through *fine-tuning*. The blog \npost [Fine-Tune Whisper with ü§ó Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step \nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\n\n### Evaluated Use\n\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only ‚Äúintended‚Äù uses or to draw reasonable guidelines around what is or is not research.\n\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\n\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\n\n\n## Training Data\n\nNo information provided.\n\n## Performance and Limitations\n\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf). \n\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\n\n\n## Broader Implications\n\nWe anticipate that Whisper models‚Äô transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box ‚Äì their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\n\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\n\n\n### BibTeX entry and citation info\n```bibtex\n@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n```', '{"pipeline_tag":"automatic-speech-recognition","library_name":"transformers","framework":"transformers","params":808878080,"storage_bytes":5096208915,"files_count":13,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["WhisperForConditionalGeneration"],"model_type":"whisper","tokenizer_config":{"bos_token":"<|endoftext|>","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:openai:whisper","source_url":"https://github.com/openai/whisper"},{"type":"has_code","target_id":"github:Dao-AILab:flash-attention","source_url":"https://github.com/Dao-AILab/flash-attention"},{"type":"based_on_paper","target_id":"arxiv:2212.04356","source_url":"https://arxiv.org/abs/2212.04356"}]', NULL, 'MIT', 'approved', 80, 'a089c94c8d214552eaa3d94aea7901c7', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-3.3-70B-Instruct', 'huggingface--meta-llama--llama-3.3-70b-instruct', 'Llama-3.3-70B-Instruct', 'meta-llama', '', '["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","conversational","en","fr","it","pt","hi","es","th","de","arxiv:2204.05149","base_model:meta-llama/llama-3.1-70b","base_model:finetune:meta-llama/llama-3.1-70b","license:llama3.3","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 2589, 412422, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct","fetched_at":"2025-12-08T10:39:52.034Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":70553706496,"storage_bytes":282254659966,"files_count":53,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","chat_template":"{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][''role''] == ''system'' %}\n    {%- set system_message = messages[0][''content'']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject(''equalto'', ''code_interpreter'') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][''content'']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there''s no first user message!\") }}\n{%- endif %}\n    {{- ''<|start_header_id|>user<|end_header_id|>\\n\\n'' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == ''ipython'' or message.role == ''tool'' or ''tool_calls'' in message) %}\n        {{- ''<|start_header_id|>'' + message[''role''] + ''<|end_header_id|>\\n\\n''+ message[''content''] | trim + ''<|eot_id|>'' }}\n    {%- elif ''tool_calls'' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + ''=\"'' + arg_val + ''\"'' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' -}}\n            {{- ''{\"name\": \"'' + tool_call.name + ''\", '' }}\n            {{- ''\"parameters\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we''re in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' }}\n{%- endif %}\n","eos_token":"<|eot_id|>","pad_token":"<|finetune_right_pad_id|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2204.05149","source_url":"https://arxiv.org/abs/2204.05149"}]', NULL, 'llama3.3', 'approved', 40, 'a4fdef161fdb669b7745ae9be68eb611', NULL, NULL, CURRENT_TIMESTAMP);
