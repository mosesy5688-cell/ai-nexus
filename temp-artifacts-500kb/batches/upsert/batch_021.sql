/* LOGS:
Downloading image for github-meta-llama-llama-cookbook from https://github.com/meta-llama.png
Image converted to WebP: data/images/github-meta-llama-llama-cookbook.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-keon-awesome-nlp', 'github--keon--awesome-nlp', 'awesome-nlp', 'keon', 'A curated list of resources dedicated to Natural Language Processing !Awesome NLP Logo Read this in English, Traditional Chinese _Please read the contribution guidelines before contributing. Please add your favourite NLP resource by raising a pull request_ * Research Summaries and Trends * Prominent NLP Research Labs * Tutorials * Reading Content * Videos and Courses * Books * Libraries * Node.js * Python * C++ * Java * Kotlin * Scala * R * Clojure * Ruby * Rust * NLP++ * Julia * Services * A...', '["awesome","awesome-list","deep-learning","language","machine-learning","natural-language-processing","nlp","text-mining"]', 'other', 18022, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/keon/awesome-nlp","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# awesome-nlp\n\n[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\nA curated list of resources dedicated to Natural Language Processing\n\n![Awesome NLP Logo](/images/logo.jpg)\n\nRead this in [English](./README.md), [Traditional Chinese](./README-ZH-TW.md)\n\n_Please read the [contribution guidelines](contributing.md) before contributing. Please add your favourite NLP resource by raising a [pull request](https://github.com/keonkim/awesome-nlp/pulls)_\n\n## Contents\n\n* [Research Summaries and Trends](#research-summaries-and-trends)\n* [Prominent NLP Research Labs](#prominent-nlp-research-labs)\n* [Tutorials](#tutorials)\n  * [Reading Content](#reading-content)\n  * [Videos and Courses](#videos-and-online-courses)\n  * [Books](#books)\n* [Libraries](#libraries)\n  * [Node.js](#node-js)\n  * [Python](#python)\n  * [C++](#c++)\n  * [Java](#java)\n  * [Kotlin](#kotlin)\n  * [Scala](#scala)\n  * [R](#R)\n  * [Clojure](#clojure)\n  * [Ruby](#ruby)\n  * [Rust](#rust)\n  * [NLP++](#NLP++)\n  * [Julia](#julia)\n* [Services](#services)\n* [Annotation Tools](#annotation-tools)\n* [Datasets](#datasets)\n* [NLP in Korean](#nlp-in-korean)\n* [NLP in Arabic](#nlp-in-arabic)\n* [NLP in Chinese](#nlp-in-chinese)\n* [NLP in German](#nlp-in-german)\n* [NLP in Polish](#nlp-in-polish)\n* [NLP in Spanish](#nlp-in-spanish)\n* [NLP in Indic Languages](#nlp-in-indic-languages)\n* [NLP in Thai](#nlp-in-thai)\n* [NLP in Danish](#nlp-in-danish)\n* [NLP in Vietnamese](#nlp-in-vietnamese)\n* [NLP for Dutch](#nlp-for-dutch)\n* [NLP in Indonesian](#nlp-in-indonesian)\n* [NLP in Urdu](#nlp-in-urdu)\n* [NLP in Persian](#nlp-in-persian)\n* [NLP in Ukrainian](#nlp-in-ukrainian)\n* [NLP in Hungarian](#nlp-in-hungarian)\n* [NLP in Portuguese](#nlp-in-portuguese)\n* [Other Languages](#other-languages)\n* [Credits](#credits)\n\n## Research Summaries and Trends\n\n* [NLP-Overview](https://nlpoverview.com/) is an up-to-date overview of deep learning techniques applied to NLP, including theory, implementations, applications, and state-of-the-art results. This is a great Deep NLP Introduction for researchers.\n* [NLP-Progress](https://nlpprogress.com/) tracks the progress in Natural Language Processing, including the datasets and the current state-of-the-art for the most common NLP tasks\n* [NLP''s ImageNet moment has arrived](https://thegradient.pub/nlp-imagenet/)\n* [ACL 2018 Highlights: Understanding Representation and Evaluation in More Challenging Settings](http://ruder.io/acl-2018-highlights/)\n* [Four deep learning trends from ACL 2017. Part One: Linguistic Structure and Word Embeddings](https://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-1.html)\n* [Four deep learning trends from ACL 2017. Part Two: Interpretability and Attention](https://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-2.html)\n* [Highlights of EMNLP 2017: Exciting Datasets, Return of the Clusters, and More!](http://blog.aylien.com/highlights-emnlp-2017-exciting-datasets-return-clusters/)\n* [Deep Learning for Natural Language Processing (NLP): Advancements & Trends](https://tryolabs.com/blog/2017/12/12/deep-learning-for-nlp-advancements-and-trends-in-2017/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=The%20Wild%20Week%20in%20AI)\n* [Survey of the State of the Art in Natural Language Generation](https://arxiv.org/abs/1703.09902)\n\n## Prominent NLP Research Labs\n[Back to Top](#contents)\n\n* [The Berkeley NLP Group](http://nlp.cs.berkeley.edu/index.shtml) - Notable contributions include a tool to reconstruct long dead languages, referenced [here](https://www.bbc.com/news/science-environment-21427896) and by taking corpora from 637 languages currently spoken in Asia and the Pacific and recreating their descendant.\n* [Language Technologies Institute, Carnegie Mellon University](http://www.cs.cmu.edu/~nasmith/nlp-cl.html) - Notable projects include [Avenue Project](http://www.cs.cmu.edu/~avenue/), a syntax driven machine translation system for endangered languages like Quechua and Aymara and previously, [Noah''s Ark](http://www.cs.cmu.edu/~ark/) which created [AQMAR](http://www.cs.cmu.edu/~ark/AQMAR/) to improve NLP tools for Arabic.\n* [NLP research group, Columbia University](http://www1.cs.columbia.edu/nlp/index.cgi) - Responsible for creating BOLT ( interactive error handling for speech translation systems) and an un-named project to characterize laughter in dialogue.\n* [The Center or Language and Speech Processing, John Hopkins University](http://clsp.jhu.edu/) - Recently in the news for developing speech recognition software to create a diagnostic test or Parkinson''s Disease, [here](https://www.clsp.jhu.edu/2019/03/27/speech-recognition-software-and-machine-learning-tools-are-being-used-to-create-diagnostic-test-for-parkinsons-disease/#.XNFqrIkzYdU).\n* [Computational Linguistics and Information Processing Group, University of Maryland](https://wiki.umiacs.umd.edu/clip/index.php/Main_Page) - Notable contributions include [Human-Computer Cooperation or Word-by-Word Question Answering](http://www.umiacs.umd.edu/~jbg/projects/IIS-1652666) and modeling development of phonetic representations. \n* [Penn Natural Language Processing, University of Pennsylvania](https://nlp.cis.upenn.edu/)- Famous for creating the [Penn Treebank](https://www.seas.upenn.edu/~pdtb/).\n* [The Stanford Nautral Language Processing Group](https://nlp.stanford.edu/)- One of the top NLP research labs in the world, notable for creating [Stanford CoreNLP](https://nlp.stanford.edu/software/corenlp.shtml) and their [coreference resolution system](https://nlp.stanford.edu/software/dcoref.shtml)\n\n\n## Tutorials\n[Back to Top](#contents)\n\n### Reading Content\n\nGeneral Machine Learning\n\n* [Machine Learning 101](https://docs.google.com/presentation/d/1kSuQyW5DTnkVaZEjGYCkfOxvzCqGEFzWBy4e9Uedd9k/edit?usp=sharing) from Google''s Senior Creative Engineer explains Machine Learning for engineer''s and executives alike\n* [AI Playbook](https://aiplaybook.a16z.com/) - a16z AI playbook is a great link to forward to your managers or content for your presentations\n* [Ruder''s Blog](http://ruder.io/#open) by [Sebastian Ruder](https://twitter.com/seb_ruder) for commentary on the best of NLP Research\n* [How To Label Data](https://www.lighttag.io/how-to-label-data/) guide to managing larger linguistic annotation projects\n* [Depends on the Definition](https://www.depends-on-the-definition.com/) collection of blog posts covering a wide array of NLP topics with detailed implementation\n\nIntroductions and Guides to NLP\n\n* [Understand & Implement Natural Language Processing](https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/)\n* [NLP in Python](http://github.com/NirantK/nlp-python-deep-learning) - Collection of Github notebooks\n* [Natural Language Processing: An Introduction](https://academic.oup.com/jamia/article/18/5/544/829676) - Oxford\n* [Deep Learning for NLP with Pytorch](https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html)\n* [Hands-On NLTK Tutorial](https://github.com/hb20007/hands-on-nltk-tutorial) - NLTK Tutorials, Jupyter notebooks\n* [Natural Language Processing with Python â€“ Analyzing Text with the Natural Language Toolkit](https://www.nltk.org/book/) - An online and print book introducing NLP concepts using NLTK. The book''s authors also wrote the NLTK library.\n* [Train a new language model from scratch](https://huggingface.co/blog/how-to-train) - Hugging Face ðŸ¤—\n* [The Super Duper NLP Repo (SDNLPR)](https://notebooks.quantumstat.com/): Collection of Colab notebooks covering a wide array of NLP task implementations.\n\nBlogs and Newsletters\n\n* [Deep Learning, NLP, and Representations](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)\n* [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert/) and [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n* [Natural Language Processing](https://nlpers.blogspot.com/) by Hal DaumÃ© III\n* [arXiv: Natural Language Processing (Almost) from Scratch](https://arxiv.org/pdf/1103.0398.pdf)\n* [Karpathy''s The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness)\n* [Machine Learning Mastery: Deep Learning for Natural Language Processing](https://machinelearningmastery.com/category/natural-language-processing)\n* [Visual NLP Paper Summaries](https://amitness.com/categories/#nlp)\n\n### Videos and Online Courses\n[Back to Top](#contents)\n\n* [Advanced Natural Language Processing](https://people.cs.umass.edu/~miyyer/cs685_f20/) - CS 685, UMass Amherst CS\n* [Deep Natural Language Processing](https://github.com/oxford-cs-deepnlp-2017/lectures) - Lectures series from Oxford\n* [Deep Learning for Natural Language Processing (cs224-n)](https://web.stanford.edu/class/cs224n/) - Richard Socher and Christopher Manning''s Stanford Course\n* [Neural Networks for NLP](http://phontron.com/class/nn4nlp2017/) - Carnegie Mellon Language Technology Institute there\n* [Deep NLP Course](https://github.com/yandexdataschool/nlp_course) by Yandex Data School, covering important ideas from text embedding to machine translation including sequence modeling, language models and so on.\n* [fast.ai Code-First Intro to Natural Language Processing](https://www.fast.ai/2019/07/08/fastai-nlp/) - This covers a blend of traditional NLP topics (including regex, SVD, naive bayes, tokenization) and recent neural network approaches (including RNNs, seq2seq, GRUs, and the Transformer), as well as addressing urgent ethical issues, such as bias and disinformation. Find the Jupyter Notebooks [here](https://github.com/fastai/course-nlp)\n* [Machine Learning University - Accelerated Natural Language Processing](https://www.youtube.com/playlist?list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw) - Lectures go from introduction to NLP and text processing to Recurrent Neural Networks and Transformers.\nMaterial can be found [here](https://github.com/aws-samples/aws-machine-learning-university-accelerated-nlp).\n* [Applied Natural Language Processing](https://www.youtube.com/playlist?list=PLH-xYrxjfO2WyR3pOAB006CYMhNt4wTqp)- Lecture series from IIT Madras taking from the basics all the way to autoencoders and everything. The github notebooks for this course are also available [here](https://github.com/Ramaseshanr/anlp)\n\n\n### Books\n\n* [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) - free, by Prof. Dan Jurafsy\n* [Natural Language Processing](https://github.com/jacobeisenstein/gt-nlp-class) - free, NLP notes by Dr. Jacob Eisenstein at GeorgiaTech\n* [NLP with PyTorch](https://github.com/joosthub/PyTorchNLPBook) - Brian & Delip Rao\n* [Text Mining in R](https://www.tidytextmining.com)\n* [Natural Language Processing with Python](https://www.nltk.org/book/)\n* [Practical Natural Language Processing](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/)\n* [Natural Language Processing with Spark NLP](https://www.oreilly.com/library/view/natural-language-processing/9781492047759/)\n* [Deep Learning for Natural Language Processing](https://www.manning.com/books/deep-learning-for-natural-language-processing) by Stephan Raaijmakers\n* [Real-World Natural Language Processing](https://www.manning.com/books/real-world-natural-language-processing) - by Masato Hagiwara\n* [Natural Language Processing in Action, Second Edition](https://www.manning.com/books/natural-language-processing-in-action-second-edition) - by Hobson Lane and Maria Dyshel\n## Libraries\n\n[Back to Top](#contents)\n\n* <a id="node-js">**Node.js and Javascript** - Node.js Libaries for NLP</a> | [Back to Top](#contents)\n  * [Twitter-text](https://github.com/twitter/twitter-text) - A JavaScript implementation of Twitter''s text processing library\n  * [Knwl.js](https://github.com/benhmoore/Knwl.js) - A Natural Language Processor in JS\n  * [Retext](https://github.com/retextjs/retext) - Extensible system for analyzing and manipulating natural language\n  * [NLP Compromise](https://github.com/spencermountain/compromise) - Natural Language processing in the browser\n  * [Natural](https://github.com/NaturalNode/natural) - general natural language facilities for node\n  * [Poplar](https://github.com/synyi/poplar) - A web-based annotation tool for natural language processing (NLP)\n  * [NLP.js](https://github.com/axa-group/nlp.js) - An NLP library for building bots\n  * [node-question-answering](https://github.com/huggingface/node-question-answering) - Fast and production-ready question answering w/ DistilBERT in Node.js\n\n* <a id="python"> **Python** - Python NLP Libraries</a> | [Back to Top](#contents)\n  - [sentimental-onix](https://github.com/sloev/sentimental-onix) Sentiment models for spacy using onnx\n  - [TextAttack](https://github.com/QData/TextAttack) - Adversarial attacks, adversarial training, and data augmentation in NLP\n  - [TextBlob](http://textblob.readthedocs.org/) - Providing a consistent API for diving into common natural language processing (NLP) tasks. Stands on the giant shoulders of [Natural Language Toolkit (NLTK)](https://www.nltk.org/) and [Pattern](https://github.com/clips/pattern), and plays nicely with both :+1:\n  - [spaCy](https://github.com/explosion/spaCy) - Industrial strength NLP with Python and Cython :+1:\n  - [Speedster](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/speedster) - Automatically apply SOTA optimization techniques to achieve the maximum inference speed-up on your hardware\n    - [textacy](https://github.com/chartbeat-labs/textacy) - Higher level NLP built on spaCy\n  - [gensim](https://radimrehurek.com/gensim/index.html) - Python library to conduct unsupervised semantic modelling from plain text :+1:\n  - [scattertext](https://github.com/JasonKessler/scattertext) - Python library to produce d3 visualizations of how language differs between corpora\n  - [GluonNLP](https://github.com/dmlc/gluon-nlp) - A deep learning toolkit for NLP, built on MXNet/Gluon, for research prototyping and industrial deployment of state-of-the-art models on a wide range of NLP tasks.\n  - [AllenNLP](https://github.com/allenai/allennlp) - An NLP research library, built on PyTorch, for developing state-of-the-art deep learning models on a wide variety of linguistic tasks.\n  - [PyTorch-NLP](https://github.com/PetrochukM/PyTorch-NLP) - NLP research toolkit designed to support rapid prototyping with better data loaders, word vector loaders, neural network layer representations, common NLP metrics such as BLEU\n  - [Rosetta](https://github.com/columbia-applied-data-science/rosetta) - Text processing tools and wrappers (e.g. Vowpal Wabbit)\n  - [PyNLPl](https://github.com/proycon/pynlpl) - Python Natural Language Processing Library. General purpose NLP library for Python, handles some specific formats like ARPA language models, Moses phrasetables, GIZA++ alignments.\n  - [foliapy](https://github.com/proycon/foliapy) - Python library for working with [FoLiA](https://proycon.github.io/folia/), an XML format for linguistic annotation.\n  - [PySS3](https://github.com/sergioburdisso/pyss3) - Python package that implements a novel white-box machine learning model for text classification, called SS3. Since SS3 has the ability to visually explain its rationale, this package also comes with easy-to-use interactive visualizations tools ([online demos](http://tworld.io/ss3/)).\n  - [jPTDP](https://github.com/datquocnguyen/jPTDP) - A toolkit for joint part-of-speech (POS) tagging and dependency parsing. jPTDP provides pre-trained models for 40+ languages.\n  - [BigARTM](https://github.com/bigartm/bigartm) - a fast library for topic modelling\n  - [Snips NLU](https://github.com/snipsco/snips-nlu) - A production ready library for intent parsing\n  - [Chazutsu](https://github.com/chakki-works/chazutsu) - A library for downloading&parsing standard NLP research datasets\n  - [Word Forms](https://github.com/gutfeeling/word_forms) - Word forms can accurately generate all possible forms of an English word\n  - [Multilingual Latent Dirichlet Allocation (LDA)](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA) - A multilingual and extensible document clustering pipeline\n  - [Natural Language Toolkit (NLTK)](https://www.nltk.org/) - A library containing a wide variety of NLP functionality, supporting over 50 corpora.\n  - [NLP Architect](https://github.com/NervanaSystems/nlp-architect) - A library for exploring the state-of-the-art deep learning topologies and techniques for NLP and NLU\n  - [Flair](https://github.com/zalandoresearch/flair) - A very simple framework for state-of-the-art multilingual NLP built on PyTorch. Includes BERT, ELMo and Flair embeddings.\n  - [Kashgari](https://github.com/BrikerMan/Kashgari) - Simple, Keras-powered multilingual NLP framework, allows you to build your models in 5 minutes for named entity recognition (NER), part-of-speech tagging (PoS) and text classification tasks. Includes BERT and word2vec embedding.\n  - [FARM](https://github.com/deepset-ai/FARM) - Fast & easy transfer learning for NLP. Harvesting language models for the industry. Focus on Question Answering.\n  - [Haystack](https://github.com/deepset-ai/haystack) - End-to-end Python framework for building natural language search interfaces to data. Leverages Transformers and the State-of-the-Art of NLP. Supports DPR, Elasticsearch, HuggingFaceâ€™s Modelhub, and much more!\n  - [Rita DSL](https://github.com/zaibacu/rita-dsl) - a DSL, loosely based on [RUTA on Apache UIMA](https://uima.apache.org/ruta.html). Allows to define language patterns (rule-based NLP) which are then translated into [spaCy](https://spacy.io/), or if you prefer less features and lightweight - regex patterns.\n  - [Transformers](https://github.com/huggingface/transformers) - Natural Language Processing for TensorFlow 2.0 and PyTorch.\n  - [Tokenizers](https://github.com/huggingface/tokenizers) - Tokenizers optimized for Research and Production.\n  - [fairSeq](https://github.com/pytorch/fairseq) Facebook AI Research implementations of SOTA seq2seq models in Pytorch. \n  - [corex_topic](https://github.com/gregversteeg/corex_topic) - Hierarchical Topic Modeling with Minimal Domain Knowledge\n  - [Sockeye](https://github.com/awslabs/sockeye) - Neural Machine Translation (NMT) toolkit that powers Amazon Translate.\n  - [DL Translate](https://github.com/xhlulu/dl-translate) - A deep learning-based translation library for 50 languages, built on `transformers` and Facebook''s mBART Large.\n  - [Jury](https://github.com/obss/jury) - Evaluation of NLP model outputs offering various automated metrics.\n  - [python-ucto](https://github.com/proycon/python-ucto) - Unicode-aware regular-expression based tokenizer for various languages. Python binding to C++ library, supports [FoLiA format](https://proycon.github.io/folia).\n\n- <a id="c++">**C++** - C++ Libraries</a> | [Back to Top](#contents)\n  - [InsNet](https://github.com/chncwang/InsNet) - A neural network library for building instance-dependent NLP models with padding-free dynamic batching.\n  - [MIT Information Extraction Toolkit](https://github.com/mit-nlp/MITIE) - C, C++, and Python tools for named entity recognition and relation extraction\n  - [CRF++](https://taku910.github.io/crfpp/) - Open source implementation of Conditional Random Fields (CRFs) for segmenting/labeling sequential data & other Natural Language Processing tasks.\n  - [CRFsuite](http://www.chokkan.org/software/crfsuite/) - CRFsuite is an implementation of Conditional Random Fields (CRFs) for labeling sequential data.\n  - [BLLIP Parser](https://github.com/BLLIP/bllip-parser) - BLLIP Natural Language Parser (also known as the Charniak-Johnson parser)\n  - [colibri-core](https://github.com/proycon/colibri-core) - C++ library, command line tools, and Python binding for extracting and working with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.\n  - [ucto](https://github.com/LanguageMachines/ucto) - Unicode-aware regular-expression based tokenizer for various languages. Tool and C++ library. Supports FoLiA format.\n  - [libfolia](https://github.com/LanguageMachines/libfolia) - C++ library for the [FoLiA format](https://proycon.github.io/folia/)\n  - [frog](https://github.com/LanguageMachines/frog) - Memory-based NLP suite developed for Dutch: PoS tagger, lemmatiser, dependency parser, NER, shallow parser, morphological analyzer.\n  - [MeTA](https://github.com/meta-toolkit/meta) - [MeTA : ModErn Text Analysis](https://meta-toolkit.org/) is a C++ Data Sciences Toolkit that facilitates mining big text data.\n  - [Mecab (Japanese)](https://taku910.github.io/mecab/)\n  - [Moses](http://statmt.org/moses/)\n  - [StarSpace](https://github.com/facebookresearch/StarSpace) - a library from Facebook for creating embeddings of word-level, paragraph-level, document-level and for text classification\n\n- <a id="java">**Java** - Java NLP Libraries</a> | [Back to Top](#contents)\n  - [Stanford NLP](https://nlp.stanford.edu/software/index.shtml)\n  - [OpenNLP](https://opennlp.apache.org/)\n  - [NLP4J](https://emorynlp.github.io/nlp4j/)\n  - [Word2vec in Java](https://deeplearning4j.org/docs/latest/deeplearning4j-nlp-word2vec)\n  - [ReVerb](https://github.com/knowitall/reverb/) Web-Scale Open Information Extraction\n  - [OpenRegex](https://github.com/knowitall/openregex) An efficient and flexible token-based regular expression language and engine.\n  - [CogcompNLP](https://github.com/CogComp/cogcomp-nlp) - Core libraries developed in the U of Illinois'' Cognitive Computation Group.\n  - [MALLET](http://mallet.cs.umass.edu/) - MAchine Learning for LanguagE Toolkit - package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.\n  - [RDRPOSTagger](https://github.com/datquocnguyen/RDRPOSTagger) - A robust POS tagging toolkit available (in both Java & Python) together with pre-trained models for 40+ languages.\n\n- <a id="kotlin">**Kotlin** - Kotlin NLP Libraries</a> | [Back to Top](#contents)\n  - [Lingua](https://github.com/pemistahl/lingua/) A language detection library for Kotlin and Java, suitable for long and short text alike\n  - [Kotidgy](https://github.com/meiblorn/kotidgy) â€” an index-based text data generator written in Kotlin\n\n- <a id="scala">**Scala** - Scala NLP Libraries</a> | [Back to Top](#contents)\n  - [Saul](https://github.com/CogComp/saul) - Library for developing NLP systems, including built in modules like SRL, POS, etc.\n  - [ATR4S](https://github.com/ispras/atr4s) - Toolkit with state-of-the-art [automatic term recognition](https://en.wikipedia.org/wiki/Terminology_extraction) methods.\n  - [tm](https://github.com/ispras/tm) - Implementation of topic modeling based on regularized multilingual [PLSA](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis).\n  - [word2vec-scala](https://github.com/Refefer/word2vec-scala) - Scala interface to word2vec model; includes operations on vectors like word-distance and word-analogy.\n  - [Epic](https://github.com/dlwh/epic) - Epic is a high performance statistical parser written in Scala, along with a framework for building complex structured prediction models.\n  - [Spark NLP](https://github.com/JohnSnowLabs/spark-nlp) - Spark NLP is a natural language processing library built on top of Apache Spark ML that provides simple, performant & accurate NLP annotations for machine learning pipelines that scale easily in a distributed environment.\n\n- <a id="R">**R** - R NLP Libraries</a> | [Back to Top](#contents)\n  - [text2vec](https://github.com/dselivanov/text2vec) - Fast vectorization, topic modeling, distances and GloVe word embeddings in R.\n  - [wordVectors](https://github.com/bmschmidt/wordVectors) - An R package for creating and exploring word2vec and other word embedding models\n  - [RMallet](https://github.com/mimno/RMallet) - R package to interface with the Java machine learning tool MALLET\n  - [dfr-browser](https://github.com/agoldst/dfr-browser) - Creates d3 visualizations for browsing topic models of text in a web browser.\n  - [dfrtopics](https://github.com/agoldst/dfrtopics) - R package for exploring topic models of text.\n  - [sentiment_classifier](https://github.com/kevincobain2000/sentiment_classifier) - Sentiment Classification using Word Sense Disambiguation and WordNet Reader\n  - [jProcessing](https://github.com/kevincobain2000/jProcessing) - Japanese Natural Langauge Processing Libraries, with Japanese sentiment classification\n  - [corporaexplorer](https://kgjerde.github.io/corporaexplorer/) - An R package for dynamic exploration of text collections\n  - [tidytext](https://github.com/juliasilge/tidytext) - Text mining using tidy tools\n  - [spacyr](https://github.com/quanteda/spacyr) - R wrapper to spaCy NLP\n  - [CRAN Task View: Natural Language Processing](https://github.com/cran-task-views/NaturalLanguageProcessing/)\n\n- <a id="clojure">**Clojure**</a> | [Back to Top](#contents)\n  - [Clojure-openNLP](https://github.com/dakrone/clojure-opennlp) - Natural Language Processing in Clojure (opennlp)\n  - [Infections-clj](https://github.com/r0man/inflections-clj) - Rails-like inflection library for Clojure and ClojureScript\n  - [postagga](https://github.com/fekr/postagga) - A library to parse natural language in Clojure and ClojureScript\n\n- <a id="ruby">**Ruby**</a> | [Back to Top](#contents)\n  - Kevin Dias''s [A collection of Natural Language Processing (NLP) Ruby libraries, tools and software](https://github.com/diasks2/ruby-nlp)\n  - [Practical Natural Language Processing done in Ruby](https://github.com/arbox/nlp-with-ruby)\n\n- <a id="rust">**Rust**</a> | [Back to Top](#contents)\n  - [whatlang](https://github.com/greyblake/whatlang-rs) â€” Natural language recognition library based on trigrams\n  - [snips-nlu-rs](https://github.com/snipsco/snips-nlu-rs) - A production ready library for intent parsing\n  - [rust-bert](https://github.com/guillaume-be/rust-bert) - Ready-to-use NLP pipelines and Transformer-based models\n\n- <a id="NLP++">**NLP++** - NLP++ Language</a> | [Back to Top](#contents)\n  - [VSCode Language Extension](https://marketplace.visualstudio.com/items?itemName=dehilster.nlp) - NLP++ Language Extension for VSCode\n  - [nlp-engine](https://github.com/VisualText/nlp-engine) - NLP++ engine to run NLP++ code on Linux including a full English parser\n  - [VisualText](http://visualtext.org) - Homepage for the NLP++ Language\n  - [NLP++ Wiki](http://wiki.naturalphilosophy.org/index.php?title=NLP%2B%2B) - Wiki entry for the NLP++ language\n\n- <a id="julia">**Julia**</a> | [Back to Top](#contents)\n  - [CorpusLoaders](https://github.com/JuliaText/CorpusLoaders.jl) - A variety of loaders for various NLP corpora\n  - [Languages](https://github.com/JuliaText/Languages.jl) - A package for working with human languages\n  - [TextAnalysis](https://github.com/JuliaText/TextAnalysis.jl) - Julia package for text analysis\n  - [TextModels](https://github.com/JuliaText/TextModels.jl) - Neural Network based models for Natural Language Processing\n  - [WordTokenizers](https://github.com/JuliaText/WordTokenizers.jl) - High performance tokenizers for natural language processing and other related tasks\n  - [Word2Vec](https://github.com/JuliaText/Word2Vec.jl) - Julia interface to word2vec\n\n### Services\n\nNLP as API with higher level functionality such as NER, Topic tagging and so on | [Back to Top](#contents)\n\n- [Wit-ai](https://github.com/wit-ai/wit) - Natural Language Interface for apps and devices\n- [IBM Watson''s Natural Language Understanding](https://github.com/watson-developer-cloud/natural-language-understanding-nodejs) - API and Github demo\n- [Amazon Comprehend](https://aws.amazon.com/comprehend/) - NLP and ML suite covers most common tasks like NER, tagging, and sentiment analysis\n- [Google Cloud Natural Language API](https://cloud.google.com/natural-language/) - Syntax Analysis, NER, Sentiment Analysis, and Content tagging in atleast 9 languages include English and Chinese (Simplified and Traditional).\n- [ParallelDots](https://www.paralleldots.com/text-analysis-apis) - High level Text Analysis API Service ranging from Sentiment Analysis to Intent Analysis\n- [Microsoft Cognitive Service](https://azure.microsoft.com/en-us/services/cognitive-services/text-analytics/)\n- [TextRazor](https://www.textrazor.com/)\n- [Rosette](https://www.rosette.com/)\n- [Textalytic](https://www.textalytic.com) - Natural Language Processing in the Browser with sentiment analysis, named entity extraction, POS tagging, word frequencies, topic modeling, word clouds, and more\n- [NLP Cloud](https://nlpcloud.io) - SpaCy NLP models (custom and pre-trained ones) served through a RESTful API for named entity recognition (NER), POS tagging, and more.\n- [Cloudmersive](https://cloudmersive.com/nlp-api) - Unified and free NLP APIs that perform actions such as speech tagging, text rephrasing, language translation/detection, and sentence parsing\n\n### Annotation Tools\n\n- [GATE](https://gate.ac.uk/overview.html) - General Architecture and Text Engineering is 15+ years old, free and open source\n- [Anafora](https://github.com/weitechen/anafora) is free and open source, web-based raw text annotation tool\n- [brat](https://brat.nlplab.org/) - brat rapid annotation tool is an online environment for collaborative text annotation\n- [doccano](https://github.com/chakki-works/doccano) - doccano is free, open-source, and provides annotation features for text classification, sequence labeling and sequence to sequence\n- [INCEpTION](https://inception-project.github.io) - A semantic annotation platform offering intelligent assistance and knowledge management\n- [tagtog](https://www.tagtog.net/), team-first web tool to find, create, maintain, and share datasets - costs $\n- [prodigy](https://prodi.gy/) is an annotation tool powered by active learning, costs $\n- [LightTag](https://lighttag.io) - Hosted and managed text annotation tool for teams, costs $\n- [rstWeb](https://corpling.uis.georgetown.edu/rstweb/info/) - open source local or online tool for discourse tree annotations\n- [GitDox](https://corpling.uis.georgetown.edu/gitdox/) - open source server annotation tool with GitHub version control and validation for XML data and collaborative spreadsheet grids\n- [Label Studio](https://www.heartex.ai/) - Hosted and managed text annotation tool for teams, freemium based, costs $\n- [Datasaur](https://datasaur.ai/) support various NLP tasks for individual or teams, freemium based\n- [Konfuzio](https://konfuzio.com/en/) - team-first hosted and on-prem text, image and PDF annotation tool powered by active learning, freemium based, costs $\n- [UBIAI](https://ubiai.tools/) - Easy-to-use text annotation tool for teams with most comprehensive auto-annotation features. Supports NER, relations and document classification as well as OCR annotation for invoice labeling, costs $\n- [Shoonya](https://github.com/AI4Bharat/Shoonya-Backend) - Shoonya is free and open source data annotation platform with wide varities of organization and workspace level management system. Shoonya is data agnostic, can be used by teams to annotate data with various level of verification stages at scale.\n- [Annotation Lab](https://www.johnsnowlabs.com/annotation-lab/) - Free End-to-End No-Code platform for text annotation and DL model training/tuning. Out-of-the-box support for Named Entity Recognition, Classification, Relation extraction and Assertion Status Spark NLP models. Unlimited support for users, teams, projects, documents. Not FOSS. \n- [FLAT](https://github.com/proycon/flat) - FLAT is a web-based linguistic annotation environment based around the [FoLiA format](http://proycon.github.io/folia), a rich XML-based format for linguistic annotation. Free and open source.\n\n\n## Techniques\n\n### Text Embeddings\n\n#### Word Embeddings\n\n- Thumb Rule: **fastText >> GloVe > word2vec**\n\n- [word2vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) - [implementation](https://code.google.com/archive/p/word2vec/) - [explainer blog](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)\n- [glove](https://nlp.stanford.edu/pubs/glove.pdf) - [explainer blog](https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/)\n- fasttext - [implementation](https://github.com/facebookresearch/fastText) - [paper](https://arxiv.org/abs/1607.04606) - [explainer blog](https://towardsdatascience.com/fasttext-under-the-hood-11efc57b2b3)\n\n#### Sentence and Language Model Based Word Embeddings\n\n[Back to Top](#contents)\n\n- ElMo - [Deep Contextualized Word Representations](https://arxiv.org/abs/1802.05365) - [PyTorch implmentation](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md) - [TF Implementation](https://github.com/allenai/bilm-tf)\n- ULMFiT - [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146) by Jeremy Howard and Sebastian Ruder\n- InferSent - [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](https://arxiv.org/abs/1705.02364) by facebook\n- CoVe - [Learned in Translation: Contextualized Word Vectors](https://arxiv.org/abs/1708.00107)\n- Pargraph vectors - from [Distributed Representations of Sentences and Documents](https://cs.stanford.edu/~quocle/paragraph_vector.pdf). See [doc2vec tutorial at gensim](https://rare-technologies.com/doc2vec-tutorial/)\n- [sense2vec](https://arxiv.org/abs/1511.06388) - on word sense disambiguation\n- [Skip Thought Vectors](https://arxiv.org/abs/1506.06726) - word representation method\n- [Adaptive skip-gram](https://arxiv.org/abs/1502.07257) - similar approach, with adaptive properties\n- [Sequence to Sequence Learning](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) - word vectors for machine translation\n\n### Question Answering and Knowledge Extraction\n\n[Back to Top](#contents)\n\n- [DrQA](https://github.com/facebookresearch/DrQA) - Open Domain Question Answering work by Facebook Research on Wikipedia data\n- [Document-QA](https://github.com/allenai/document-qa) - Simple and Effective Multi-Paragraph Reading Comprehension by AllenAI\n- [Template-Based Information Extraction without the Templates](https://www.usna.edu/Users/cs/nchamber/pubs/acl2011-chambers-templates.pdf)\n- [Privee: An Architecture for Automatically Analyzing Web Privacy Policies](https://www.sebastianzimmeck.de/zimmeckAndBellovin2014Privee.pdf)\n\n## Datasets\n\n[Back to Top](#contents)\n\n- [nlp-datasets](https://github.com/niderhoff/nlp-datasets) great collection of nlp datasets\n- [gensim-data](https://github.com/RaRe-Technologies/gensim-data) - Data repository for pretrained NLP models and NLP corpora.\n- [tiny_qa_benchmark_pp](https://github.com/vincentkoc/tiny_qa_benchmark_pp/) - Repository of tiny NLP multi-lingual QA datasets and library to generate your own synthetic copies.\n\n## Multilingual NLP Frameworks\n\n[Back to Top](#contents)\n\n- [UDPipe](https://github.com/ufal/udpipe) is a trainable pipeline for tokenizing, tagging, lemmatizing and parsing Universal Treebanks and other CoNLL-U files. Primarily written in C++, offers a fast and reliable solution for multilingual NLP processing.\n- [NLP-Cube](https://github.com/adobe/NLP-Cube) : Natural Language Processing Pipeline - Sentence Splitting, Tokenization, Lemmatization, Part-of-speech Tagging and Dependency Parsing. New platform, written in Python with Dynet 2.0. Offers standalone (CLI/Python bindings) and server functionality (REST API).\n- [UralicNLP](https://github.com/mikahama/uralicNLP) is an NLP library mostly for many endangered Uralic languages such as Sami languages, Mordvin languages, Mari languages, Komi languages and so on. Also some non-endangered languages are supported such as Finnish together with non-Uralic languages such as Swedish and Arabic. UralicNLP can do morphological analysis, generation, lemmatization and disambiguation.\n\n## NLP in Korean\n\n[Back to Top](#contents)\n\n### Libraries\n\n- [KoNLPy](http://konlpy.org) - Python package for Korean natural language processing.\n- [Mecab (Korean)](https://eunjeon.blogspot.com/) - C++ library for Korean NLP\n- [KoalaNLP](https://koalanlp.github.io/koalanlp/) - Scala library for Korean Natural Language Processing.\n- [KoNLP](https://cran.r-project.org/package=KoNLP) - R package for Korean Natural language processing\n\n### Blogs and Tutorials\n\n- [dsindex''s blog](https://dsindex.github.io/)\n- [Kangwon University''s NLP course in Korean](http://cs.kangwon.ac.kr/~leeck/NLP/)\n\n### Datasets\n\n- [KAIST Corpus](http://semanticweb.kaist.ac.kr/home/index.php/KAIST_Corpus) - A corpus from the Korea Advanced Institute of Science and Technology in Korean.\n- [Naver Sentiment Movie Corpus in Korean](https://github.com/e9t/nsmc/)\n- [Chosun Ilbo archive](http://srchdb1.chosun.com/pdf/i_archive/) - dataset in Korean from one of the major newspapers in South Korea, the Chosun Ilbo.\n- [Chat data](https://github.com/songys/Chatbot_data) - Chatbot data in Korean\n- [Petitions](https://github.com/akngs/petitions) - Collect expired petition data from the Blue House National Petition Site.\n- [Korean Parallel corpora](https://github.com/j-min/korean-parallel-corpora) - Neural Machine Translation(NMT) Dataset for **Korean to French** & **Korean to English**\n- [KorQuAD](https://korquad.github.io/) - Korean SQuAD dataset with Wiki HTML source. Mentions both v1.0 and v2.1 at the time of adding to Awesome NLP\n\n## NLP in Arabic\n\n[Back to Top](#contents)\n\n### Libraries\n\n- [goarabic](https://github.com/01walid/goarabic) - Go package for Arabic text processing\n- [jsastem](https://github.com/ejtaal/jsastem) - Javascript for Arabic stemming\n- [PyArabic](https://pypi.org/project/PyArabic/) - Python libraries for Arabic\n- [RFTokenizer](https://github.com/amir-zeldes/RFTokenizer) - trainable Python segmenter for Arabic, Hebrew and Coptic\n\n### Datasets\n\n- [Multidomain Datasets](https://github.com/hadyelsahar/large-arabic-sentiment-analysis-resouces) - Largest Available Multi-Domain Resources for Arabic Sentiment Analysis\n- [LABR](https://github.com/mohamedadaly/labr) - LArge Arabic Book Reviews dataset\n- [Arabic Stopwords](https://github.com/mohataher/arabic-stop-words) - A list of Arabic stopwords from various resources\n\n## NLP in Chinese\n\n[Back to Top](#contents)\n\n### Libraries\n\n- [jieba](https://github.com/fxsjy/jieba#jieba-1) - Python package for Words Segmentation Utilities in Chinese\n- [SnowNLP](https://github.com/isnowfy/snownlp) - Python package for Chinese NLP\n- [FudanNLP](https://github.com/FudanNLP/fnlp) - Java library for Chinese text processing\n- [HanLP](https://github.com/hankcs/HanLP) - The multilingual NLP library\n\n### Anthology\n- [funNLP](https://github.com/fighting41love/funNLP) - Collection of NLP tools and resources mainly for Chinese\n\n## NLP in German\n\n- [German-NLP](https://github.com/adbar/German-NLP) - Curated list of open-access/open-source/off-the-shelf resources and tools developed with a particular focus on German\n\n## NLP in Polish\n\n- [Polish-NLP](https://github.com/ksopyla/awesome-nlp-polish) - A curated list of resources dedicated to Natural Language Processing (NLP) in polish. Models, tools, datasets.\n\n## NLP in Spanish\n\n[Back to Top](#contents)\n\n### Libraries\n\n- [spanlp](https://github.com/jfreddypuentes/spanlp) - Python library to detect, censor and clean profanity, vulgarities, hateful words, racism, xenophobia and bullying in texts written in Spanish. It contains data of 21 Spanish-speaking countries.\n\n### Data\n\n- [Columbian Political Speeches](https://github.com/dav009/LatinamericanTextResources)\n- [Copenhagen Treebank](https://mbkromann.github.io/copenhagen-dependency-treebank/)\n- [Spanish Billion words corpus with Word2Vec embeddings](https://github.com/crscardellino/sbwce)\n- [Compilation of Spanish Unannotated Corpora](https://github.com/josecannete/spanish-unannotated-corpora)\n\n### Word and Sentence Embeddings\n- [Spanish Word Embeddings Computed with Different Methods and from Different Corpora](https://github.com/dccuchile/spanish-word-embeddings)\n- [Spanish Word Embeddings Computed from Large Corpora and Different Sizes Using fastText](https://github.com/BotCenter/spanishWordEmbeddings)\n- [Spanish Sentence Embeddings Computed from Large Corpora Using sent2vec](https://github.com/BotCenter/spanishSent2Vec)\n- [Beto - BERT for Spanish](https://github.com/dccuchile/beto)\n\n\n## NLP in Indic languages\n\n[Back to Top](#contents)\n\n### Data, Corpora and Treebanks\n\n- [Hindi Dependency Treebank](https://ltrc.iiit.ac.in/treebank_H2014/) - A multi-representational multi-layered treebank for Hindi and Urdu\n- [Universal Dependencies Treebank in Hindi](https://universaldependencies.org/treebanks/hi_hdtb/index.html)\n  - [Parallel Universal Dependencies Treebank in Hindi](http://universaldependencies.org/treebanks/hi_pud/index.html) - A smaller part of the above-mentioned treebank.\n- [ISI FIRE Stopwords List (Hindi and Bangla)](https://www.isical.ac.in/~fire/data/)\n- [Peter Graham''s Stopwords List](https://github.com/6/stopwords-json)\n- [NLTK Corpus](https://www.nltk.org/book/ch02.html) 60k Words POS Tagged, Bangla, Hindi, Marathi, Telugu\n- [Hindi Movie Reviews Dataset](https://github.com/goru001/nlp-for-hindi) ~1k Samples, 3 polarity classes\n- [BBC News Hindi Dataset](https://github.com/NirantK/hindi2vec/releases/tag/bbc-hindi-v0.1) 4.3k Samples, 14 classes\n- [IIT Patna Hindi ABSA Dataset](https://github.com/pnisarg/ABSA) 5.4k Samples, 12 Domains, 4k aspect terms, aspect and sentence level polarity in 4 classes\n- [Bangla ABSA](https://github.com/AtikRahman/Bangla_Datasets_ABSA) 5.5k Samples, 2 Domains, 10 aspect terms\n- [IIT Patna Movie Review Sentiment Dataset](https://www.iitp.ac.in/~ai-nlp-ml/resources.html) 2k Samples, 3 polarity labels\n\n#### Corpora/Datasets that need a login/access can be gained via email\n\n- [SAIL 2015](http://amitavadas.com/SAIL/) Twitter and Facebook labelled sentiment samples in Hindi, Bengali, Tamil, Telugu.\n- [IIT Bombay NLP Resources](http://www.cfilt.iitb.ac.in/Sentiment_Analysis_Resources.html) Sentiwordnet, Movie and Tourism parallel labelled corpora, polarity labelled sense annotated corpus, Marathi polarity labelled corpus.\n- [TDIL-IC aggregates a lot of useful resources and provides access to otherwise gated datasets](https://tdil-dc.in/index.php?option=com_catalogue&task=viewTools&id=83&lang=en)\n\n### Language Models and Word Embeddings\n\n- [Hindi2Vec](https://nirantk.com/hindi2vec/) and [nlp-for-hindi](https://github.com/goru001/nlp-for-hindi) ULMFIT style languge model\n- [IIT Patna Bilingual Word Embeddings Hi-En](https://www.iitp.ac.in/~ai-nlp-ml/resources.html)\n- [Fasttext word embeddings in a whole bunch of languages, trained on Common Crawl](https://fasttext.cc/docs/en/crawl-vectors.html)\n- [Hindi and Bengali Word2Vec](https://github.com/Kyubyong/wordvectors)\n- [Hindi and Urdu Elmo Model](https://github.com/HIT-SCIR/ELMoForManyLangs)\n- [Sanskrit Albert](https://huggingface.co/surajp/albert-base-sanskrit) Trained on Sanskrit Wikipedia and OSCAR corpus\n\n### Libraries and Tooling\n\n- [Multi-Task Deep Morphological Analyzer](https://github.com/Saurav0074/mt-dma) Deep Network based Morphological Parser for Hindi and Urdu\n- [Anoop Kunchukuttan](https://github.com/anoopkunchukuttan/indic_nlp_library) 18 Languages, whole host of features from tokenization to translation\n- [SivaReddy''s Dependency Parser](http://sivareddy.in/downloads) Dependency Parser and Pos Tagger for Kannada, Hindi and Telugu. [Python3 Port](https://github.com/CalmDownKarm/sivareddydependencyparser)\n- [iNLTK](https://github.com/goru001/inltk) - A Natural Language Toolkit for Indic Languages (Indian subcontinent languages) built on top of Pytorch/Fastai, which aims to provide out of the box support for common NLP tasks.\n\n## NLP in Thai\n\n[Back to Top](#contents)\n\n### Libraries\n\n- [PyThaiNLP](https://github.com/PyThaiNLP/pythainlp) - Thai NLP in Python Package\n- [JTCC](https://github.com/wittawatj/jtcc) - A character cluster library in Java\n- [CutKum](https://github.com/pucktada/cutkum) - Word segmentation with deep learning in TensorFlow\n- [Thai Language Toolkit](https://pypi.python.org/pypi/tltk/) - Based on a paper by Wirote Aroonmanakun in 2002 with included dataset\n- [SynThai](https://github.com/KenjiroAI/SynThai) - Word segmentation and POS tagging using deep learning in Python\n\n### Data\n\n- [Inter-BEST](https://www.nectec.or.th/corpus/index.php?league=pm) - A text corpus with 5 million words with word segmentation\n- [Prime Minister 29](https://github.com/PyThaiNLP/lexicon-thai/tree/master/thai-corpus/Prime%20Minister%2029) - Dataset containing speeches of the current Prime Minister of Thailand\n\n## NLP in Danish\n\n- [Named Entity Recognition for Danish](https://github.com/ITUnlp/daner)\n- [DaNLP](https://github.com/alexandrainst/danlp) - NLP resources in Danish\n- [Awesome Danish](https://github.com/fnielsen/awesome-danish) - A curated list of awesome resources for Danish language technology\n\n## NLP in Vietnamese\n\n### Libraries\n\n- [underthesea](https://github.com/undertheseanlp/underthesea) - Vietnamese NLP Toolkit\n- [vn.vitk](https://github.com/phuonglh/vn.vitk) - A Vietnamese Text Processing Toolkit\n- [VnCoreNLP](https://github.com/vncorenlp/VnCoreNLP) - A Vietnamese natural language processing toolkit\n- [PhoBERT](https://github.com/VinAIResearch/PhoBERT) - Pre-trained language models for Vietnamese\n- [pyvi](https://github.com/trungtv/pyvi) - Python Vietnamese Core NLP Toolkit\n\n### Data\n\n- [Vietnamese treebank](https://vlsp.hpda.vn/demo/?page=resources&lang=en) - 10,000 sentences for the constituency parsing task\n- [BKTreeBank](https://arxiv.org/pdf/1710.05519.pdf) - a Vietnamese Dependency Treebank\n- [UD_Vietnamese](https://github.com/UniversalDependencies/UD_Vietnamese-VTB) - Vietnamese Universal Dependency Treebank\n- [VIVOS](https://ailab.hcmus.edu.vn/vivos/) - a free Vietnamese speech corpus consisting of 15 hours of recording speech by AILab\n- [VNTQcorpus(big).txt](http://viet.jnlp.org/download-du-lieu-tu-vung-corpus) - 1.75 million sentences in news\n- [ViText2SQL](https://github.com/VinAIResearch/ViText2SQL) - A dataset for Vietnamese Text-to-SQL semantic parsing (EMNLP-2020 Findings)\n- [EVB Corpus](https://github.com/qhungngo/EVBCorpus) - 20,000,000 words (20 million) from 15 bilingual books, 100 parallel English-Vietnamese / Vietnamese-English texts, 250 parallel law and ordinance texts, 5,000 news articles, and 2,000 film subtitles.\n\n\n## NLP for Dutch\n\n[Back to Top](#contents)\n\n- [python-frog](https://github.com/proycon/python-frog) - Python binding to Frog, an NLP suite for Dutch. (pos tagging, lemmatisation, dependency parsing, NER)\n- [SimpleNLG_NL](https://github.com/rfdj/SimpleNLG-NL) - Dutch surface realiser used for Natural Language Generation in Dutch, based on the SimpleNLG implementation for English and French.\n- [Alpino](https://github.com/rug-compling/alpino) - Dependency parser for Dutch (also does PoS tagging and Lemmatisation).\n- [Kaldi NL](https://github.com/opensource-spraakherkenning-nl/Kaldi_NL) - Dutch Speech Recognition models based on [Kaldi](http://kaldi-asr.org/).\n- [spaCy](https://spacy.io/) - [Dutch model](https://spacy.io/models/nl) available. - Industrial strength NLP with Python and Cython. \n\n\n## NLP in Indonesian\n\n### Datasets\n- Kompas and Tempo collections at [ILPS](http://ilps.science.uva.nl/resources/bahasa/)\n- [PANL10N for PoS tagging](http://www.panl10n.net/english/outputs/Indonesia/UI/0802/UI-1M-tagged.zip): 39K sentences and 900K word tokens\n- [IDN for PoS tagging](https://github.com/famrashel/idn-tagged-corpus): This corpus contains 10K sentences and 250K word tokens\n- [Indonesian Treebank](https://github.com/famrashel/idn-treebank) and [Universal Dependencies-Indonesian](https://github.com/UniversalDependencies/UD_Indonesian-GSD)\n- [IndoSum](https://github.com/kata-ai/indosum) for text summarization and classification both\n- [Wordnet-Bahasa](http://wn-msa.sourceforge.net/) - large, free, semantic dictionary\n- IndoBenchmark [IndoNLU](https://github.com/indobenchmark/indonlu) includes pre-trained language model (IndoBERT), FastText model, Indo4B corpus, and several NLU benchmark datasets\n\n### Libraries & Embedding\n- Natural language toolkit [bahasa](https://github.com/kangfend/bahasa)\n- [Indonesian Word Embedding](https://github.com/galuhsahid/indonesian-word-embedding)\n- Pretrained [Indonesian fastText Text Embedding](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.id.zip) trained on Wikipedia\n- IndoBenchmark [IndoNLU](https://github.com/indobenchmark/indonlu) includes pretrained language model (IndoBERT), FastText model, Indo4B corpus, and several NLU benchmark datasets\n\n## NLP in Urdu\n\n### Datasets\n- [Collection of Urdu datasets](https://github.com/mirfan899/Urdu) for POS, NER and NLP tasks\n\n### Libraries\n- [Natural Language Processing library](https://github.com/urduhack/urduhack) for ( ðŸ‡µðŸ‡°)Urdu language\n\n## NLP in Persian\n\n[Back to Top](#contents)\n\n### Libraries\n- [Hazm](https://github.com/roshan-research/hazm) - Persian NLP Toolkit.\n- [Parsivar](https://github.com/ICTRC/Parsivar): A Language Processing Toolkit for Persian\n- [Perke](https://github.com/AlirezaTheH/perke): Perke is a Python keyphrase extraction package for Persian language. It provides an end-to-end keyphrase extraction pipeline in which each component can be easily modified or extended to develop new models.\n- [Perstem](https://github.com/jonsafari/perstem): Persian stemmer, morphological analyzer, transliterator, and partial part-of-speech tagger\n- [ParsiAnalyzer](https://github.com/NarimanN2/ParsiAnalyzer): Persian Analyzer For Elasticsearch\n- [virastar](https://github.com/aziz/virastar): Cleaning up Persian text!\n\n### Datasets\n- [Bijankhan Corpus](https://dbrg.ut.ac.ir/Ø¨ÛŒÚ˜Ù†%E2%80%8CØ®Ø§Ù†/): Bijankhan corpus is a tagged corpus that is suitable for natural language processing research on the Persian (Farsi) language. This collection is gathered form daily news and common texts. In this collection all documents are categorized into different subjects such as political, cultural and so on. Totally, there are 4300 different subjects. The Bijankhan collection contains about 2.6 millions manually tagged words with a tag set that contains 40 Persian POS tags.\n- [Uppsala Persian Corpus (UPC)](https://sites.google.com/site/mojganserajicom/home/upc): Uppsala Persian Corpus (UPC) is a large, freely available Persian corpus. The corpus is a modified version of the Bijankhan corpus with additional sentence segmentation and consistent tokenization containing 2,704,028 tokens and annotated with 31 part-of-speech tags. The part-of-speech tags are listed with explanations in [this table](https://sites.google.com/site/mojganserajicom/home/upc/Table_tag.pdf).\n- [Large-Scale Colloquial Persian](http://hdl.handle.net/11234/1-3195): Large Scale Colloquial Persian Dataset (LSCP) is hierarchically organized in asemantic taxonomy that focuses on multi-task informal Persian language understanding as a comprehensive problem. LSCP includes 120M sentences from 27M casual Persian tweets with its dependency relations in syntactic annotation, Part-of-speech tags, sentiment polarity and automatic translation of original Persian sentences in English (EN), German (DE), Czech (CS), Italian (IT) and Hindi (HI) spoken languages. Learn more about this project at [LSCP webpage](https://iasbs.ac.ir/~ansari/lscp/).\n- [ArmanPersoNERCorpus](https://github.com/HaniehP/PersianNER): The dataset includes 250,015 tokens and 7,682 Persian sentences in total. It is available in 3 folds to be used in turn as training and test sets. Each file contains one token, along with its manually annotated named-entity tag, per line. Each sentence is separated with a newline. The NER tags are in IOB format.\n- [FarsiYar PersianNER](https://github.com/Text-Mining/Persian-NER): The dataset includes about 25,000,000 tokens and about 1,000,000 Persian sentences in total based on [Persian Wikipedia Corpus](https://github.com/Text-Mining/Persian-Wikipedia-Corpus). The NER tags are in IOB format. More than 1000 volunteers contributed tag improvements to this dataset via web panel or android app. They release updated tags every two weeks.\n- [PERLEX](http://farsbase.net/PERLEX.html): The first Persian dataset for relation extraction, which is an expert translated version of the â€œSemeval-2010-Task-8â€ dataset. Link to the relevant publication.\n- [Persian Syntactic Dependency Treebank](http://dadegan.ir/catalog/perdt): This treebank is supplied for free noncommercial use. For commercial uses feel free to contact us. The number of annotated sentences is 29,982 sentences including samples from almost all verbs of the Persian valency lexicon.\n- [Uppsala Persian Dependency Treebank (UPDT)](http://stp.lingfil.uu.se/~mojgan/UPDT.html): Dependency-based syntactically annotated corpus.\n- [Hamshahri](https://dbrg.ut.ac.ir/hamshahri/): Hamshahri collection is a standard reliable Persian text collection that was used at Cross Language Evaluation Forum (CLEF) during years 2008 and 2009 for evaluation of Persian information retrieval systems.\n\n\n## NLP in Ukrainian\n\n[Back to Top](#contents)\n\n- [awesome-ukrainian-nlp](https://github.com/asivokon/awesome-ukrainian-nlp) - a curated list of Ukrainian NLP datasets, models, etc.\n- [UkrainianLT](https://github.com/Helsinki-NLP/UkrainianLT) - another curated list with a focus on machine translation and speech processing\n\n\n## NLP in Hungarian\n\n[Back to Top](#contents)\n\n- [awesome-hungarian-nlp](https://github.com/oroszgy/awesome-hungarian-nlp): A curated list of free resources dedicated to Hungarian Natural Language Processing.\n\n## NLP in Portuguese\n\n[Back to Top](#contents)\n\n- [Portuguese-nlp](https://github.com/ajdavidl/Portuguese-NLP) - a List of resources and tools developed with focus on Portuguese.\n\n## Other Languages\n\n- Russian: [pymorphy2](https://github.com/kmike/pymorphy2) - a good pos-tagger for Russian\n- Asian Languages: Thai, Lao, Chinese, Japanese, and Korean [ICU Tokenizer](https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-icu-tokenizer.html) implementation in ElasticSearch\n- Ancient Languages: [CLTK](https://github.com/cltk/cltk): The Classical Language Toolkit is a Python library and collection of texts for doing NLP in ancient languages\n- Hebrew: [NLPH_Resources](https://github.com/NLPH/NLPH_Resources) - A collection of papers, corpora and linguistic resources for NLP in Hebrew\n\n[Back to Top](#contents)\n\n[Credits](./CREDITS.md) for initial curators and sources\n\n## License\n[License](./LICENSE) - CC0\n', '{"language":null,"stars":18022,"forks":2724,"watchers":18022,"open_issues":10,"topics":["awesome","awesome-list","deep-learning","language","machine-learning","natural-language-processing","nlp","text-mining"],"default_branch":"master","size_kb":555,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:sindresorhus:awesome","source_url":"https://github.com/sindresorhus/awesome"},{"type":"has_code","target_id":"github:keonkim:awesome-nlp","source_url":"https://github.com/keonkim/awesome-nlp"},{"type":"has_code","target_id":"github:NirantK:nlp-python-deep-learning","source_url":"http://github.com/NirantK/nlp-python-deep-learning"},{"type":"has_code","target_id":"github:hb20007:hands-on-nltk-tutorial","source_url":"https://github.com/hb20007/hands-on-nltk-tutorial"},{"type":"has_code","target_id":"github:oxford-cs-deepnlp-2017:lectures","source_url":"https://github.com/oxford-cs-deepnlp-2017/lectures"},{"type":"has_code","target_id":"github:yandexdataschool:nlp_course","source_url":"https://github.com/yandexdataschool/nlp_course"},{"type":"has_code","target_id":"github:fastai:course-nlp","source_url":"https://github.com/fastai/course-nlp"},{"type":"has_code","target_id":"github:aws-samples:aws-machine-learning-university-accelerated-nlp","source_url":"https://github.com/aws-samples/aws-machine-learning-university-accelerated-nlp"},{"type":"has_code","target_id":"github:Ramaseshanr:anlp","source_url":"https://github.com/Ramaseshanr/anlp"},{"type":"has_code","target_id":"github:jacobeisenstein:gt-nlp-class","source_url":"https://github.com/jacobeisenstein/gt-nlp-class"},{"type":"has_code","target_id":"github:joosthub:PyTorchNLPBook","source_url":"https://github.com/joosthub/PyTorchNLPBook"},{"type":"has_code","target_id":"github:twitter:twitter-text","source_url":"https://github.com/twitter/twitter-text"},{"type":"has_code","target_id":"github:benhmoore:Knwl.js","source_url":"https://github.com/benhmoore/Knwl.js"},{"type":"has_code","target_id":"github:retextjs:retext","source_url":"https://github.com/retextjs/retext"},{"type":"has_code","target_id":"github:spencermountain:compromise","source_url":"https://github.com/spencermountain/compromise"},{"type":"has_code","target_id":"github:NaturalNode:natural","source_url":"https://github.com/NaturalNode/natural"},{"type":"has_code","target_id":"github:synyi:poplar","source_url":"https://github.com/synyi/poplar"},{"type":"has_code","target_id":"github:axa-group:nlp.js","source_url":"https://github.com/axa-group/nlp.js"},{"type":"has_code","target_id":"github:huggingface:node-question-answering","source_url":"https://github.com/huggingface/node-question-answering"},{"type":"has_code","target_id":"github:sloev:sentimental-onix","source_url":"https://github.com/sloev/sentimental-onix"},{"type":"has_code","target_id":"github:QData:TextAttack","source_url":"https://github.com/QData/TextAttack"},{"type":"has_code","target_id":"github:clips:pattern","source_url":"https://github.com/clips/pattern"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:nebuly-ai:nebullvm","source_url":"https://github.com/nebuly-ai/nebullvm"},{"type":"has_code","target_id":"github:chartbeat-labs:textacy","source_url":"https://github.com/chartbeat-labs/textacy"},{"type":"has_code","target_id":"github:JasonKessler:scattertext","source_url":"https://github.com/JasonKessler/scattertext"},{"type":"has_code","target_id":"github:dmlc:gluon-nlp","source_url":"https://github.com/dmlc/gluon-nlp"},{"type":"has_code","target_id":"github:allenai:allennlp","source_url":"https://github.com/allenai/allennlp"},{"type":"has_code","target_id":"github:PetrochukM:PyTorch-NLP","source_url":"https://github.com/PetrochukM/PyTorch-NLP"},{"type":"has_code","target_id":"github:columbia-applied-data-science:rosetta","source_url":"https://github.com/columbia-applied-data-science/rosetta"},{"type":"has_code","target_id":"github:proycon:pynlpl","source_url":"https://github.com/proycon/pynlpl"},{"type":"has_code","target_id":"github:proycon:foliapy","source_url":"https://github.com/proycon/foliapy"},{"type":"has_code","target_id":"github:sergioburdisso:pyss3","source_url":"https://github.com/sergioburdisso/pyss3"},{"type":"has_code","target_id":"github:datquocnguyen:jPTDP","source_url":"https://github.com/datquocnguyen/jPTDP"},{"type":"has_code","target_id":"github:bigartm:bigartm","source_url":"https://github.com/bigartm/bigartm"},{"type":"has_code","target_id":"github:snipsco:snips-nlu","source_url":"https://github.com/snipsco/snips-nlu"},{"type":"has_code","target_id":"github:chakki-works:chazutsu","source_url":"https://github.com/chakki-works/chazutsu"},{"type":"has_code","target_id":"github:gutfeeling:word_forms","source_url":"https://github.com/gutfeeling/word_forms"},{"type":"has_code","target_id":"github:ArtificiAI:Multilingual-Latent-Dirichlet-Allocation-LDA","source_url":"https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA"},{"type":"has_code","target_id":"github:NervanaSystems:nlp-architect","source_url":"https://github.com/NervanaSystems/nlp-architect"},{"type":"has_code","target_id":"github:zalandoresearch:flair","source_url":"https://github.com/zalandoresearch/flair"},{"type":"has_code","target_id":"github:BrikerMan:Kashgari","source_url":"https://github.com/BrikerMan/Kashgari"},{"type":"has_code","target_id":"github:deepset-ai:FARM","source_url":"https://github.com/deepset-ai/FARM"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:zaibacu:rita-dsl","source_url":"https://github.com/zaibacu/rita-dsl"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:tokenizers","source_url":"https://github.com/huggingface/tokenizers"},{"type":"has_code","target_id":"github:pytorch:fairseq","source_url":"https://github.com/pytorch/fairseq"},{"type":"has_code","target_id":"github:gregversteeg:corex_topic","source_url":"https://github.com/gregversteeg/corex_topic"},{"type":"has_code","target_id":"github:awslabs:sockeye","source_url":"https://github.com/awslabs/sockeye"},{"type":"has_code","target_id":"github:xhlulu:dl-translate","source_url":"https://github.com/xhlulu/dl-translate"},{"type":"has_code","target_id":"github:obss:jury","source_url":"https://github.com/obss/jury"},{"type":"has_code","target_id":"github:proycon:python-ucto","source_url":"https://github.com/proycon/python-ucto"},{"type":"has_code","target_id":"github:chncwang:InsNet","source_url":"https://github.com/chncwang/InsNet"},{"type":"has_code","target_id":"github:mit-nlp:MITIE","source_url":"https://github.com/mit-nlp/MITIE"},{"type":"has_code","target_id":"github:BLLIP:bllip-parser","source_url":"https://github.com/BLLIP/bllip-parser"},{"type":"has_code","target_id":"github:proycon:colibri-core","source_url":"https://github.com/proycon/colibri-core"},{"type":"has_code","target_id":"github:LanguageMachines:ucto","source_url":"https://github.com/LanguageMachines/ucto"},{"type":"has_code","target_id":"github:LanguageMachines:libfolia","source_url":"https://github.com/LanguageMachines/libfolia"},{"type":"has_code","target_id":"github:LanguageMachines:frog","source_url":"https://github.com/LanguageMachines/frog"},{"type":"has_code","target_id":"github:meta-toolkit:meta","source_url":"https://github.com/meta-toolkit/meta"},{"type":"has_code","target_id":"github:facebookresearch:StarSpace","source_url":"https://github.com/facebookresearch/StarSpace"},{"type":"has_code","target_id":"github:knowitall:reverb","source_url":"https://github.com/knowitall/reverb"},{"type":"has_code","target_id":"github:knowitall:openregex","source_url":"https://github.com/knowitall/openregex"},{"type":"has_code","target_id":"github:CogComp:cogcomp-nlp","source_url":"https://github.com/CogComp/cogcomp-nlp"},{"type":"has_code","target_id":"github:datquocnguyen:RDRPOSTagger","source_url":"https://github.com/datquocnguyen/RDRPOSTagger"},{"type":"has_code","target_id":"github:pemistahl:lingua","source_url":"https://github.com/pemistahl/lingua"},{"type":"has_code","target_id":"github:meiblorn:kotidgy","source_url":"https://github.com/meiblorn/kotidgy"},{"type":"has_code","target_id":"github:CogComp:saul","source_url":"https://github.com/CogComp/saul"},{"type":"has_code","target_id":"github:ispras:atr4s","source_url":"https://github.com/ispras/atr4s"},{"type":"has_code","target_id":"github:ispras:tm","source_url":"https://github.com/ispras/tm"},{"type":"has_code","target_id":"github:Refefer:word2vec-scala","source_url":"https://github.com/Refefer/word2vec-scala"},{"type":"has_code","target_id":"github:dlwh:epic","source_url":"https://github.com/dlwh/epic"},{"type":"has_code","target_id":"github:JohnSnowLabs:spark-nlp","source_url":"https://github.com/JohnSnowLabs/spark-nlp"},{"type":"has_code","target_id":"github:dselivanov:text2vec","source_url":"https://github.com/dselivanov/text2vec"},{"type":"has_code","target_id":"github:bmschmidt:wordVectors","source_url":"https://github.com/bmschmidt/wordVectors"},{"type":"has_code","target_id":"github:mimno:RMallet","source_url":"https://github.com/mimno/RMallet"},{"type":"has_code","target_id":"github:agoldst:dfr-browser","source_url":"https://github.com/agoldst/dfr-browser"},{"type":"has_code","target_id":"github:agoldst:dfrtopics","source_url":"https://github.com/agoldst/dfrtopics"},{"type":"has_code","target_id":"github:kevincobain2000:sentiment_classifier","source_url":"https://github.com/kevincobain2000/sentiment_classifier"},{"type":"has_code","target_id":"github:kevincobain2000:jProcessing","source_url":"https://github.com/kevincobain2000/jProcessing"},{"type":"has_code","target_id":"github:juliasilge:tidytext","source_url":"https://github.com/juliasilge/tidytext"},{"type":"has_code","target_id":"github:quanteda:spacyr","source_url":"https://github.com/quanteda/spacyr"},{"type":"has_code","target_id":"github:cran-task-views:NaturalLanguageProcessing","source_url":"https://github.com/cran-task-views/NaturalLanguageProcessing"},{"type":"has_code","target_id":"github:dakrone:clojure-opennlp","source_url":"https://github.com/dakrone/clojure-opennlp"},{"type":"has_code","target_id":"github:r0man:inflections-clj","source_url":"https://github.com/r0man/inflections-clj"},{"type":"has_code","target_id":"github:fekr:postagga","source_url":"https://github.com/fekr/postagga"},{"type":"has_code","target_id":"github:diasks2:ruby-nlp","source_url":"https://github.com/diasks2/ruby-nlp"},{"type":"has_code","target_id":"github:arbox:nlp-with-ruby","source_url":"https://github.com/arbox/nlp-with-ruby"},{"type":"has_code","target_id":"github:greyblake:whatlang-rs","source_url":"https://github.com/greyblake/whatlang-rs"},{"type":"has_code","target_id":"github:snipsco:snips-nlu-rs","source_url":"https://github.com/snipsco/snips-nlu-rs"},{"type":"has_code","target_id":"github:guillaume-be:rust-bert","source_url":"https://github.com/guillaume-be/rust-bert"},{"type":"has_code","target_id":"github:VisualText:nlp-engine","source_url":"https://github.com/VisualText/nlp-engine"},{"type":"has_code","target_id":"github:JuliaText:CorpusLoaders.jl","source_url":"https://github.com/JuliaText/CorpusLoaders.jl"},{"type":"has_code","target_id":"github:JuliaText:Languages.jl","source_url":"https://github.com/JuliaText/Languages.jl"},{"type":"has_code","target_id":"github:JuliaText:TextAnalysis.jl","source_url":"https://github.com/JuliaText/TextAnalysis.jl"},{"type":"has_code","target_id":"github:JuliaText:TextModels.jl","source_url":"https://github.com/JuliaText/TextModels.jl"},{"type":"has_code","target_id":"github:JuliaText:WordTokenizers.jl","source_url":"https://github.com/JuliaText/WordTokenizers.jl"},{"type":"has_code","target_id":"github:JuliaText:Word2Vec.jl","source_url":"https://github.com/JuliaText/Word2Vec.jl"},{"type":"has_code","target_id":"github:wit-ai:wit","source_url":"https://github.com/wit-ai/wit"},{"type":"has_code","target_id":"github:watson-developer-cloud:natural-language-understanding-nodejs","source_url":"https://github.com/watson-developer-cloud/natural-language-understanding-nodejs"},{"type":"has_code","target_id":"github:weitechen:anafora","source_url":"https://github.com/weitechen/anafora"},{"type":"has_code","target_id":"github:chakki-works:doccano","source_url":"https://github.com/chakki-works/doccano"},{"type":"has_code","target_id":"github:AI4Bharat:Shoonya-Backend","source_url":"https://github.com/AI4Bharat/Shoonya-Backend"},{"type":"has_code","target_id":"github:proycon:flat","source_url":"https://github.com/proycon/flat"},{"type":"has_code","target_id":"github:facebookresearch:fastText","source_url":"https://github.com/facebookresearch/fastText"},{"type":"has_code","target_id":"github:allenai:allennlp","source_url":"https://github.com/allenai/allennlp"},{"type":"has_code","target_id":"github:allenai:bilm-tf","source_url":"https://github.com/allenai/bilm-tf"},{"type":"has_code","target_id":"github:facebookresearch:DrQA","source_url":"https://github.com/facebookresearch/DrQA"},{"type":"has_code","target_id":"github:allenai:document-qa","source_url":"https://github.com/allenai/document-qa"},{"type":"has_code","target_id":"github:niderhoff:nlp-datasets","source_url":"https://github.com/niderhoff/nlp-datasets"},{"type":"has_code","target_id":"github:RaRe-Technologies:gensim-data","source_url":"https://github.com/RaRe-Technologies/gensim-data"},{"type":"has_code","target_id":"github:vincentkoc:tiny_qa_benchmark_pp","source_url":"https://github.com/vincentkoc/tiny_qa_benchmark_pp"},{"type":"has_code","target_id":"github:ufal:udpipe","source_url":"https://github.com/ufal/udpipe"},{"type":"has_code","target_id":"github:adobe:NLP-Cube","source_url":"https://github.com/adobe/NLP-Cube"},{"type":"has_code","target_id":"github:mikahama:uralicNLP","source_url":"https://github.com/mikahama/uralicNLP"},{"type":"has_code","target_id":"github:e9t:nsmc","source_url":"https://github.com/e9t/nsmc"},{"type":"has_code","target_id":"github:songys:Chatbot_data","source_url":"https://github.com/songys/Chatbot_data"},{"type":"has_code","target_id":"github:akngs:petitions","source_url":"https://github.com/akngs/petitions"},{"type":"has_code","target_id":"github:j-min:korean-parallel-corpora","source_url":"https://github.com/j-min/korean-parallel-corpora"},{"type":"has_code","target_id":"github:01walid:goarabic","source_url":"https://github.com/01walid/goarabic"},{"type":"has_code","target_id":"github:ejtaal:jsastem","source_url":"https://github.com/ejtaal/jsastem"},{"type":"has_code","target_id":"github:amir-zeldes:RFTokenizer","source_url":"https://github.com/amir-zeldes/RFTokenizer"},{"type":"has_code","target_id":"github:hadyelsahar:large-arabic-sentiment-analysis-resouces","source_url":"https://github.com/hadyelsahar/large-arabic-sentiment-analysis-resouces"},{"type":"has_code","target_id":"github:mohamedadaly:labr","source_url":"https://github.com/mohamedadaly/labr"},{"type":"has_code","target_id":"github:mohataher:arabic-stop-words","source_url":"https://github.com/mohataher/arabic-stop-words"},{"type":"has_code","target_id":"github:fxsjy:jieba","source_url":"https://github.com/fxsjy/jieba#jieba-1"},{"type":"has_code","target_id":"github:isnowfy:snownlp","source_url":"https://github.com/isnowfy/snownlp"},{"type":"has_code","target_id":"github:FudanNLP:fnlp","source_url":"https://github.com/FudanNLP/fnlp"},{"type":"has_code","target_id":"github:hankcs:HanLP","source_url":"https://github.com/hankcs/HanLP"},{"type":"has_code","target_id":"github:fighting41love:funNLP","source_url":"https://github.com/fighting41love/funNLP"},{"type":"has_code","target_id":"github:adbar:German-NLP","source_url":"https://github.com/adbar/German-NLP"},{"type":"has_code","target_id":"github:ksopyla:awesome-nlp-polish","source_url":"https://github.com/ksopyla/awesome-nlp-polish"},{"type":"has_code","target_id":"github:jfreddypuentes:spanlp","source_url":"https://github.com/jfreddypuentes/spanlp"},{"type":"has_code","target_id":"github:dav009:LatinamericanTextResources","source_url":"https://github.com/dav009/LatinamericanTextResources"},{"type":"has_code","target_id":"github:crscardellino:sbwce","source_url":"https://github.com/crscardellino/sbwce"},{"type":"has_code","target_id":"github:josecannete:spanish-unannotated-corpora","source_url":"https://github.com/josecannete/spanish-unannotated-corpora"},{"type":"has_code","target_id":"github:dccuchile:spanish-word-embeddings","source_url":"https://github.com/dccuchile/spanish-word-embeddings"},{"type":"has_code","target_id":"github:BotCenter:spanishWordEmbeddings","source_url":"https://github.com/BotCenter/spanishWordEmbeddings"},{"type":"has_code","target_id":"github:BotCenter:spanishSent2Vec","source_url":"https://github.com/BotCenter/spanishSent2Vec"},{"type":"has_code","target_id":"github:dccuchile:beto","source_url":"https://github.com/dccuchile/beto"},{"type":"has_code","target_id":"github:6:stopwords-json","source_url":"https://github.com/6/stopwords-json"},{"type":"has_code","target_id":"github:goru001:nlp-for-hindi","source_url":"https://github.com/goru001/nlp-for-hindi"},{"type":"has_code","target_id":"github:NirantK:hindi2vec","source_url":"https://github.com/NirantK/hindi2vec"},{"type":"has_code","target_id":"github:pnisarg:ABSA","source_url":"https://github.com/pnisarg/ABSA"},{"type":"has_code","target_id":"github:AtikRahman:Bangla_Datasets_ABSA","source_url":"https://github.com/AtikRahman/Bangla_Datasets_ABSA"},{"type":"has_code","target_id":"github:goru001:nlp-for-hindi","source_url":"https://github.com/goru001/nlp-for-hindi"},{"type":"has_code","target_id":"github:Kyubyong:wordvectors","source_url":"https://github.com/Kyubyong/wordvectors"},{"type":"has_code","target_id":"github:HIT-SCIR:ELMoForManyLangs","source_url":"https://github.com/HIT-SCIR/ELMoForManyLangs"},{"type":"has_code","target_id":"github:Saurav0074:mt-dma","source_url":"https://github.com/Saurav0074/mt-dma"},{"type":"has_code","target_id":"github:anoopkunchukuttan:indic_nlp_library","source_url":"https://github.com/anoopkunchukuttan/indic_nlp_library"},{"type":"has_code","target_id":"github:CalmDownKarm:sivareddydependencyparser","source_url":"https://github.com/CalmDownKarm/sivareddydependencyparser"},{"type":"has_code","target_id":"github:goru001:inltk","source_url":"https://github.com/goru001/inltk"},{"type":"has_code","target_id":"github:PyThaiNLP:pythainlp","source_url":"https://github.com/PyThaiNLP/pythainlp"},{"type":"has_code","target_id":"github:wittawatj:jtcc","source_url":"https://github.com/wittawatj/jtcc"},{"type":"has_code","target_id":"github:pucktada:cutkum","source_url":"https://github.com/pucktada/cutkum"},{"type":"has_code","target_id":"github:KenjiroAI:SynThai","source_url":"https://github.com/KenjiroAI/SynThai"},{"type":"has_code","target_id":"github:PyThaiNLP:lexicon-thai","source_url":"https://github.com/PyThaiNLP/lexicon-thai"},{"type":"has_code","target_id":"github:ITUnlp:daner","source_url":"https://github.com/ITUnlp/daner"},{"type":"has_code","target_id":"github:alexandrainst:danlp","source_url":"https://github.com/alexandrainst/danlp"},{"type":"has_code","target_id":"github:fnielsen:awesome-danish","source_url":"https://github.com/fnielsen/awesome-danish"},{"type":"has_code","target_id":"github:undertheseanlp:underthesea","source_url":"https://github.com/undertheseanlp/underthesea"},{"type":"has_code","target_id":"github:phuonglh:vn.vitk","source_url":"https://github.com/phuonglh/vn.vitk"},{"type":"has_code","target_id":"github:vncorenlp:VnCoreNLP","source_url":"https://github.com/vncorenlp/VnCoreNLP"},{"type":"has_code","target_id":"github:VinAIResearch:PhoBERT","source_url":"https://github.com/VinAIResearch/PhoBERT"},{"type":"has_code","target_id":"github:trungtv:pyvi","source_url":"https://github.com/trungtv/pyvi"},{"type":"has_code","target_id":"github:UniversalDependencies:UD_Vietnamese-VTB","source_url":"https://github.com/UniversalDependencies/UD_Vietnamese-VTB"},{"type":"has_code","target_id":"github:VinAIResearch:ViText2SQL","source_url":"https://github.com/VinAIResearch/ViText2SQL"},{"type":"has_code","target_id":"github:qhungngo:EVBCorpus","source_url":"https://github.com/qhungngo/EVBCorpus"},{"type":"has_code","target_id":"github:proycon:python-frog","source_url":"https://github.com/proycon/python-frog"},{"type":"has_code","target_id":"github:rfdj:SimpleNLG-NL","source_url":"https://github.com/rfdj/SimpleNLG-NL"},{"type":"has_code","target_id":"github:rug-compling:alpino","source_url":"https://github.com/rug-compling/alpino"},{"type":"has_code","target_id":"github:opensource-spraakherkenning-nl:Kaldi_NL","source_url":"https://github.com/opensource-spraakherkenning-nl/Kaldi_NL"},{"type":"has_code","target_id":"github:famrashel:idn-tagged-corpus","source_url":"https://github.com/famrashel/idn-tagged-corpus"},{"type":"has_code","target_id":"github:famrashel:idn-treebank","source_url":"https://github.com/famrashel/idn-treebank"},{"type":"has_code","target_id":"github:UniversalDependencies:UD_Indonesian-GSD","source_url":"https://github.com/UniversalDependencies/UD_Indonesian-GSD"},{"type":"has_code","target_id":"github:kata-ai:indosum","source_url":"https://github.com/kata-ai/indosum"},{"type":"has_code","target_id":"github:indobenchmark:indonlu","source_url":"https://github.com/indobenchmark/indonlu"},{"type":"has_code","target_id":"github:kangfend:bahasa","source_url":"https://github.com/kangfend/bahasa"},{"type":"has_code","target_id":"github:galuhsahid:indonesian-word-embedding","source_url":"https://github.com/galuhsahid/indonesian-word-embedding"},{"type":"has_code","target_id":"github:indobenchmark:indonlu","source_url":"https://github.com/indobenchmark/indonlu"},{"type":"has_code","target_id":"github:mirfan899:Urdu","source_url":"https://github.com/mirfan899/Urdu"},{"type":"has_code","target_id":"github:urduhack:urduhack","source_url":"https://github.com/urduhack/urduhack"},{"type":"has_code","target_id":"github:roshan-research:hazm","source_url":"https://github.com/roshan-research/hazm"},{"type":"has_code","target_id":"github:ICTRC:Parsivar","source_url":"https://github.com/ICTRC/Parsivar"},{"type":"has_code","target_id":"github:AlirezaTheH:perke","source_url":"https://github.com/AlirezaTheH/perke"},{"type":"has_code","target_id":"github:jonsafari:perstem","source_url":"https://github.com/jonsafari/perstem"},{"type":"has_code","target_id":"github:NarimanN2:ParsiAnalyzer","source_url":"https://github.com/NarimanN2/ParsiAnalyzer"},{"type":"has_code","target_id":"github:aziz:virastar","source_url":"https://github.com/aziz/virastar"},{"type":"has_code","target_id":"github:HaniehP:PersianNER","source_url":"https://github.com/HaniehP/PersianNER"},{"type":"has_code","target_id":"github:Text-Mining:Persian-NER","source_url":"https://github.com/Text-Mining/Persian-NER"},{"type":"has_code","target_id":"github:Text-Mining:Persian-Wikipedia-Corpus","source_url":"https://github.com/Text-Mining/Persian-Wikipedia-Corpus"},{"type":"has_code","target_id":"github:asivokon:awesome-ukrainian-nlp","source_url":"https://github.com/asivokon/awesome-ukrainian-nlp"},{"type":"has_code","target_id":"github:Helsinki-NLP:UkrainianLT","source_url":"https://github.com/Helsinki-NLP/UkrainianLT"},{"type":"has_code","target_id":"github:oroszgy:awesome-hungarian-nlp","source_url":"https://github.com/oroszgy/awesome-hungarian-nlp"},{"type":"has_code","target_id":"github:ajdavidl:Portuguese-NLP","source_url":"https://github.com/ajdavidl/Portuguese-NLP"},{"type":"has_code","target_id":"github:kmike:pymorphy2","source_url":"https://github.com/kmike/pymorphy2"},{"type":"has_code","target_id":"github:cltk:cltk","source_url":"https://github.com/cltk/cltk"},{"type":"has_code","target_id":"github:NLPH:NLPH_Resources","source_url":"https://github.com/NLPH/NLPH_Resources"}]', NULL, 'CC0-1.0', 'approved', 80, '66f6c4e1c256156496c918dd3e8d25ea', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-keon-awesome-nlp from https://github.com/keon.png
Image converted to WebP: data/images/github-keon-awesome-nlp.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-microsoft-LightGBM', 'github--microsoft--lightgbm', 'LightGBM', 'microsoft', '<img src=https://github.com/microsoft/LightGBM/blob/master/docs/logo/LightGBM_logo_black_text.svg width=300 /> Light Gradient Boosting Machine =============================== LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages: - Faster training speed and higher efficiency. - Lower memory usage. - Better accuracy. - Support of parallel, distributed, and GPU learning. - Capable of handl...', '["data-mining","decision-trees","distributed","gbdt","gbm","gbrt","gradient-boosting","kaggle","lightgbm","machine-learning","microsoft","parallel","python","r","c++"]', 'other', 17914, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/microsoft/LightGBM","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<img src=https://github.com/microsoft/LightGBM/blob/master/docs/logo/LightGBM_logo_black_text.svg width=300 />\n\nLight Gradient Boosting Machine\n===============================\n\n[![C++ GitHub Actions Build Status](https://github.com/microsoft/LightGBM/actions/workflows/cpp.yml/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions/workflows/cpp.yml)\n[![Python-package GitHub Actions Build Status](https://github.com/microsoft/LightGBM/actions/workflows/python_package.yml/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions/workflows/python_package.yml)\n[![R-package GitHub Actions Build Status](https://github.com/microsoft/LightGBM/actions/workflows/r_package.yml/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions/workflows/r_package.yml)\n[![CUDA Version GitHub Actions Build Status](https://github.com/microsoft/LightGBM/actions/workflows/cuda.yml/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions/workflows/cuda.yml)\n[![SWIG Wrapper GitHub Actions Build Status](https://github.com/microsoft/LightGBM/actions/workflows/swig.yml/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions/workflows/swig.yml)\n[![Static Analysis GitHub Actions Build Status](https://github.com/microsoft/LightGBM/actions/workflows/static_analysis.yml/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions/workflows/static_analysis.yml)\n[![Azure Pipelines Build Status](https://lightgbm-ci.visualstudio.com/lightgbm-ci/_apis/build/status/Microsoft.LightGBM?branchName=master)](https://lightgbm-ci.visualstudio.com/lightgbm-ci/_build/latest?definitionId=1)\n[![Appveyor Build Status](https://ci.appveyor.com/api/projects/status/1ys5ot401m0fep6l/branch/master?svg=true)](https://ci.appveyor.com/project/guolinke/lightgbm/branch/master)\n[![Documentation Status](https://readthedocs.org/projects/lightgbm/badge/?version=latest)](https://lightgbm.readthedocs.io/)\n[![Link checks](https://github.com/microsoft/LightGBM/actions/workflows/lychee.yml/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions/workflows/lychee.yml)\n[![License](https://img.shields.io/github/license/microsoft/lightgbm.svg)](https://github.com/microsoft/LightGBM/blob/master/LICENSE)\n[![EffVer Versioning](https://img.shields.io/badge/version_scheme-EffVer-0097a7)](https://jacobtomlinson.dev/effver)\n[![StackOverflow questions](https://img.shields.io/stackexchange/stackoverflow/t/lightgbm?logo=stackoverflow&logoColor=white&label=StackOverflow%20questions)](https://stackoverflow.com/questions/tagged/lightgbm?sort=votes)\n[![Python Versions](https://img.shields.io/pypi/pyversions/lightgbm.svg?logo=python&logoColor=white)](https://pypi.org/project/lightgbm)\n[![PyPI Version](https://img.shields.io/pypi/v/lightgbm.svg?logo=pypi&logoColor=white)](https://pypi.org/project/lightgbm)\n[![conda Version](https://img.shields.io/conda/vn/conda-forge/lightgbm?logo=conda-forge&logoColor=white&label=conda)](https://anaconda.org/conda-forge/lightgbm)\n[![CRAN Version](https://www.r-pkg.org/badges/version/lightgbm)](https://cran.r-project.org/package=lightgbm)\n[![NuGet Version](https://img.shields.io/nuget/v/lightgbm?logo=nuget&logoColor=white)](https://www.nuget.org/packages/LightGBM)\n\nLightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\n- Faster training speed and higher efficiency.\n- Lower memory usage.\n- Better accuracy.\n- Support of parallel, distributed, and GPU learning.\n- Capable of handling large-scale data.\n\nFor further details, please refer to [Features](https://github.com/microsoft/LightGBM/blob/master/docs/Features.rst).\n\nBenefiting from these advantages, LightGBM is being widely-used in many [winning solutions](https://github.com/microsoft/LightGBM/blob/master/examples/README.md#machine-learning-challenge-winning-solutions) of machine learning competitions.\n\n[Comparison experiments](https://github.com/microsoft/LightGBM/blob/master/docs/Experiments.rst#comparison-experiment) on public datasets show that LightGBM can outperform existing boosting frameworks on both efficiency and accuracy, with significantly lower memory consumption. What''s more, [distributed learning experiments](https://github.com/microsoft/LightGBM/blob/master/docs/Experiments.rst#parallel-experiment) show that LightGBM can achieve a linear speed-up by using multiple machines for training in specific settings.\n\nGet Started and Documentation\n-----------------------------\n\nOur primary documentation is at https://lightgbm.readthedocs.io/ and is generated from this repository. If you are new to LightGBM, follow [the installation instructions](https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html) on that site.\n\nNext you may want to read:\n\n- [**Examples**](https://github.com/microsoft/LightGBM/tree/master/examples) showing command line usage of common tasks.\n- [**Features**](https://github.com/microsoft/LightGBM/blob/master/docs/Features.rst) and algorithms supported by LightGBM.\n- [**Parameters**](https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst) is an exhaustive list of customization you can make.\n- [**Distributed Learning**](https://github.com/microsoft/LightGBM/blob/master/docs/Parallel-Learning-Guide.rst) and [**GPU Learning**](https://github.com/microsoft/LightGBM/blob/master/docs/GPU-Tutorial.rst) can speed up computation.\n- [**FLAML**](https://www.microsoft.com/en-us/research/project/fast-and-lightweight-automl-for-large-scale-data/articles/flaml-a-fast-and-lightweight-automl-library/) provides automated tuning for LightGBM ([code examples](https://microsoft.github.io/FLAML/docs/Examples/AutoML-for-LightGBM/)).\n- [**Optuna Hyperparameter Tuner**](https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258) provides automated tuning for LightGBM hyperparameters ([code examples](https://github.com/optuna/optuna-examples/blob/main/lightgbm/lightgbm_tuner_simple.py)).\n- [**Understanding LightGBM Parameters (and How to Tune Them using Neptune)**](https://neptune.ai/blog/lightgbm-parameters-guide).\n\nDocumentation for contributors:\n\n- [**How we update readthedocs.io**](https://github.com/microsoft/LightGBM/blob/master/docs/README.rst).\n- Check out the [**Development Guide**](https://github.com/microsoft/LightGBM/blob/master/docs/Development-Guide.rst).\n\nNews\n----\n\nPlease refer to changelogs at [GitHub releases](https://github.com/microsoft/LightGBM/releases) page.\n\nExternal (Unofficial) Repositories\n----------------------------------\n\nProjects listed here offer alternative ways to use LightGBM.\nThey are not maintained or officially endorsed by the `LightGBM` development team.\n\nJPMML (Java PMML converter): https://github.com/jpmml/jpmml-lightgbm\n\nNyoka (Python PMML converter): https://github.com/SoftwareAG/nyoka\n\nTreelite (model compiler for efficient deployment): https://github.com/dmlc/treelite\n\nlleaves (LLVM-based model compiler for efficient inference): https://github.com/siboehm/lleaves\n\nHummingbird (model compiler into tensor computations): https://github.com/microsoft/hummingbird\n\nGBNet (use `LightGBM` as a [PyTorch Module](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html)): https://github.com/mthorrell/gbnet\n\ncuML Forest Inference Library (GPU-accelerated inference): https://github.com/rapidsai/cuml\n\ndaal4py (Intel CPU-accelerated inference): https://github.com/intel/scikit-learn-intelex/tree/master/daal4py\n\nm2cgen (model appliers for various languages): https://github.com/BayesWitnesses/m2cgen\n\nleaves (Go model applier): https://github.com/dmitryikh/leaves\n\nONNXMLTools (ONNX converter): https://github.com/onnx/onnxmltools\n\nSHAP (model output explainer): https://github.com/slundberg/shap\n\nShapash (model visualization and interpretation): https://github.com/MAIF/shapash\n\ndtreeviz (decision tree visualization and model interpretation): https://github.com/parrt/dtreeviz\n\nsupertree (interactive visualization of decision trees): https://github.com/mljar/supertree\n\nSynapseML (LightGBM on Spark): https://github.com/microsoft/SynapseML\n\nKubeflow Fairing (LightGBM on Kubernetes): https://github.com/kubeflow/fairing\n\nKubeflow Operator (LightGBM on Kubernetes): https://github.com/kubeflow/xgboost-operator\n\nlightgbm_ray (LightGBM on Ray): https://github.com/ray-project/lightgbm_ray\n\nRay (distributed computing framework): https://github.com/ray-project/ray\n\nMars (LightGBM on Mars): https://github.com/mars-project/mars\n\nML.NET (.NET/C#-package): https://github.com/dotnet/machinelearning\n\nLightGBM.NET (.NET/C#-package): https://github.com/rca22/LightGBM.Net\n\nLightGBM Ruby (Ruby gem): https://github.com/ankane/lightgbm-ruby\n\nLightGBM4j (Java high-level binding): https://github.com/metarank/lightgbm4j\n\nLightGBM4J (JVM interface for LightGBM written in Scala): https://github.com/seek-oss/lightgbm4j\n\nJulia-package: https://github.com/IQVIA-ML/LightGBM.jl\n\nlightgbm3 (Rust binding): https://github.com/Mottl/lightgbm3-rs\n\nMLServer (inference server for LightGBM): https://github.com/SeldonIO/MLServer\n\nMLflow (experiment tracking, model monitoring framework): https://github.com/mlflow/mlflow\n\nFLAML (AutoML library for hyperparameter optimization): https://github.com/microsoft/FLAML\n\nMLJAR AutoML (AutoML on tabular data): https://github.com/mljar/mljar-supervised\n\nOptuna (hyperparameter optimization framework): https://github.com/optuna/optuna\n\nLightGBMLSS (probabilistic modelling with LightGBM): https://github.com/StatMixedML/LightGBMLSS\n\nmlforecast (time series forecasting with LightGBM): https://github.com/Nixtla/mlforecast\n\nskforecast (time series forecasting with LightGBM): https://github.com/JoaquinAmatRodrigo/skforecast\n\n`{bonsai}` (R `{parsnip}`-compliant interface): https://github.com/tidymodels/bonsai\n\n`{mlr3extralearners}` (R `{mlr3}`-compliant interface): https://github.com/mlr-org/mlr3extralearners\n\nlightgbm-transform (feature transformation binding): https://github.com/microsoft/lightgbm-transform\n\n`postgresml` (LightGBM training and prediction in SQL, via a Postgres extension): https://github.com/postgresml/postgresml\n\n`pyodide` (run `lightgbm` Python-package in a web browser): https://github.com/pyodide/pyodide\n\n`vaex-ml` (Python DataFrame library with its own interface to LightGBM): https://github.com/vaexio/vaex\n\nSupport\n-------\n\n- Ask a question [on Stack Overflow with the `lightgbm` tag](https://stackoverflow.com/questions/ask?tags=lightgbm), we monitor this for new questions.\n- Open **bug reports** and **feature requests** on [GitHub issues](https://github.com/microsoft/LightGBM/issues).\n\nHow to Contribute\n-----------------\n\nCheck [CONTRIBUTING](https://github.com/microsoft/LightGBM/blob/master/CONTRIBUTING.md) page.\n\nMicrosoft Open Source Code of Conduct\n-------------------------------------\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\nReference Papers\n----------------\n\nYu Shi, Guolin Ke, Zhuoming Chen, Shuxin Zheng, Tie-Yan Liu. "Quantized Training of Gradient Boosting Decision Trees" ([link](https://proceedings.neurips.cc/paper/2022/hash/77911ed9e6e864ca1a3d165b2c3cb258-Abstract.html)). Advances in Neural Information Processing Systems 35 (NeurIPS 2022), pp. 18822-18833.\n\nGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu. "[LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html)". Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 3149-3157.\n\nQi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, Tie-Yan Liu. "[A Communication-Efficient Parallel Algorithm for Decision Tree](https://proceedings.neurips.cc/paper/2016/hash/10a5ab2db37feedfdeaab192ead4ac0e-Abstract.html)". Advances in Neural Information Processing Systems 29 (NIPS 2016), pp. 1279-1287.\n\nHuan Zhang, Si Si and Cho-Jui Hsieh. "[GPU Acceleration for Large-scale Tree Boosting](https://arxiv.org/abs/1706.08359)". SysML Conference, 2018.\n\nLicense\n-------\n\nThis project is licensed under the terms of the MIT license. See [LICENSE](https://github.com/microsoft/LightGBM/blob/master/LICENSE) for additional details.\n', '{"language":"C++","stars":17914,"forks":3965,"watchers":17914,"open_issues":471,"topics":["data-mining","decision-trees","distributed","gbdt","gbm","gbrt","gradient-boosting","kaggle","lightgbm","machine-learning","microsoft","parallel","python","r"],"default_branch":"master","size_kb":24815,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:optuna:optuna-examples","source_url":"https://github.com/optuna/optuna-examples"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:jpmml:jpmml-lightgbm","source_url":"https://github.com/jpmml/jpmml-lightgbm"},{"type":"has_code","target_id":"github:SoftwareAG:nyoka","source_url":"https://github.com/SoftwareAG/nyoka"},{"type":"has_code","target_id":"github:dmlc:treelite","source_url":"https://github.com/dmlc/treelite"},{"type":"has_code","target_id":"github:siboehm:lleaves","source_url":"https://github.com/siboehm/lleaves"},{"type":"has_code","target_id":"github:microsoft:hummingbird","source_url":"https://github.com/microsoft/hummingbird"},{"type":"has_code","target_id":"github:mthorrell:gbnet","source_url":"https://github.com/mthorrell/gbnet"},{"type":"has_code","target_id":"github:rapidsai:cuml","source_url":"https://github.com/rapidsai/cuml"},{"type":"has_code","target_id":"github:intel:scikit-learn-intelex","source_url":"https://github.com/intel/scikit-learn-intelex"},{"type":"has_code","target_id":"github:BayesWitnesses:m2cgen","source_url":"https://github.com/BayesWitnesses/m2cgen"},{"type":"has_code","target_id":"github:dmitryikh:leaves","source_url":"https://github.com/dmitryikh/leaves"},{"type":"has_code","target_id":"github:onnx:onnxmltools","source_url":"https://github.com/onnx/onnxmltools"},{"type":"has_code","target_id":"github:slundberg:shap","source_url":"https://github.com/slundberg/shap"},{"type":"has_code","target_id":"github:MAIF:shapash","source_url":"https://github.com/MAIF/shapash"},{"type":"has_code","target_id":"github:parrt:dtreeviz","source_url":"https://github.com/parrt/dtreeviz"},{"type":"has_code","target_id":"github:mljar:supertree","source_url":"https://github.com/mljar/supertree"},{"type":"has_code","target_id":"github:microsoft:SynapseML","source_url":"https://github.com/microsoft/SynapseML"},{"type":"has_code","target_id":"github:kubeflow:fairing","source_url":"https://github.com/kubeflow/fairing"},{"type":"has_code","target_id":"github:kubeflow:xgboost-operator","source_url":"https://github.com/kubeflow/xgboost-operator"},{"type":"has_code","target_id":"github:ray-project:lightgbm_ray","source_url":"https://github.com/ray-project/lightgbm_ray"},{"type":"has_code","target_id":"github:ray-project:ray","source_url":"https://github.com/ray-project/ray"},{"type":"has_code","target_id":"github:mars-project:mars","source_url":"https://github.com/mars-project/mars"},{"type":"has_code","target_id":"github:dotnet:machinelearning","source_url":"https://github.com/dotnet/machinelearning"},{"type":"has_code","target_id":"github:rca22:LightGBM.Net","source_url":"https://github.com/rca22/LightGBM.Net"},{"type":"has_code","target_id":"github:ankane:lightgbm-ruby","source_url":"https://github.com/ankane/lightgbm-ruby"},{"type":"has_code","target_id":"github:metarank:lightgbm4j","source_url":"https://github.com/metarank/lightgbm4j"},{"type":"has_code","target_id":"github:seek-oss:lightgbm4j","source_url":"https://github.com/seek-oss/lightgbm4j"},{"type":"has_code","target_id":"github:IQVIA-ML:LightGBM.jl","source_url":"https://github.com/IQVIA-ML/LightGBM.jl"},{"type":"has_code","target_id":"github:Mottl:lightgbm3-rs","source_url":"https://github.com/Mottl/lightgbm3-rs"},{"type":"has_code","target_id":"github:SeldonIO:MLServer","source_url":"https://github.com/SeldonIO/MLServer"},{"type":"has_code","target_id":"github:mlflow:mlflow","source_url":"https://github.com/mlflow/mlflow"},{"type":"has_code","target_id":"github:microsoft:FLAML","source_url":"https://github.com/microsoft/FLAML"},{"type":"has_code","target_id":"github:mljar:mljar-supervised","source_url":"https://github.com/mljar/mljar-supervised"},{"type":"has_code","target_id":"github:optuna:optuna","source_url":"https://github.com/optuna/optuna"},{"type":"has_code","target_id":"github:StatMixedML:LightGBMLSS","source_url":"https://github.com/StatMixedML/LightGBMLSS"},{"type":"has_code","target_id":"github:Nixtla:mlforecast","source_url":"https://github.com/Nixtla/mlforecast"},{"type":"has_code","target_id":"github:JoaquinAmatRodrigo:skforecast","source_url":"https://github.com/JoaquinAmatRodrigo/skforecast"},{"type":"has_code","target_id":"github:tidymodels:bonsai","source_url":"https://github.com/tidymodels/bonsai"},{"type":"has_code","target_id":"github:mlr-org:mlr3extralearners","source_url":"https://github.com/mlr-org/mlr3extralearners"},{"type":"has_code","target_id":"github:microsoft:lightgbm-transform","source_url":"https://github.com/microsoft/lightgbm-transform"},{"type":"has_code","target_id":"github:postgresml:postgresml","source_url":"https://github.com/postgresml/postgresml"},{"type":"has_code","target_id":"github:pyodide:pyodide","source_url":"https://github.com/pyodide/pyodide"},{"type":"has_code","target_id":"github:vaexio:vaex","source_url":"https://github.com/vaexio/vaex"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"}]', NULL, 'MIT', 'approved', 80, 'd94c4aee547622b9de8eac8d47e8a208', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-microsoft-LightGBM from https://github.com/microsoft.png
Image converted to WebP: data/images/github-microsoft-LightGBM.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-marimo-team-marimo', 'github--marimo-team--marimo', 'marimo', 'marimo-team', '<p align="center"> <img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/marimo-logotype-thick.svg"> </p> <p align="center"> <em>A reactive Python notebook that''s reproducible, git-friendly, and deployable as scripts or apps.</em> </p> <p align="center"> <a href="https://docs.marimo.io" target="_blank"><strong>Docs</strong></a> Â· <a href="https://marimo.io/discord?ref=readme" target="_blank"><strong>Discord</strong></a> Â· <a href="https://docs.marimo.io/examples/" t...', '["artificial-intelligence","dag","data-science","data-visualization","dataflow","developer-tools","machine-learning","notebooks","pipeline","python","reactive","sql","web-app","python"]', 'other', 17676, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/marimo-team/marimo","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/marimo-logotype-thick.svg">\n</p>\n\n<p align="center">\n  <em>A reactive Python notebook that''s reproducible, git-friendly, and deployable as scripts or apps.</em>\n</p>\n\n<p align="center">\n  <a href="https://docs.marimo.io" target="_blank"><strong>Docs</strong></a> Â·\n  <a href="https://marimo.io/discord?ref=readme" target="_blank"><strong>Discord</strong></a> Â·\n  <a href="https://docs.marimo.io/examples/" target="_blank"><strong>Examples</strong></a> Â·\n  <a href="https://marimo.io/gallery/" target="_blank"><strong>Gallery</strong></a> Â·\n  <a href="https://www.youtube.com/@marimo-team/" target="_blank"><strong>YouTube</strong></a>\n</p>\n\n<p align="center">\n  <b>English</b>\n  <b> | </b>\n  <a href="https://github.com/marimo-team/marimo/blob/main/README_Traditional_Chinese.md" target="_blank"><b>ç¹é«”ä¸­æ–‡</b></a>\n  <b> | </b>\n  <a href="https://github.com/marimo-team/marimo/blob/main/README_Chinese.md" target="_blank"><b>ç®€ä½“ä¸­æ–‡</b></a>\n  <b> | </b>\n  <a href="https://github.com/marimo-team/marimo/blob/main/README_Japanese.md" target="_blank"><b>æ—¥æœ¬èªž</b></a>\n  <b> | </b>\n  <a href="https://github.com/marimo-team/marimo/blob/main/README_Spanish.md" target="_blank"><b>EspaÃ±ol</b></a>\n</p>\n\n<p align="center">\n  <a href="https://pypi.org/project/marimo/"><img src="https://img.shields.io/pypi/v/marimo?color=%2334D058&label=pypi"/></a>\n  <a href="https://anaconda.org/conda-forge/marimo"><img src="https://img.shields.io/conda/vn/conda-forge/marimo.svg"/></a>\n  <a href="https://marimo.io/discord?ref=readme"><img src="https://shields.io/discord/1059888774789730424" alt="discord"/></a>\n  <img alt="Pepy Total Downloads" src="https://img.shields.io/pepy/dt/marimo?label=pypi%20%7C%20downloads"/>\n  <img alt="Conda Downloads" src="https://img.shields.io/conda/d/conda-forge/marimo"/>\n  <a href="https://github.com/marimo-team/marimo/blob/main/LICENSE"><img src="https://img.shields.io/pypi/l/marimo"/></a>\n</p>\n\n**marimo** is a reactive Python notebook: run a cell or interact with a UI\nelement, and marimo automatically runs dependent cells (or <a href="#expensive-notebooks">marks them as stale</a>), keeping code and outputs\nconsistent. marimo notebooks are stored as pure Python (with first-class SQL support), executable as scripts,\nand deployable as apps.\n\n**Highlights**.\n\n- ðŸš€ **batteries-included:** replaces `jupyter`, `streamlit`, `jupytext`, `ipywidgets`, `papermill`, and more\n- âš¡ï¸ **reactive**: run a cell, and marimo reactively [runs all dependent cells](https://docs.marimo.io/guides/reactivity.html) or <a href="#expensive-notebooks">marks them as stale</a>\n- ðŸ–ï¸ **interactive:** [bind sliders, tables, plots, and more](https://docs.marimo.io/guides/interactivity.html) to Python â€” no callbacks required\n- ðŸ **git-friendly:** stored as `.py` files\n- ðŸ›¢ï¸ **designed for data**: query dataframes, databases, warehouses, or lakehouses [with SQL](https://docs.marimo.io/guides/working_with_data/sql.html), filter and search [dataframes](https://docs.marimo.io/guides/working_with_data/dataframes.html)\n- ðŸ¤– **AI-native**: [generate cells with AI](https://docs.marimo.io/guides/generate_with_ai/) tailored for data work\n- ðŸ”¬ **reproducible:** [no hidden state](https://docs.marimo.io/guides/reactivity.html#no-hidden-state), deterministic execution, [built-in package management](https://docs.marimo.io/guides/package_management/)\n- ðŸƒ **executable:** [execute as a Python script](https://docs.marimo.io/guides/scripts.html), parameterized by CLI args\n- ðŸ›œ **shareable**: [deploy as an interactive web app](https://docs.marimo.io/guides/apps.html) or [slides](https://docs.marimo.io/guides/apps.html#slides-layout), [run in the browser via WASM](https://docs.marimo.io/guides/wasm.html)\n- ðŸ§© **reusable:** [import functions and classes](https://docs.marimo.io/guides/reusing_functions/) from one notebook to another\n- ðŸ§ª **testable:** [run pytest](https://docs.marimo.io/guides/testing/) on notebooks\n- âŒ¨ï¸ **a modern editor**: [GitHub Copilot](https://docs.marimo.io/guides/editor_features/ai_completion.html#github-copilot), [AI assistants](https://docs.marimo.io/guides/editor_features/ai_completion.html), vim keybindings, variable explorer, and [more](https://docs.marimo.io/guides/editor_features/index.html)\n- ðŸ§‘â€ðŸ’» **use your favorite editor**: run in [VS Code or Cursor](https://marketplace.visualstudio.com/items?itemName=marimo-team.vscode-marimo), or edit in neovim, Zed, [or any other text editor](https://docs.marimo.io/guides/editor_features/watching/)\n\n```python\npip install marimo && marimo tutorial intro\n```\n\n_Try marimo at [our online playground](https://marimo.app/l/c7h6pz), which runs entirely in the browser!_\n\n_Jump to the [quickstart](#quickstart) for a primer on our CLI._\n\n## A reactive programming environment\n\nmarimo guarantees your notebook code, outputs, and program state are consistent. This [solves many problems](https://docs.marimo.io/faq.html#faq-problems) associated with traditional notebooks like Jupyter.\n\n**A reactive programming environment.**\nRun a cell and marimo _reacts_ by automatically running the cells that\nreference its variables, eliminating the error-prone task of manually\nre-running cells. Delete a cell and marimo scrubs its variables from program\nmemory, eliminating hidden state.\n\n<img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/reactive.gif" width="700px" />\n\n<a name="expensive-notebooks"></a>\n\n**Compatible with expensive notebooks.** marimo lets you [configure the runtime\nto be\nlazy](https://docs.marimo.io/guides/configuration/runtime_configuration.html),\nmarking affected cells as stale instead of automatically running them. This\ngives you guarantees on program state while preventing accidental execution of\nexpensive cells.\n\n**Synchronized UI elements.** Interact with [UI\nelements](https://docs.marimo.io/guides/interactivity.html) like [sliders](https://docs.marimo.io/api/inputs/slider.html#slider),\n[dropdowns](https://docs.marimo.io/api/inputs/dropdown.html), [dataframe\ntransformers](https://docs.marimo.io/api/inputs/dataframe.html), and [chat\ninterfaces](https://docs.marimo.io/api/inputs/chat.html), and the cells that\nuse them are automatically re-run with their latest values.\n\n<img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-ui.gif" width="700px" />\n\n**Interactive dataframes.** [Page through, search, filter, and\nsort](https://docs.marimo.io/guides/working_with_data/dataframes.html)\nmillions of rows blazingly fast, no code required.\n\n<img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/docs-df.gif" width="700px" />\n\n**Generate cells with data-aware AI.** [Generate code with an AI\nassistant](https://docs.marimo.io/guides/editor_features/ai_completion/) that is highly\nspecialized for working with data, with context about your variables in memory;\n[zero-shot entire notebooks](https://docs.marimo.io/guides/generate_with_ai/text_to_notebook/).\nCustomize the system prompt, bring your own API keys, or use local models.\n\n<img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-generate-with-ai.gif" width="700px" />\n\n**Query data with SQL.** Build [SQL](https://docs.marimo.io/guides/working_with_data/sql.html) queries\nthat depend on Python values and execute them against dataframes, databases, lakehouses,\nCSVs, Google Sheets, or anything else using our built-in SQL engine, which\nreturns the result as a Python dataframe.\n\n<img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-sql-cell.png" width="700px" />\n\nYour notebooks are still pure Python, even if they use SQL.\n\n**Dynamic markdown.** Use markdown parametrized by Python variables to tell\ndynamic stories that depend on Python data.\n\n**Built-in package management.** marimo has built-in support for all major\npackage managers, letting you [install packages on import](https://docs.marimo.io/guides/editor_features/package_management.html). marimo can even\n[serialize package\nrequirements](https://docs.marimo.io/guides/package_management/inlining_dependencies/)\nin notebook files, and auto install them in\nisolated venv sandboxes.\n\n**Deterministic execution order.** Notebooks are executed in a deterministic\norder, based on variable references instead of cells'' positions on the page.\nOrganize your notebooks to best fit the stories you''d like to tell.\n\n**Performant runtime.** marimo runs only those cells that need to be run by\nstatically analyzing your code.\n\n**Batteries-included.** marimo comes with GitHub Copilot, AI assistants, Ruff\ncode formatting, HTML export, fast code completion, a [VS Code\nextension](https://marketplace.visualstudio.com/items?itemName=marimo-team.vscode-marimo),\nan interactive dataframe viewer, and [many more](https://docs.marimo.io/guides/editor_features/index.html)\nquality-of-life features.\n\n## Quickstart\n\n_The [marimo concepts\nplaylist](https://www.youtube.com/watch?v=3N6lInzq5MI&list=PLNJXGo8e1XT9jP7gPbRdm1XwloZVFvLEq)\non our [YouTube channel](https://www.youtube.com/@marimo-team) gives an\noverview of many features._\n\n**Installation.** In a terminal, run\n\n```bash\npip install marimo  # or conda install -c conda-forge marimo\nmarimo tutorial intro\n```\n\nTo install with additional dependencies that unlock SQL cells, AI completion, and more,\nrun\n\n```bash\npip install "marimo[recommended]"\n```\n\n**Create notebooks.**\n\nCreate or edit notebooks with\n\n```bash\nmarimo edit\n```\n\n**Run apps.** Run your notebook as a web app, with Python\ncode hidden and uneditable:\n\n```bash\nmarimo run your_notebook.py\n```\n\n<img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/docs-model-comparison.gif" style="border-radius: 8px" width="450px" />\n\n**Execute as scripts.** Execute a notebook as a script at the\ncommand line:\n\n```bash\npython your_notebook.py\n```\n\n**Automatically convert Jupyter notebooks.** Automatically convert Jupyter\nnotebooks to marimo notebooks with the CLI\n\n```bash\nmarimo convert your_notebook.ipynb > your_notebook.py\n```\n\nor use our [web interface](https://marimo.io/convert).\n\n**Tutorials.**\nList all tutorials:\n\n```bash\nmarimo tutorial --help\n```\n\n**Share cloud-based notebooks.** Use\n[molab](https://molab.marimo.io/notebooks), a cloud-based marimo notebook\nservice similar to Google Colab, to create and share notebook links.\n\n## Questions?\n\nSee the [FAQ](https://docs.marimo.io/faq.html) at our docs.\n\n## Learn more\n\nmarimo is easy to get started with, with lots of room for power users.\nFor example, here''s an embedding visualizer made in marimo\n([video](https://marimo.io/videos/landing/full.mp4)):\n\n<img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/embedding.gif" width="700px" />\n\nCheck out our [docs](https://docs.marimo.io),\n[usage examples](https://docs.marimo.io/examples/), and our [gallery](https://marimo.io/gallery) to learn more.\n\n<table border="0">\n  <tr>\n    <td>\n      <a target="_blank" href="https://docs.marimo.io/getting_started/key_concepts.html">\n        <img src="https://docs.marimo.io/_static/reactive.gif" style="max-height: 150px; width: auto; display: block" />\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.marimo.io/api/inputs/index.html">\n        <img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-ui.gif" style="max-height: 150px; width: auto; display: block" />\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.marimo.io/guides/working_with_data/plotting.html">\n        <img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/docs-intro.gif" style="max-height: 150px; width: auto; display: block" />\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.marimo.io/api/layouts/index.html">\n        <img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/outputs.gif" style="max-height: 150px; width: auto; display: block" />\n      </a>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <a target="_blank" href="https://docs.marimo.io/getting_started/key_concepts.html"> Tutorial </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.marimo.io/api/inputs/index.html"> Inputs </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.marimo.io/guides/working_with_data/plotting.html"> Plots </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.marimo.io/api/layouts/index.html"> Layout </a>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <a target="_blank" href="https://marimo.app/l/c7h6pz">\n        <img src="https://marimo.io/shield.svg"/>\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://marimo.app/l/0ue871">\n        <img src="https://marimo.io/shield.svg"/>\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://marimo.app/l/lxp1jk">\n        <img src="https://marimo.io/shield.svg"/>\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://marimo.app/l/14ovyr">\n        <img src="https://marimo.io/shield.svg"/>\n      </a>\n    </td>\n  </tr>\n</table>\n\n## Contributing\n\nWe appreciate all contributions! You don''t need to be an expert to help out.\nPlease see [CONTRIBUTING.md](https://github.com/marimo-team/marimo/blob/main/CONTRIBUTING.md) for more details on how to get\nstarted.\n\n> Questions? Reach out to us [on Discord](https://marimo.io/discord?ref=readme).\n\n## Community\n\nWe''re building a community. Come hang out with us!\n\n- ðŸŒŸ [Star us on GitHub](https://github.com/marimo-team/marimo)\n- ðŸ’¬ [Chat with us on Discord](https://marimo.io/discord?ref=readme)\n- ðŸ“§ [Subscribe to our Newsletter](https://marimo.io/newsletter)\n- â˜ï¸ [Join our Cloud Waitlist](https://marimo.io/cloud)\n- âœï¸ [Start a GitHub Discussion](https://github.com/marimo-team/marimo/discussions)\n- ðŸ¦‹ [Follow us on Bluesky](https://bsky.app/profile/marimo.io)\n- ðŸ¦ [Follow us on Twitter](https://twitter.com/marimo_io)\n- ðŸŽ¥ [Subscribe on YouTube](https://www.youtube.com/@marimo-team)\n- ðŸ•´ï¸ [Follow us on LinkedIn](https://www.linkedin.com/company/marimo-io)\n\n**A NumFOCUS affiliated project.** marimo is a core part of the broader Python\necosystem and is a member of the NumFOCUS community, which includes projects\nsuch as NumPy, SciPy, and Matplotlib.\n\n<img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/numfocus_affiliated_project.png" height="40px" />\n\n\n## Inspiration âœ¨\n\nmarimo is a **reinvention** of the Python notebook as a reproducible, interactive,\nand shareable Python program, instead of an error-prone JSON scratchpad.\n\nWe believe that the tools we use shape the way we think â€” better tools, for\nbetter minds. With marimo, we hope to provide the Python community with a\nbetter programming environment to do research and communicate it; to experiment\nwith code and share it; to learn computational science and teach it.\n\nOur inspiration comes from many places and projects, especially\n[Pluto.jl](https://github.com/fonsp/Pluto.jl),\n[ObservableHQ](https://observablehq.com/tutorials), and\n[Bret Victor''s essays](http://worrydream.com/). marimo is part of\na greater movement toward reactive dataflow programming. From\n[IPyflow](https://github.com/ipyflow/ipyflow), [streamlit](https://github.com/streamlit/streamlit),\n[TensorFlow](https://github.com/tensorflow/tensorflow),\n[PyTorch](https://github.com/pytorch/pytorch/tree/main),\n[JAX](https://github.com/google/jax), and\n[React](https://github.com/facebook/react), the ideas of functional,\ndeclarative, and reactive programming are transforming a broad range of tools\nfor the better.\n\n<p align="right">\n  <img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/marimo-logotype-horizontal.png" height="200px">\n</p>\n', '{"language":"Python","stars":17676,"forks":822,"watchers":17676,"open_issues":527,"topics":["artificial-intelligence","dag","data-science","data-visualization","dataflow","developer-tools","machine-learning","notebooks","pipeline","python","reactive","sql","web-app"],"default_branch":"main","size_kb":187194,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:marimo-team:marimo","source_url":"https://github.com/marimo-team/marimo"},{"type":"has_code","target_id":"github:marimo-team:marimo","source_url":"https://github.com/marimo-team/marimo"},{"type":"has_code","target_id":"github:marimo-team:marimo","source_url":"https://github.com/marimo-team/marimo"},{"type":"has_code","target_id":"github:marimo-team:marimo","source_url":"https://github.com/marimo-team/marimo"},{"type":"has_code","target_id":"github:marimo-team:marimo","source_url":"https://github.com/marimo-team/marimo"},{"type":"has_code","target_id":"github:marimo-team:marimo","source_url":"https://github.com/marimo-team/marimo"},{"type":"has_code","target_id":"github:marimo-team:marimo","source_url":"https://github.com/marimo-team/marimo"},{"type":"has_code","target_id":"github:marimo-team:marimo","source_url":"https://github.com/marimo-team/marimo"},{"type":"has_code","target_id":"github:fonsp:Pluto.jl","source_url":"https://github.com/fonsp/Pluto.jl"},{"type":"has_code","target_id":"github:ipyflow:ipyflow","source_url":"https://github.com/ipyflow/ipyflow"},{"type":"has_code","target_id":"github:streamlit:streamlit","source_url":"https://github.com/streamlit/streamlit"},{"type":"has_code","target_id":"github:tensorflow:tensorflow","source_url":"https://github.com/tensorflow/tensorflow"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:google:jax","source_url":"https://github.com/google/jax"},{"type":"has_code","target_id":"github:facebook:react","source_url":"https://github.com/facebook/react"}]', NULL, 'Apache-2.0', 'approved', 80, 'fbe56949c1c26aa89c02a9efedb030d1', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-marimo-team-marimo from https://github.com/marimo-team.png
Image converted to WebP: data/images/github-marimo-team-marimo.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-AUTOMATIC1111-stable-diffusion-webui', 'github--automatic1111--stable-diffusion-webui', 'stable-diffusion-webui', 'AUTOMATIC1111', 'A web interface for Stable Diffusion, implemented using Gradio library. Detailed feature showcase with images: - Original txt2img and img2img modes - One click install and run script (but you still must install python and git) - Outpainting - Inpainting - Color Sketch - Prompt Matrix - Stable Diffusion Upscale - Attention, specify parts of text that the model should pay more attention to - a man in a - will pay more attention to tuxedo - a man in a - alternative syntax - select text and press...', '["ai","ai-art","deep-learning","diffusion","gradio","image-generation","image2image","img2img","pytorch","stable-diffusion","text2image","torch","txt2img","unstable","upscaling","web","python"]', 'other', 158820, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Stable Diffusion web UI\nA web interface for Stable Diffusion, implemented using Gradio library.\n\n![](screenshot.png)\n\n## Features\n[Detailed feature showcase with images](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features):\n- Original txt2img and img2img modes\n- One click install and run script (but you still must install python and git)\n- Outpainting\n- Inpainting\n- Color Sketch\n- Prompt Matrix\n- Stable Diffusion Upscale\n- Attention, specify parts of text that the model should pay more attention to\n    - a man in a `((tuxedo))` - will pay more attention to tuxedo\n    - a man in a `(tuxedo:1.21)` - alternative syntax\n    - select text and press `Ctrl+Up` or `Ctrl+Down` (or `Command+Up` or `Command+Down` if you''re on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user)\n- Loopback, run img2img processing multiple times\n- X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters\n- Textual Inversion\n    - have as many embeddings as you want and use any names you like for them\n    - use multiple embeddings with different numbers of vectors per token\n    - works with half precision floating point numbers\n    - train embeddings on 8GB (also reports of 6GB working)\n- Extras tab with:\n    - GFPGAN, neural network that fixes faces\n    - CodeFormer, face restoration tool as an alternative to GFPGAN\n    - RealESRGAN, neural network upscaler\n    - ESRGAN, neural network upscaler with a lot of third party models\n    - SwinIR and Swin2SR ([see here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/2092)), neural network upscalers\n    - LDSR, Latent diffusion super resolution upscaling\n- Resizing aspect ratio options\n- Sampling method selection\n    - Adjust sampler eta values (noise multiplier)\n    - More advanced noise setting options\n- Interrupt processing at any time\n- 4GB video card support (also reports of 2GB working)\n- Correct seeds for batches\n- Live prompt token length validation\n- Generation parameters\n     - parameters you used to generate images are saved with that image\n     - in PNG chunks for PNG, in EXIF for JPEG\n     - can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI\n     - can be disabled in settings\n     - drag and drop an image/text-parameters to promptbox\n- Read Generation Parameters Button, loads parameters in promptbox to UI\n- Settings page\n- Running arbitrary python code from UI (must run with `--allow-code` to enable)\n- Mouseover hints for most UI elements\n- Possible to change defaults/mix/max/step values for UI elements via text config\n- Tiling support, a checkbox to create images that can be tiled like textures\n- Progress bar and live image generation preview\n    - Can use a separate neural network to produce previews with almost none VRAM or compute requirement\n- Negative prompt, an extra text field that allows you to list what you don''t want to see in generated image\n- Styles, a way to save part of prompt and easily apply them via dropdown later\n- Variations, a way to generate same image but with tiny differences\n- Seed resizing, a way to generate same image but at slightly different resolution\n- CLIP interrogator, a button that tries to guess prompt from an image\n- Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway\n- Batch Processing, process a group of files using img2img\n- Img2img Alternative, reverse Euler method of cross attention control\n- Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions\n- Reloading checkpoints on the fly\n- Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one\n- [Custom scripts](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts) with many extensions from community\n- [Composable-Diffusion](https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/), a way to use multiple prompts at once\n     - separate prompts using uppercase `AND`\n     - also supports weights for prompts: `a cat :1.2 AND a dog AND a penguin :2.2`\n- No token limit for prompts (original stable diffusion lets you use up to 75 tokens)\n- DeepDanbooru integration, creates danbooru style tags for anime prompts\n- [xformers](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers), major speed increase for select cards: (add `--xformers` to commandline args)\n- via extension: [History tab](https://github.com/yfszzx/stable-diffusion-webui-images-browser): view, direct and delete images conveniently within the UI\n- Generate forever option\n- Training tab\n     - hypernetworks and embeddings options\n     - Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime)\n- Clip skip\n- Hypernetworks\n- Loras (same as Hypernetworks but more pretty)\n- A separate UI where you can choose, with preview, which embeddings, hypernetworks or Loras to add to your prompt\n- Can select to load a different VAE from settings screen\n- Estimated completion time in progress bar\n- API\n- Support for dedicated [inpainting model](https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion) by RunwayML\n- via extension: [Aesthetic Gradients](https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients), a way to generate images with a specific aesthetic by using clip images embeds (implementation of [https://github.com/vicgalle/stable-diffusion-aesthetic-gradients](https://github.com/vicgalle/stable-diffusion-aesthetic-gradients))\n- [Stable Diffusion 2.0](https://github.com/Stability-AI/stablediffusion) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20) for instructions\n- [Alt-Diffusion](https://arxiv.org/abs/2211.06679) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#alt-diffusion) for instructions\n- Now without any bad letters!\n- Load checkpoints in safetensors format\n- Eased resolution restriction: generated image''s dimensions must be a multiple of 8 rather than 64\n- Now with a license!\n- Reorder elements in the UI from settings screen\n- [Segmind Stable Diffusion](https://huggingface.co/segmind/SSD-1B) support\n\n## Installation and Running\nMake sure the required [dependencies](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies) are met and follow the instructions available for:\n- [NVidia](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs) (recommended)\n- [AMD](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs) GPUs.\n- [Intel CPUs, Intel GPUs (both integrated and discrete)](https://github.com/openvinotoolkit/stable-diffusion-webui/wiki/Installation-on-Intel-Silicon) (external wiki page)\n- [Ascend NPUs](https://github.com/wangshuai09/stable-diffusion-webui/wiki/Install-and-run-on-Ascend-NPUs) (external wiki page)\n\nAlternatively, use online services (like Google Colab):\n\n- [List of Online Services](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services)\n\n### Installation on Windows 10/11 with NVidia-GPUs using release package\n1. Download `sd.webui.zip` from [v1.0.0-pre](https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/tag/v1.0.0-pre) and extract its contents.\n2. Run `update.bat`.\n3. Run `run.bat`.\n> For more details see [Install-and-Run-on-NVidia-GPUs](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs)\n\n### Automatic Installation on Windows\n1. Install [Python 3.10.6](https://www.python.org/downloads/release/python-3106/) (Newer version of Python does not support torch), checking "Add Python to PATH".\n2. Install [git](https://git-scm.com/download/win).\n3. Download the stable-diffusion-webui repository, for example by running `git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`.\n4. Run `webui-user.bat` from Windows Explorer as normal, non-administrator, user.\n\n### Automatic Installation on Linux\n1. Install the dependencies:\n```bash\n# Debian-based:\nsudo apt install wget git python3 python3-venv libgl1 libglib2.0-0\n# Red Hat-based:\nsudo dnf install wget git python3 gperftools-libs libglvnd-glx\n# openSUSE-based:\nsudo zypper install wget git python3 libtcmalloc4 libglvnd\n# Arch-based:\nsudo pacman -S wget git python3\n```\nIf your system is very new, you need to install python3.11 or python3.10:\n```bash\n# Ubuntu 24.04\nsudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt update\nsudo apt install python3.11\n\n# Manjaro/Arch\nsudo pacman -S yay\nyay -S python311 # do not confuse with python3.11 package\n\n# Only for 3.11\n# Then set up env variable in launch script\nexport python_cmd="python3.11"\n# or in webui-user.sh\npython_cmd="python3.11"\n```\n2. Navigate to the directory you would like the webui to be installed and execute the following command:\n```bash\nwget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh\n```\nOr just clone the repo wherever you want:\n```bash\ngit clone https://github.com/AUTOMATIC1111/stable-diffusion-webui\n```\n\n3. Run `webui.sh`.\n4. Check `webui-user.sh` for options.\n### Installation on Apple Silicon\n\nFind the instructions [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon).\n\n## Contributing\nHere''s how to add code to this repo: [Contributing](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)\n\n## Documentation\n\nThe documentation was moved from this README over to the project''s [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki).\n\nFor the purposes of getting Google and other search engines to crawl the wiki, here''s a link to the (not for humans) [crawlable wiki](https://github-wiki-see.page/m/AUTOMATIC1111/stable-diffusion-webui/wiki).\n\n## Credits\nLicenses for borrowed code can be found in `Settings -> Licenses` screen, and also in `html/licenses.html` file.\n\n- Stable Diffusion - https://github.com/Stability-AI/stablediffusion, https://github.com/CompVis/taming-transformers, https://github.com/mcmonkey4eva/sd3-ref\n- k-diffusion - https://github.com/crowsonkb/k-diffusion.git\n- Spandrel - https://github.com/chaiNNer-org/spandrel implementing\n  - GFPGAN - https://github.com/TencentARC/GFPGAN.git\n  - CodeFormer - https://github.com/sczhou/CodeFormer\n  - ESRGAN - https://github.com/xinntao/ESRGAN\n  - SwinIR - https://github.com/JingyunLiang/SwinIR\n  - Swin2SR - https://github.com/mv-lab/swin2sr\n- LDSR - https://github.com/Hafiidz/latent-diffusion\n- MiDaS - https://github.com/isl-org/MiDaS\n- Ideas for optimizations - https://github.com/basujindal/stable-diffusion\n- Cross Attention layer optimization - Doggettx - https://github.com/Doggettx/stable-diffusion, original idea for prompt editing.\n- Cross Attention layer optimization - InvokeAI, lstein - https://github.com/invoke-ai/InvokeAI (originally http://github.com/lstein/stable-diffusion)\n- Sub-quadratic Cross Attention layer optimization - Alex Birch (https://github.com/Birch-san/diffusers/pull/1), Amin Rezaei (https://github.com/AminRezaei0x443/memory-efficient-attention)\n- Textual Inversion - Rinon Gal - https://github.com/rinongal/textual_inversion (we''re not using his code, but we are using his ideas).\n- Idea for SD upscale - https://github.com/jquesnelle/txt2imghd\n- Noise generation for outpainting mk2 - https://github.com/parlance-zz/g-diffuser-bot\n- CLIP interrogator idea and borrowing some code - https://github.com/pharmapsychotic/clip-interrogator\n- Idea for Composable Diffusion - https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch\n- xformers - https://github.com/facebookresearch/xformers\n- DeepDanbooru - interrogator for anime diffusers https://github.com/KichangKim/DeepDanbooru\n- Sampling in float32 precision from a float16 UNet - marunine for the idea, Birch-san for the example Diffusers implementation (https://github.com/Birch-san/diffusers-play/tree/92feee6)\n- Instruct pix2pix - Tim Brooks (star), Aleksander Holynski (star), Alexei A. Efros (no star) - https://github.com/timothybrooks/instruct-pix2pix\n- Security advice - RyotaK\n- UniPC sampler - Wenliang Zhao - https://github.com/wl-zhao/UniPC\n- TAESD - Ollin Boer Bohan - https://github.com/madebyollin/taesd\n- LyCORIS - KohakuBlueleaf\n- Restart sampling - lambertae - https://github.com/Newbeeer/diffusion_restart_sampling\n- Hypertile - tfernd - https://github.com/tfernd/HyperTile\n- Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user.\n- (You)\n', '{"language":"Python","stars":158820,"forks":29490,"watchers":158820,"open_issues":2442,"topics":["ai","ai-art","deep-learning","diffusion","gradio","image-generation","image2image","img2img","pytorch","stable-diffusion","text2image","torch","txt2img","unstable","upscaling","web"],"default_branch":"master","size_kb":36442,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:yfszzx:stable-diffusion-webui-images-browser","source_url":"https://github.com/yfszzx/stable-diffusion-webui-images-browser"},{"type":"has_code","target_id":"github:runwayml:stable-diffusion","source_url":"https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui-aesthetic-gradients","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients"},{"type":"has_code","target_id":"github:vicgalle:stable-diffusion-aesthetic-gradients](https:","source_url":"https://github.com/vicgalle/stable-diffusion-aesthetic-gradients](https:"},{"type":"has_code","target_id":"github:Stability-AI:stablediffusion","source_url":"https://github.com/Stability-AI/stablediffusion"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:openvinotoolkit:stable-diffusion-webui","source_url":"https://github.com/openvinotoolkit/stable-diffusion-webui"},{"type":"has_code","target_id":"github:wangshuai09:stable-diffusion-webui","source_url":"https://github.com/wangshuai09/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui.git`.","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`."},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:Stability-AI:stablediffusion,","source_url":"https://github.com/Stability-AI/stablediffusion,"},{"type":"has_code","target_id":"github:CompVis:taming-transformers,","source_url":"https://github.com/CompVis/taming-transformers,"},{"type":"has_code","target_id":"github:mcmonkey4eva:sd3-ref","source_url":"https://github.com/mcmonkey4eva/sd3-ref"},{"type":"has_code","target_id":"github:crowsonkb:k-diffusion.git","source_url":"https://github.com/crowsonkb/k-diffusion.git"},{"type":"has_code","target_id":"github:chaiNNer-org:spandrel","source_url":"https://github.com/chaiNNer-org/spandrel"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN.git","source_url":"https://github.com/TencentARC/GFPGAN.git"},{"type":"has_code","target_id":"github:sczhou:CodeFormer","source_url":"https://github.com/sczhou/CodeFormer"},{"type":"has_code","target_id":"github:xinntao:ESRGAN","source_url":"https://github.com/xinntao/ESRGAN"},{"type":"has_code","target_id":"github:JingyunLiang:SwinIR","source_url":"https://github.com/JingyunLiang/SwinIR"},{"type":"has_code","target_id":"github:mv-lab:swin2sr","source_url":"https://github.com/mv-lab/swin2sr"},{"type":"has_code","target_id":"github:Hafiidz:latent-diffusion","source_url":"https://github.com/Hafiidz/latent-diffusion"},{"type":"has_code","target_id":"github:isl-org:MiDaS","source_url":"https://github.com/isl-org/MiDaS"},{"type":"has_code","target_id":"github:basujindal:stable-diffusion","source_url":"https://github.com/basujindal/stable-diffusion"},{"type":"has_code","target_id":"github:Doggettx:stable-diffusion,","source_url":"https://github.com/Doggettx/stable-diffusion,"},{"type":"has_code","target_id":"github:invoke-ai:InvokeAI","source_url":"https://github.com/invoke-ai/InvokeAI"},{"type":"has_code","target_id":"github:lstein:stable-diffusion","source_url":"http://github.com/lstein/stable-diffusion"},{"type":"has_code","target_id":"github:Birch-san:diffusers","source_url":"https://github.com/Birch-san/diffusers"},{"type":"has_code","target_id":"github:AminRezaei0x443:memory-efficient-attention","source_url":"https://github.com/AminRezaei0x443/memory-efficient-attention"},{"type":"has_code","target_id":"github:rinongal:textual_inversion","source_url":"https://github.com/rinongal/textual_inversion"},{"type":"has_code","target_id":"github:jquesnelle:txt2imghd","source_url":"https://github.com/jquesnelle/txt2imghd"},{"type":"has_code","target_id":"github:parlance-zz:g-diffuser-bot","source_url":"https://github.com/parlance-zz/g-diffuser-bot"},{"type":"has_code","target_id":"github:pharmapsychotic:clip-interrogator","source_url":"https://github.com/pharmapsychotic/clip-interrogator"},{"type":"has_code","target_id":"github:energy-based-model:Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch","source_url":"https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch"},{"type":"has_code","target_id":"github:facebookresearch:xformers","source_url":"https://github.com/facebookresearch/xformers"},{"type":"has_code","target_id":"github:KichangKim:DeepDanbooru","source_url":"https://github.com/KichangKim/DeepDanbooru"},{"type":"has_code","target_id":"github:Birch-san:diffusers-play","source_url":"https://github.com/Birch-san/diffusers-play"},{"type":"has_code","target_id":"github:timothybrooks:instruct-pix2pix","source_url":"https://github.com/timothybrooks/instruct-pix2pix"},{"type":"has_code","target_id":"github:wl-zhao:UniPC","source_url":"https://github.com/wl-zhao/UniPC"},{"type":"has_code","target_id":"github:madebyollin:taesd","source_url":"https://github.com/madebyollin/taesd"},{"type":"has_code","target_id":"github:Newbeeer:diffusion_restart_sampling","source_url":"https://github.com/Newbeeer/diffusion_restart_sampling"},{"type":"has_code","target_id":"github:tfernd:HyperTile","source_url":"https://github.com/tfernd/HyperTile"}]', NULL, 'AGPL-3.0', 'approved', 80, '14ea93d459ddb16be471bacb57307e6c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-AUTOMATIC1111-stable-diffusion-webui from https://github.com/AUTOMATIC1111.png
Image converted to WebP: data/images/github-AUTOMATIC1111-stable-diffusion-webui.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-opencv-opencv', 'github--opencv--opencv', 'opencv', 'opencv', '* Homepage: <https://opencv.org> * Courses: <https://opencv.org/courses> * Docs: <https://docs.opencv.org/4.x/> * Q&A forum: <https://forum.opencv.org> * previous forum (read only): <http://answers.opencv.org> * Issue tracking: <https://github.com/opencv/opencv/issues> * Additional OpenCV functionality: <https://github.com/opencv/opencv_contrib> * Donate to OpenCV: <https://opencv.org/support/> Please read the contribution guidelines before starting work on a pull request. * One pull request ...', '["c-plus-plus","computer-vision","deep-learning","image-processing","opencv","c++"]', 'other', 85191, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/opencv/opencv","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '## OpenCV: Open Source Computer Vision Library\n\n\n### Resources\n\n* Homepage: <https://opencv.org>\n  * Courses: <https://opencv.org/courses>\n* Docs: <https://docs.opencv.org/4.x/>\n* Q&A forum: <https://forum.opencv.org>\n  * previous forum (read only): <http://answers.opencv.org>\n* Issue tracking: <https://github.com/opencv/opencv/issues>\n* Additional OpenCV functionality: <https://github.com/opencv/opencv_contrib>\n* Donate to OpenCV: <https://opencv.org/support/>\n\n\n### Contributing\n\nPlease read the [contribution guidelines](https://github.com/opencv/opencv/wiki/How_to_contribute) before starting work on a pull request.\n\n#### Summary of the guidelines:\n\n* One pull request per issue;\n* Choose the right base branch;\n* Include tests and documentation;\n* Clean up "oops" commits before submitting;\n* Follow the [coding style guide](https://github.com/opencv/opencv/wiki/Coding_Style_Guide).\n\n### Additional Resources\n\n* [Submit your OpenCV-based project](https://form.jotform.com/233105358823151) for inclusion in Community Friday on opencv.org\n* [Subscribe to the OpenCV YouTube Channel](http://youtube.com/@opencvofficial) featuring OpenCV Live, an hour-long streaming show\n* [Follow OpenCV on LinkedIn](http://linkedin.com/company/opencv/) for daily posts showing the state-of-the-art in computer vision & AI\n* [Apply to be an OpenCV Volunteer](https://form.jotform.com/232745316792159) to help organize events and online campaigns as well as amplify them\n* [Follow OpenCV on Mastodon](http://mastodon.social/@opencv) in the Fediverse\n* [Follow OpenCV on Twitter](https://twitter.com/opencvlive)\n* [OpenCV.ai](https://opencv.ai): Computer Vision and AI development services from the OpenCV team.\n', '{"language":"C++","stars":85191,"forks":56386,"watchers":85191,"open_issues":2709,"topics":["c-plus-plus","computer-vision","deep-learning","image-processing","opencv"],"default_branch":"4.x","size_kb":551435,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:opencv:opencv","source_url":"https://github.com/opencv/opencv"},{"type":"has_code","target_id":"github:opencv:opencv_contrib>","source_url":"https://github.com/opencv/opencv_contrib>"},{"type":"has_code","target_id":"github:opencv:opencv","source_url":"https://github.com/opencv/opencv"},{"type":"has_code","target_id":"github:opencv:opencv","source_url":"https://github.com/opencv/opencv"}]', NULL, 'Apache-2.0', 'approved', 50, '84038d88524e6cb7775a0c4d84743d12', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-opencv-opencv from https://github.com/opencv.png
Image converted to WebP: data/images/github-opencv-opencv.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-infiniflow-ragflow', 'github--infiniflow--ragflow', 'ragflow', 'infiniflow', '<div align="center"> <a href="https://demo.ragflow.io/"> <img src="web/src/assets/logo-with-text.svg" width="520" alt="ragflow logo"> </a> </div> <p align="center"> <a href="./README.md"><img alt="README in English" src="https://img.shields.io/badge/English-DBEDFA"></a> <a href="./README_zh.md"><img alt="ç®€ä½“ä¸­æ–‡ç‰ˆè‡ªè¿°æ–‡ä»¶" src="https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-DFE0E5"></a> <a href="./README_tzh.md"><img alt="ç¹é«”ç‰ˆä¸­æ–‡è‡ªè¿°æ–‡ä»¶" src="https://img.shields.io/badge/ç¹é«”ä¸­æ–‡-DFE0E5"></a> <a href="./README_ja.md"><img...', '["agent","agentic","agentic-ai","agentic-workflow","ai","ai-search","deep-learning","deep-research","deepseek","deepseek-r1","document-parser","document-understanding","graphrag","llm","mcp","multi-agent","ollama","openai","rag","retrieval-augmented-generation","python"]', 'other', 68943, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/infiniflow/ragflow","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n<a href="https://demo.ragflow.io/">\n<img src="web/src/assets/logo-with-text.svg" width="520" alt="ragflow logo">\n</a>\n</div>\n\n<p align="center">\n  <a href="./README.md"><img alt="README in English" src="https://img.shields.io/badge/English-DBEDFA"></a>\n  <a href="./README_zh.md"><img alt="ç®€ä½“ä¸­æ–‡ç‰ˆè‡ªè¿°æ–‡ä»¶" src="https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-DFE0E5"></a>\n  <a href="./README_tzh.md"><img alt="ç¹é«”ç‰ˆä¸­æ–‡è‡ªè¿°æ–‡ä»¶" src="https://img.shields.io/badge/ç¹é«”ä¸­æ–‡-DFE0E5"></a>\n  <a href="./README_ja.md"><img alt="æ—¥æœ¬èªžã®README" src="https://img.shields.io/badge/æ—¥æœ¬èªž-DFE0E5"></a>\n  <a href="./README_ko.md"><img alt="í•œêµ­ì–´" src="https://img.shields.io/badge/í•œêµ­ì–´-DFE0E5"></a>\n  <a href="./README_id.md"><img alt="Bahasa Indonesia" src="https://img.shields.io/badge/Bahasa Indonesia-DFE0E5"></a>\n  <a href="./README_pt_br.md"><img alt="PortuguÃªs(Brasil)" src="https://img.shields.io/badge/PortuguÃªs(Brasil)-DFE0E5"></a>\n</p>\n\n<p align="center">\n    <a href="https://x.com/intent/follow?screen_name=infiniflowai" target="_blank">\n        <img src="https://img.shields.io/twitter/follow/infiniflow?logo=X&color=%20%23f5f5f5" alt="follow on X(Twitter)">\n    </a>\n    <a href="https://demo.ragflow.io" target="_blank">\n        <img alt="Static Badge" src="https://img.shields.io/badge/Online-Demo-4e6b99">\n    </a>\n    <a href="https://hub.docker.com/r/infiniflow/ragflow" target="_blank">\n        <img src="https://img.shields.io/docker/pulls/infiniflow/ragflow?label=Docker%20Pulls&color=0db7ed&logo=docker&logoColor=white&style=flat-square" alt="docker pull infiniflow/ragflow:v0.22.1">\n    </a>\n    <a href="https://github.com/infiniflow/ragflow/releases/latest">\n        <img src="https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&label=Latest%20Release" alt="Latest Release">\n    </a>\n    <a href="https://github.com/infiniflow/ragflow/blob/main/LICENSE">\n        <img height="21" src="https://img.shields.io/badge/License-Apache--2.0-ffffff?labelColor=d4eaf7&color=2e6cc4" alt="license">\n    </a>\n    <a href="https://deepwiki.com/infiniflow/ragflow">\n        <img alt="Ask DeepWiki" src="https://deepwiki.com/badge.svg">\n    </a>\n</p>\n\n<h4 align="center">\n  <a href="https://ragflow.io/docs/dev/">Document</a> |\n  <a href="https://github.com/infiniflow/ragflow/issues/4214">Roadmap</a> |\n  <a href="https://twitter.com/infiniflowai">Twitter</a> |\n  <a href="https://discord.gg/NjYzJD3GM3">Discord</a> |\n  <a href="https://demo.ragflow.io">Demo</a>\n</h4>\n\n<div align="center" style="margin-top:20px;margin-bottom:20px;">\n<img src="https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/ragflow-octoverse.png" width="1200"/>\n</div>\n\n<div align="center">\n<a href="https://trendshift.io/repositories/9064" target="_blank"><img src="https://trendshift.io/api/badge/repositories/9064" alt="infiniflow%2Fragflow | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n</div>\n\n<details open>\n<summary><b>ðŸ“• Table of Contents</b></summary>\n\n- ðŸ’¡ [What is RAGFlow?](#-what-is-ragflow)\n- ðŸŽ® [Demo](#-demo)\n- ðŸ“Œ [Latest Updates](#-latest-updates)\n- ðŸŒŸ [Key Features](#-key-features)\n- ðŸ”Ž [System Architecture](#-system-architecture)\n- ðŸŽ¬ [Get Started](#-get-started)\n- ðŸ”§ [Configurations](#-configurations)\n- ðŸ”§ [Build a Docker image](#-build-a-docker-image)\n- ðŸ”¨ [Launch service from source for development](#-launch-service-from-source-for-development)\n- ðŸ“š [Documentation](#-documentation)\n- ðŸ“œ [Roadmap](#-roadmap)\n- ðŸ„ [Community](#-community)\n- ðŸ™Œ [Contributing](#-contributing)\n\n</details>\n\n## ðŸ’¡ What is RAGFlow?\n\n[RAGFlow](https://ragflow.io/) is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs. It offers a streamlined RAG workflow adaptable to enterprises of any scale. Powered by a converged context engine and pre-built agent templates, RAGFlow enables developers to transform complex data into high-fidelity, production-ready AI systems with exceptional efficiency and precision.\n\n## ðŸŽ® Demo\n\nTry our demo at [https://demo.ragflow.io](https://demo.ragflow.io).\n\n<div align="center" style="margin-top:20px;margin-bottom:20px;">\n<img src="https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/chunking.gif" width="1200"/>\n<img src="https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/agentic-dark.gif" width="1200"/>\n</div>\n\n## ðŸ”¥ Latest Updates\n\n- 2025-11-19 Supports Gemini 3 Pro.\n- 2025-11-12 Supports data synchronization from Confluence, S3, Notion, Discord, Google Drive.\n- 2025-10-23 Supports MinerU & Docling as document parsing methods.\n- 2025-10-15 Supports orchestrable ingestion pipeline.\n- 2025-08-08 Supports OpenAI''s latest GPT-5 series models.\n- 2025-08-01 Supports agentic workflow and MCP.\n- 2025-05-23 Adds a Python/JavaScript code executor component to Agent.\n- 2025-05-05 Supports cross-language query.\n- 2025-03-19 Supports using a multi-modal model to make sense of images within PDF or DOCX files.\n\n## ðŸŽ‰ Stay Tuned\n\nâ­ï¸ Star our repository to stay up-to-date with exciting new features and improvements! Get instant notifications for new\nreleases! ðŸŒŸ\n\n<div align="center" style="margin-top:20px;margin-bottom:20px;">\n<img src="https://github.com/user-attachments/assets/18c9707e-b8aa-4caf-a154-037089c105ba" width="1200"/>\n</div>\n\n## ðŸŒŸ Key Features\n\n### ðŸ­ **"Quality in, quality out"**\n\n- [Deep document understanding](./deepdoc/README.md)-based knowledge extraction from unstructured data with complicated\n  formats.\n- Finds "needle in a data haystack" of literally unlimited tokens.\n\n### ðŸ± **Template-based chunking**\n\n- Intelligent and explainable.\n- Plenty of template options to choose from.\n\n### ðŸŒ± **Grounded citations with reduced hallucinations**\n\n- Visualization of text chunking to allow human intervention.\n- Quick view of the key references and traceable citations to support grounded answers.\n\n### ðŸ” **Compatibility with heterogeneous data sources**\n\n- Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.\n\n### ðŸ›€ **Automated and effortless RAG workflow**\n\n- Streamlined RAG orchestration catered to both personal and large businesses.\n- Configurable LLMs as well as embedding models.\n- Multiple recall paired with fused re-ranking.\n- Intuitive APIs for seamless integration with business.\n\n## ðŸ”Ž System Architecture\n\n<div align="center" style="margin-top:20px;margin-bottom:20px;">\n<img src="https://github.com/user-attachments/assets/31b0dd6f-ca4f-445a-9457-70cb44a381b2" width="1000"/>\n</div>\n\n## ðŸŽ¬ Get Started\n\n### ðŸ“ Prerequisites\n\n- CPU >= 4 cores\n- RAM >= 16 GB\n- Disk >= 50 GB\n- Docker >= 24.0.0 & Docker Compose >= v2.26.1\n- [gVisor](https://gvisor.dev/docs/user_guide/install/): Required only if you intend to use the code executor (sandbox) feature of RAGFlow.\n\n> [!TIP]\n> If you have not installed Docker on your local machine (Windows, Mac, or Linux), see [Install Docker Engine](https://docs.docker.com/engine/install/).\n\n### ðŸš€ Start up the server\n\n1. Ensure `vm.max_map_count` >= 262144:\n\n   > To check the value of `vm.max_map_count`:\n   >\n   > ```bash\n   > $ sysctl vm.max_map_count\n   > ```\n   >\n   > Reset `vm.max_map_count` to a value at least 262144 if it is not.\n   >\n   > ```bash\n   > # In this case, we set it to 262144:\n   > $ sudo sysctl -w vm.max_map_count=262144\n   > ```\n   >\n   > This change will be reset after a system reboot. To ensure your change remains permanent, add or update the\n   > `vm.max_map_count` value in **/etc/sysctl.conf** accordingly:\n   >\n   > ```bash\n   > vm.max_map_count=262144\n   > ```\n   >\n2. Clone the repo:\n\n   ```bash\n   $ git clone https://github.com/infiniflow/ragflow.git\n   ```\n3. Start up the server using the pre-built Docker images:\n\n> [!CAUTION]\n> All Docker images are built for x86 platforms. We don''t currently offer Docker images for ARM64.\n> If you are on an ARM64 platform, follow [this guide](https://ragflow.io/docs/dev/build_docker_image) to build a Docker image compatible with your system.\n\n> The command below downloads the `v0.22.1` edition of the RAGFlow Docker image. See the following table for descriptions of different RAGFlow editions. To download a RAGFlow edition different from `v0.22.1`, update the `RAGFLOW_IMAGE` variable accordingly in **docker/.env** before using `docker compose` to start the server.\n\n```bash\n   $ cd ragflow/docker\n  \n   # git checkout v0.22.1\n   # Optional: use a stable tag (see releases: https://github.com/infiniflow/ragflow/releases)\n   # This step ensures the **entrypoint.sh** file in the code matches the Docker image version.\n   \n   # Use CPU for DeepDoc tasks:\n   $ docker compose -f docker-compose.yml up -d\n\n   # To use GPU to accelerate DeepDoc tasks:\n   # sed -i ''1i DEVICE=gpu'' .env\n   # docker compose -f docker-compose.yml up -d\n```\n\n> Note: Prior to `v0.22.0`, we provided both images with embedding models and slim images without embedding models. Details as follows:\n\n| RAGFlow image tag | Image size (GB) | Has embedding models? | Stable?                  |\n| ----------------- | --------------- | --------------------- | ------------------------ |\n| v0.21.1           | &approx;9       | âœ”ï¸                    | Stable release           |\n| v0.21.1-slim      | &approx;2       | âŒ                    | Stable release           |\n\n> Starting with `v0.22.0`, we ship only the slim edition and no longer append the **-slim** suffix to the image tag.\n\n4. Check the server status after having the server up and running:\n\n   ```bash\n   $ docker logs -f docker-ragflow-cpu-1\n   ```\n\n   _The following output confirms a successful launch of the system:_\n\n   ```bash\n\n         ____   ___    ______ ______ __\n        / __ \ /   |  / ____// ____// /____  _      __\n       / /_/ // /| | / / __ / /_   / // __ \| | /| / /\n      / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ /\n     /_/ |_|/_/  |_|\____//_/    /_/ \____/ |__/|__/\n\n    * Running on all addresses (0.0.0.0)\n   ```\n\n   > If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a `network anormal`\n   > error because, at that moment, your RAGFlow may not be fully initialized.\n   >\n5. In your web browser, enter the IP address of your server and log in to RAGFlow.\n\n   > With the default settings, you only need to enter `http://IP_OF_YOUR_MACHINE` (**sans** port number) as the default\n   > HTTP serving port `80` can be omitted when using the default configurations.\n   >\n6. In [service_conf.yaml.template](./docker/service_conf.yaml.template), select the desired LLM factory in `user_default_llm` and update\n   the `API_KEY` field with the corresponding API key.\n\n   > See [llm_api_key_setup](https://ragflow.io/docs/dev/llm_api_key_setup) for more information.\n   >\n\n   _The show is on!_\n\n## ðŸ”§ Configurations\n\nWhen it comes to system configurations, you will need to manage the following files:\n\n- [.env](./docker/.env): Keeps the fundamental setups for the system, such as `SVR_HTTP_PORT`, `MYSQL_PASSWORD`, and\n  `MINIO_PASSWORD`.\n- [service_conf.yaml.template](./docker/service_conf.yaml.template): Configures the back-end services. The environment variables in this file will be automatically populated when the Docker container starts. Any environment variables set within the Docker container will be available for use, allowing you to customize service behavior based on the deployment environment.\n- [docker-compose.yml](./docker/docker-compose.yml): The system relies on [docker-compose.yml](./docker/docker-compose.yml) to start up.\n\n> The [./docker/README](./docker/README.md) file provides a detailed description of the environment settings and service\n> configurations which can be used as `${ENV_VARS}` in the [service_conf.yaml.template](./docker/service_conf.yaml.template) file.\n\nTo update the default HTTP serving port (80), go to [docker-compose.yml](./docker/docker-compose.yml) and change `80:80`\nto `<YOUR_SERVING_PORT>:80`.\n\nUpdates to the above configurations require a reboot of all containers to take effect:\n\n> ```bash\n> $ docker compose -f docker-compose.yml up -d\n> ```\n\n### Switch doc engine from Elasticsearch to Infinity\n\nRAGFlow uses Elasticsearch by default for storing full text and vectors. To switch to [Infinity](https://github.com/infiniflow/infinity/), follow these steps:\n\n1. Stop all running containers:\n\n   ```bash\n   $ docker compose -f docker/docker-compose.yml down -v\n   ```\n\n> [!WARNING]\n> `-v` will delete the docker container volumes, and the existing data will be cleared.\n\n2. Set `DOC_ENGINE` in **docker/.env** to `infinity`.\n3. Start the containers:\n\n   ```bash\n   $ docker compose -f docker-compose.yml up -d\n   ```\n\n> [!WARNING]\n> Switching to Infinity on a Linux/arm64 machine is not yet officially supported.\n\n## ðŸ”§ Build a Docker image\n\nThis image is approximately 2 GB in size and relies on external LLM and embedding services.\n\n```bash\ngit clone https://github.com/infiniflow/ragflow.git\ncd ragflow/\ndocker build --platform linux/amd64 -f Dockerfile -t infiniflow/ragflow:nightly .\n```\n\n## ðŸ”¨ Launch service from source for development\n\n1. Install `uv` and `pre-commit`, or skip this step if they are already installed:\n\n   ```bash\n   pipx install uv pre-commit\n   ```\n2. Clone the source code and install Python dependencies:\n\n   ```bash\n   git clone https://github.com/infiniflow/ragflow.git\n   cd ragflow/\n   uv sync --python 3.10 # install RAGFlow dependent python modules\n   uv run download_deps.py\n   pre-commit install\n   ```\n3. Launch the dependent services (MinIO, Elasticsearch, Redis, and MySQL) using Docker Compose:\n\n   ```bash\n   docker compose -f docker/docker-compose-base.yml up -d\n   ```\n\n   Add the following line to `/etc/hosts` to resolve all hosts specified in **docker/.env** to `127.0.0.1`:\n\n   ```\n   127.0.0.1       es01 infinity mysql minio redis sandbox-executor-manager\n   ```\n4. If you cannot access HuggingFace, set the `HF_ENDPOINT` environment variable to use a mirror site:\n\n   ```bash\n   export HF_ENDPOINT=https://hf-mirror.com\n   ```\n5. If your operating system does not have jemalloc, please install it as follows:\n\n   ```bash\n   # Ubuntu\n   sudo apt-get install libjemalloc-dev\n   # CentOS\n   sudo yum install jemalloc\n   # OpenSUSE\n   sudo zypper install jemalloc\n   # macOS\n   sudo brew install jemalloc\n   ```\n6. Launch backend service:\n\n   ```bash\n   source .venv/bin/activate\n   export PYTHONPATH=$(pwd)\n   bash docker/launch_backend_service.sh\n   ```\n7. Install frontend dependencies:\n\n   ```bash\n   cd web\n   npm install\n   ```\n8. Launch frontend service:\n\n   ```bash\n   npm run dev\n   ```\n\n   _The following output confirms a successful launch of the system:_\n\n   ![](https://github.com/user-attachments/assets/0daf462c-a24d-4496-a66f-92533534e187)\n9. Stop RAGFlow front-end and back-end service after development is complete:\n\n   ```bash\n   pkill -f "ragflow_server.py|task_executor.py"\n   ```\n\n## ðŸ“š Documentation\n\n- [Quickstart](https://ragflow.io/docs/dev/)\n- [Configuration](https://ragflow.io/docs/dev/configurations)\n- [Release notes](https://ragflow.io/docs/dev/release_notes)\n- [User guides](https://ragflow.io/docs/dev/category/guides)\n- [Developer guides](https://ragflow.io/docs/dev/category/developers)\n- [References](https://ragflow.io/docs/dev/category/references)\n- [FAQs](https://ragflow.io/docs/dev/faq)\n\n## ðŸ“œ Roadmap\n\nSee the [RAGFlow Roadmap 2025](https://github.com/infiniflow/ragflow/issues/4214)\n\n## ðŸ„ Community\n\n- [Discord](https://discord.gg/NjYzJD3GM3)\n- [Twitter](https://twitter.com/infiniflowai)\n- [GitHub Discussions](https://github.com/orgs/infiniflow/discussions)\n\n## ðŸ™Œ Contributing\n\nRAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community.\nIf you would like to be a part, review our [Contribution Guidelines](https://ragflow.io/docs/dev/contributing) first.\n', '{"language":"Python","stars":68943,"forks":7475,"watchers":68943,"open_issues":2956,"topics":["agent","agentic","agentic-ai","agentic-workflow","ai","ai-search","deep-learning","deep-research","deepseek","deepseek-r1","document-parser","document-understanding","graphrag","llm","mcp","multi-agent","ollama","openai","rag","retrieval-augmented-generation"],"default_branch":"main","size_kb":96735,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:infiniflow:ragflow","source_url":"https://github.com/infiniflow/ragflow"},{"type":"has_code","target_id":"github:infiniflow:ragflow","source_url":"https://github.com/infiniflow/ragflow"},{"type":"has_code","target_id":"github:infiniflow:ragflow","source_url":"https://github.com/infiniflow/ragflow"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:infiniflow:ragflow.git","source_url":"https://github.com/infiniflow/ragflow.git"},{"type":"has_code","target_id":"github:infiniflow:ragflow","source_url":"https://github.com/infiniflow/ragflow"},{"type":"has_code","target_id":"github:infiniflow:infinity","source_url":"https://github.com/infiniflow/infinity"},{"type":"has_code","target_id":"github:infiniflow:ragflow.git","source_url":"https://github.com/infiniflow/ragflow.git"},{"type":"has_code","target_id":"github:infiniflow:ragflow.git","source_url":"https://github.com/infiniflow/ragflow.git"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:infiniflow:ragflow","source_url":"https://github.com/infiniflow/ragflow"},{"type":"has_code","target_id":"github:orgs:infiniflow","source_url":"https://github.com/orgs/infiniflow"}]', NULL, 'Apache-2.0', 'approved', 80, '1bf14b9f5e66e050a308364cc1171fc9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-infiniflow-ragflow from https://github.com/infiniflow.png
Image converted to WebP: data/images/github-infiniflow-ragflow.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-dair-ai-Prompt-Engineering-Guide', 'github--dair-ai--prompt-engineering-guide', 'Prompt-Engineering-Guide', 'dair-ai', '<h5 align="center"> Sponsored by&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://serpapi.com/"><img src="https://cdn.rawgit.com/standard/standard/master/docs/logos/serpapi.png" height=35 valign="middle"></a> </h5> Prompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics. Prompt engineering skills help to better understand the capabilities and limitations of large language mod...', '["agent","agents","ai-agents","chatgpt","deep-learning","generative-ai","language-model","llms","openai","prompt-engineering","rag","mdx"]', 'other', 67389, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/dair-ai/Prompt-Engineering-Guide","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '# Prompt Engineering Guide\n\n<h5 align="center">\n  Sponsored by&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://serpapi.com/"><img src="https://cdn.rawgit.com/standard/standard/master/docs/logos/serpapi.png" height=35 valign="middle"></a>\n</h5>\n\nPrompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics. Prompt engineering skills help to better understand the capabilities and limitations of large language models (LLMs). Researchers use prompt engineering to improve the capacity of LLMs on a wide range of common and complex tasks such as question answering and arithmetic reasoning. Developers use prompt engineering to design robust and effective prompting techniques that interface with LLMs and other tools.\n\nMotivated by the high interest in developing with LLMs, we have created this new prompt engineering guide that contains all the latest papers, learning guides, lectures, references, and tools related to prompt engineering for LLMs.\n\nðŸŒ [Prompt Engineering Guide (Web Version)](https://www.promptingguide.ai/)\n\nðŸŽ‰ We are excited to launch our new prompt engineering, RAG, and AI Agents courses under the DAIR.AI Academy. [Join Now](https://dair-ai.thinkific.com/bundles/pro)!\n\nThe courses are meant to compliment this guide and provide a more hands-on approach to learning about prompt engineering, context engineering, and AI Agents. \n\nUse code PROMPTING20 to get an extra 20% off.\n\nHappy Prompting!\n\n---\n## Announcements / Updates\n\n- ðŸŽ“ We now offer self-paced prompt engineering courses under our DAIR.AI Academy. [Join Now](https://dair-ai.thinkific.com/bundles/pro)! \n- ðŸŽ“ New course on Prompt Engineering for LLMs announced! [Enroll here](https://maven.com/dair-ai/prompt-engineering-llms)!\n- ðŸ’¼ We now offer several [services](https://www.promptingguide.ai/services) like corporate training, consulting, and talks.\n- ðŸŒ We now support 13 languages! Welcoming more translations.\n- ðŸ‘©â€ðŸŽ“ We crossed 3 million learners in January 2024!\n- ðŸŽ‰ We have launched a new web version of the guide [here](https://www.promptingguide.ai/)\n- ðŸ”¥ We reached #1 on Hacker News on 21 Feb 2023\n- ðŸŽ‰ The First Prompt Engineering Lecture went live [here](https://youtu.be/dOxUroR57xs)\n\n[Join our Discord](https://discord.com/invite/SKgkVT8BGJ)\n\n[Follow us on Twitter](https://twitter.com/dair_ai)\n\n[Subscribe to our YouTube](https://www.youtube.com/channel/UCyna_OxOWL7IEuOwb7WhmxQ)\n\n[Subscribe to our Newsletter](https://nlpnews.substack.com/)\n\n---\n\n## Guides\nYou can also find the most up-to-date guides on our new website [https://www.promptingguide.ai/](https://www.promptingguide.ai/).\n\n- [Prompt Engineering - Introduction](https://www.promptingguide.ai/introduction)\n  - [Prompt Engineering - LLM Settings](https://www.promptingguide.ai/introduction/settings)\n  - [Prompt Engineering - Basics of Prompting](https://www.promptingguide.ai/introduction/basics)\n  - [Prompt Engineering - Prompt Elements](https://www.promptingguide.ai/introduction/elements)\n  - [Prompt Engineering - General Tips for Designing Prompts](https://www.promptingguide.ai/introduction/tips)\n  - [Prompt Engineering - Examples of Prompts](https://www.promptingguide.ai/introduction/examples)\n- [Prompt Engineering - Techniques](https://www.promptingguide.ai/techniques)\n  - [Prompt Engineering - Zero-Shot Prompting](https://www.promptingguide.ai/techniques/zeroshot)\n  - [Prompt Engineering - Few-Shot Prompting](https://www.promptingguide.ai/techniques/fewshot)\n  - [Prompt Engineering - Chain-of-Thought Prompting](https://www.promptingguide.ai/techniques/cot)\n  - [Prompt Engineering - Self-Consistency](https://www.promptingguide.ai/techniques/consistency)\n  - [Prompt Engineering - Generate Knowledge Prompting](https://www.promptingguide.ai/techniques/knowledge)\n  - [Prompt Engineering - Prompt Chaining](https://www.promptingguide.ai/techniques/prompt_chaining)\n  - [Prompt Engineering - Tree of Thoughts (ToT)](https://www.promptingguide.ai/techniques/tot)\n  - [Prompt Engineering - Retrieval Augmented Generation](https://www.promptingguide.ai/techniques/rag)\n  - [Prompt Engineering - Automatic Reasoning and Tool-use (ART)](https://www.promptingguide.ai/techniques/art)\n  - [Prompt Engineering - Automatic Prompt Engineer](https://www.promptingguide.ai/techniques/ape)\n  - [Prompt Engineering - Active-Prompt](https://www.promptingguide.ai/techniques/activeprompt)\n  - [Prompt Engineering - Directional Stimulus Prompting](https://www.promptingguide.ai/techniques/dsp)\n  - [Prompt Engineering - Program-Aided Language Models](https://www.promptingguide.ai/techniques/pal)\n  - [Prompt Engineering - ReAct Prompting](https://www.promptingguide.ai/techniques/react)\n  - [Prompt Engineering - Multimodal CoT Prompting](https://www.promptingguide.ai/techniques/multimodalcot)\n  - [Prompt Engineering - Graph Prompting](https://www.promptingguide.ai/techniques/graph)\n- [Prompt Engineering - Applications](https://www.promptingguide.ai/applications)\n  - [Prompt Engineering - Function Calling](https://www.promptingguide.ai/applications/function_calling)\n  - [Prompt Engineering - Generating Data](https://www.promptingguide.ai/applications/generating)\n  - [Prompt Engineering - Generating Synthetic Dataset for RAG](https://www.promptingguide.ai/applications/synthetic_rag)\n  - [Prompt Engineering - Takling Generated Datasets Diversity](https://www.promptingguide.ai/applications/generating_textbooks)\n  - [Prompt Engineering - Generating Code](https://www.promptingguide.ai/applications/coding)\n  - [Prompt Engineering - Graduate Job Classification Case Study](https://www.promptingguide.ai/applications/workplace_casestudy)\n- [Prompt Engineering - Prompt Hub](https://www.promptingguide.ai/prompts)\n  - [Prompt Engineering - Classification](https://www.promptingguide.ai/prompts/classification)\n  - [Prompt Engineering - Coding](https://www.promptingguide.ai/prompts/coding)\n  - [Prompt Engineering - Creativity](https://www.promptingguide.ai/prompts/creativity)\n  - [Prompt Engineering - Evaluation](https://www.promptingguide.ai/prompts/evaluation)\n  - [Prompt Engineering - Information Extraction](https://www.promptingguide.ai/prompts/information-extraction)\n  - [Prompt Engineering - Image Generation](https://www.promptingguide.ai/prompts/image-generation)\n  - [Prompt Engineering - Mathematics](https://www.promptingguide.ai/prompts/mathematics)\n  - [Prompt Engineering - Question Answering](https://www.promptingguide.ai/prompts/question-answering)\n  - [Prompt Engineering - Reasoning](https://www.promptingguide.ai/prompts/reasoning)\n  - [Prompt Engineering - Text Summarization](https://www.promptingguide.ai/prompts/text-summarization)\n  - [Prompt Engineering - Truthfulness](https://www.promptingguide.ai/prompts/truthfulness)\n  - [Prompt Engineering - Adversarial Prompting](https://www.promptingguide.ai/prompts/adversarial-prompting)\n- [Prompt Engineering - Models](https://www.promptingguide.ai/models)\n  - [Prompt Engineering - ChatGPT](https://www.promptingguide.ai/models/chatgpt)\n  - [Prompt Engineering - Code Llama](https://www.promptingguide.ai/models/code-llama)\n  - [Prompt Engineering - Flan](https://www.promptingguide.ai/models/flan)\n  - [Prompt Engineering - Gemini](https://www.promptingguide.ai/models/gemini)\n  - [Prompt Engineering - GPT-4](https://www.promptingguide.ai/models/gpt-4)\n  - [Prompt Engineering - LLaMA](https://www.promptingguide.ai/models/llama)\n  - [Prompt Engineering - Mistral 7B](https://www.promptingguide.ai/models/mistral-7b)\n  - [Prompt Engineering - Mixtral](https://www.promptingguide.ai/models/mixtral)\n  - [Prompt Engineering - OLMo](https://www.promptingguide.ai/models/olmo)\n  - [Prompt Engineering - Phi-2](https://www.promptingguide.ai/models/phi-2)\n  - [Prompt Engineering - Model Collection](https://www.promptingguide.ai/models/collection)\n- [Prompt Engineering - Risks and Misuses](https://www.promptingguide.ai/risks)\n  - [Prompt Engineering - Adversarial Prompting](https://www.promptingguide.ai/risks/adversarial)\n  - [Prompt Engineering - Factuality](https://www.promptingguide.ai/risks/factuality)\n  - [Prompt Engineering - Biases](https://www.promptingguide.ai/risks/biases)\n- [Prompt Engineering - Papers](https://www.promptingguide.ai/papers)\n  - [Prompt Engineering - Overviews](https://www.promptingguide.ai/papers#overviews)\n  - [Prompt Engineering - Approaches](https://www.promptingguide.ai/papers#approaches)\n  - [Prompt Engineering - Applications](https://www.promptingguide.ai/papers#applications)\n  - [Prompt Engineering - Collections](https://www.promptingguide.ai/papers#collections)\n- [Prompt Engineering - Tools](https://www.promptingguide.ai/tools)\n- [Prompt Engineering - Notebooks](https://www.promptingguide.ai/notebooks)\n- [Prompt Engineering - Datasets](https://www.promptingguide.ai/datasets)\n- [Prompt Engineering - Additional Readings](https://www.promptingguide.ai/readings)\n\n\n---\n## Lecture\n\nWe have published a 1 hour lecture that provides a comprehensive overview of prompting techniques, applications, and tools.\n- [Video Lecture](https://youtu.be/dOxUroR57xs)\n- [Notebook with code](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/notebooks/pe-lecture.ipynb)\n- [Slides](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/lecture/Prompt-Engineering-Lecture-Elvis.pdf)\n\n---\n## Running the guide locally\n\nTo run the guide locally, for example to check the correct implementation of a new translation, you will need to:\n\n1. Install Node >=18.0.0\n1. Install `pnpm` if not present in your system. Check [here](https://pnpm.io/installation) for detailed instructions.\n1. Install the dependencies: `pnpm i next react react-dom nextra nextra-theme-docs`\n1. Boot the guide with `pnpm dev`\n2. Browse the guide at `http://localhost:3000/`\n\n---\n## Appearances\nSome places where we have been featured:\n- Wall Street Journal - [ChatGPT Can Give Great Answers. But Only If You Know How to Ask the Right Question](https://www.wsj.com/articles/chatgpt-ask-the-right-question-12d0f035)\n- Forbes - [Mom, Dad, I Want To Be A Prompt Engineer](https://www.forbes.com/sites/craigsmith/2023/04/05/mom-dad-i-want-to-be-a-prompt-engineer/?sh=7f1213159c8e)\n- Markettechpost - [Best Free Prompt Engineering Resources (2023)](https://www.marktechpost.com/2023/04/04/best-free-prompt-engineering-resources-2023/)\n\n\n---\nIf you are using the guide for your work or research, please cite us as follows:\n\n```\n@article{Saravia_Prompt_Engineering_Guide_2022,\nauthor = {Saravia, Elvis},\njournal = {https://github.com/dair-ai/Prompt-Engineering-Guide},\nmonth = {12},\ntitle = {{Prompt Engineering Guide}},\nyear = {2022}\n}\n```\n\n## License\n\n[MIT License](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/LICENSE.md)\n\n\nFeel free to open a PR if you think something is missing here. Always welcome feedback and suggestions. Just open an issue!\n', '{"language":"MDX","stars":67389,"forks":7087,"watchers":67389,"open_issues":231,"topics":["agent","agents","ai-agents","chatgpt","deep-learning","generative-ai","language-model","llms","openai","prompt-engineering","rag"],"default_branch":"main","size_kb":70677,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:dair-ai:Prompt-Engineering-Guide","source_url":"https://github.com/dair-ai/Prompt-Engineering-Guide"},{"type":"has_code","target_id":"github:dair-ai:Prompt-Engineering-Guide","source_url":"https://github.com/dair-ai/Prompt-Engineering-Guide"},{"type":"has_code","target_id":"github:dair-ai:Prompt-Engineering-Guide},","source_url":"https://github.com/dair-ai/Prompt-Engineering-Guide},"},{"type":"has_code","target_id":"github:dair-ai:Prompt-Engineering-Guide","source_url":"https://github.com/dair-ai/Prompt-Engineering-Guide"}]', NULL, 'MIT', 'approved', 80, 'bcd11aa7e7a7cd811e1b6ce06e43d4f7', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-dair-ai-Prompt-Engineering-Guide from https://github.com/dair-ai.png
Image converted to WebP: data/images/github-dair-ai-Prompt-Engineering-Guide.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-CorentinJ-Real-Time-Voice-Cloning', 'github--corentinj--real-time-voice-cloning', 'Real-Time-Voice-Cloning', 'CorentinJ', 'This repository is an implementation of Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis (SV2TTS) with a vocoder that works in real-time. This was my master''s thesis. SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text. **Video demonstration** (click...', '["deep-learning","python","pytorch","tensorflow","tts","voice-cloning","python"]', 'other', 58981, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/CorentinJ/Real-Time-Voice-Cloning","fetched_at":"2025-12-08T10:39:52.043Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Real-Time Voice Cloning\n\nThis repository is an implementation of [Transfer Learning from Speaker Verification to\nMultispeaker Text-To-Speech Synthesis](https://arxiv.org/pdf/1806.04558.pdf) (SV2TTS) with a vocoder that works in real-time. This was my [master''s thesis](https://matheo.uliege.be/handle/2268.2/6801).\n\nSV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.\n\n**Video demonstration** (click the picture):\n\n[![Toolbox demo](https://i.imgur.com/8lFUlgz.png)](https://www.youtube.com/watch?v=-O_hYhToKoA)\n\n### Papers implemented\n\n| URL                                                    | Designation            | Title                                                                                    | Implementation source                                   |\n| ------------------------------------------------------ | ---------------------- | ---------------------------------------------------------------------------------------- | ------------------------------------------------------- |\n| [**1806.04558**](https://arxiv.org/pdf/1806.04558.pdf) | **SV2TTS**             | **Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis** | This repo                                               |\n| [1802.08435](https://arxiv.org/pdf/1802.08435.pdf)     | WaveRNN (vocoder)      | Efficient Neural Audio Synthesis                                                         | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN) |\n| [1703.10135](https://arxiv.org/pdf/1703.10135.pdf)     | Tacotron (synthesizer) | Tacotron: Towards End-to-End Speech Synthesis                                            | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN) |\n| [1710.10467](https://arxiv.org/pdf/1710.10467.pdf)     | GE2E (encoder)         | Generalized End-To-End Loss for Speaker Verification                                     | This repo                                               |\n\n## Heads up\n\nLike everything else in Deep Learning, this repo has quickly gotten old. Many SaaS apps (often paying) will give you a better audio quality than this repository will. If you wish for an open-source solution with a high voice quality:\n\n- Check out [paperswithcode](https://paperswithcode.com/task/speech-synthesis/) for other repositories and recent research in the field of speech synthesis.\n- Check out [Chatterbox](https://github.com/resemble-ai/chatterbox) for a similar project up to date with the 2025 SOTA in voice cloning\n\n## Running the toolbox\n\nBoth Windows and Linux are supported.\n1. Install [ffmpeg](https://ffmpeg.org/download.html#get-packages). This is necessary for reading audio files. Check if it''s installed by running in a command line\n```\nffmpeg\n```\n2. Install uv for python package management\n```\n# On Windows:\npowershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"\n# On Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Alternatively, on any platform if you have pip installed you can do\npip install -U uv\n```\n3. Run one of the following commands\n```\n# Run the toolbox if you have an NVIDIA GPU\nuv run --extra cuda demo_toolbox.py\n# Use this if you don''t\nuv run --extra cpu demo_toolbox.py\n\n# Run in command line if you don''t want the GUI\nuv run --extra cuda demo_cli.py\nuv run --extra cpu demo_cli.py\n```\nUv will automatically create a .venv directory for you with an appropriate python environment. [Open an issue](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues) if this fails for you\n\n### (Optional) Download Pretrained Models\n\nPretrained models are now downloaded automatically. If this doesn''t work for you, you can manually download them [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models).\n\n### (Optional) Download Datasets\n\nFor playing with the toolbox alone, I only recommend downloading [`LibriSpeech/train-clean-100`](https://www.openslr.org/resources/12/train-clean-100.tar.gz). Extract the contents as `<datasets_root>/LibriSpeech/train-clean-100` where `<datasets_root>` is a directory of your choosing. Other datasets are supported in the toolbox, see [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets). You''re free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.\n', '{"language":"Python","stars":58981,"forks":9391,"watchers":58981,"open_issues":172,"topics":["deep-learning","python","pytorch","tensorflow","tts","voice-cloning"],"default_branch":"master","size_kb":369680,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:fatchord:WaveRNN","source_url":"https://github.com/fatchord/WaveRNN"},{"type":"has_code","target_id":"github:fatchord:WaveRNN","source_url":"https://github.com/fatchord/WaveRNN"},{"type":"has_code","target_id":"github:resemble-ai:chatterbox","source_url":"https://github.com/resemble-ai/chatterbox"},{"type":"has_code","target_id":"github:CorentinJ:Real-Time-Voice-Cloning","source_url":"https://github.com/CorentinJ/Real-Time-Voice-Cloning"},{"type":"has_code","target_id":"github:CorentinJ:Real-Time-Voice-Cloning","source_url":"https://github.com/CorentinJ/Real-Time-Voice-Cloning"},{"type":"has_code","target_id":"github:CorentinJ:Real-Time-Voice-Cloning","source_url":"https://github.com/CorentinJ/Real-Time-Voice-Cloning"}]', NULL, 'NOASSERTION', 'approved', 65, '072f82d256fbe882b3f3b139e090f96e', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-CorentinJ-Real-Time-Voice-Cloning from https://github.com/CorentinJ.png
Image converted to WebP: data/images/github-CorentinJ-Real-Time-Voice-Cloning.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-coqui-ai-TTS', 'github--coqui-ai--tts', 'TTS', 'coqui-ai', '- ðŸ“£ â“TTSv2 is here with 16 languages and better performance across the board. - ðŸ“£ â“TTS fine-tuning code is out. Check the example recipes. - ðŸ“£ â“TTS can now stream with <200ms latency. - ðŸ“£ â“TTS, our production TTS model that can speak 13 languages, is released Blog Post, Demo, Docs - ðŸ“£ ðŸ¶Bark is now available for inference with unconstrained voice cloning. Docs - ðŸ“£ You can use ~1100 Fairseq models with ðŸ¸TTS. - ðŸ“£ ðŸ¸TTS now supports ðŸ¢Tortoise with faster inference. Docs <div align="cent...', '["deep-learning","glow-tts","hifigan","melgan","multi-speaker-tts","python","pytorch","speaker-encoder","speaker-encodings","speech","speech-synthesis","tacotron","text-to-speech","tts","tts-model","vocoder","voice-cloning","voice-conversion","voice-synthesis","python"]', 'other', 43766, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/coqui-ai/TTS","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '\n## ðŸ¸Coqui.ai News\n- ðŸ“£ â“TTSv2 is here with 16 languages and better performance across the board.\n- ðŸ“£ â“TTS fine-tuning code is out. Check the [example recipes](https://github.com/coqui-ai/TTS/tree/dev/recipes/ljspeech).\n- ðŸ“£ â“TTS can now stream with <200ms latency.\n- ðŸ“£ â“TTS, our production TTS model that can speak 13 languages, is released [Blog Post](https://coqui.ai/blog/tts/open_xtts), [Demo](https://huggingface.co/spaces/coqui/xtts), [Docs](https://tts.readthedocs.io/en/dev/models/xtts.html)\n- ðŸ“£ [ðŸ¶Bark](https://github.com/suno-ai/bark) is now available for inference with unconstrained voice cloning. [Docs](https://tts.readthedocs.io/en/dev/models/bark.html)\n- ðŸ“£ You can use [~1100 Fairseq models](https://github.com/facebookresearch/fairseq/tree/main/examples/mms) with ðŸ¸TTS.\n- ðŸ“£ ðŸ¸TTS now supports ðŸ¢Tortoise with faster inference. [Docs](https://tts.readthedocs.io/en/dev/models/tortoise.html)\n\n<div align="center">\n<img src="https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2" />\n\n## <img src="https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png" height="56"/>\n\n\n**ðŸ¸TTS is a library for advanced Text-to-Speech generation.**\n\nðŸš€ Pretrained models in +1100 languages.\n\nðŸ› ï¸ Tools for training new models and fine-tuning existing models in any language.\n\nðŸ“š Utilities for dataset analysis and curation.\n______________________________________________________________________\n\n[![Discord](https://img.shields.io/discord/1037326658807533628?color=%239B59B6&label=chat%20on%20discord)](https://discord.gg/5eXr5seRrv)\n[![License](<https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg>)](https://opensource.org/licenses/MPL-2.0)\n[![PyPI version](https://badge.fury.io/py/TTS.svg)](https://badge.fury.io/py/TTS)\n[![Covenant](https://camo.githubusercontent.com/7d620efaa3eac1c5b060ece5d6aacfcc8b81a74a04d05cd0398689c01c4463bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e7472696275746f72253230436f76656e616e742d76322e3025323061646f707465642d6666363962342e737667)](https://github.com/coqui-ai/TTS/blob/master/CODE_OF_CONDUCT.md)\n[![Downloads](https://pepy.tech/badge/tts)](https://pepy.tech/project/tts)\n[![DOI](https://zenodo.org/badge/265612440.svg)](https://zenodo.org/badge/latestdoi/265612440)\n\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/aux_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/data_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/docker.yaml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/inference_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/style_check.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/text_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/tts_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/vocoder_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests0.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests1.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests2.yml/badge.svg)\n[![Docs](<https://readthedocs.org/projects/tts/badge/?version=latest&style=plastic>)](https://tts.readthedocs.io/en/latest/)\n\n</div>\n\n______________________________________________________________________\n\n## ðŸ’¬ Where to ask questions\nPlease use our dedicated channels for questions and discussion. Help is much more valuable if it''s shared publicly so that more people can benefit from it.\n\n| Type                            | Platforms                               |\n| ------------------------------- | --------------------------------------- |\n| ðŸš¨ **Bug Reports**              | [GitHub Issue Tracker]                  |\n| ðŸŽ **Feature Requests & Ideas** | [GitHub Issue Tracker]                  |\n| ðŸ‘©â€ðŸ’» **Usage Questions**          | [GitHub Discussions]                    |\n| ðŸ—¯ **General Discussion**       | [GitHub Discussions] or [Discord]   |\n\n[github issue tracker]: https://github.com/coqui-ai/tts/issues\n[github discussions]: https://github.com/coqui-ai/TTS/discussions\n[discord]: https://discord.gg/5eXr5seRrv\n[Tutorials and Examples]: https://github.com/coqui-ai/TTS/wiki/TTS-Notebooks-and-Tutorials\n\n\n## ðŸ”— Links and Resources\n| Type                            | Links                               |\n| ------------------------------- | --------------------------------------- |\n| ðŸ’¼ **Documentation**              | [ReadTheDocs](https://tts.readthedocs.io/en/latest/)\n| ðŸ’¾ **Installation**               | [TTS/README.md](https://github.com/coqui-ai/TTS/tree/dev#installation)|\n| ðŸ‘©â€ðŸ’» **Contributing**               | [CONTRIBUTING.md](https://github.com/coqui-ai/TTS/blob/main/CONTRIBUTING.md)|\n| ðŸ“Œ **Road Map**                   | [Main Development Plans](https://github.com/coqui-ai/TTS/issues/378)\n| ðŸš€ **Released Models**            | [TTS Releases](https://github.com/coqui-ai/TTS/releases) and [Experimental Models](https://github.com/coqui-ai/TTS/wiki/Experimental-Released-Models)|\n| ðŸ“° **Papers**                    | [TTS Papers](https://github.com/erogol/TTS-papers)|\n\n\n## ðŸ¥‡ TTS Performance\n<p align="center"><img src="https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png" width="800" /></p>\n\nUnderlined "TTS*" and "Judy*" are **internal** ðŸ¸TTS models that are not released open-source. They are here to show the potential. Models prefixed with a dot (.Jofish .Abe and .Janice) are real human voices.\n\n## Features\n- High-performance Deep Learning models for Text2Speech tasks.\n    - Text2Spec models (Tacotron, Tacotron2, Glow-TTS, SpeedySpeech).\n    - Speaker Encoder to compute speaker embeddings efficiently.\n    - Vocoder models (MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN)\n- Fast and efficient model training.\n- Detailed training logs on the terminal and Tensorboard.\n- Support for Multi-speaker TTS.\n- Efficient, flexible, lightweight but feature complete `Trainer API`.\n- Released and ready-to-use models.\n- Tools to curate Text2Speech datasets under```dataset_analysis```.\n- Utilities to use and test your models.\n- Modular (but not too much) code base enabling easy implementation of new ideas.\n\n## Model Implementations\n### Spectrogram models\n- Tacotron: [paper](https://arxiv.org/abs/1703.10135)\n- Tacotron2: [paper](https://arxiv.org/abs/1712.05884)\n- Glow-TTS: [paper](https://arxiv.org/abs/2005.11129)\n- Speedy-Speech: [paper](https://arxiv.org/abs/2008.03802)\n- Align-TTS: [paper](https://arxiv.org/abs/2003.01950)\n- FastPitch: [paper](https://arxiv.org/pdf/2006.06873.pdf)\n- FastSpeech: [paper](https://arxiv.org/abs/1905.09263)\n- FastSpeech2: [paper](https://arxiv.org/abs/2006.04558)\n- SC-GlowTTS: [paper](https://arxiv.org/abs/2104.05557)\n- Capacitron: [paper](https://arxiv.org/abs/1906.03402)\n- OverFlow: [paper](https://arxiv.org/abs/2211.06892)\n- Neural HMM TTS: [paper](https://arxiv.org/abs/2108.13320)\n- Delightful TTS: [paper](https://arxiv.org/abs/2110.12612)\n\n### End-to-End Models\n- â“TTS: [blog](https://coqui.ai/blog/tts/open_xtts)\n- VITS: [paper](https://arxiv.org/pdf/2106.06103)\n- ðŸ¸ YourTTS: [paper](https://arxiv.org/abs/2112.02418)\n- ðŸ¢ Tortoise: [orig. repo](https://github.com/neonbjb/tortoise-tts)\n- ðŸ¶ Bark: [orig. repo](https://github.com/suno-ai/bark)\n\n### Attention Methods\n- Guided Attention: [paper](https://arxiv.org/abs/1710.08969)\n- Forward Backward Decoding: [paper](https://arxiv.org/abs/1907.09006)\n- Graves Attention: [paper](https://arxiv.org/abs/1910.10288)\n- Double Decoder Consistency: [blog](https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/)\n- Dynamic Convolutional Attention: [paper](https://arxiv.org/pdf/1910.10288.pdf)\n- Alignment Network: [paper](https://arxiv.org/abs/2108.10447)\n\n### Speaker Encoder\n- GE2E: [paper](https://arxiv.org/abs/1710.10467)\n- Angular Loss: [paper](https://arxiv.org/pdf/2003.11982.pdf)\n\n### Vocoders\n- MelGAN: [paper](https://arxiv.org/abs/1910.06711)\n- MultiBandMelGAN: [paper](https://arxiv.org/abs/2005.05106)\n- ParallelWaveGAN: [paper](https://arxiv.org/abs/1910.11480)\n- GAN-TTS discriminators: [paper](https://arxiv.org/abs/1909.11646)\n- WaveRNN: [origin](https://github.com/fatchord/WaveRNN/)\n- WaveGrad: [paper](https://arxiv.org/abs/2009.00713)\n- HiFiGAN: [paper](https://arxiv.org/abs/2010.05646)\n- UnivNet: [paper](https://arxiv.org/abs/2106.07889)\n\n### Voice Conversion\n- FreeVC: [paper](https://arxiv.org/abs/2210.15418)\n\nYou can also help us implement more models.\n\n## Installation\nðŸ¸TTS is tested on Ubuntu 18.04 with **python >= 3.9, < 3.12.**.\n\nIf you are only interested in [synthesizing speech](https://tts.readthedocs.io/en/latest/inference.html) with the released ðŸ¸TTS models, installing from PyPI is the easiest option.\n\n```bash\npip install TTS\n```\n\nIf you plan to code or train models, clone ðŸ¸TTS and install it locally.\n\n```bash\ngit clone https://github.com/coqui-ai/TTS\npip install -e .[all,dev,notebooks]  # Select the relevant extras\n```\n\nIf you are on Ubuntu (Debian), you can also run following commands for installation.\n\n```bash\n$ make system-deps  # intended to be used on Ubuntu (Debian). Let us know if you have a different OS.\n$ make install\n```\n\nIf you are on Windows, ðŸ‘‘@GuyPaddock wrote installation instructions [here](https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system).\n\n\n## Docker Image\nYou can also try TTS without install with the docker image.\nSimply run the following command and you will be able to run TTS without installing it.\n\n```bash\ndocker run --rm -it -p 5002:5002 --entrypoint /bin/bash ghcr.io/coqui-ai/tts-cpu\npython3 TTS/server/server.py --list_models #To get the list of available models\npython3 TTS/server/server.py --model_name tts_models/en/vctk/vits # To start a server\n```\n\nYou can then enjoy the TTS server [here](http://[::1]:5002/)\nMore details about the docker images (like GPU support) can be found [here](https://tts.readthedocs.io/en/latest/docker_images.html)\n\n\n## Synthesizing speech by ðŸ¸TTS\n\n### ðŸ Python API\n\n#### Running a multi-speaker and multi-lingual model\n\n```python\nimport torch\nfrom TTS.api import TTS\n\n# Get device\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\n\n# List available ðŸ¸TTS models\nprint(TTS().list_models())\n\n# Init TTS\ntts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to(device)\n\n# Run TTS\n# â— Since this model is multi-lingual voice cloning model, we must set the target speaker_wav and language\n# Text to speech list of amplitude values as output\nwav = tts.tts(text="Hello world!", speaker_wav="my/cloning/audio.wav", language="en")\n# Text to speech to a file\ntts.tts_to_file(text="Hello world!", speaker_wav="my/cloning/audio.wav", language="en", file_path="output.wav")\n```\n\n#### Running a single speaker model\n\n```python\n# Init TTS with the target model name\ntts = TTS(model_name="tts_models/de/thorsten/tacotron2-DDC", progress_bar=False).to(device)\n\n# Run TTS\ntts.tts_to_file(text="Ich bin eine Testnachricht.", file_path=OUTPUT_PATH)\n\n# Example voice cloning with YourTTS in English, French and Portuguese\ntts = TTS(model_name="tts_models/multilingual/multi-dataset/your_tts", progress_bar=False).to(device)\ntts.tts_to_file("This is voice cloning.", speaker_wav="my/cloning/audio.wav", language="en", file_path="output.wav")\ntts.tts_to_file("C''est le clonage de la voix.", speaker_wav="my/cloning/audio.wav", language="fr-fr", file_path="output.wav")\ntts.tts_to_file("Isso Ã© clonagem de voz.", speaker_wav="my/cloning/audio.wav", language="pt-br", file_path="output.wav")\n```\n\n#### Example voice conversion\n\nConverting the voice in `source_wav` to the voice of `target_wav`\n\n```python\ntts = TTS(model_name="voice_conversion_models/multilingual/vctk/freevc24", progress_bar=False).to("cuda")\ntts.voice_conversion_to_file(source_wav="my/source.wav", target_wav="my/target.wav", file_path="output.wav")\n```\n\n#### Example voice cloning together with the voice conversion model.\nThis way, you can clone voices by using any model in ðŸ¸TTS.\n\n```python\n\ntts = TTS("tts_models/de/thorsten/tacotron2-DDC")\ntts.tts_with_vc_to_file(\n    "Wie sage ich auf Italienisch, dass ich dich liebe?",\n    speaker_wav="target/speaker.wav",\n    file_path="output.wav"\n)\n```\n\n#### Example text to speech using **Fairseq models in ~1100 languages** ðŸ¤¯.\nFor Fairseq models, use the following name format: `tts_models/<lang-iso_code>/fairseq/vits`.\nYou can find the language ISO codes [here](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)\nand learn about the Fairseq models [here](https://github.com/facebookresearch/fairseq/tree/main/examples/mms).\n\n```python\n# TTS with on the fly voice conversion\napi = TTS("tts_models/deu/fairseq/vits")\napi.tts_with_vc_to_file(\n    "Wie sage ich auf Italienisch, dass ich dich liebe?",\n    speaker_wav="target/speaker.wav",\n    file_path="output.wav"\n)\n```\n\n### Command-line `tts`\n\n<!-- begin-tts-readme -->\n\nSynthesize speech on command line.\n\nYou can either use your trained model or choose a model from the provided list.\n\nIf you don''t specify any models, then it uses LJSpeech based English model.\n\n#### Single Speaker Models\n\n- List provided models:\n\n  ```\n  $ tts --list_models\n  ```\n\n- Get model info (for both tts_models and vocoder_models):\n\n  - Query by type/name:\n    The model_info_by_name uses the name as it from the --list_models.\n    ```\n    $ tts --model_info_by_name "<model_type>/<language>/<dataset>/<model_name>"\n    ```\n    For example:\n    ```\n    $ tts --model_info_by_name tts_models/tr/common-voice/glow-tts\n    $ tts --model_info_by_name vocoder_models/en/ljspeech/hifigan_v2\n    ```\n  - Query by type/idx:\n    The model_query_idx uses the corresponding idx from --list_models.\n\n    ```\n    $ tts --model_info_by_idx "<model_type>/<model_query_idx>"\n    ```\n\n    For example:\n\n    ```\n    $ tts --model_info_by_idx tts_models/3\n    ```\n\n  - Query info for model info by full name:\n    ```\n    $ tts --model_info_by_name "<model_type>/<language>/<dataset>/<model_name>"\n    ```\n\n- Run TTS with default models:\n\n  ```\n  $ tts --text "Text for TTS" --out_path output/path/speech.wav\n  ```\n\n- Run TTS and pipe out the generated TTS wav file data:\n\n  ```\n  $ tts --text "Text for TTS" --pipe_out --out_path output/path/speech.wav | aplay\n  ```\n\n- Run a TTS model with its default vocoder model:\n\n  ```\n  $ tts --text "Text for TTS" --model_name "<model_type>/<language>/<dataset>/<model_name>" --out_path output/path/speech.wav\n  ```\n\n  For example:\n\n  ```\n  $ tts --text "Text for TTS" --model_name "tts_models/en/ljspeech/glow-tts" --out_path output/path/speech.wav\n  ```\n\n- Run with specific TTS and vocoder models from the list:\n\n  ```\n  $ tts --text "Text for TTS" --model_name "<model_type>/<language>/<dataset>/<model_name>" --vocoder_name "<model_type>/<language>/<dataset>/<model_name>" --out_path output/path/speech.wav\n  ```\n\n  For example:\n\n  ```\n  $ tts --text "Text for TTS" --model_name "tts_models/en/ljspeech/glow-tts" --vocoder_name "vocoder_models/en/ljspeech/univnet" --out_path output/path/speech.wav\n  ```\n\n- Run your own TTS model (Using Griffin-Lim Vocoder):\n\n  ```\n  $ tts --text "Text for TTS" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n  ```\n\n- Run your own TTS and Vocoder models:\n\n  ```\n  $ tts --text "Text for TTS" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n      --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json\n  ```\n\n#### Multi-speaker Models\n\n- List the available speakers and choose a <speaker_id> among them:\n\n  ```\n  $ tts --model_name "<language>/<dataset>/<model_name>"  --list_speaker_idxs\n  ```\n\n- Run the multi-speaker TTS model with the target speaker ID:\n\n  ```\n  $ tts --text "Text for TTS." --out_path output/path/speech.wav --model_name "<language>/<dataset>/<model_name>"  --speaker_idx <speaker_id>\n  ```\n\n- Run your own multi-speaker TTS model:\n\n  ```\n  $ tts --text "Text for TTS" --out_path output/path/speech.wav --model_path path/to/model.pth --config_path path/to/config.json --speakers_file_path path/to/speaker.json --speaker_idx <speaker_id>\n  ```\n\n### Voice Conversion Models\n\n```\n$ tts --out_path output/path/speech.wav --model_name "<language>/<dataset>/<model_name>" --source_wav <path/to/speaker/wav> --target_wav <path/to/reference/wav>\n```\n\n<!-- end-tts-readme -->\n\n## Directory Structure\n```\n|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)\n|- utils/           (common utilities.)\n|- TTS\n    |- bin/             (folder for all the executables.)\n      |- train*.py                  (train your target model.)\n      |- ...\n    |- tts/             (text to speech models)\n        |- layers/          (model layer definitions)\n        |- models/          (model definitions)\n        |- utils/           (model specific utilities.)\n    |- speaker_encoder/ (Speaker Encoder models.)\n        |- (same)\n    |- vocoder/         (Vocoder models.)\n        |- (same)\n```\n', '{"language":"Python","stars":43766,"forks":5836,"watchers":43766,"open_issues":9,"topics":["deep-learning","glow-tts","hifigan","melgan","multi-speaker-tts","python","pytorch","speaker-encoder","speaker-encodings","speech","speech-synthesis","tacotron","text-to-speech","tts","tts-model","vocoder","voice-cloning","voice-conversion","voice-synthesis"],"default_branch":"dev","size_kb":170196,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:suno-ai:bark","source_url":"https://github.com/suno-ai/bark"},{"type":"has_code","target_id":"github:facebookresearch:fairseq","source_url":"https://github.com/facebookresearch/fairseq"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:tts","source_url":"https://github.com/coqui-ai/tts"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:erogol:TTS-papers","source_url":"https://github.com/erogol/TTS-papers"},{"type":"has_code","target_id":"github:neonbjb:tortoise-tts","source_url":"https://github.com/neonbjb/tortoise-tts"},{"type":"has_code","target_id":"github:suno-ai:bark","source_url":"https://github.com/suno-ai/bark"},{"type":"has_code","target_id":"github:fatchord:WaveRNN","source_url":"https://github.com/fatchord/WaveRNN"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:facebookresearch:fairseq","source_url":"https://github.com/facebookresearch/fairseq"}]', NULL, 'MPL-2.0', 'approved', 80, '5747c748ef83dceaa79bf0b85569e273', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-coqui-ai-TTS from https://github.com/coqui-ai.png
Image converted to WebP: data/images/github-coqui-ai-TTS.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-hpcaitech-ColossalAI', 'github--hpcaitech--colossalai', 'ColossalAI', 'hpcaitech', '<div id="top" align="center"> Colossal-AI: Making large AI models cheaper, faster, and more accessible <h3> <a href="https://arxiv.org/abs/2110.14883"> Paper </a> | <a href="https://www.colossalai.org/"> Documentation </a> | <a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples"> Examples </a> | <a href="https://github.com/hpcaitech/ColossalAI/discussions"> Forum </a> | <a href="https://colossalai.org/zh-Hans/docs/get_started/bonus/">GPU Cloud Playground </a> | <a href="https://...', '["ai","big-model","data-parallelism","deep-learning","distributed-computing","foundation-models","heterogeneous-training","hpc","inference","large-scale","model-parallelism","pipeline-parallelism","python"]', 'other', 41289, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/hpcaitech/ColossalAI","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '# Colossal-AI\n<div id="top" align="center">\n\n   [![logo](https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/colossal-ai_logo_vertical.png)](https://www.colossalai.org/)\n\n   Colossal-AI: Making large AI models cheaper, faster, and more accessible\n\n   <h3> <a href="https://arxiv.org/abs/2110.14883"> Paper </a> |\n   <a href="https://www.colossalai.org/"> Documentation </a> |\n   <a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples"> Examples </a> |\n   <a href="https://github.com/hpcaitech/ColossalAI/discussions"> Forum </a> |\n   <a href="https://colossalai.org/zh-Hans/docs/get_started/bonus/">GPU Cloud Playground </a> |\n   <a href="https://hpc-ai.com/blog"> Blog </a></h3>\n\n   [![GitHub Repo stars](https://img.shields.io/github/stars/hpcaitech/ColossalAI?style=social)](https://github.com/hpcaitech/ColossalAI/stargazers)\n   [![Build](https://github.com/hpcaitech/ColossalAI/actions/workflows/build_on_schedule.yml/badge.svg)](https://github.com/hpcaitech/ColossalAI/actions/workflows/build_on_schedule.yml)\n   [![Documentation](https://readthedocs.org/projects/colossalai/badge/?version=latest)](https://colossalai.readthedocs.io/en/latest/?badge=latest)\n   [![CodeFactor](https://www.codefactor.io/repository/github/hpcaitech/colossalai/badge)](https://www.codefactor.io/repository/github/hpcaitech/colossalai)\n   [![HuggingFace badge](https://img.shields.io/badge/%F0%9F%A4%97HuggingFace-Join-yellow)](https://huggingface.co/hpcai-tech)\n   [![slack badge](https://img.shields.io/badge/Slack-join-blueviolet?logo=slack&amp)](https://github.com/hpcaitech/public_assets/tree/main/colossalai/contact/slack)\n   [![WeChat badge](https://img.shields.io/badge/å¾®ä¿¡-åŠ å…¥-green?logo=wechat&amp)](https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png)\n\n\n   | [English](README.md) | [ä¸­æ–‡](docs/README-zh-Hans.md) |\n\n</div>\n\n## Instantly Run Colossal-AI on Enterprise-Grade GPUs\n\nSkip the setup. Access a powerful, pre-configured Colossal-AI environment on [**HPC-AI Cloud**](https://hpc-ai.com/?utm_source=github&utm_medium=social&utm_campaign=promotion-colossalai).\n\nTrain your models and scale your AI workload in one click!\n\n* **NVIDIA Blackwell B200s**: Experience the next generation of AI performance ([See Benchmarks](https://hpc-ai.com/blog/b200)). Now available on cloud from **$2.47/hr**.\n* **Cost-Effective H200 Cluster**: Get premier performance with on-demand rental from just **$1.99/hr**.\n\n[**Get Started Now & Claim Your Free Credits â†’**](https://hpc-ai.com/?utm_source=github&utm_medium=social&utm_campaign=promotion-colossalai)\n\n<div align="center">\n   <a href="https://hpc-ai.com/?utm_source=github&utm_medium=social&utm_campaign=promotion-colossalai">\n   <img src="https://github.com/hpcaitech/public_assets/blob/main/colossalai/img/2-3.png" width="850" />\n   </a>\n</div>\n\n### Colossal-AI Benchmark\n\nTo see how these performance gains translate to real-world applications, we conducted a large language model training benchmark using Colossal-AI on Llama-like models. The tests were run on both 8-card and 16-card configurations for 7B and 70B models, respectively.\n\n|              GPU              |  GPUs  | Model Size |    Parallelism    | Batch Size per DP | Seqlen | Throughput | TFLOPS/GPU  | Peak Mem(MiB)  |\n| :-----------------------------: | :--------: | :-------------: | :------------------: | :-----------: | :--------------: | :-------------: | :-------------: | :-------------: |\n|         H200            |     8     |      7B       |   zero2(dp8)     | 36 |        4096     |       17.13 samp/s     |       534.18     |       119040.02     |\n|         H200            |     16     |      70B       |   zero2     | 48 |        4096     |       3.27 samp/s     |       469.1     |       150032.23     |\n|         B200            |     8     |      7B       |   zero1(dp2)+tp2+pp4     | 128 |        4096     |       25.83 samp/s     |       805.69     |       100119.77     |\n|         H200            |     16     |      70B       |   zero1(dp2)+tp2+pp4     | 128 |        4096     |       5.66 samp/s     |       811.79     |       100072.02     |\n\nThe results from the Colossal-AI benchmark provide the most practical insight. For the 7B model on 8 cards, the **B200 achieved a 50% higher throughput** and a significant increase in TFLOPS per GPU. For the 70B model on 16 cards, the B200 again demonstrated a clear advantage, with **over 70% higher throughput and TFLOPS per GPU**. These numbers show that the B200''s performance gains translate directly to faster training times for large-scale models.\n\n## Latest News\n* [2025/02] [DeepSeek 671B Fine-Tuning Guide Revealedâ€”Unlock the Upgraded DeepSeek Suite with One Click, AI Players Ecstatic!](https://company.hpc-ai.com/blog/shocking-release-deepseek-671b-fine-tuning-guide-revealed-unlock-the-upgraded-deepseek-suite-with-one-click-ai-players-ecstatic)\n* [2024/12] [The development cost of video generation models has saved by 50%! Open-source solutions are now available with H200 GPU vouchers](https://company.hpc-ai.com/blog/the-development-cost-of-video-generation-models-has-saved-by-50-open-source-solutions-are-now-available-with-h200-gpu-vouchers) [[code]](https://github.com/hpcaitech/Open-Sora/blob/main/scripts/train.py) [[vouchers]](https://colossalai.org/zh-Hans/docs/get_started/bonus/)\n* [2024/10] [How to build a low-cost Sora-like app? Solutions for you](https://company.hpc-ai.com/blog/how-to-build-a-low-cost-sora-like-app-solutions-for-you)\n* [2024/09] [Singapore Startup HPC-AI Tech Secures 50 Million USD in Series A Funding to Build the Video Generation AI Model and GPU Platform](https://company.hpc-ai.com/blog/singapore-startup-hpc-ai-tech-secures-50-million-usd-in-series-a-funding-to-build-the-video-generation-ai-model-and-gpu-platform)\n* [2024/09] [Reducing AI Large Model Training Costs by 30% Requires Just a Single Line of Code From FP8 Mixed Precision Training Upgrades](https://company.hpc-ai.com/blog/reducing-ai-large-model-training-costs-by-30-requires-just-a-single-line-of-code-from-fp8-mixed-precision-training-upgrades)\n* [2024/06] [Open-Sora Continues Open Source: Generate Any 16-Second 720p HD Video with One Click, Model Weights Ready to Use](https://hpc-ai.com/blog/open-sora-from-hpc-ai-tech-team-continues-open-source-generate-any-16-second-720p-hd-video-with-one-click-model-weights-ready-to-use)\n* [2024/05] [Large AI Models Inference Speed Doubled, Colossal-Inference Open Source Release](https://hpc-ai.com/blog/colossal-inference)\n* [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open Source with Single-Shot 16-Second Video Generation and 720p Resolution](https://hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-and-720p-resolution-in-open-source)\n* [2024/04] [Most cost-effective solutions for inference, fine-tuning and pretraining, tailored to LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-inference-fine-tuning-and-pretraining-tailored-to-llama3-series)\n\n## Table of Contents\n<ul>\n <li><a href="#Why-Colossal-AI">Why Colossal-AI</a> </li>\n <li><a href="#Features">Features</a> </li>\n <li>\n   <a href="#Colossal-AI-in-the-Real-World">Colossal-AI for Real World Applications</a>\n   <ul>\n     <li><a href="#Open-Sora">Open-Sora: Revealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models</a></li>\n     <li><a href="#Colossal-LLaMA-2">Colossal-LLaMA-2: One Half-Day of Training Using a Few Hundred Dollars Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-Free Domain-Specific Llm Solution</a></li>\n     <li><a href="#ColossalChat">ColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline</a></li>\n     <li><a href="#AIGC">AIGC: Acceleration of Stable Diffusion</a></li>\n     <li><a href="#Biomedicine">Biomedicine: Acceleration of AlphaFold Protein Structure</a></li>\n   </ul>\n </li>\n <li>\n   <a href="#Parallel-Training-Demo">Parallel Training Demo</a>\n   <ul>\n     <li><a href="#LLaMA3">LLaMA 1/2/3 </a></li>\n     <li><a href="#MoE">MoE</a></li>\n     <li><a href="#GPT-3">GPT-3</a></li>\n     <li><a href="#GPT-2">GPT-2</a></li>\n     <li><a href="#BERT">BERT</a></li>\n     <li><a href="#PaLM">PaLM</a></li>\n     <li><a href="#OPT">OPT</a></li>\n     <li><a href="#ViT">ViT</a></li>\n     <li><a href="#Recommendation-System-Models">Recommendation System Models</a></li>\n   </ul>\n </li>\n <li>\n   <a href="#Single-GPU-Training-Demo">Single GPU Training Demo</a>\n   <ul>\n     <li><a href="#GPT-2-Single">GPT-2</a></li>\n     <li><a href="#PaLM-Single">PaLM</a></li>\n   </ul>\n </li>\n <li>\n   <a href="#Inference">Inference</a>\n   <ul>\n     <li><a href="#Colossal-Inference">Colossal-Inference: Large AI  Models Inference Speed Doubled</a></li>\n     <li><a href="#Grok-1">Grok-1: 314B model of PyTorch + HuggingFace Inference</a></li>\n     <li><a href="#SwiftInfer">SwiftInfer:Breaks the Length Limit of LLM for Multi-Round Conversations with 46% Acceleration</a></li>\n   </ul>\n </li>\n <li>\n   <a href="#Installation">Installation</a>\n   <ul>\n     <li><a href="#PyPI">PyPI</a></li>\n     <li><a href="#Install-From-Source">Install From Source</a></li>\n   </ul>\n </li>\n <li><a href="#Use-Docker">Use Docker</a></li>\n <li><a href="#Community">Community</a></li>\n <li><a href="#Contributing">Contributing</a></li>\n <li><a href="#Cite-Us">Cite Us</a></li>\n</ul>\n\n## Why Colossal-AI\n<div align="center">\n   <a href="https://youtu.be/KnXSfjqkKN0">\n   <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/JamesDemmel_Colossal-AI.png" width="600" />\n   </a>\n\n   Prof. James Demmel (UC Berkeley): Colossal-AI makes training AI models efficient, easy, and scalable.\n</div>\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n## Features\n\nColossal-AI provides a collection of parallel components for you. We aim to support you to write your\ndistributed deep learning models just like how you write your model on your laptop. We provide user-friendly tools to kickstart\ndistributed training and inference in a few lines.\n\n- Parallelism strategies\n  - Data Parallelism\n  - Pipeline Parallelism\n  - 1D, [2D](https://arxiv.org/abs/2104.05343), [2.5D](https://arxiv.org/abs/2105.14500), [3D](https://arxiv.org/abs/2105.14450) Tensor Parallelism\n  - [Sequence Parallelism](https://arxiv.org/abs/2105.13120)\n  - [Zero Redundancy Optimizer (ZeRO)](https://arxiv.org/abs/1910.02054)\n  - [Auto-Parallelism](https://arxiv.org/abs/2302.02599)\n\n- Heterogeneous Memory Management\n  - [PatrickStar](https://arxiv.org/abs/2108.05818)\n\n- Friendly Usage\n  - Parallelism based on the configuration file\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n## Colossal-AI in the Real World\n### Open-Sora\n\n[Open-Sora](https://github.com/hpcaitech/Open-Sora)ï¼šRevealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models\n[[code]](https://github.com/hpcaitech/Open-Sora)\n[[blog]](https://hpc-ai.com/blog/open-sora-from-hpc-ai-tech-team-continues-open-source-generate-any-16-second-720p-hd-video-with-one-click-model-weights-ready-to-use)\n[[Model weights]](https://github.com/hpcaitech/Open-Sora?tab=readme-ov-file#model-weights)\n[[Demo]](https://github.com/hpcaitech/Open-Sora?tab=readme-ov-file#-latest-demo)\n[[GPU Cloud Playground]](https://cloud.luchentech.com/)\n[[OpenSora Image]](https://cloud.luchentech.com/doc/docs/image/open-sora/)\n\n<div align="center">\n   <a href="https://youtu.be/ilMQpU71ddI?si=J4JSPzZ03ycYmlki">\n   <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/sora/opensora-v1.2.png" width="700" />\n   </a>\n</div>\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n### Colossal-LLaMA-2\n\n[[GPU Cloud Playground]](https://cloud.luchentech.com/)\n[[LLaMA3 Image]](https://cloud.luchentech.com/doc/docs/image/llama)\n\n- 7B: One half-day of training using a few hundred dollars yields similar results to mainstream large models, open-source and commercial-free domain-specific LLM solution.\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Colossal-LLaMA-2)\n[[blog]](https://www.hpc-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-large-models-open-source-and-commercial-free-domain-specific-llm-solution)\n[[HuggingFace model weights]](https://huggingface.co/hpcai-tech/Colossal-LLaMA-2-7b-base)\n[[Modelscope model weights]](https://www.modelscope.cn/models/colossalai/Colossal-LLaMA-2-7b-base/summary)\n\n- 13B: Construct refined 13B private model with just $5000 USD.\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Colossal-LLaMA-2)\n[[blog]](https://hpc-ai.com/blog/colossal-llama-2-13b)\n[[HuggingFace model weights]](https://huggingface.co/hpcai-tech/Colossal-LLaMA-2-13b-base)\n[[Modelscope model weights]](https://www.modelscope.cn/models/colossalai/Colossal-LLaMA-2-13b-base/summary)\n\n|              Model              |  Backbone  | Tokens Consumed |     MMLU (5-shot)    | CMMLU (5-shot)| AGIEval (5-shot) | GAOKAO (0-shot) | CEval (5-shot)  |\n| :-----------------------------: | :--------: | :-------------: | :------------------: | :-----------: | :--------------: | :-------------: | :-------------: |\n|          Baichuan-7B            |     -      |      1.2T       |    42.32 (42.30)     | 44.53 (44.02) |        38.72     |       36.74     |       42.80     |\n|       Baichuan-13B-Base         |     -      |      1.4T       |    50.51 (51.60)     | 55.73 (55.30) |        47.20     |       51.41     |       53.60     |\n|       Baichuan2-7B-Base         |     -      |      2.6T       |    46.97 (54.16)     | 57.67 (57.07) |        45.76     |       52.60     |       54.00     |\n|       Baichuan2-13B-Base        |     -      |      2.6T       |    54.84 (59.17)     | 62.62 (61.97) |        52.08     |       58.25     |       58.10     |\n|           ChatGLM-6B            |     -      |      1.0T       |    39.67 (40.63)     |   41.17 (-)   |        40.10     |       36.53     |       38.90     |\n|          ChatGLM2-6B            |     -      |      1.4T       |    44.74 (45.46)     |   49.40 (-)   |        46.36     |       45.49     |       51.70     |\n|          InternLM-7B            |     -      |      1.6T       |    46.70 (51.00)     |   52.00 (-)   |        44.77     |       61.64     |       52.80     |\n|            Qwen-7B              |     -      |      2.2T       |    54.29 (56.70)     | 56.03 (58.80) |        52.47     |       56.42     |       59.60     |\n|           Llama-2-7B            |     -      |      2.0T       |    44.47 (45.30)     |   32.97 (-)   |        32.60     |       25.46     |         -       |\n| Linly-AI/Chinese-LLaMA-2-7B-hf  | Llama-2-7B |      1.0T       |        37.43         |     29.92     |        32.00     |       27.57     |         -       |\n| wenge-research/yayi-7b-llama2   | Llama-2-7B |        -        |        38.56         |     31.52     |        30.99     |       25.95     |         -       |\n| ziqingyang/chinese-llama-2-7b   | Llama-2-7B |        -        |        33.86         |     34.69     |        34.52     |       25.18     |        34.2     |\n| TigerResearch/tigerbot-7b-base  | Llama-2-7B |      0.3T       |        43.73         |     42.04     |        37.64     |       30.61     |         -       |\n|  LinkSoul/Chinese-Llama-2-7b    | Llama-2-7B |        -        |        48.41         |     38.31     |        38.45     |       27.72     |         -       |\n|       FlagAlpha/Atom-7B         | Llama-2-7B |      0.1T       |        49.96         |     41.10     |        39.83     |       33.00     |         -       |\n| IDEA-CCNL/Ziya-LLaMA-13B-v1.1   | Llama-13B  |      0.11T      |        50.25         |     40.99     |        40.04     |       30.54     |         -       |\n|  **Colossal-LLaMA-2-7b-base**   | Llama-2-7B |   **0.0085T**   |        53.06         |     49.89     |        51.48     |       58.82     |        50.2     |\n|  **Colossal-LLaMA-2-13b-base**  | Llama-2-13B |   **0.025T**    |        56.42         |     61.80     |        54.69     |       69.53     |        60.3     |\n\n\n### ColossalChat\n\n<div align="center">\n   <a href="https://www.youtube.com/watch?v=HcTiHzApHm0">\n   <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chat/ColossalChat%20YouTube.png" width="700" />\n   </a>\n</div>\n\n[ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat): An open-source solution for cloning [ChatGPT](https://openai.com/blog/chatgpt/) with a complete RLHF pipeline.\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat)\n[[blog]](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b)\n[[demo]](https://www.youtube.com/watch?v=HcTiHzApHm0)\n[[tutorial]](https://www.youtube.com/watch?v=-qFBZFmOJfg)\n\n<p id="ColossalChat-Speed" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chat/ColossalChat%20Speed.jpg" width=450/>\n</p>\n\n- Up to 10 times faster for RLHF PPO Stage3 Training\n\n<p id="ColossalChat_scaling" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT%20scaling.png" width=800/>\n</p>\n\n- Up to 7.73 times faster for single server training and 1.42 times faster for single-GPU inference\n\n<p id="ColossalChat-1GPU" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT-1GPU.jpg" width=450/>\n</p>\n\n- Up to 10.3x growth in model capacity on one GPU\n- A mini demo training process requires only 1.62GB of GPU memory (any consumer-grade GPU)\n\n<p id="ColossalChat-LoRA" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/LoRA%20data.jpg" width=600/>\n</p>\n\n- Increase the capacity of the fine-tuning model by up to 3.7 times on a single GPU\n- Keep at a sufficiently high running speed\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n\n### AIGC\nAcceleration of AIGC (AI-Generated Content) models such as [Stable Diffusion v1](https://github.com/CompVis/stable-diffusion) and [Stable Diffusion v2](https://github.com/Stability-AI/stablediffusion).\n<p id="diffusion_train" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20v2.png" width=800/>\n</p>\n\n- [Training](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion): Reduce Stable Diffusion memory consumption by up to 5.6x and hardware cost by up to 46x (from A100 to RTX3060).\n\n<p id="diffusion_demo" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/DreamBooth.png" width=800/>\n</p>\n\n- [DreamBooth Fine-tuning](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/dreambooth): Personalize your model using just 3-5 images of the desired subject.\n\n<p id="inference-sd" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20Inference.jpg" width=800/>\n</p>\n\n- [Inference](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion): Reduce inference GPU memory consumption by 2.5x.\n\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n### Biomedicine\nAcceleration of [AlphaFold Protein Structure](https://alphafold.ebi.ac.uk/)\n\n<p id="FastFold" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/FastFold.jpg" width=800/>\n</p>\n\n- [FastFold](https://github.com/hpcaitech/FastFold): Accelerating training and inference on GPU Clusters, faster data processing, inference sequence containing more than 10000 residues.\n\n<p id="FastFold-Intel" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/data%20preprocessing%20with%20Intel.jpg" width=600/>\n</p>\n\n- [FastFold with Intel](https://github.com/hpcaitech/FastFold): 3x inference acceleration and 39% cost reduce.\n\n<p id="xTrimoMultimer" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/xTrimoMultimer_Table.jpg" width=800/>\n</p>\n\n- [xTrimoMultimer](https://github.com/biomap-research/xTrimoMultimer): accelerating structure prediction of protein monomers and multimer by 11x.\n\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n## Parallel Training Demo\n### LLaMA3\n<p align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/LLaMA3-70B-H100.png" width=600/>\n</p>\n\n- 70 billion parameter LLaMA3 model training accelerated by 18%\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama)\n[[GPU Cloud Playground]](https://cloud.luchentech.com/)\n[[LLaMA3 Image]](https://cloud.luchentech.com/doc/docs/image/llama)\n\n### LLaMA2\n<p align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/llama2_pretraining.png" width=600/>\n</p>\n\n- 70 billion parameter LLaMA2 model training accelerated by 195%\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama)\n[[blog]](https://www.hpc-ai.tech/blog/70b-llama2-training)\n\n### LLaMA1\n<p align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/LLaMA_pretraining.png" width=600/>\n</p>\n\n- 65-billion-parameter large model pretraining accelerated by 38%\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama)\n[[blog]](https://www.hpc-ai.tech/blog/large-model-pretraining)\n\n### MoE\n<p align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/MOE_training.png" width=800/>\n</p>\n\n- Enhanced MoE parallelism, Open-source MoE model training can be 9 times more efficient\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/openmoe)\n[[blog]](https://www.hpc-ai.tech/blog/enhanced-moe-parallelism-open-source-moe-model-training-can-be-9-times-more-efficient)\n\n### GPT-3\n<p align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT3-v5.png" width=700/>\n</p>\n\n- Save 50% GPU resources and 10.7% acceleration\n\n### GPT-2\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2.png" width=800/>\n\n- 11x lower GPU memory consumption, and superlinear scaling efficiency with Tensor Parallelism\n\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/(updated)GPT-2.png" width=800>\n\n- 24x larger model size on the same hardware\n- over 3x acceleration\n### BERT\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BERT.png" width=800/>\n\n- 2x faster training, or 50% longer sequence length\n\n### PaLM\n- [PaLM-colossalai](https://github.com/hpcaitech/PaLM-colossalai): Scalable implementation of Google''s Pathways Language Model ([PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)).\n\n### OPT\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/OPT_update.png" width=800/>\n\n- [Open Pretrained Transformer (OPT)](https://github.com/facebookresearch/metaseq), a 175-Billion parameter AI language model released by Meta, which stimulates AI programmers to perform various downstream tasks and application deployments because of public pre-trained model weights.\n- 45% speedup fine-tuning OPT at low cost in lines. [[Example]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/opt) [[Online Serving]](https://colossalai.org/docs/advanced_tutorials/opt_service)\n\nPlease visit our [documentation](https://www.colossalai.org/) and [examples](https://github.com/hpcaitech/ColossalAI/tree/main/examples) for more details.\n\n### ViT\n<p align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/ViT.png" width="450" />\n</p>\n\n- 14x larger batch size, and 5x faster training for Tensor Parallelism = 64\n\n### Recommendation System Models\n- [Cached Embedding](https://github.com/hpcaitech/CachedEmbedding), utilize software cache to train larger embedding tables with a smaller GPU memory budget.\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n## Single GPU Training Demo\n\n### GPT-2\n<p id="GPT-2-Single" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2-GPU1.png" width=450/>\n</p>\n\n- 20x larger model size on the same hardware\n\n<p id="GPT-2-NVME" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2-NVME.png" width=800/>\n</p>\n\n- 120x larger model size on the same hardware (RTX 3080)\n\n### PaLM\n<p id="PaLM-Single" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/PaLM-GPU1.png" width=450/>\n</p>\n\n- 34x larger model size on the same hardware\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n\n## Inference\n### Colossal-Inference\n<p align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference/colossal-inference-v1-1.png" width=1000/>\n</p>\n\n<p align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference/colossal-inference-v1-2.png" width=1000/>\n</p>\n\n - Large AI models inference speed doubled, compared to the offline inference performance of vLLM in some cases.\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/colossalai/inference)\n[[blog]](https://hpc-ai.com/blog/colossal-inference)\n[[GPU Cloud Playground]](https://cloud.luchentech.com/)\n[[LLaMA3 Image]](https://cloud.luchentech.com/doc/docs/image/llama)\n\n### Grok-1\n<p id="Grok-1" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/grok-1-inference.jpg" width=600/>\n</p>\n\n - 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use Python + PyTorch + HuggingFace version for Inference.\n\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1)\n[[blog]](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)\n[[HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/grok-1)\n[[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/models/colossalai/grok-1-pytorch/summary)\n\n### SwiftInfer\n<p id="SwiftInfer" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/SwiftInfer.jpg" width=800/>\n</p>\n\n- [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance improved by 46%, open source solution breaks the length limit of LLM for multi-round conversations\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n## Installation\n\nRequirements:\n- PyTorch >= 2.2\n- Python >= 3.7\n- CUDA >= 11.0\n- [NVIDIA GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher)\n- Linux OS\n\nIf you encounter any problem with installation, you may want to raise an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.\n\n### Install from PyPI\n\nYou can easily install Colossal-AI with the following command. **By default, we do not build PyTorch extensions during installation.**\n\n```bash\npip install colossalai\n```\n\n**Note: only Linux is supported for now.**\n\nHowever, if you want to build the PyTorch extensions during installation, you can set `BUILD_EXT=1`.\n\n```bash\nBUILD_EXT=1 pip install colossalai\n```\n\n**Otherwise, CUDA kernels will be built during runtime when you actually need them.**\n\nWe also keep releasing the nightly version to PyPI every week. This allows you to access the unreleased features and bug fixes in the main branch.\nInstallation can be made via\n\n```bash\npip install colossalai-nightly\n```\n\n### Download From Source\n\n> The version of Colossal-AI will be in line with the main branch of the repository. Feel free to raise an issue if you encounter any problems. :)\n\n```shell\ngit clone https://github.com/hpcaitech/ColossalAI.git\ncd ColossalAI\n\n# install colossalai\npip install .\n```\n\nBy default, we do not compile CUDA/C++ kernels. ColossalAI will build them during runtime.\nIf you want to install and enable CUDA kernel fusion (compulsory installation when using fused optimizer):\n\n```shell\nBUILD_EXT=1 pip install .\n```\n\nFor Users with CUDA 10.2, you can still build ColossalAI from source. However, you need to manually download the cub library and copy it to the corresponding directory.\n\n```bash\n# clone the repository\ngit clone https://github.com/hpcaitech/ColossalAI.git\ncd ColossalAI\n\n# download the cub library\nwget https://github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip\nunzip 1.8.0.zip\ncp -r cub-1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/\n\n# install\nBUILD_EXT=1 pip install .\n```\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n## Use Docker\n\n### Pull from DockerHub\n\nYou can directly pull the docker image from our [DockerHub page](https://hub.docker.com/r/hpcaitech/colossalai). The image is automatically uploaded upon release.\n\n\n### Build On Your Own\n\nRun the following command to build a docker image from Dockerfile provided.\n\n> Building Colossal-AI from scratch requires GPU support, you need to use Nvidia Docker Runtime as the default when doing `docker build`. More details can be found [here](https://stackoverflow.com/questions/59691207/docker-build-with-nvidia-runtime).\n> We recommend you install Colossal-AI from our [project page](https://www.colossalai.org) directly.\n\n\n```bash\ncd ColossalAI\ndocker build -t colossalai ./docker\n```\n\nRun the following command to start the docker container in interactive mode.\n\n```bash\ndocker run -ti --gpus all --rm --ipc=host colossalai bash\n```\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n## Community\n\nJoin the Colossal-AI community on [Forum](https://github.com/hpcaitech/ColossalAI/discussions),\n[Slack](https://join.slack.com/t/colossalaiworkspace/shared_invite/zt-z7b26eeb-CBp7jouvu~r0~lcFzX832w),\nand [WeChat(å¾®ä¿¡)](https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png "qrcode") to share your suggestions, feedback, and questions with our engineering team.\n\n## Contributing\nReferring to the successful attempts of [BLOOM](https://bigscience.huggingface.co/) and [Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion), any and all developers and partners with computing powers, datasets, models are welcome to join and build the Colossal-AI community, making efforts towards the era of big AI models!\n\nYou may contact us or participate in the following ways:\n1. [Leaving a Star â­](https://github.com/hpcaitech/ColossalAI/stargazers) to show your like and support. Thanks!\n2. Posting an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose), or submitting a PR on GitHub follow the guideline in [Contributing](https://github.com/hpcaitech/ColossalAI/blob/main/CONTRIBUTING.md)\n3. Send your official proposal to email contact@hpcaitech.com\n\nThanks so much to all of our amazing contributors!\n\n<a href="https://github.com/hpcaitech/ColossalAI/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=hpcaitech/ColossalAI"  width="800px"/>\n</a>\n\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n\n## CI/CD\n\nWe leverage the power of [GitHub Actions](https://github.com/features/actions) to automate our development, release and deployment workflows. Please check out this [documentation](.github/workflows/README.md) on how the automated workflows are operated.\n\n\n## Cite Us\n\nThis project is inspired by some related projects (some by our team and some by other organizations). We would like to credit these amazing projects as listed in the [Reference List](./docs/REFERENCE.md).\n\nTo cite this project, you can use the following BibTeX citation.\n\n```\n@inproceedings{10.1145/3605573.3605613,\nauthor = {Li, Shenggui and Liu, Hongxin and Bian, Zhengda and Fang, Jiarui and Huang, Haichen and Liu, Yuliang and Wang, Boxiang and You, Yang},\ntitle = {Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training},\nyear = {2023},\nisbn = {9798400708435},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3605573.3605613},\ndoi = {10.1145/3605573.3605613},\nabstract = {The success of Transformer models has pushed the deep learning model scale to billions of parameters, but the memory limitation of a single GPU has led to an urgent need for training on multi-GPU clusters. However, the best practice for choosing the optimal parallel strategy is still lacking, as it requires domain expertise in both deep learning and parallel computing. The Colossal-AI system addressed the above challenge by introducing a unified interface to scale your sequential code of model training to distributed environments. It supports parallel training methods such as data, pipeline, tensor, and sequence parallelism and is integrated with heterogeneous training and zero redundancy optimizer. Compared to the baseline system, Colossal-AI can achieve up to 2.76 times training speedup on large-scale models.},\nbooktitle = {Proceedings of the 52nd International Conference on Parallel Processing},\npages = {766â€“775},\nnumpages = {10},\nkeywords = {datasets, gaze detection, text tagging, neural networks},\nlocation = {Salt Lake City, UT, USA},\nseries = {ICPP ''23}\n}\n```\n\nColossal-AI has been accepted as official tutorial by top conferences [NeurIPS](https://nips.cc/), [SC](https://sc22.supercomputing.org/), [AAAI](https://aaai.org/Conferences/AAAI-23/),\n[PPoPP](https://ppopp23.sigplan.org/), [CVPR](https://cvpr2023.thecvf.com/), [ISC](https://www.isc-hpc.com/), [NVIDIA GTC](https://www.nvidia.com/en-us/on-demand/session/gtcspring23-S51482/) ,etc.\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n', '{"language":"Python","stars":41289,"forks":4540,"watchers":41289,"open_issues":481,"topics":["ai","big-model","data-parallelism","deep-learning","distributed-computing","foundation-models","heterogeneous-training","hpc","inference","large-scale","model-parallelism","pipeline-parallelism"],"default_branch":"main","size_kb":66459,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:public_assets","source_url":"https://github.com/hpcaitech/public_assets"},{"type":"has_code","target_id":"github:hpcaitech:public_assets","source_url":"https://github.com/hpcaitech/public_assets"},{"type":"has_code","target_id":"github:hpcaitech:Open-Sora","source_url":"https://github.com/hpcaitech/Open-Sora"},{"type":"has_code","target_id":"github:hpcaitech:Open-Sora","source_url":"https://github.com/hpcaitech/Open-Sora"},{"type":"has_code","target_id":"github:hpcaitech:Open-Sora","source_url":"https://github.com/hpcaitech/Open-Sora"},{"type":"has_code","target_id":"github:hpcaitech:Open-Sora","source_url":"https://github.com/hpcaitech/Open-Sora?tab=readme-ov-file#model-weights"},{"type":"has_code","target_id":"github:hpcaitech:Open-Sora","source_url":"https://github.com/hpcaitech/Open-Sora?tab=readme-ov-file#-latest-demo"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:CompVis:stable-diffusion","source_url":"https://github.com/CompVis/stable-diffusion"},{"type":"has_code","target_id":"github:Stability-AI:stablediffusion","source_url":"https://github.com/Stability-AI/stablediffusion"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:FastFold","source_url":"https://github.com/hpcaitech/FastFold"},{"type":"has_code","target_id":"github:hpcaitech:FastFold","source_url":"https://github.com/hpcaitech/FastFold"},{"type":"has_code","target_id":"github:biomap-research:xTrimoMultimer","source_url":"https://github.com/biomap-research/xTrimoMultimer"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:PaLM-colossalai","source_url":"https://github.com/hpcaitech/PaLM-colossalai"},{"type":"has_code","target_id":"github:facebookresearch:metaseq","source_url":"https://github.com/facebookresearch/metaseq"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:CachedEmbedding","source_url":"https://github.com/hpcaitech/CachedEmbedding"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:SwiftInfer","source_url":"https://github.com/hpcaitech/SwiftInfer"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI.git","source_url":"https://github.com/hpcaitech/ColossalAI.git"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI.git","source_url":"https://github.com/hpcaitech/ColossalAI.git"},{"type":"has_code","target_id":"github:NVIDIA:cub","source_url":"https://github.com/NVIDIA/cub"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:features:actions","source_url":"https://github.com/features/actions"}]', NULL, 'Apache-2.0', 'approved', 80, 'ef5c449867797e26cd61ab916b071a34', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-hpcaitech-ColossalAI from https://github.com/hpcaitech.png
Image converted to WebP: data/images/github-hpcaitech-ColossalAI.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-floodsung-Deep-Learning-Papers-Reading-Roadmap', 'github--floodsung--deep-learning-papers-reading-roadmap', 'Deep-Learning-Papers-Reading-Roadmap', 'floodsung', '>If you are a newcomer to the Deep Learning area, the first question you may have is "Which paper should I start reading from?" >Here is a reading roadmap of Deep Learning papers! The roadmap is constructed in accordance with the following four guidelines: - From outline to detail - From old to state-of-the-art - from generic to specific areas - focus on state-of-the-art You will find many papers that are quite new but really worth reading. I would continue adding papers to this roadmap. ----...', '["deep-learning","python"]', 'other', 39379, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Deep Learning Papers Reading Roadmap\n\n>If you are a newcomer to the Deep Learning area, the first question you may have is "Which paper should I start reading from?"\n\n>Here is a reading roadmap of Deep Learning papers!\n\nThe roadmap is constructed in accordance with the following four guidelines:\n\n- From outline to detail\n- From old to state-of-the-art\n- from generic to specific areas\n- focus on state-of-the-art\n\nYou will find many papers that are quite new but really worth reading.\n\nI would continue adding papers to this roadmap.\n\n\n---------------------------------------\n\n# 1 Deep Learning History and Basics\n\n## 1.0 Book\n\n**[0]** Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. "**Deep learning**." An MIT Press book. (2015). [[html]](http://www.deeplearningbook.org/) **(Deep Learning Bible, you can read this book while reading following papers.)** :star::star::star::star::star:\n\n## 1.1 Survey\n\n**[1]** LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. "**Deep learning**." Nature 521.7553 (2015): 436-444. [[pdf]](http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf) **(Three Giants'' Survey)** :star::star::star::star::star:\n\n## 1.2 Deep Belief Network(DBN)(Milestone of Deep Learning Eve)\n\n**[2]** Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. "**A fast learning algorithm for deep belief nets**." Neural computation 18.7 (2006): 1527-1554. [[pdf]](http://www.cs.toronto.edu/~hinton/absps/ncfast.pdf)**(Deep Learning Eve)** :star::star::star:\n\n**[3]** Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. "**Reducing the dimensionality of data with neural networks**." Science 313.5786 (2006): 504-507. [[pdf]](http://www.cs.toronto.edu/~hinton/science.pdf) **(Milestone, Show the promise of deep learning)** :star::star::star:\n\n## 1.3 ImageNet Evolutionï¼ˆDeep Learning broke out from hereï¼‰\n\n**[4]** Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "**Imagenet classification with deep convolutional neural networks**." Advances in neural information processing systems. 2012. [[pdf]](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) **(AlexNet, Deep Learning Breakthrough)** :star::star::star::star::star:\n\n**[5]** Simonyan, Karen, and Andrew Zisserman. "**Very deep convolutional networks for large-scale image recognition**." arXiv preprint arXiv:1409.1556 (2014). [[pdf]](https://arxiv.org/pdf/1409.1556.pdf) **(VGGNet,Neural Networks become very deep!)** :star::star::star:\n\n**[6]** Szegedy, Christian, et al. "**Going deeper with convolutions**." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf) **(GoogLeNet)** :star::star::star:\n\n**[7]** He, Kaiming, et al. "**Deep residual learning for image recognition**." arXiv preprint arXiv:1512.03385 (2015). [[pdf]](https://arxiv.org/pdf/1512.03385.pdf) **(ResNet,Very very deep networks, CVPR best paper)** :star::star::star::star::star:\n\n## 1.4 Speech Recognition Evolution\n\n**[8]** Hinton, Geoffrey, et al. "**Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups**." IEEE Signal Processing Magazine 29.6 (2012): 82-97. [[pdf]](http://cs224d.stanford.edu/papers/maas_paper.pdf) **(Breakthrough in speech recognition)**:star::star::star::star:\n\n**[9]** Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. "**Speech recognition with deep recurrent neural networks**." 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [[pdf]](http://arxiv.org/pdf/1303.5778.pdf) **(RNN)**:star::star::star:\n\n**[10]** Graves, Alex, and Navdeep Jaitly. "**Towards End-To-End Speech Recognition with Recurrent Neural Networks**." ICML. Vol. 14. 2014. [[pdf]](http://www.jmlr.org/proceedings/papers/v32/graves14.pdf):star::star::star:\n\n**[11]** Sak, HaÅŸim, et al. "**Fast and accurate recurrent neural network acoustic models for speech recognition**." arXiv preprint arXiv:1507.06947 (2015). [[pdf]](http://arxiv.org/pdf/1507.06947) **(Google Speech Recognition System)** :star::star::star:\n\n**[12]** Amodei, Dario, et al. "**Deep speech 2: End-to-end speech recognition in english and mandarin**." arXiv preprint arXiv:1512.02595 (2015). [[pdf]](https://arxiv.org/pdf/1512.02595.pdf) **(Baidu Speech Recognition System)** :star::star::star::star:\n\n**[13]** W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig "**Achieving Human Parity in Conversational Speech Recognition**." arXiv preprint arXiv:1610.05256 (2016). [[pdf]](https://arxiv.org/pdf/1610.05256v1) **(State-of-the-art in speech recognition, Microsoft)** :star::star::star::star:\n\n>After reading above papers, you will have a basic understanding of the Deep Learning history, the basic architectures of Deep Learning model(including CNN, RNN, LSTM) and how deep learning can be applied to image and speech recognition issues. The following papers will take you in-depth understanding of the Deep Learning method, Deep Learning in different areas of application and the frontiers. I suggest that you can choose the following papers based on your interests and research direction.\n\n#2 Deep Learning Method\n\n## 2.1 Model\n\n**[14]** Hinton, Geoffrey E., et al. "**Improving neural networks by preventing co-adaptation of feature detectors**." arXiv preprint arXiv:1207.0580 (2012). [[pdf]](https://arxiv.org/pdf/1207.0580.pdf) **(Dropout)** :star::star::star:\n\n**[15]** Srivastava, Nitish, et al. "**Dropout: a simple way to prevent neural networks from overfitting**." Journal of Machine Learning Research 15.1 (2014): 1929-1958. [[pdf]](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) :star::star::star:\n\n**[16]** Ioffe, Sergey, and Christian Szegedy. "**Batch normalization: Accelerating deep network training by reducing internal covariate shift**." arXiv preprint arXiv:1502.03167 (2015). [[pdf]](http://arxiv.org/pdf/1502.03167) **(An outstanding Work in 2015)** :star::star::star::star:\n\n**[17]** Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. "**Layer normalization**." arXiv preprint arXiv:1607.06450 (2016). [[pdf]](https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&utm_medium=refer&utm_campaign=promote) **(Update of Batch Normalization)** :star::star::star::star:\n\n**[18]** Courbariaux, Matthieu, et al. "**Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 orâˆ’1**." [[pdf]](https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf) **(New Model,Fast)**  :star::star::star:\n\n**[19]** Jaderberg, Max, et al. "**Decoupled neural interfaces using synthetic gradients**." arXiv preprint arXiv:1608.05343 (2016). [[pdf]](https://arxiv.org/pdf/1608.05343) **(Innovation of Training Method,Amazing Work)** :star::star::star::star::star:\n\n**[20]** Chen, Tianqi, Ian Goodfellow, and Jonathon Shlens. "Net2net: Accelerating learning via knowledge transfer." arXiv preprint arXiv:1511.05641 (2015). [[pdf]](https://arxiv.org/abs/1511.05641) **(Modify previously trained network to reduce training epochs)** :star::star::star:\n\n**[21]** Wei, Tao, et al. "Network Morphism." arXiv preprint arXiv:1603.01670 (2016). [[pdf]](https://arxiv.org/abs/1603.01670) **(Modify previously trained network to reduce training epochs)** :star::star::star:\n\n## 2.2 Optimization\n\n**[22]** Sutskever, Ilya, et al. "**On the importance of initialization and momentum in deep learning**." ICML (3) 28 (2013): 1139-1147. [[pdf]](http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf) **(Momentum optimizer)** :star::star:\n\n**[23]** Kingma, Diederik, and Jimmy Ba. "**Adam: A method for stochastic optimization**." arXiv preprint arXiv:1412.6980 (2014). [[pdf]](http://arxiv.org/pdf/1412.6980) **(Maybe used most often currently)** :star::star::star:\n\n**[24]** Andrychowicz, Marcin, et al. "**Learning to learn by gradient descent by gradient descent**." arXiv preprint arXiv:1606.04474 (2016). [[pdf]](https://arxiv.org/pdf/1606.04474) **(Neural Optimizer,Amazing Work)** :star::star::star::star::star:\n\n**[25]** Han, Song, Huizi Mao, and William J. Dally. "**Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding**." CoRR, abs/1510.00149 2 (2015). [[pdf]](https://pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf) **(ICLR best paper, new direction to make NN running fast,DeePhi Tech Startup)** :star::star::star::star::star:\n\n**[26]** Iandola, Forrest N., et al. "**SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 1MB model size**." arXiv preprint arXiv:1602.07360 (2016). [[pdf]](http://arxiv.org/pdf/1602.07360) **(Also a new direction to optimize NN,DeePhi Tech Startup)** :star::star::star::star:\n\n**[27]** Glorat Xavier, Bengio Yoshua, et al. "**Understanding the difficulty of training deep forward neural networks**." Proceedings of the thirteenth International Conference on Artificial Intelligence and Statistics, PMLR 9:249-256,2010. [[pdf]](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) :star::star::star::star:\n\n## 2.3 Unsupervised Learning / Deep Generative Model\n\n**[28]** Le, Quoc V. "**Building high-level features using large scale unsupervised learning**." 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [[pdf]](http://arxiv.org/pdf/1112.6209.pdf&embed) **(Milestone, Andrew Ng, Google Brain Project, Cat)** :star::star::star::star:\n\n\n**[29]** Kingma, Diederik P., and Max Welling. "**Auto-encoding variational bayes**." arXiv preprint arXiv:1312.6114 (2013). [[pdf]](http://arxiv.org/pdf/1312.6114) **(VAE)** :star::star::star::star:\n\n**[30]** Goodfellow, Ian, et al. "**Generative adversarial nets**." Advances in Neural Information Processing Systems. 2014. [[pdf]](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) **(GAN,super cool idea)** :star::star::star::star::star:\n\n**[31]** Radford, Alec, Luke Metz, and Soumith Chintala. "**Unsupervised representation learning with deep convolutional generative adversarial networks**." arXiv preprint arXiv:1511.06434 (2015). [[pdf]](http://arxiv.org/pdf/1511.06434) **(DCGAN)** :star::star::star::star:\n\n**[32]** Gregor, Karol, et al. "**DRAW: A recurrent neural network for image generation**." arXiv preprint arXiv:1502.04623 (2015). [[pdf]](http://jmlr.org/proceedings/papers/v37/gregor15.pdf) **(VAE with attention, outstanding work)** :star::star::star::star::star:\n\n**[33]** Oord, Aaron van den, Nal Kalchbrenner, and Koray Kavukcuoglu. "**Pixel recurrent neural networks**." arXiv preprint arXiv:1601.06759 (2016). [[pdf]](http://arxiv.org/pdf/1601.06759) **(PixelRNN)** :star::star::star::star:\n\n**[34]** Oord, Aaron van den, et al. "Conditional image generation with PixelCNN decoders." arXiv preprint arXiv:1606.05328 (2016). [[pdf]](https://arxiv.org/pdf/1606.05328) **(PixelCNN)** :star::star::star::star:\n\n**[34]** S. Mehri et al., "**SampleRNN: An Unconditional End-to-End Neural Audio Generation Model**." arXiv preprint 	arXiv:1612.07837 (2016). [[pdf]](https://arxiv.org/pdf/1612.07837.pdf) :star::star::star::star::star:\n\n## 2.4 RNN / Sequence-to-Sequence Model\n\n**[35]** Graves, Alex. "**Generating sequences with recurrent neural networks**." arXiv preprint arXiv:1308.0850 (2013). [[pdf]](http://arxiv.org/pdf/1308.0850) **(LSTM, very nice generating result, show the power of RNN)** :star::star::star::star:\n\n**[36]** Cho, Kyunghyun, et al. "**Learning phrase representations using RNN encoder-decoder for statistical machine translation**." arXiv preprint arXiv:1406.1078 (2014). [[pdf]](http://arxiv.org/pdf/1406.1078) **(First Seq-to-Seq Paper)** :star::star::star::star:\n\n**[37]** Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. "**Sequence to sequence learning with neural networks**." Advances in neural information processing systems. 2014. [[pdf]](https://arxiv.org/pdf/1409.3215.pdf) **(Outstanding Work)** :star::star::star::star::star:\n\n**[38]** Bahdanau, Dzmitry, KyungHyun Cho, and Yoshua Bengio. "**Neural Machine Translation by Jointly Learning to Align and Translate**." arXiv preprint arXiv:1409.0473 (2014). [[pdf]](https://arxiv.org/pdf/1409.0473v7.pdf) :star::star::star::star:\n\n**[39]** Vinyals, Oriol, and Quoc Le. "**A neural conversational model**." arXiv preprint arXiv:1506.05869 (2015). [[pdf]](http://arxiv.org/pdf/1506.05869.pdf%20(http://arxiv.org/pdf/1506.05869.pdf)) **(Seq-to-Seq on Chatbot)** :star::star::star:\n\n## 2.5 Neural Turing Machine\n\n**[40]** Graves, Alex, Greg Wayne, and Ivo Danihelka. "**Neural turing machines**." arXiv preprint arXiv:1410.5401 (2014). [[pdf]](http://arxiv.org/pdf/1410.5401.pdf) **(Basic Prototype of Future Computer)** :star::star::star::star::star:\n\n**[41]** Zaremba, Wojciech, and Ilya Sutskever. "**Reinforcement learning neural Turing machines**." arXiv preprint arXiv:1505.00521 362 (2015). [[pdf]](https://pdfs.semanticscholar.org/f10e/071292d593fef939e6ef4a59baf0bb3a6c2b.pdf) :star::star::star:\n\n**[42]** Weston, Jason, Sumit Chopra, and Antoine Bordes. "**Memory networks**." arXiv preprint arXiv:1410.3916 (2014). [[pdf]](http://arxiv.org/pdf/1410.3916) :star::star::star:\n\n\n**[43]** Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. "**End-to-end memory networks**." Advances in neural information processing systems. 2015. [[pdf]](http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf) :star::star::star::star:\n\n**[44]** Vinyals, Oriol, Meire Fortunato, and Navdeep Jaitly. "**Pointer networks**." Advances in Neural Information Processing Systems. 2015. [[pdf]](http://papers.nips.cc/paper/5866-pointer-networks.pdf) :star::star::star::star:\n\n**[45]** Graves, Alex, et al. "**Hybrid computing using a neural network with dynamic external memory**." Nature (2016). [[pdf]](https://www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf) **(Milestone,combine above papers'' ideas)** :star::star::star::star::star:\n\n## 2.6 Deep Reinforcement Learning\n\n**[46]** Mnih, Volodymyr, et al. "**Playing atari with deep reinforcement learning**." arXiv preprint arXiv:1312.5602 (2013). [[pdf]](http://arxiv.org/pdf/1312.5602.pdf)) **(First Paper named deep reinforcement learning)** :star::star::star::star:\n\n**[47]** Mnih, Volodymyr, et al. "**Human-level control through deep reinforcement learning**." Nature 518.7540 (2015): 529-533. [[pdf]](https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf) **(Milestone)** :star::star::star::star::star:\n\n**[48]** Wang, Ziyu, Nando de Freitas, and Marc Lanctot. "**Dueling network architectures for deep reinforcement learning**." arXiv preprint arXiv:1511.06581 (2015). [[pdf]](http://arxiv.org/pdf/1511.06581) **(ICLR best paper,great idea)**  :star::star::star::star:\n\n**[49]** Mnih, Volodymyr, et al. "**Asynchronous methods for deep reinforcement learning**." arXiv preprint arXiv:1602.01783 (2016). [[pdf]](http://arxiv.org/pdf/1602.01783) **(State-of-the-art method)** :star::star::star::star::star:\n\n**[50]** Lillicrap, Timothy P., et al. "**Continuous control with deep reinforcement learning**." arXiv preprint arXiv:1509.02971 (2015). [[pdf]](http://arxiv.org/pdf/1509.02971) **(DDPG)** :star::star::star::star:\n\n**[51]** Gu, Shixiang, et al. "**Continuous Deep Q-Learning with Model-based Acceleration**." arXiv preprint arXiv:1603.00748 (2016). [[pdf]](http://arxiv.org/pdf/1603.00748) **(NAF)** :star::star::star::star:\n\n**[52]** Schulman, John, et al. "**Trust region policy optimization**." CoRR, abs/1502.05477 (2015). [[pdf]](http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf) **(TRPO)** :star::star::star::star:\n\n**[53]** Silver, David, et al. "**Mastering the game of Go with deep neural networks and tree search**." Nature 529.7587 (2016): 484-489. [[pdf]](http://willamette.edu/~levenick/cs448/goNature.pdf) **(AlphaGo)** :star::star::star::star::star:\n\n## 2.7 Deep Transfer Learning / Lifelong Learning / especially for RL\n\n**[54]** Bengio, Yoshua. "**Deep Learning of Representations for Unsupervised and Transfer Learning**." ICML Unsupervised and Transfer Learning 27 (2012): 17-36. [[pdf]](http://www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf) **(A Tutorial)** :star::star::star:\n\n**[55]** Silver, Daniel L., Qiang Yang, and Lianghao Li. "**Lifelong Machine Learning Systems: Beyond Learning Algorithms**." AAAI Spring Symposium: Lifelong Machine Learning. 2013. [[pdf]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.696.7800&rep=rep1&type=pdf) **(A brief discussion about lifelong learning)**  :star::star::star:\n\n**[56]** Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. "**Distilling the knowledge in a neural network**." arXiv preprint arXiv:1503.02531 (2015). [[pdf]](http://arxiv.org/pdf/1503.02531) **(Godfather''s Work)** :star::star::star::star:\n\n**[57]** Rusu, Andrei A., et al. "**Policy distillation**." arXiv preprint arXiv:1511.06295 (2015). [[pdf]](http://arxiv.org/pdf/1511.06295) **(RL domain)** :star::star::star:\n\n**[58]** Parisotto, Emilio, Jimmy Lei Ba, and Ruslan Salakhutdinov. "**Actor-mimic: Deep multitask and transfer reinforcement learning**." arXiv preprint arXiv:1511.06342 (2015). [[pdf]](http://arxiv.org/pdf/1511.06342) **(RL domain)** :star::star::star:\n\n**[59]** Rusu, Andrei A., et al. "**Progressive neural networks**." arXiv preprint arXiv:1606.04671 (2016). [[pdf]](https://arxiv.org/pdf/1606.04671) **(Outstanding Work, A novel idea)** :star::star::star::star::star:\n\n\n## 2.8 One Shot Deep Learning\n\n**[60]** Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. "**Human-level concept learning through probabilistic program induction**." Science 350.6266 (2015): 1332-1338. [[pdf]](http://clm.utexas.edu/compjclub/wp-content/uploads/2016/02/lake2015.pdf) **(No Deep Learning,but worth reading)** :star::star::star::star::star:\n\n**[61]** Koch, Gregory, Richard Zemel, and Ruslan Salakhutdinov. "**Siamese Neural Networks for One-shot Image Recognition**."(2015) [[pdf]](http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf) :star::star::star:\n\n**[62]** Santoro, Adam, et al. "**One-shot Learning with Memory-Augmented Neural Networks**." arXiv preprint arXiv:1605.06065 (2016). [[pdf]](http://arxiv.org/pdf/1605.06065) **(A basic step to one shot learning)** :star::star::star::star:\n\n**[63]** Vinyals, Oriol, et al. "**Matching Networks for One Shot Learning**." arXiv preprint arXiv:1606.04080 (2016). [[pdf]](https://arxiv.org/pdf/1606.04080) :star::star::star:\n\n**[64]** Hariharan, Bharath, and Ross Girshick. "**Low-shot visual object recognition**." arXiv preprint arXiv:1606.02819 (2016). [[pdf]](http://arxiv.org/pdf/1606.02819) **(A step to large data)** :star::star::star::star:\n\n\n# 3 Applications\n\n## 3.1 NLP(Natural Language Processing)\n\n**[1]** Antoine Bordes, et al. "**Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing**." AISTATS(2012) [[pdf]](https://www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php?id=en%3Apubli&cache=cache&media=en:bordes12aistats.pdf) :star::star::star::star:\n\n**[2]** Mikolov, et al. "**Distributed representations of words and phrases and their compositionality**." ANIPS(2013): 3111-3119 [[pdf]](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) **(word2vec)** :star::star::star:\n\n**[3]** Sutskever, et al. "**â€œSequence to sequence learning with neural networks**." ANIPS(2014) [[pdf]](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) :star::star::star:\n\n**[4]** Ankit Kumar, et al. "**â€œAsk Me Anything: Dynamic Memory Networks for Natural Language Processing**." arXiv preprint arXiv:1506.07285(2015) [[pdf]](https://arxiv.org/abs/1506.07285) :star::star::star::star:\n\n**[5]** Yoon Kim, et al. "**Character-Aware Neural Language Models**." NIPS(2015) arXiv preprint arXiv:1508.06615(2015) [[pdf]](https://arxiv.org/abs/1508.06615) :star::star::star::star:\n\n**[6]** Jason Weston, et al. "**Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks**." arXiv preprint arXiv:1502.05698(2015) [[pdf]](https://arxiv.org/abs/1502.05698) **(bAbI tasks)** :star::star::star:\n\n**[7]** Karl Moritz Hermann, et al. "**Teaching Machines to Read and Comprehend**." arXiv preprint arXiv:1506.03340(2015) [[pdf]](https://arxiv.org/abs/1506.03340) **(CNN/DailyMail cloze style questions)** :star::star:\n\n**[8]** Alexis Conneau, et al. "**Very Deep Convolutional Networks for Natural Language Processing**." arXiv preprint arXiv:1606.01781(2016) [[pdf]](https://arxiv.org/abs/1606.01781) **(state-of-the-art in text classification)** :star::star::star:\n\n**[9]** Armand Joulin, et al. "**Bag of Tricks for Efficient Text Classification**." arXiv preprint arXiv:1607.01759(2016) [[pdf]](https://arxiv.org/abs/1607.01759) **(slightly worse than state-of-the-art, but a lot faster)** :star::star::star:\n\n## 3.2 Object Detection\n\n**[1]** Szegedy, Christian, Alexander Toshev, and Dumitru Erhan. "**Deep neural networks for object detection**." Advances in Neural Information Processing Systems. 2013. [[pdf]](http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf) :star::star::star:\n\n**[2]** Girshick, Ross, et al. "**Rich feature hierarchies for accurate object detection and semantic segmentation**." Proceedings of the IEEE conference on computer vision and pattern recognition. 2014. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf) **(RCNN)** :star::star::star::star::star:\n\n**[3]** He, Kaiming, et al. "**Spatial pyramid pooling in deep convolutional networks for visual recognition**." European Conference on Computer Vision. Springer International Publishing, 2014. [[pdf]](http://arxiv.org/pdf/1406.4729) **(SPPNet)** :star::star::star::star:\n\n**[4]** Girshick, Ross. "**Fast r-cnn**." Proceedings of the IEEE International Conference on Computer Vision. 2015. [[pdf]](https://pdfs.semanticscholar.org/8f67/64a59f0d17081f2a2a9d06f4ed1cdea1a0ad.pdf) :star::star::star::star:\n\n**[5]** Ren, Shaoqing, et al. "**Faster R-CNN: Towards real-time object detection with region proposal networks**." Advances in neural information processing systems. 2015. [[pdf]](https://arxiv.org/pdf/1506.01497.pdf) :star::star::star::star:\n\n**[6]** Redmon, Joseph, et al. "**You only look once: Unified, real-time object detection**." arXiv preprint arXiv:1506.02640 (2015). [[pdf]](http://homes.cs.washington.edu/~ali/papers/YOLO.pdf) **(YOLO,Oustanding Work, really practical)** :star::star::star::star::star:\n\n**[7]** Liu, Wei, et al. "**SSD: Single Shot MultiBox Detector**." arXiv preprint arXiv:1512.02325 (2015). [[pdf]](http://arxiv.org/pdf/1512.02325) :star::star::star:\n\n**[8]** Dai, Jifeng, et al. "**R-FCN: Object Detection via\nRegion-based Fully Convolutional Networks**." arXiv preprint arXiv:1605.06409 (2016). [[pdf]](https://arxiv.org/abs/1605.06409) :star::star::star::star:\n\n**[9]** He, Gkioxari, et al. "**Mask R-CNN**" arXiv preprint arXiv:1703.06870 (2017). [[pdf]](https://arxiv.org/abs/1703.06870) :star::star::star::star:\n\n**[10]** Bochkovskiy, Alexey, et al. "**YOLOv4: Optimal Speed and Accuracy of Object Detection.**"  arXiv preprint arXiv:2004.10934 (2020). [[pdf]](https://arxiv.org/pdf/2004.10934) :star::star::star::star:\n\n\n**[11]** Tan, Mingxing, et al. â€œ**EfficientDet: Scalable and Efficient Object Detection.**" arXiv preprint arXiv:1911.09070 (2019). [[pdf]](https://arxiv.org/pdf/1911.09070) :star::star::star::star::star:\n\n\n## 3.3 Visual Tracking\n\n**[1]** Wang, Naiyan, and Dit-Yan Yeung. "**Learning a deep compact image representation for visual tracking**." Advances in neural information processing systems. 2013. [[pdf]](http://papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf) **(First Paper to do visual tracking using Deep Learning,DLT Tracker)** :star::star::star:\n\n**[2]** Wang, Naiyan, et al. "**Transferring rich feature hierarchies for robust visual tracking**." arXiv preprint arXiv:1501.04587 (2015). [[pdf]](http://arxiv.org/pdf/1501.04587) **(SO-DLT)** :star::star::star::star:\n\n**[3]** Wang, Lijun, et al. "**Visual tracking with fully convolutional networks**." Proceedings of the IEEE International Conference on Computer Vision. 2015. [[pdf]](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf) **(FCNT)** :star::star::star::star:\n\n**[4]** Held, David, Sebastian Thrun, and Silvio Savarese. "**Learning to Track at 100 FPS with Deep Regression Networks**." arXiv preprint arXiv:1604.01802 (2016). [[pdf]](http://arxiv.org/pdf/1604.01802) **(GOTURN,Really fast as a deep learning method,but still far behind un-deep-learning methods)** :star::star::star::star:\n\n**[5]** Bertinetto, Luca, et al. "**Fully-Convolutional Siamese Networks for Object Tracking**." arXiv preprint arXiv:1606.09549 (2016). [[pdf]](https://arxiv.org/pdf/1606.09549) **(SiameseFC,New state-of-the-art for real-time object tracking)** :star::star::star::star:\n\n**[6]** Martin Danelljan, Andreas Robinson, Fahad Khan, Michael Felsberg. "**Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking**." ECCV (2016) [[pdf]](http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/C-COT_ECCV16.pdf) **(C-COT)** :star::star::star::star:\n\n**[7]** Nam, Hyeonseob, Mooyeol Baek, and Bohyung Han. "**Modeling and Propagating CNNs in a Tree Structure for Visual Tracking**." arXiv preprint arXiv:1608.07242 (2016). [[pdf]](https://arxiv.org/pdf/1608.07242) **(VOT2016 Winner,TCNN)** :star::star::star::star:\n\n## 3.4 Image Caption\n**[1]** Farhadi,Ali,etal. "**Every picture tells a story: Generating sentences from images**". In Computer VisionECCV 2010. Springer Berlin Heidelberg:15-29, 2010. [[pdf]](https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf) :star::star::star:\n\n**[2]** Kulkarni, Girish, et al. "**Baby talk: Understanding and generating image descriptions**". In Proceedings of the 24th CVPR, 2011. [[pdf]](http://tamaraberg.com/papers/generation_cvpr11.pdf):star::star::star::star:\n\n**[3]** Vinyals, Oriol, et al. "**Show and tell: A neural image caption generator**". In arXiv preprint arXiv:1411.4555, 2014. [[pdf]](https://arxiv.org/pdf/1411.4555.pdf):star::star::star:\n\n**[4]** Donahue, Jeff, et al. "**Long-term recurrent convolutional networks for visual recognition and description**". In arXiv preprint arXiv:1411.4389 ,2014. [[pdf]](https://arxiv.org/pdf/1411.4389.pdf)\n\n**[5]** Karpathy, Andrej, and Li Fei-Fei. "**Deep visual-semantic alignments for generating image descriptions**". In arXiv preprint arXiv:1412.2306, 2014. [[pdf]](https://cs.stanford.edu/people/karpathy/cvpr2015.pdf):star::star::star::star::star:\n\n**[6]** Karpathy, Andrej, Armand Joulin, and Fei Fei F. Li. "**Deep fragment embeddings for bidirectional image sentence mapping**". In Advances in neural information processing systems, 2014. [[pdf]](https://arxiv.org/pdf/1406.5679v1.pdf):star::star::star::star:\n\n**[7]** Fang, Hao, et al. "**From captions to visual concepts and back**". In arXiv preprint arXiv:1411.4952, 2014. [[pdf]](https://arxiv.org/pdf/1411.4952v3.pdf):star::star::star::star::star:\n\n**[8]** Chen, Xinlei, and C. Lawrence Zitnick. "**Learning a recurrent visual representation for image caption generation**". In arXiv preprint arXiv:1411.5654, 2014. [[pdf]](https://arxiv.org/pdf/1411.5654v1.pdf):star::star::star::star:\n\n**[9]** Mao, Junhua, et al. "**Deep captioning with multimodal recurrent neural networks (m-rnn)**". In arXiv preprint arXiv:1412.6632, 2014. [[pdf]](https://arxiv.org/pdf/1412.6632v5.pdf):star::star::star:\n\n**[10]** Xu, Kelvin, et al. "**Show, attend and tell: Neural image caption generation with visual attention**". In arXiv preprint arXiv:1502.03044, 2015. [[pdf]](https://arxiv.org/pdf/1502.03044v3.pdf):star::star::star::star::star:\n\n## 3.5 Machine Translation\n\n> Some milestone papers are listed in RNN / Seq-to-Seq topic.\n\n**[1]** Luong, Minh-Thang, et al. "**Addressing the rare word problem in neural machine translation**." arXiv preprint arXiv:1410.8206 (2014). [[pdf]](http://arxiv.org/pdf/1410.8206) :star::star::star::star:\n\n\n**[2]** Sennrich, et al. "**Neural Machine Translation of Rare Words with Subword Units**". In arXiv preprint arXiv:1508.07909, 2015. [[pdf]](https://arxiv.org/pdf/1508.07909.pdf):star::star::star:\n\n**[3]** Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. "**Effective approaches to attention-based neural machine translation**." arXiv preprint arXiv:1508.04025 (2015). [[pdf]](http://arxiv.org/pdf/1508.04025) :star::star::star::star:\n\n**[4]** Chung, et al. "**A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation**". In arXiv preprint arXiv:1603.06147, 2016. [[pdf]](https://arxiv.org/pdf/1603.06147.pdf):star::star:\n\n**[5]** Lee, et al. "**Fully Character-Level Neural Machine Translation without Explicit Segmentation**". In arXiv preprint arXiv:1610.03017, 2016. [[pdf]](https://arxiv.org/pdf/1610.03017.pdf):star::star::star::star::star:\n\n**[6]** Wu, Schuster, Chen, Le, et al. "**Google''s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation**". In arXiv preprint arXiv:1609.08144v2, 2016. [[pdf]](https://arxiv.org/pdf/1609.08144v2.pdf) **(Milestone)** :star::star::star::star:\n\n## 3.6 Robotics\n\n**[1]** KoutnÃ­k, Jan, et al. "**Evolving large-scale neural networks for vision-based reinforcement learning**." Proceedings of the 15th annual conference on Genetic and evolutionary computation. ACM, 2013. [[pdf]](http://repository.supsi.ch/4550/1/koutnik2013gecco.pdf) :star::star::star:\n\n**[2]** Levine, Sergey, et al. "**End-to-end training of deep visuomotor policies**." Journal of Machine Learning Research 17.39 (2016): 1-40. [[pdf]](http://www.jmlr.org/papers/volume17/15-522/15-522.pdf) :star::star::star::star::star:\n\n**[3]** Pinto, Lerrel, and Abhinav Gupta. "**Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours**." arXiv preprint arXiv:1509.06825 (2015). [[pdf]](http://arxiv.org/pdf/1509.06825) :star::star::star:\n\n**[4]** Levine, Sergey, et al. "**Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection**." arXiv preprint arXiv:1603.02199 (2016). [[pdf]](http://arxiv.org/pdf/1603.02199) :star::star::star::star:\n\n**[5]** Zhu, Yuke, et al. "**Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning**." arXiv preprint arXiv:1609.05143 (2016). [[pdf]](https://arxiv.org/pdf/1609.05143) :star::star::star::star:\n\n**[6]** Yahya, Ali, et al. "**Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search**." arXiv preprint arXiv:1610.00673 (2016). [[pdf]](https://arxiv.org/pdf/1610.00673) :star::star::star::star:\n\n**[7]** Gu, Shixiang, et al. "**Deep Reinforcement Learning for Robotic Manipulation**." arXiv preprint arXiv:1610.00633 (2016). [[pdf]](https://arxiv.org/pdf/1610.00633) :star::star::star::star:\n\n**[8]** A Rusu, M Vecerik, Thomas RothÃ¶rl, N Heess, R Pascanu, R Hadsell."**Sim-to-Real Robot Learning from Pixels with Progressive Nets**." arXiv preprint arXiv:1610.04286 (2016). [[pdf]](https://arxiv.org/pdf/1610.04286.pdf) :star::star::star::star:\n\n**[9]** Mirowski, Piotr, et al. "**Learning to navigate in complex environments**." arXiv preprint arXiv:1611.03673 (2016). [[pdf]](https://arxiv.org/pdf/1611.03673) :star::star::star::star:\n\n## 3.7 Art\n\n**[1]** Mordvintsev, Alexander; Olah, Christopher; Tyka, Mike (2015). "**Inceptionism: Going Deeper into Neural Networks**". Google Research. [[html]](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html) **(Deep Dream)**\n:star::star::star::star:\n\n**[2]** Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. "**A neural algorithm of artistic style**." arXiv preprint arXiv:1508.06576 (2015). [[pdf]](http://arxiv.org/pdf/1508.06576) **(Outstanding Work, most successful method currently)** :star::star::star::star::star:\n\n**[3]** Zhu, Jun-Yan, et al. "**Generative Visual Manipulation on the Natural Image Manifold**." European Conference on Computer Vision. Springer International Publishing, 2016. [[pdf]](https://arxiv.org/pdf/1609.03552) **(iGAN)** :star::star::star::star:\n\n**[4]** Champandard, Alex J. "**Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks**." arXiv preprint arXiv:1603.01768 (2016). [[pdf]](http://arxiv.org/pdf/1603.01768) **(Neural Doodle)** :star::star::star::star:\n\n**[5]** Zhang, Richard, Phillip Isola, and Alexei A. Efros. "**Colorful Image Colorization**." arXiv preprint arXiv:1603.08511 (2016). [[pdf]](http://arxiv.org/pdf/1603.08511) :star::star::star::star:\n\n**[6]** Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. "**Perceptual losses for real-time style transfer and super-resolution**." arXiv preprint arXiv:1603.08155 (2016). [[pdf]](https://arxiv.org/pdf/1603.08155.pdf) :star::star::star::star:\n\n**[7]** Vincent Dumoulin, Jonathon Shlens and Manjunath Kudlur. "**A learned representation for artistic style**." arXiv preprint arXiv:1610.07629 (2016). [[pdf]](https://arxiv.org/pdf/1610.07629v1.pdf) :star::star::star::star:\n\n**[8]** Gatys, Leon and Ecker, et al."**Controlling Perceptual Factors in Neural Style Transfer**." arXiv preprint arXiv:1611.07865 (2016). [[pdf]](https://arxiv.org/pdf/1611.07865.pdf) **(control style transfer over spatial location,colour information and across spatial scale)**:star::star::star::star:\n\n**[9]** Ulyanov, Dmitry and Lebedev, Vadim, et al. "**Texture Networks: Feed-forward Synthesis of Textures and Stylized Images**." arXiv preprint arXiv:1603.03417(2016). [[pdf]](http://arxiv.org/abs/1603.03417) **(texture generation and style transfer)** :star::star::star::star:\n\n**[10]** Yijun Li, Ming-Yu Liu ,Xueting Li, Ming-Hsuan Yang,Jan Kautz (NVIDIA). "**A Closed-form Solution to Photorealistic Image Stylization**." arXiv preprint arXiv:1802.06474(2018). [[pdf]](https://arxiv.org/pdf/1802.06474.pdf) **(Very fast and ultra realistic style transfer)** :star::star::star::star:\n\n## 3.8 Object Segmentation\n\n**[1]** J. Long, E. Shelhamer, and T. Darrell, â€œ**Fully convolutional networks for semantic segmentation**.â€ in CVPR, 2015. [[pdf]](https://arxiv.org/pdf/1411.4038v2.pdf) :star::star::star::star::star:\n\n**[2]** L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. "**Semantic image segmentation with deep convolutional nets and fully connected crfs**." In ICLR, 2015. [[pdf]](https://arxiv.org/pdf/1606.00915v1.pdf) :star::star::star::star::star:\n\n**[3]** Pinheiro, P.O., Collobert, R., Dollar, P. "**Learning to segment object candidates.**" In: NIPS. 2015. [[pdf]](https://arxiv.org/pdf/1506.06204v2.pdf) :star::star::star::star:\n\n**[4]** Dai, J., He, K., Sun, J. "**Instance-aware semantic segmentation via multi-task network cascades**." in CVPR. 2016 [[pdf]](https://arxiv.org/pdf/1512.04412v1.pdf) :star::star::star:\n\n**[5]** Dai, J., He, K., Sun, J. "**Instance-sensitive Fully Convolutional Networks**." arXiv preprint arXiv:1603.08678 (2016). [[pdf]](https://arxiv.org/pdf/1603.08678v1.pdf) :star::star::star:\n\n\n', '{"language":"Python","stars":39379,"forks":7339,"watchers":39379,"open_issues":91,"topics":["deep-learning"],"default_branch":"master","size_kb":3638,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[]', NULL, NULL, 'pending', 70, '33c0664ff7300dbf71c36e2eb87e075b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-floodsung-Deep-Learning-Papers-Reading-Roadmap from https://github.com/floodsung.png
Image converted to WebP: data/images/github-floodsung-Deep-Learning-Papers-Reading-Roadmap.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-naptha-tesseract.js', 'github--naptha--tesseract.js', 'tesseract.js', 'naptha', '<p align="center"> <a href="https://tesseract.projectnaptha.com/"> <picture> <source media="(prefers-color-scheme: dark)" srcset="./docs/images/tesseract_dark.png"> <img width="256px" height="256px" alt="Tesseract.js" src="./docs/images/tesseract.png"> </picture> </a> </p> !Lint & Test !CodeQL !npm !jsDelivr hits (npm) Tesseract.js is a javascript library that gets words in almost any language out of images. (Demo) Image Recognition Video Real-time Recognition <p align="center"> <a href="http...', '["deep-learning","javascript","ocr","tesseract","webassembly","javascript"]', 'other', 37560, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/naptha/tesseract.js","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <a href="https://tesseract.projectnaptha.com/">\n    <picture>\n      <source media="(prefers-color-scheme: dark)" srcset="./docs/images/tesseract_dark.png">\n      <img width="256px" height="256px" alt="Tesseract.js" src="./docs/images/tesseract.png">\n    </picture>\n  </a>\n</p>\n\n![Lint & Test](https://github.com/naptha/tesseract.js/workflows/Node.js%20CI/badge.svg)\n![CodeQL](https://github.com/naptha/tesseract.js/workflows/CodeQL/badge.svg)\n[![Gitpod Ready-to-Code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://github.com/naptha/tesseract.js) \n[![Financial Contributors on Open Collective](https://opencollective.com/tesseractjs/all/badge.svg?label=financial+contributors)](https://opencollective.com/tesseractjs) [![npm version](https://badge.fury.io/js/tesseract.js.svg)](https://badge.fury.io/js/tesseract.js)\n[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://github.com/naptha/tesseract.js/graphs/commit-activity)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Code Style](https://badgen.net/badge/code%20style/airbnb/ff5a5f?icon=airbnb)](https://github.com/airbnb/javascript)\n![npm](https://img.shields.io/npm/dm/tesseract.js?label=npm%20downloads)\n![jsDelivr hits (npm)](https://img.shields.io/jsdelivr/npm/hm/tesseract.js?label=jsdelivr%20hits)\n\nTesseract.js is a javascript library that gets words in [almost any language](./docs/tesseract_lang_list.md) out of images. ([Demo](http://tesseract.projectnaptha.com/))\n\nImage Recognition\n\n[![fancy demo gif](./docs/images/demo.gif)](http://tesseract.projectnaptha.com)\n\nVideo Real-time Recognition\n\n<p align="center">\n  <a href="https://github.com/jeromewu/tesseract.js-video"><img alt="Tesseract.js Video" src="./docs/images/video-demo.gif"></a>\n</p>\n\nTesseract.js works in the browser using [webpack](https://webpack.js.org/), esm, or plain script tags with a [CDN](#CDN) and on the server with [Node.js](https://nodejs.org/en/).\nAfter you [install it](#installation), using it is as simple as:\n\n```javascript\nimport { createWorker } from ''tesseract.js'';\n\n(async () => {\n  const worker = await createWorker(''eng'');\n  const ret = await worker.recognize(''https://tesseract.projectnaptha.com/img/eng_bw.png'');\n  console.log(ret.data.text);\n  await worker.terminate();\n})();\n```\nWhen recognizing multiple images, users should create a worker once, run `worker.recognize` for each image, and then run `worker.terminate()` once at the end (rather than running the above snippet for every image). \n\n## Installation\nTesseract.js works with a `<script>` tag via local copy or CDN, with webpack via `npm` and on Node.js with `npm/yarn`.\n\n### CDN\n```html\n<!-- v5 -->\n<script src=''https://cdn.jsdelivr.net/npm/tesseract.js@5/dist/tesseract.min.js''></script>\n```\nAfter including the script the `Tesseract` variable will be globally available and a worker can be created using `Tesseract.createWorker`.\n\nAlternatively, an ESM build (used with `import` syntax) can be found at `https://cdn.jsdelivr.net/npm/tesseract.js@5/dist/tesseract.esm.min.js`. \n\n### Node.js\n\n**Requires Node.js v14 or higher**\n\n```shell\n# For latest version\nnpm install tesseract.js\nyarn add tesseract.js\n\n# For old versions\nnpm install tesseract.js@3.0.3\nyarn add tesseract.js@3.0.3\n```\n\n## Project Scope\nTesseract.js aims to bring the [Tesseract](https://github.com/tesseract-ocr/tesseract) OCR engine (a separate project) to the browser and Node.js, and works by wrapping a [WebAssembly port](https://github.com/naptha/tesseract.js-core) of Tesseract.  This project does not modify core Tesseract features.  Most notably, **Tesseract.js does not support PDF files and does not modify the Tesseract recognition model to improve accuracy.**\n\nIf your project requires features outside of this scope, consider the [Scribe.js library](https://github.com/scribeocr/scribe.js).  Scribe.js is an alternative library created to accommodate common feature requests that are outside of the scope of this repo.  Scribe.js includes improvements to the Tesseract recognition model and supports extracting text from PDF documents, among other features.  For more information see [Scribe.js vs. Tesseract.js](https://github.com/scribeocr/scribe.js/blob/master/docs/scribe_vs_tesseract.md).\n\n## Documentation\n\n* [Workers vs. Schedulers](./docs/workers_vs_schedulers.md)\n* [Examples](./docs/examples.md)\n* [Supported Image Formats](./docs/image-format.md)\n* [API](./docs/api.md)\n* [Local Installation](./docs/local-installation.md)\n* [FAQ](./docs/faq.md)\n\n## Community Projects and Examples\nThe following are examples and projects built by the community using Tesseract.js. Officially supported examples are found in the [examples](https://github.com/naptha/tesseract.js/tree/master/examples) directory. \n\n- Projects\n   - Scribe OCR: web application for scanning documents (images and PDFs)\n      - Site at [scribeocr.com](https://scribeocr.com/), repo at [github.com/scribeocr/scribeocr](https://github.com/scribeocr/scribeocr)\n   - Chrome Extension (with Manifest V3): https://github.com/Tshetrim/Image-To-Text-OCR-extension-for-ChatGPT\n- Examples\n   - Converting PDF to text: https://github.com/racosa/pdf2text-ocr\n   - Use `blocks` output to generate granular data [word/symbol level]: https://github.com/Kishlay-notabot/tesseract-bbox-examples\n   - Electron: https://github.com/Balearica/tesseract.js-electron\n   - Typescript: https://github.com/Balearica/tesseract.js-typescript\n \nIf you have a project or example repo that uses Tesseract.js, feel free to add it to this list using a pull request. Examples submitted should be well documented such that new users can run them; projects should be functional and actively maintained.\n\n## Major changes in v6\nVersion 6 changes are documented in [this issue](https://github.com/naptha/tesseract.js/issues/993).  Highlights are below.\n - Fixed memory leak in previous versions\n - Overall reductions in runtime and memory usage\n - Breaking changes:\n    - All outputs formats other than `text` are disabled by default.\n      - To re-enable the `hocr` output (for example), set the following: `worker.recognize(image, {}, { hocr: true })`\n    - Minor changes to the structure of the JavaScript object (`blocks`) output\n    - See [this issue](https://github.com/naptha/tesseract.js/issues/993) for full list\n\n## Major changes in v5\nVersion 5 changes are documented in [this issue](https://github.com/naptha/tesseract.js/issues/820).  Highlights are below.\n\n - Significantly smaller files by default (54% smaller for English, 73% smaller for Chinese)\n    - This results in a ~50% reduction in runtime for first-time users (who do not have the files cached yet)\n - Significantly lower memory usage\n - Breaking changes:\n    - `createWorker` arguments changed\n       - Setting non-default language and OEM now happens in `createWorker`\n          - E.g. `createWorker("chi_sim", 1)`\n    - `worker.initialize` and `worker.loadLanguage` functions should be deleted from code\n    - See [this issue](https://github.com/naptha/tesseract.js/issues/820) for full list\n\nUpgrading from v2 to v5?  See [this guide](https://github.com/naptha/tesseract.js/issues/771).\n\n## Major changes in v4\nVersion 4 includes many new features and bug fixes--see [this issue](https://github.com/naptha/tesseract.js/issues/662) for a full list.  Several highlights are below. \n\n- Added rotation preprocessing options (including auto-rotate) for significantly better accuracy\n- Processed images (rotated, grayscale, binary) can now be retrieved\n- Improved support for parallel processing (schedulers)\n- Breaking changes:\n  - `createWorker` is now async\n  - `getPDF` function replaced by `pdf` recognize option\n\n## Contributing\n\n### Development\nTo run a development copy of Tesseract.js do the following:\n```shell\n# First we clone the repository\ngit clone https://github.com/naptha/tesseract.js.git\ncd tesseract.js\n\n# Then we install the dependencies\nnpm install\n\n# And finally we start the development server\nnpm start\n```\n\nThe development server will be available at http://localhost:3000/examples/browser/basic-efficient.html in your favorite browser.\nIt will automatically rebuild `tesseract.min.js` and `worker.min.js` when you change files in the **src** folder.\n\n### Building Static Files\nTo build the compiled static files just execute the following:\n```shell\nnpm run build\n```\nThis will output the files into the `dist` directory.\n\n### Run Tests\n**Always confirm the automated tests pass before submitting a pull request.**  To run the automated tests locally, run the following commands.\n```shell\nnpm run lint\nnpm run test\n```\n\n## Contributors\n\n### Code Contributors\n\nThis project exists thanks to all the people who contribute. [[Contribute](https://github.com/naptha/tesseract.js?tab=readme-ov-file#contributing)].\n<a href="https://github.com/naptha/tesseract.js/graphs/contributors"><img src="https://opencollective.com/tesseractjs/contributors.svg?width=890&button=false" /></a>\n\n### Financial Contributors\n\nBecome a financial contributor and help us sustain our community. [[Contribute](https://opencollective.com/tesseractjs/contribute)]\n\n#### Individuals\n\n<a href="https://opencollective.com/tesseractjs"><img src="https://opencollective.com/tesseractjs/individuals.svg?width=890"></a>\n\n#### Organizations\n\nSupport this project with your organization. Your logo will show up here with a link to your website. [[Contribute](https://opencollective.com/tesseractjs/contribute)]\n\n<a href="https://opencollective.com/tesseractjs/organization/0/website"><img src="https://opencollective.com/tesseractjs/organization/0/avatar.svg"></a>\n<a href="https://opencollective.com/tesseractjs/organization/1/website"><img src="https://opencollective.com/tesseractjs/organization/1/avatar.svg"></a>\n<a href="https://opencollective.com/tesseractjs/organization/2/website"><img src="https://opencollective.com/tesseractjs/organization/2/avatar.svg"></a>\n<a href="https://opencollective.com/tesseractjs/organization/3/website"><img src="https://opencollective.com/tesseractjs/organization/3/avatar.svg"></a>\n<a href="https://opencollective.com/tesseractjs/organization/4/website"><img src="https://opencollective.com/tesseractjs/organization/4/avatar.svg"></a>\n<a href="https://opencollective.com/tesseractjs/organization/5/website"><img src="https://opencollective.com/tesseractjs/organization/5/avatar.svg"></a>\n<a href="https://opencollective.com/tesseractjs/organization/6/website"><img src="https://opencollective.com/tesseractjs/organization/6/avatar.svg"></a>\n<a href="https://opencollective.com/tesseractjs/organization/7/website"><img src="https://opencollective.com/tesseractjs/organization/7/avatar.svg"></a>\n<a href="https://opencollective.com/tesseractjs/organization/8/website"><img src="https://opencollective.com/tesseractjs/organization/8/avatar.svg"></a>\n<a href="https://opencollective.com/tesseractjs/organization/9/website"><img src="https://opencollective.com/tesseractjs/organization/9/avatar.svg"></a>\n', '{"language":"JavaScript","stars":37560,"forks":2347,"watchers":37560,"open_issues":32,"topics":["deep-learning","javascript","ocr","tesseract","webassembly"],"default_branch":"master","size_kb":109409,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:airbnb:javascript","source_url":"https://github.com/airbnb/javascript"},{"type":"has_code","target_id":"github:jeromewu:tesseract.js-video\"><img","source_url":"https://github.com/jeromewu/tesseract.js-video\"><img"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:naptha:tesseract.js-core","source_url":"https://github.com/naptha/tesseract.js-core"},{"type":"has_code","target_id":"github:scribeocr:scribe.js","source_url":"https://github.com/scribeocr/scribe.js"},{"type":"has_code","target_id":"github:scribeocr:scribe.js","source_url":"https://github.com/scribeocr/scribe.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:scribeocr:scribeocr","source_url":"https://github.com/scribeocr/scribeocr"},{"type":"has_code","target_id":"github:Tshetrim:Image-To-Text-OCR-extension-for-ChatGPT","source_url":"https://github.com/Tshetrim/Image-To-Text-OCR-extension-for-ChatGPT"},{"type":"has_code","target_id":"github:racosa:pdf2text-ocr","source_url":"https://github.com/racosa/pdf2text-ocr"},{"type":"has_code","target_id":"github:Kishlay-notabot:tesseract-bbox-examples","source_url":"https://github.com/Kishlay-notabot/tesseract-bbox-examples"},{"type":"has_code","target_id":"github:Balearica:tesseract.js-electron","source_url":"https://github.com/Balearica/tesseract.js-electron"},{"type":"has_code","target_id":"github:Balearica:tesseract.js-typescript","source_url":"https://github.com/Balearica/tesseract.js-typescript"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js.git","source_url":"https://github.com/naptha/tesseract.js.git"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js?tab=readme-ov-file#contributing"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"}]', NULL, 'Apache-2.0', 'approved', 80, 'c15e542340f89ea1bab186fa33a95835', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-naptha-tesseract.js from https://github.com/naptha.png
Image converted to WebP: data/images/github-naptha-tesseract.js.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-TencentARC-GFPGAN', 'github--tencentarc--gfpgan', 'GFPGAN', 'TencentARC', '<p align="center"> <img src="assets/gfpgan_logo.png" height=130> </p> <div align="center"> <!-- <a href="https://twitter.com/_Xintao_" style="text-decoration:none;"> <img src="https://user-images.githubusercontent.com/17445847/187162058-c764ced6-952f-404b-ac85-ba95cce18e7b.png" width="4%" alt="" /> </a> --> </div> 1. :boom: **Updated** online demo: . Here is the backup. 1. :boom: **Updated** online demo: 1. Colab Demo for GFPGAN <a href="https://colab.research.google.com/drive/1sVsoBd9AjckIXT...', '["deep-learning","face-restoration","gan","gfpgan","image-restoration","pytorch","super-resolution","python"]', 'other', 37261, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/TencentARC/GFPGAN","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <img src="assets/gfpgan_logo.png" height=130>\n</p>\n\n## <div align="center"><b><a href="README.md">English</a> | <a href="README_CN.md">ç®€ä½“ä¸­æ–‡</a></b></div>\n\n<div align="center">\n<!-- <a href="https://twitter.com/_Xintao_" style="text-decoration:none;">\n    <img src="https://user-images.githubusercontent.com/17445847/187162058-c764ced6-952f-404b-ac85-ba95cce18e7b.png" width="4%" alt="" />\n</a> -->\n\n[![download](https://img.shields.io/github/downloads/TencentARC/GFPGAN/total.svg)](https://github.com/TencentARC/GFPGAN/releases)\n[![PyPI](https://img.shields.io/pypi/v/gfpgan)](https://pypi.org/project/gfpgan/)\n[![Open issue](https://img.shields.io/github/issues/TencentARC/GFPGAN)](https://github.com/TencentARC/GFPGAN/issues)\n[![Closed issue](https://img.shields.io/github/issues-closed/TencentARC/GFPGAN)](https://github.com/TencentARC/GFPGAN/issues)\n[![LICENSE](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/TencentARC/GFPGAN/blob/master/LICENSE)\n[![python lint](https://github.com/TencentARC/GFPGAN/actions/workflows/pylint.yml/badge.svg)](https://github.com/TencentARC/GFPGAN/blob/master/.github/workflows/pylint.yml)\n[![Publish-pip](https://github.com/TencentARC/GFPGAN/actions/workflows/publish-pip.yml/badge.svg)](https://github.com/TencentARC/GFPGAN/blob/master/.github/workflows/publish-pip.yml)\n</div>\n\n1. :boom: **Updated** online demo: [![Replicate](https://img.shields.io/static/v1?label=Demo&message=Replicate&color=blue)](https://replicate.com/tencentarc/gfpgan). Here is the [backup](https://replicate.com/xinntao/gfpgan).\n1. :boom: **Updated** online demo: [![Huggingface Gradio](https://img.shields.io/static/v1?label=Demo&message=Huggingface%20Gradio&color=orange)](https://huggingface.co/spaces/Xintao/GFPGAN)\n1. [Colab Demo](https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo) for GFPGAN <a href="https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="google colab logo"></a>; (Another [Colab Demo](https://colab.research.google.com/drive/1Oa1WwKB4M4l1GmR7CtswDVgOCOeSLChA?usp=sharing) for the original paper model)\n\n<!-- 3. Online demo: [Replicate.ai](https://replicate.com/xinntao/gfpgan) (may need to sign in, return the whole image)\n4. Online demo: [Baseten.co](https://app.baseten.co/applications/Q04Lz0d/operator_views/8qZG6Bg) (backed by GPU, returns the whole image)\n5. We provide a *clean* version of GFPGAN, which can run without CUDA extensions. So that it can run in **Windows** or on **CPU mode**. -->\n\n> :rocket: **Thanks for your interest in our work. You may also want to check our new updates on the *tiny models* for *anime images and videos* in [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN/blob/master/docs/anime_video_model.md)** :blush:\n\nGFPGAN aims at developing a **Practical Algorithm for Real-world Face Restoration**.<br>\nIt leverages rich and diverse priors encapsulated in a pretrained face GAN (*e.g.*, StyleGAN2) for blind face restoration.\n\n:question: Frequently Asked Questions can be found in [FAQ.md](FAQ.md).\n\n:triangular_flag_on_post: **Updates**\n\n- :white_check_mark: Add [RestoreFormer](https://github.com/wzhouxiff/RestoreFormer) inference codes.\n- :white_check_mark: Add [V1.4 model](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth), which produces slightly more details and better identity than V1.3.\n- :white_check_mark: Add **[V1.3 model](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth)**, which produces **more natural** restoration results, and better results on *very low-quality* / *high-quality* inputs. See more in [Model zoo](#european_castle-model-zoo), [Comparisons.md](Comparisons.md)\n- :white_check_mark: Integrated to [Huggingface Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See [Gradio Web Demo](https://huggingface.co/spaces/akhaliq/GFPGAN).\n- :white_check_mark: Support enhancing non-face regions (background) with [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN).\n- :white_check_mark: We provide a *clean* version of GFPGAN, which does not require CUDA extensions.\n- :white_check_mark: We provide an updated model without colorizing faces.\n\n---\n\nIf GFPGAN is helpful in your photos/projects, please help to :star: this repo or recommend it to your friends. Thanks:blush:\nOther recommended projects:<br>\n:arrow_forward: [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN): A practical algorithm for general image restoration<br>\n:arrow_forward: [BasicSR](https://github.com/xinntao/BasicSR): An open-source image and video restoration toolbox<br>\n:arrow_forward: [facexlib](https://github.com/xinntao/facexlib): A collection that provides useful face-relation functions<br>\n:arrow_forward: [HandyView](https://github.com/xinntao/HandyView): A PyQt5-based image viewer that is handy for view and comparison<br>\n\n---\n\n### :book: GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior\n\n> [[Paper](https://arxiv.org/abs/2101.04061)] &emsp; [[Project Page](https://xinntao.github.io/projects/gfpgan)] &emsp; [Demo] <br>\n> [Xintao Wang](https://xinntao.github.io/), [Yu Li](https://yu-li.github.io/), [Honglun Zhang](https://scholar.google.com/citations?hl=en&user=KjQLROoAAAAJ), [Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en) <br>\n> Applied Research Center (ARC), Tencent PCG\n\n<p align="center">\n  <img src="https://xinntao.github.io/projects/GFPGAN_src/gfpgan_teaser.jpg">\n</p>\n\n---\n\n## :wrench: Dependencies and Installation\n\n- Python >= 3.7 (Recommend to use [Anaconda](https://www.anaconda.com/download/#linux) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html))\n- [PyTorch >= 1.7](https://pytorch.org/)\n- Option: NVIDIA GPU + [CUDA](https://developer.nvidia.com/cuda-downloads)\n- Option: Linux\n\n### Installation\n\nWe now provide a *clean* version of GFPGAN, which does not require customized CUDA extensions. <br>\nIf you want to use the original model in our paper, please see [PaperModel.md](PaperModel.md) for installation.\n\n1. Clone repo\n\n    ```bash\n    git clone https://github.com/TencentARC/GFPGAN.git\n    cd GFPGAN\n    ```\n\n1. Install dependent packages\n\n    ```bash\n    # Install basicsr - https://github.com/xinntao/BasicSR\n    # We use BasicSR for both training and inference\n    pip install basicsr\n\n    # Install facexlib - https://github.com/xinntao/facexlib\n    # We use face detection and face restoration helper in the facexlib package\n    pip install facexlib\n\n    pip install -r requirements.txt\n    python setup.py develop\n\n    # If you want to enhance the background (non-face) regions with Real-ESRGAN,\n    # you also need to install the realesrgan package\n    pip install realesrgan\n    ```\n\n## :zap: Quick Inference\n\nWe take the v1.3 version for an example. More models can be found [here](#european_castle-model-zoo).\n\nDownload pre-trained models: [GFPGANv1.3.pth](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth)\n\n```bash\nwget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models\n```\n\n**Inference!**\n\n```bash\npython inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2\n```\n\n```console\nUsage: python inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2 [options]...\n\n  -h                   show this help\n  -i input             Input image or folder. Default: inputs/whole_imgs\n  -o output            Output folder. Default: results\n  -v version           GFPGAN model version. Option: 1 | 1.2 | 1.3. Default: 1.3\n  -s upscale           The final upsampling scale of the image. Default: 2\n  -bg_upsampler        background upsampler. Default: realesrgan\n  -bg_tile             Tile size for background sampler, 0 for no tile during testing. Default: 400\n  -suffix              Suffix of the restored faces\n  -only_center_face    Only restore the center face\n  -aligned             Input are aligned faces\n  -ext                 Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto\n```\n\nIf you want to use the original model in our paper, please see [PaperModel.md](PaperModel.md) for installation and inference.\n\n## :european_castle: Model Zoo\n\n| Version | Model Name  | Description |\n| :---: | :---:        |     :---:      |\n| V1.3 | [GFPGANv1.3.pth](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth) | Based on V1.2; **more natural** restoration results; better results on very low-quality / high-quality inputs. |\n| V1.2 | [GFPGANCleanv1-NoCE-C2.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth) | No colorization; no CUDA extensions are required. Trained with more data with pre-processing. |\n| V1 | [GFPGANv1.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/GFPGANv1.pth) | The paper model, with colorization. |\n\nThe comparisons are in [Comparisons.md](Comparisons.md).\n\nNote that V1.3 is not always better than V1.2. You may need to select different models based on your purpose and inputs.\n\n| Version | Strengths  | Weaknesses |\n| :---: | :---:        |     :---:      |\n|V1.3 |  âœ“ natural outputs<br> âœ“better results on very low-quality inputs <br> âœ“ work on relatively high-quality inputs <br>âœ“ can have repeated (twice) restorations | âœ— not very sharp <br> âœ— have a slight change on identity |\n|V1.2 |  âœ“ sharper output <br> âœ“ with beauty makeup | âœ— some outputs are unnatural |\n\nYou can find **more models (such as the discriminators)** here: [[Google Drive](https://drive.google.com/drive/folders/17rLiFzcUMoQuhLnptDsKolegHWwJOnHu?usp=sharing)], OR [[Tencent Cloud è…¾è®¯å¾®äº‘](https://share.weiyun.com/ShYoCCoc)]\n\n## :computer: Training\n\nWe provide the training codes for GFPGAN (used in our paper). <br>\nYou could improve it according to your own needs.\n\n**Tips**\n\n1. More high quality faces can improve the restoration quality.\n2. You may need to perform some pre-processing, such as beauty makeup.\n\n**Procedures**\n\n(You can try a simple version ( `options/train_gfpgan_v1_simple.yml`) that does not require face component landmarks.)\n\n1. Dataset preparation: [FFHQ](https://github.com/NVlabs/ffhq-dataset)\n\n1. Download pre-trained models and other data. Put them in the `experiments/pretrained_models` folder.\n    1. [Pre-trained StyleGAN2 model: StyleGAN2_512_Cmul1_FFHQ_B12G4_scratch_800k.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/StyleGAN2_512_Cmul1_FFHQ_B12G4_scratch_800k.pth)\n    1. [Component locations of FFHQ: FFHQ_eye_mouth_landmarks_512.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/FFHQ_eye_mouth_landmarks_512.pth)\n    1. [A simple ArcFace model: arcface_resnet18.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/arcface_resnet18.pth)\n\n1. Modify the configuration file `options/train_gfpgan_v1.yml` accordingly.\n\n1. Training\n\n> python -m torch.distributed.launch --nproc_per_node=4 --master_port=22021 gfpgan/train.py -opt options/train_gfpgan_v1.yml --launcher pytorch\n\n## :scroll: License and Acknowledgement\n\nGFPGAN is released under Apache License Version 2.0.\n\n## BibTeX\n\n    @InProceedings{wang2021gfpgan,\n        author = {Xintao Wang and Yu Li and Honglun Zhang and Ying Shan},\n        title = {Towards Real-World Blind Face Restoration with Generative Facial Prior},\n        booktitle={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n        year = {2021}\n    }\n\n## :e-mail: Contact\n\nIf you have any question, please email `xintao.wang@outlook.com` or `xintaowang@tencent.com`.\n', '{"language":"Python","stars":37261,"forks":6251,"watchers":37261,"open_issues":397,"topics":["deep-learning","face-restoration","gan","gfpgan","image-restoration","pytorch","super-resolution"],"default_branch":"master","size_kb":5467,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:xinntao:Real-ESRGAN","source_url":"https://github.com/xinntao/Real-ESRGAN"},{"type":"has_code","target_id":"github:wzhouxiff:RestoreFormer","source_url":"https://github.com/wzhouxiff/RestoreFormer"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"},{"type":"has_code","target_id":"github:xinntao:Real-ESRGAN","source_url":"https://github.com/xinntao/Real-ESRGAN"},{"type":"has_code","target_id":"github:xinntao:Real-ESRGAN","source_url":"https://github.com/xinntao/Real-ESRGAN"},{"type":"has_code","target_id":"github:xinntao:BasicSR","source_url":"https://github.com/xinntao/BasicSR"},{"type":"has_code","target_id":"github:xinntao:facexlib","source_url":"https://github.com/xinntao/facexlib"},{"type":"has_code","target_id":"github:xinntao:HandyView","source_url":"https://github.com/xinntao/HandyView"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN.git","source_url":"https://github.com/TencentARC/GFPGAN.git"},{"type":"has_code","target_id":"github:xinntao:BasicSR","source_url":"https://github.com/xinntao/BasicSR"},{"type":"has_code","target_id":"github:xinntao:facexlib","source_url":"https://github.com/xinntao/facexlib"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:NVlabs:ffhq-dataset","source_url":"https://github.com/NVlabs/ffhq-dataset"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"}]', NULL, 'NOASSERTION', 'approved', 80, 'deafc64194ae3c25aa7fc67b5e376707', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-TencentARC-GFPGAN from https://github.com/TencentARC.png
Image converted to WebP: data/images/github-TencentARC-GFPGAN.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-babysor-MockingBird', 'github--babysor--mockingbird', 'MockingBird', 'babysor', '> ðŸš§ While I no longer actively update this repo, you can find me continuously pushing this tech forward to good side and open-source. I''m also building an optimized and cloud hosted version: https://noiz.ai/ and it''s free but not ready for commersial use now. > !mockingbird <a href="https://trendshift.io/repositories/3869" target="_blank"><img src="https://trendshift.io/api/badge/repositories/3869" alt="babysor%2FMockingBird | Trendshift" style="width: 250px; height: 55px;" width="250" heigh...', '["ai","deep-learning","pytorch","speech","text-to-speech","tts","python"]', 'other', 36801, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/babysor/MockingBird","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '> ðŸš§ While I no longer actively update this repo, you can find me continuously pushing this tech forward to good side and open-source. I''m also building an optimized and cloud hosted version: https://noiz.ai/ and it''s free but not ready for commersial use now.\n>\n![mockingbird](https://user-images.githubusercontent.com/12797292/131216767-6eb251d6-14fc-4951-8324-2722f0cd4c63.jpg)\n<a href="https://trendshift.io/repositories/3869" target="_blank"><img src="https://trendshift.io/api/badge/repositories/3869" alt="babysor%2FMockingBird | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n\n[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](http://choosealicense.com/licenses/mit/)\n\n> English | [ä¸­æ–‡](README-CN.md)| [ä¸­æ–‡Linux](README-LINUX-CN.md)\n\n## Features\nðŸŒ **Chinese** supported mandarin and tested with multiple datasets: aidatatang_200zh, magicdata, aishell3, data_aishell, and etc.\n\nðŸ¤© **PyTorch** worked for pytorch, tested in version of 1.9.0(latest in August 2021), with GPU Tesla T4 and GTX 2060\n\nðŸŒ **Windows + Linux** run in both Windows OS and linux OS (even in M1 MACOS)\n\nðŸ¤© **Easy & Awesome** effect with only newly-trained synthesizer, by reusing the pretrained encoder/vocoder\n\nðŸŒ **Webserver Ready** to serve your result with remote calling\n\n### [DEMO VIDEO](https://www.bilibili.com/video/BV17Q4y1B7mY/)\n\n## Quick Start\n\n### 1. Install Requirements\n#### 1.1 General Setup\n> Follow the original repo to test if you got all environment ready.\n**Python 3.7 or higher ** is needed to run the toolbox.\n\n* Install [PyTorch](https://pytorch.org/get-started/locally/).\n> If you get an `ERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu102 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2 )` This error is probably due to a low version of python, try using 3.9 and it will install successfully\n* Install [ffmpeg](https://ffmpeg.org/download.html#get-packages).\n* Run `pip install -r requirements.txt` to install the remaining necessary packages.\n> The recommended environment here is `Repo Tag 0.0.1` `Pytorch1.9.0 with Torchvision0.10.0 and cudatoolkit10.2` `requirements.txt` `webrtcvad-wheels` because `requirements. txt` was exported a few months ago, so it doesn''t work with newer versions\n* Install webrtcvad `pip install webrtcvad-wheels`(If you need)\n\nor\n- install dependencies withÂ `conda`Â orÂ `mamba`\n\n  ```conda env create -n env_name -f env.yml```\n\n  ```mamba env create -n env_name -f env.yml```\n\n  will create a virtual environment where necessary dependencies are installed. Switch to the new environment byÂ `conda activate env_name`Â and enjoy it.\n  > env.yml only includes the necessary dependencies to run the projectï¼Œtemporarily without monotonic-align. You can check the official website to install the GPU version of pytorch.\n\n#### 1.2 Setup with a M1 Mac\n> The following steps are a workaround to directly use the original `demo_toolbox.py`without the changing of codes.\n>\n  >  Since the major issue comes with the PyQt5 packages used in `demo_toolbox.py` not compatible with M1 chips, were one to attempt on training models with the M1 chip, either that person can forgo `demo_toolbox.py`, or one can try the `web.py` in the project.\n\n##### 1.2.1 Install `PyQt5`, with [ref](https://stackoverflow.com/a/68038451/20455983) here.\n  * Create and open a Rosetta Terminal, with [ref](https://dev.to/courier/tips-and-tricks-to-setup-your-apple-m1-for-development-547g) here.\n  * Use system Python to create a virtual environment for the project\n    ```\n    /usr/bin/python3 -m venv /PathToMockingBird/venv\n    source /PathToMockingBird/venv/bin/activate\n    ```\n  * Upgrade pip and install `PyQt5`\n    ```\n    pip install --upgrade pip\n    pip install pyqt5\n    ```\n##### 1.2.2 Install `pyworld` and `ctc-segmentation`\n\n> Both packages seem to be unique to this project and are not seen in the original [Real-Time Voice Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning) project. When installing with `pip install`, both packages lack wheels so the program tries to directly compile from c code and could not find `Python.h`.\n\n  * Install `pyworld`\n      * `brew install python` `Python.h` can come with Python installed by brew\n      * `export CPLUS_INCLUDE_PATH=/opt/homebrew/Frameworks/Python.framework/Headers` The filepath of brew-installed `Python.h` is unique to M1 MacOS and listed above. One needs to manually add the path to the environment variables.\n      * `pip install pyworld` that should do.\n\n\n  * Install`ctc-segmentation`\n    > Same method does not apply to `ctc-segmentation`, and one needs to compile it from the source code on [github](https://github.com/lumaku/ctc-segmentation).\n    * `git clone https://github.com/lumaku/ctc-segmentation.git`\n    * `cd ctc-segmentation`\n    * `source /PathToMockingBird/venv/bin/activate` If the virtual environment hasn''t been deployed, activate it.\n    * `cythonize -3 ctc_segmentation/ctc_segmentation_dyn.pyx`\n    * `/usr/bin/arch -x86_64 python setup.py build` Build with x86 architecture.\n    * `/usr/bin/arch -x86_64 python setup.py install --optimize=1 --skip-build`Install with x86 architecture.\n\n##### 1.2.3 Other dependencies\n  * `/usr/bin/arch -x86_64 pip install torch torchvision torchaudio` Pip installing `PyTorch` as an example, articulate that it''s installed with x86 architecture\n  * `pip install ffmpeg`  Install ffmpeg\n  * `pip install -r requirements.txt` Install other requirements.\n\n##### 1.2.4 Run the Inference Time (with Toolbox)\n  > To run the project on x86 architecture. [ref](https://youtrack.jetbrains.com/issue/PY-46290/Allow-running-Python-under-Rosetta-2-in-PyCharm-for-Apple-Silicon).\n  * `vim /PathToMockingBird/venv/bin/pythonM1` Create an executable file `pythonM1` to condition python interpreter at `/PathToMockingBird/venv/bin`.\n  * Write in the following content:\n    ```\n    #!/usr/bin/env zsh\n    mydir=${0:a:h}\n    /usr/bin/arch -x86_64 $mydir/python "$@"\n    ```\n  * `chmod +x pythonM1` Set the file as executable.\n  * If using PyCharm IDE, configure project interpreter to `pythonM1`([steps here](https://www.jetbrains.com/help/pycharm/configuring-python-interpreter.html#add-existing-interpreter)), if using command line python, run `/PathToMockingBird/venv/bin/pythonM1 demo_toolbox.py`\n\n\n### 2. Prepare your models\n> Note that we are using the pretrained encoder/vocoder but not synthesizer, since the original model is incompatible with the Chinese symbols. It means the demo_cli is not working at this moment, so additional synthesizer models are required.\n\nYou can either train your models or use existing ones:\n\n#### 2.1 Train encoder with your dataset (Optional)\n\n* Preprocess with the audios and the mel spectrograms:\n`python encoder_preprocess.py <datasets_root>` Allowing parameter `--dataset {dataset}` to support the datasets you want to preprocess. Only the train set of these datasets will be used. Possible names: librispeech_other, voxceleb1, voxceleb2. Use comma to sperate multiple datasets.\n\n* Train the encoder: `python encoder_train.py my_run <datasets_root>/SV2TTS/encoder`\n> For training, the encoder uses visdom. You can disable it with `--no_visdom`, but it''s nice to have. Run "visdom" in a separate CLI/process to start your visdom server.\n\n#### 2.2 Train synthesizer with your dataset\n* Download dataset and unzip: make sure you can access all .wav in folder\n* Preprocess with the audios and the mel spectrograms:\n`python pre.py <datasets_root>`\nAllowing parameter `--dataset {dataset}` to support aidatatang_200zh, magicdata, aishell3, data_aishell, etc.If this parameter is not passed, the default dataset will be aidatatang_200zh.\n\n* Train the synthesizer:\n`python train.py --type=synth mandarin <datasets_root>/SV2TTS/synthesizer`\n\n* Go to next step when you see attention line show and loss meet your need in training folder *synthesizer/saved_models/*.\n\n#### 2.3 Use pretrained model of synthesizer\n> Thanks to the community, some models will be shared:\n\n| author | Download link | Preview Video | Info |\n| --- | ----------- | ----- |----- |\n| @author | https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g  [Baidu](https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g) 4j5d  |  | 75k steps trained by multiple datasets\n| @author | https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw  [Baidu](https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw) codeï¼šom7f  |  | 25k steps trained by multiple datasets, only works under version 0.0.1\n|@FawenYo | https://yisiou-my.sharepoint.com/:u:/g/personal/lawrence_cheng_fawenyo_onmicrosoft_com/EWFWDHzee-NNg9TWdKckCc4BC7bK2j9cCbOWn0-_tK0nOg?e=n0gGgC  | [input](https://github.com/babysor/MockingBird/wiki/audio/self_test.mp3) [output](https://github.com/babysor/MockingBird/wiki/audio/export.wav) | 200k steps with local accent of Taiwan, only works under version 0.0.1\n|@miven| https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ code: 2021 https://www.aliyundrive.com/s/AwPsbo8mcSP code: z2m0 | https://www.bilibili.com/video/BV1uh411B7AD/ | only works under version 0.0.1\n\n#### 2.4 Train vocoder (Optional)\n> note: vocoder has little difference in effect, so you may not need to train a new one.\n* Preprocess the data:\n`python vocoder_preprocess.py <datasets_root> -m <synthesizer_model_path>`\n> `<datasets_root>` replace with your dataset rootï¼Œ`<synthesizer_model_path>`replace with directory of your best trained models of sythensizer, e.g. *sythensizer\saved_mode\xxx*\n\n* Train the wavernn vocoder:\n`python vocoder_train.py mandarin <datasets_root>`\n\n* Train the hifigan vocoder\n`python vocoder_train.py mandarin <datasets_root> hifigan`\n\n### 3. Launch\n#### 3.1 Using the web server\nYou can then try to run:`python web.py` and open it in browser, default as `http://localhost:8080`\n\n#### 3.2 Using the Toolbox\nYou can then try the toolbox:\n`python demo_toolbox.py -d <datasets_root>`\n\n#### 3.3 Using the command line\nYou can then try the command:\n`python gen_voice.py <text_file.txt> your_wav_file.wav`\nyou may need to install cn2an by "pip install cn2an" for better digital number result.\n\n## Reference\n> This repository is forked from [Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning) which only support English.\n\n| URL | Designation | Title | Implementation source |\n| --- | ----------- | ----- | --------------------- |\n| [1803.09017](https://arxiv.org/abs/1803.09017) | GlobalStyleToken (synthesizer)| Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis | This repo |\n| [2010.05646](https://arxiv.org/abs/2010.05646) | HiFi-GAN (vocoder)| Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis | This repo |\n| [2106.02297](https://arxiv.org/abs/2106.02297) | Fre-GAN (vocoder)| Fre-GAN: Adversarial Frequency-consistent Audio Synthesis | This repo |\n|[**1806.04558**](https://arxiv.org/pdf/1806.04558.pdf) | **SV2TTS** | **Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis** | This repo |\n|[1802.08435](https://arxiv.org/pdf/1802.08435.pdf) | WaveRNN (vocoder) | Efficient Neural Audio Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN) |\n|[1703.10135](https://arxiv.org/pdf/1703.10135.pdf) | Tacotron (synthesizer) | Tacotron: Towards End-to-End Speech Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN)\n|[1710.10467](https://arxiv.org/pdf/1710.10467.pdf) | GE2E (encoder)| Generalized End-To-End Loss for Speaker Verification | This repo |\n\n## F Q&A\n#### 1.Where can I download the dataset?\n| Dataset | Original Source | Alternative Sources |\n| --- | ----------- | ---------------|\n| aidatatang_200zh | [OpenSLR](http://www.openslr.org/62/) | [Google Drive](https://drive.google.com/file/d/110A11KZoVe7vy6kXlLb6zVPLb_J91I_t/view?usp=sharing) |\n| magicdata | [OpenSLR](http://www.openslr.org/68/) | [Google Drive (Dev set)](https://drive.google.com/file/d/1g5bWRUSNH68ycC6eNvtwh07nX3QhOOlo/view?usp=sharing) |\n| aishell3 | [OpenSLR](https://www.openslr.org/93/) | [Google Drive](https://drive.google.com/file/d/1shYp_o4Z0X0cZSKQDtFirct2luFUwKzZ/view?usp=sharing) |\n| data_aishell | [OpenSLR](https://www.openslr.org/33/) |  |\n> After unzip aidatatang_200zh, you need to unzip all the files under `aidatatang_200zh\corpus\train`\n\n#### 2.What is`<datasets_root>`?\nIf the dataset path is `D:\data\aidatatang_200zh`,then `<datasets_root>` is`D:\data`\n\n#### 3.Not enough VRAM\nTrain the synthesizerï¼šadjust the batch_size in `synthesizer/hparams.py`\n```\n//Before\ntts_schedule = [(2,  1e-3,  20_000,  12),   # Progressive training schedule\n                (2,  5e-4,  40_000,  12),   # (r, lr, step, batch_size)\n                (2,  2e-4,  80_000,  12),   #\n                (2,  1e-4, 160_000,  12),   # r = reduction factor (# of mel frames\n                (2,  3e-5, 320_000,  12),   #     synthesized for each decoder iteration)\n                (2,  1e-5, 640_000,  12)],  # lr = learning rate\n//After\ntts_schedule = [(2,  1e-3,  20_000,  8),   # Progressive training schedule\n                (2,  5e-4,  40_000,  8),   # (r, lr, step, batch_size)\n                (2,  2e-4,  80_000,  8),   #\n                (2,  1e-4, 160_000,  8),   # r = reduction factor (# of mel frames\n                (2,  3e-5, 320_000,  8),   #     synthesized for each decoder iteration)\n                (2,  1e-5, 640_000,  8)],  # lr = learning rate\n```\n\nTrain Vocoder-Preprocess the dataï¼šadjust the batch_size in `synthesizer/hparams.py`\n```\n//Before\n### Data Preprocessing\n        max_mel_frames = 900,\n        rescale = True,\n        rescaling_max = 0.9,\n        synthesis_batch_size = 16,                  # For vocoder preprocessing and inference.\n//After\n### Data Preprocessing\n        max_mel_frames = 900,\n        rescale = True,\n        rescaling_max = 0.9,\n        synthesis_batch_size = 8,                  # For vocoder preprocessing and inference.\n```\n\nTrain Vocoder-Train the vocoderï¼šadjust the batch_size in `vocoder/wavernn/hparams.py`\n```\n//Before\n# Training\nvoc_batch_size = 100\nvoc_lr = 1e-4\nvoc_gen_at_checkpoint = 5\nvoc_pad = 2\n\n//After\n# Training\nvoc_batch_size = 6\nvoc_lr = 1e-4\nvoc_gen_at_checkpoint = 5\nvoc_pad =2\n```\n\n#### 4.If it happens `RuntimeError: Error(s) in loading state_dict for Tacotron: size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).`\nPlease refer to issue [#37](https://github.com/babysor/MockingBird/issues/37)\n\n#### 5. How to improve CPU and GPU occupancy rate?\nAdjust the batch_size as appropriate to improve\n\n\n#### 6. What if it happens `the page file is too small to complete the operation`\nPlease refer to this [video](https://www.youtube.com/watch?v=Oh6dga-Oy10&ab_channel=CodeProf) and change the virtual memory to 100G (102400), for example : When the file is placed in the D disk, the virtual memory of the D disk is changed.\n\n#### 7. When should I stop during training?\nFYI, my attention came after 18k steps and loss became lower than 0.4 after 50k steps.\n![attention_step_20500_sample_1](https://user-images.githubusercontent.com/7423248/128587252-f669f05a-f411-4811-8784-222156ea5e9d.png)\n![step-135500-mel-spectrogram_sample_1](https://user-images.githubusercontent.com/7423248/128587255-4945faa0-5517-46ea-b173-928eff999330.png)\n', '{"language":"Python","stars":36801,"forks":5271,"watchers":36801,"open_issues":481,"topics":["ai","deep-learning","pytorch","speech","text-to-speech","tts"],"default_branch":"main","size_kb":130670,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:CorentinJ:Real-Time-Voice-Cloning","source_url":"https://github.com/CorentinJ/Real-Time-Voice-Cloning"},{"type":"has_code","target_id":"github:lumaku:ctc-segmentation","source_url":"https://github.com/lumaku/ctc-segmentation"},{"type":"has_code","target_id":"github:lumaku:ctc-segmentation.git`","source_url":"https://github.com/lumaku/ctc-segmentation.git`"},{"type":"has_code","target_id":"github:babysor:MockingBird","source_url":"https://github.com/babysor/MockingBird"},{"type":"has_code","target_id":"github:babysor:MockingBird","source_url":"https://github.com/babysor/MockingBird"},{"type":"has_code","target_id":"github:CorentinJ:Real-Time-Voice-Cloning","source_url":"https://github.com/CorentinJ/Real-Time-Voice-Cloning"},{"type":"has_code","target_id":"github:fatchord:WaveRNN","source_url":"https://github.com/fatchord/WaveRNN"},{"type":"has_code","target_id":"github:fatchord:WaveRNN","source_url":"https://github.com/fatchord/WaveRNN"},{"type":"has_code","target_id":"github:babysor:MockingBird","source_url":"https://github.com/babysor/MockingBird"}]', NULL, 'NOASSERTION', 'approved', 80, '473fbc62234591e753af7239acb679bb', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-babysor-MockingBird from https://github.com/babysor.png
Image converted to WebP: data/images/github-babysor-MockingBird.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-mli-paper-reading', 'github--mli--paper-reading', 'paper-reading', 'mli', '| æ—¥æœŸ | æ ‡é¢˜ | å°é¢ | æ—¶é•¿ | è§†é¢‘ï¼ˆæ’­æ”¾æ•°ï¼‰ | | --: | -- | -- | --: | -- | | 1/10/25 | OpenAI Sora ä¸Š<br />(åŒ…å«Movie Genå’ŒHunyuanVideo) | <img src="imgs/sora.jpg" width="200px"/> | 1:04:18 | <br /> | | 9/04/24 | Llama 3.1è®ºæ–‡ç²¾è¯» Â· 5. æ¨¡åž‹è®­ç»ƒè¿‡ç¨‹ | <img src="imgs/llama3-process.jpg" width="200px"/> | 10:41| <br /> | | 8/28/24 | Llama 3.1è®ºæ–‡ç²¾è¯» Â· 4. è®­ç»ƒinfra | <img src="imgs/llama3-training-infra.webp" width="200px"/> | 25:04| <br /> | | 8/13/24 | Llama 3.1è®ºæ–‡ç²¾è¯» Â· 3. æ¨¡åž‹ | <img src="imgs/llama3-model.webp" width="200px"/> |...', '["deep-learning","paper","reading-list"]', 'other', 32098, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/mli/paper-reading","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# æ·±åº¦å­¦ä¹ è®ºæ–‡ç²¾è¯»\n\n## å½•åˆ¶å®Œæˆçš„è®ºæ–‡\n\n| æ—¥æœŸ | æ ‡é¢˜ | å°é¢ | æ—¶é•¿ | è§†é¢‘ï¼ˆæ’­æ”¾æ•°ï¼‰ |\n| --: | -- | -- | --: | -- |\n| 1/10/25 | [OpenAI Sora](https://openai.com/index/video-generation-models-as-world-simulators/) ä¸Š<br />(åŒ…å«Movie Genå’ŒHunyuanVideo) | <img src="imgs/sora.jpg" width="200px"/> | 1:04:18 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1VdcxesEAt)](https://www.bilibili.com/video/BV1VdcxesEAt/?share_source=copy_web&vd_source=5d037e935914fc22e2e978cdccf5cdfe)<br />[![](https://img.shields.io/youtube/views/5MGq7dSOghY?style=social)](https://youtu.be/5MGq7dSOghY?si=lY-OsadDsTeKf-ub)  |\n| 9/04/24 | Llama 3.1è®ºæ–‡ç²¾è¯» Â· 5. æ¨¡åž‹è®­ç»ƒè¿‡ç¨‹ | <img src="imgs/llama3-process.jpg" width="200px"/> | 10:41| [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1c8HbeaEXi)](https://www.bilibili.com/video/BV1c8HbeaEXi)<br />  |\n| 8/28/24 | Llama 3.1è®ºæ–‡ç²¾è¯» Â· 4. è®­ç»ƒinfra | <img src="imgs/llama3-training-infra.webp" width="200px"/> | 25:04| [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1b4421f7fa)](https://www.bilibili.com/video/BV1b4421f7fa)<br />[![](https://img.shields.io/youtube/views/6XidEHVjS1A?style=social)](https://www.youtube.com/watch?v=6XidEHVjS1A)  |\n| 8/13/24 | Llama 3.1è®ºæ–‡ç²¾è¯» Â· 3. æ¨¡åž‹ | <img src="imgs/llama3-model.webp" width="200px"/> | 26:14| [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Q4421Z7Tj)](https://www.bilibili.com/video/BV1Q4421Z7Tj)<br />[![](https://img.shields.io/youtube/views/G6gF-5g1Gg4?style=social)](https://www.youtube.com/watch?v=G6gF-5g1Gg4)  |\n| 8/05/24 | [Llama 3.1è®ºæ–‡ç²¾è¯» Â· 2. é¢„è®­ç»ƒæ•°æ®](https://arxiv.org/pdf/2407.21783) | <img src="imgs/llama3-pretrain-data.jpg" width="200px"/> | 23:37| [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1u142187S5)](https://www.bilibili.com/video/BV1u142187S5)[![](https://img.shields.io/youtube/views/wXFr3zIE8FM?style=social)](https://www.youtube.com/watch?v=wXFr3zIE8FM)|\n| 7/31/24 | Llama 3.1è®ºæ–‡ç²¾è¯» Â· 1. å¯¼è¨€ | <img src="imgs/llama3-intro.jpg" width="200px"/> | 18:53| [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1WM4m1y7Uh)](https://www.bilibili.com/video/BV1WM4m1y7Uh)<br />[![](https://img.shields.io/youtube/views/-PztagF3wQE?style=social)](https://www.youtube.com/watch?v=-PztagF3wQE)  |\n| 3/30/23 | [GPT-4](https://openai.com/research/gpt-4) | <img src="imgs/gpt4.jpg" width="200px"/> | 1:20:38 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1vM4y1U7b5)](https://www.bilibili.com/video/BV1vM4y1U7b5)<br />[![](https://img.shields.io/youtube/views/K0SZ9mdygTw?style=social)](https://youtu.be/K0SZ9mdygTw)  |\n| 3/23/23 | å¤§æ¨¡åž‹æ—¶ä»£ä¸‹åšç§‘ç ”çš„å››ä¸ªæ€è·¯ | <img src="imgs/limited-resources.jpg" width="200px"/> | 1:06:29 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1oX4y1d7X6)](https://www.bilibili.com/video/BV1oX4y1d7X6)<br />[![](https://img.shields.io/youtube/views/sh79Z8i15PI?style=social)](https://youtu.be/sh79Z8i15PI) |\n| 3/10/23 | [Anthropic LLM](https://arxiv.org/pdf/2204.05862.pdf) | <img src="imgs/anthropic_lm.jpg" width="200px"/> | 1:01:51 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1XY411B7nM)](https://www.bilibili.com/video/BV1XY411B7nM)<br />[![](https://img.shields.io/youtube/views/iqX0pgNDon0?style=social)](https://youtu.be/iqX0pgNDon0) |\n| 1/20/23 | [Helm](https://arxiv.org/pdf/2211.09110.pdf) å…¨é¢è¯­è¨€æ¨¡åž‹è¯„æµ‹ | <img src="imgs/helm.jpg" width="200px"/> | 1:23:37 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1z24y1B7uX)](https://www.bilibili.com/video/BV1z24y1B7uX)<br />[![](https://img.shields.io/youtube/views/WgFEw9U3BXA?style=social)](https://youtu.be/WgFEw9U3BXA) |\n| 1/11/23 | å¤šæ¨¡æ€è®ºæ–‡ä¸²è®²Â·ä¸‹ |  <img src="imgs/multimodal-2.jpg" width="200px"/> | 1:03:29 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1fA411Z772)](https://www.bilibili.com/video/BV1fA411Z772) <br />[![](https://img.shields.io/youtube/views/S1le41J76lQ?style=social)](https://youtu.be/S1le41J76lQ) |\n| 12/29/22 | [Instruct GPT](https://arxiv.org/pdf/2203.02155.pdf) | <img src="imgs/instruct-gpt.jpg" width="200px"/> | 1:07:10 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1hd4y187CR)](https://www.bilibili.com/video/BV1hd4y187CR) <br />[![](https://img.shields.io/youtube/views/zfIGAwD1jOQ?style=social)](https://youtu.be/zfIGAwD1jOQ) |\n| 12/19/22 | [Neural Corpus Indexer](https://arxiv.org/pdf/2206.02743.pdf) æ–‡æ¡£æ£€ç´¢ | <img src="imgs/nci.jpg" width="200px"/> | 55:47 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Se411w7Sn)](https://www.bilibili.com/video/BV1Se411w7Sn) <br />[![](https://img.shields.io/youtube/views/QRffZMSGJyU?style=social)](https://youtu.be/QRffZMSGJyU) |\n| 12/12/22 | å¤šæ¨¡æ€è®ºæ–‡ä¸²è®²Â·ä¸Š | <img src="imgs/multimodal-1.jpg" width="200px"/> | 1:12:27 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Vd4y1v77v)](https://www.bilibili.com/video/BV1Vd4y1v77v) <br />[![](https://img.shields.io/youtube/views/6pzBOQAXUB8?style=social)](https://youtu.be/6pzBOQAXUB8)  |\n| 11/14/22 | [OpenAI Whisper](https://cdn.openai.com/papers/whisper.pdf) ç²¾è¯» | <img src="imgs/whisper.jpg" width="200px"/> | 1:12:16 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1VG4y1t74x)](https://www.bilibili.com/video/BV1VG4y1t74x) <br />[![](https://img.shields.io/youtube/views/3eXCJd32UnM?style=social)](https://youtu.be/3eXCJd32UnM) |\n| 11/07/22 | åœ¨è®² OpenAI Whisper å‰å…ˆåšäº†ä¸€ä¸ªå‰ªè§†é¢‘å°å·¥å…· | <img src="imgs/autocut.jpg" width="200px"/> | 23:39 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Pe4y1t7de)](https://www.bilibili.com/video/BV1Pe4y1t7de) <br />[![](https://img.shields.io/youtube/views/PwVlvCPDnrI?style=social)](https://youtu.be/PwVlvCPDnrI)  |\n| 10/23/22 | [Chain of Thought](https://arxiv.org/pdf/2201.11903.pdf) è®ºæ–‡ã€ä»£ç å’Œèµ„æº | <img src="imgs/cot.jpg" width="200px"/> | 33:21 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1t8411e7Ug)](https://www.bilibili.com/video/BV1t8411e7Ug)<br />[![](https://img.shields.io/youtube/views/H4J59iG3t5o?style=social)](https://youtu.be/H4J59iG3t5o) |\n| 9/17/22 | CLIP æ”¹è¿›å·¥ä½œä¸²è®²ï¼ˆä¸‹ï¼‰ | <img src="imgs/clipx-part2.jpg" width="200px"/> | 1:04:26 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1gg411U7n4)](https://www.bilibili.com/video/BV1gg411U7n4)<br />[![](https://img.shields.io/youtube/views/ugJeBivv65s?style=social)](https://youtu.be/ugJeBivv65s) |\n| 9/2/22 | CLIP æ”¹è¿›å·¥ä½œä¸²è®²ï¼ˆä¸Šï¼‰ | <img src="imgs/clipx-part1.jpg" width="200px"/> | 1:14:43 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1FV4y1p7Lm)](https://www.bilibili.com/video/BV1FV4y1p7Lm)<br />[![](https://img.shields.io/youtube/views/x4CDhZz_Dvg?style=social)](https://youtu.be/x4CDhZz_Dvg) |\n| 7/29/22 | [ViLT](https://arxiv.org/pdf/2102.03334.pdf) è®ºæ–‡ç²¾è¯» | <img src="imgs/vilt.jpg" width="200px"/> | 1:03:26 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV14r4y1j74y)](https://www.bilibili.com/video/BV14r4y1j74y)<br />[![](https://img.shields.io/youtube/views/ug8YvZOjOCE?style=social)](https://youtu.be/ug8YvZOjOCE) |\n| 7/22/22 | ç†ç”±ã€è®ºæ®å’Œæ‹…ä¿ã€[ç ”ç©¶çš„è‰ºæœ¯](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)Â·å››ã€‘ | <img src="imgs/craft_research_p4.jpg" width="200px"/> | 44:14 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1SB4y1a75c)](https://www.bilibili.com/video/BV1SB4y1a75c) |\n| 7/15/22 | å¦‚ä½•è®²å¥½æ•…äº‹ã€æ•…äº‹é‡Œçš„è®ºç‚¹ã€[ç ”ç©¶çš„è‰ºæœ¯](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)Â·ä¸‰ã€‘| <img src="imgs/craft_research_p3.jpg" width="200px"/> | 43:56 |[![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1WB4y1v7ST)](https://www.bilibili.com/video/BV1WB4y1v7ST)|\n| 7/8/22 | [DALLÂ·E 2](https://arxiv.org/pdf/2204.06125.pdf) é€æ®µç²¾è¯» | <img src="imgs/dalle2.jpg" width="200px"/> | 1:27:54 |[![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV17r4y1u77B)](https://www.bilibili.com/video/BV17r4y1u77B)<br />[![](https://img.shields.io/youtube/views/hO57mntSMl0?style=social)](https://youtu.be/hO57mntSMl0)|\n| 7/1/22 | æ˜Žç™½é—®é¢˜çš„é‡è¦æ€§ã€[ç ”ç©¶çš„è‰ºæœ¯](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)Â·äºŒã€‘| <img src="imgs/craft_research_p2.jpg" width="200px"/> | 1:03:40 |[![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV11S4y1v7S2)](https://www.bilibili.com/video/BV11S4y1v7S2/)|\n| 6/24/22 | è·Ÿè¯»è€…å»ºç«‹è”ç³»ã€[ç ”ç©¶çš„è‰ºæœ¯](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)Â·ä¸€ã€‘ | <img src="imgs/craft_research_p1.jpg" width="200px"/> | 45:01 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1hY411T7vy)](https://www.bilibili.com/video/BV1hY411T7vy/) |\n| 6/17/22 | [Zero](https://arxiv.org/pdf/1910.02054.pdf) é€æ®µç²¾è¯» | <img src="imgs/zero.jpg" width="200px"/> | 52:21 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1tY411g7ZT)](https://www.bilibili.com/video/BV1tY411g7ZT/) |\n| 6/10/22 | [DETR](https://arxiv.org/pdf/2005.12872.pdf) é€æ®µç²¾è¯» | <img src="imgs/detr.jpg" width="200px"/> | 54:22 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1GB4y1X72R)](https://www.bilibili.com/video/BV1GB4y1X72R/) |\n| 6/3/22 | [Megatron LM](https://arxiv.org/pdf/1909.08053.pdf) é€æ®µç²¾è¯» | <img src="imgs/megatron_lm.jpg" width="200px"/> | 56:07 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1nB4y1R7Yz)](https://www.bilibili.com/video/BV1nB4y1R7Yz/) |\n| 5/27/22 | [GPipe](https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf) é€æ®µç²¾è¯» | <img src="imgs/gpipe.jpg" width="200px"/> | 58:47 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1v34y1E7zu)](https://www.bilibili.com/video/BV1v34y1E7zu/) <br />[![](https://img.shields.io/youtube/views/eXjRpS_BTbs?style=social)](https://youtu.be/eXjRpS_BTbs)  |\n| 5/5/22 | [Pathways](https://arxiv.org/pdf/2203.12533.pdf) é€æ®µç²¾è¯» | <img src="imgs/pathways.jpg" width="200px"/> | 1:02:13 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1xB4y1m7Xi)](https://www.bilibili.com/video/BV1xB4y1m7Xi/) <br />[![](https://img.shields.io/youtube/views/8hS1ZtgG0wU?style=social)](https://youtu.be/8hS1ZtgG0wU) |\n| 4/28/22 | [è§†é¢‘ç†è§£è®ºæ–‡ä¸²è®²](https://arxiv.org/pdf/2012.06567.pdf)ï¼ˆä¸‹ï¼‰ | <img src="imgs/video-survey-p2.jpg" width="200px"/> | 1:08:32 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV11Y411P7ep)](https://www.bilibili.com/video/BV11Y411P7ep/) <br />[![](https://img.shields.io/youtube/views/J2YC0-k57NM?style=social)](https://youtu.be/J2YC0-k57NM) |\n| 4/21/22 | [å‚æ•°æœåŠ¡å™¨ï¼ˆParameter Serverï¼‰](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf) é€æ®µç²¾è¯» | <img src="imgs/ps.jpg" width="200px"/> | 1:37:40 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1YA4y197G8)](https://www.bilibili.com/video/BV1YA4y197G8/) <br />[![](https://img.shields.io/youtube/views/xt-AwUrDxQk?style=social)](https://youtu.be/xt-AwUrDxQk) |\n| 4/14/22 | [è§†é¢‘ç†è§£è®ºæ–‡ä¸²è®²](https://arxiv.org/pdf/2012.06567.pdf)ï¼ˆä¸Šï¼‰ | <img src="imgs/video-survey-p1.jpg" width="200px"/> | 51:15 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1fL4y157yA)](https://www.bilibili.com/video/BV1fL4y157yA/) <br />[![](https://img.shields.io/youtube/views/gK7AGO6okhc?style=social)](https://youtu.be/gK7AGO6okhc) |\n| 3/31/22 | [I3D](https://arxiv.org/pdf/1705.07750.pdf) è®ºæ–‡ç²¾è¯» | <img src="imgs/i3d.jpg" width="200px"/> | 52:31 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1tY4y1p7hq)](https://www.bilibili.com/video/BV1tY4y1p7hq/) <br />[![](https://img.shields.io/youtube/views/9lIkKiAn6uE?style=social)](https://youtu.be/9lIkKiAn6uE) |\n| 3/24/22 | æ–¯å¦ç¦ 2022 å¹´ [AI æŒ‡æ•°æŠ¥å‘Š](https://aiindex.stanford.edu/wp-content/uploads/2022/03/2022-AI-Index-Report_Master.pdf) ç²¾è¯» | <img src="imgs/ai_index_22.jpg" width="200px"/> | 1:19:56 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1s44y1N7eu)](https://www.bilibili.com/video/BV1s44y1N7eu/) <br />[![](https://img.shields.io/youtube/views/K8h_xjQ6ufY?style=social)](https://youtu.be/K8h_xjQ6ufY) |\n| 3/17/22 | [AlphaCode](https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf) è®ºæ–‡ç²¾è¯» | <img src="imgs/alphacode.jpg" width="200px"/> | 44:00 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1ab4y1s7rc)](https://www.bilibili.com/video/BV1ab4y1s7rc/) <br />[![](https://img.shields.io/youtube/views/t8Gzkca9pW4?style=social)](https://youtu.be/t8Gzkca9pW4) |\n| 3/10/22 | [OpenAI Codex](https://arxiv.org/pdf/2107.03374.pdf) è®ºæ–‡ç²¾è¯» | <img src="imgs/codex.jpg" width="200px"/> | 47:58 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1iY41137Zi)](https://www.bilibili.com/video/BV1iY41137Zi/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1490959755963666432)](https://www.zhihu.com/zvideo/1490959755963666432)<br />[![](https://img.shields.io/youtube/views/oZriUGkQSNM?style=social)](https://youtu.be/oZriUGkQSNM) |\n| 3/3/22 | [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [GPT-3](https://arxiv.org/abs/2005.14165) ç²¾è¯» | <img src="imgs/gpt3.jpg" width="200px"/> | 1:29:58 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1AF411b7xQ)](https://www.bilibili.com/video/BV1AF411b7xQ/)<br />[![](https://img.shields.io/youtube/views/t70Bl3w7bxY?style=social)](https://youtu.be/t70Bl3w7bxY) |\n| 2/24/22 | [Two-Stream](https://proceedings.neurips.cc/paper/2014/file/00ec53c4682d36f5c4359f4ae7bd7ba1-Paper.pdf) é€æ®µç²¾è¯» |  <img src="imgs/twostream.jpg" width="200px"/> | 52:57 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1mq4y1x7RU)](https://www.bilibili.com/video/BV1mq4y1x7RU/)<br />[![](https://img.shields.io/youtube/views/vuqwKP2iDe0?style=social)](https://youtu.be/vuqwKP2iDe0) |\n| 2/10/22 | [CLIP](https://openai.com/blog/clip/) é€æ®µç²¾è¯» | <img src="imgs/clip.jpg" width="200px"/> | 1:38:25 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1SL4y1s7LQ)](https://www.bilibili.com/video/BV1SL4y1s7LQ/)<br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1475706654562299904)](https://www.zhihu.com/zvideo/1475706654562299904) <br />[![](https://img.shields.io/youtube/views/OZF1t_Hieq8?style=social)](https://youtu.be/OZF1t_Hieq8) |\n| 2/6/22 | ä½ ï¼ˆè¢«ï¼‰åæ§½è¿‡[è®ºæ–‡ä¸å¤Ÿ novel](https://perceiving-systems.blog/en/post/novelty-in-science) å—ï¼Ÿ| <img src="imgs/novelty.jpg" width="200px"/> | 14:11 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1ea41127Bq)](https://www.bilibili.com/video/BV1ea41127Bq/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1475719090198876161)](https://www.zhihu.com/zvideo/1475719090198876161) |\n| 1/23/22 | [AlphaFold 2](https://www.nature.com/articles/s41586-021-03819-2.pdf) ç²¾è¯» | <img src="imgs/alphafold_2.jpg" width="200px"/> |  1:15:28 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1oR4y1K7Xr)](https://www.bilibili.com/video/BV1oR4y1K7Xr/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1469132410537717760)](https://www.zhihu.com/zvideo/1469132410537717760)  <br />[![](https://img.shields.io/youtube/views/Oy3OCoGUr-w?style=social)](https://youtu.be/Oy3OCoGUr-w) |\n| 1/18/22 | å¦‚ä½•åˆ¤æ–­ï¼ˆä½ è‡ªå·±çš„ï¼‰ç ”ç©¶å·¥ä½œçš„ä»·å€¼ | <img src="imgs/research_value.jpg" width="200px"/> |  9:59 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1oL411c7Us)](https://www.bilibili.com/video/BV1oL411c7Us/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1475716940051869696)](https://www.zhihu.com/zvideo/1475716940051869696) |\n| 1/15/22 | [Swin Transformer](https://arxiv.org/pdf/2103.14030.pdf) ç²¾è¯» | <img src="imgs/swin_transformer.jpg" width="200px"/> | 1:00:21 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV13L4y1475U)](https://www.bilibili.com/video/BV13L4y1475U/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1466282983652691968)](https://www.zhihu.com/zvideo/1466282983652691968)   <br />[![](https://img.shields.io/youtube/views/luP3-Fs0QCo?style=social)](https://youtu.be/luP3-Fs0QCo) |\n| 1/7/22 | [æŒ‡å¯¼æ•°å­¦ç›´è§‰](https://www.nature.com/articles/s41586-021-04086-x.pdf) | <img src="imgs/math_conj.jpg" width="200px"/> | 52:51 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1YZ4y1S72j)](https://www.bilibili.com/video/BV1YZ4y1S72j/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1464060386375299072)](https://www.zhihu.com/zvideo/1464060386375299072)  <br />[![](https://img.shields.io/youtube/views/czFGjvhtss8?style=social)](https://youtu.be/czFGjvhtss8) |\n| 1/5/22 | AlphaFold 2 é¢„å‘Š | <img src="imgs/alphafold_2_preview.jpg" width="200px"/> | 03:28 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Eu411U7Te)](https://www.bilibili.com/video/BV1Eu411U7Te/) |\n| 12/20/21 | [å¯¹æ¯”å­¦ä¹ ](#contrastive_learning)è®ºæ–‡ç»¼è¿° | <img src="imgs/contrastive.jpg" width="200px"/> | 1:32:01 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV19S4y1M7hm)](https://www.bilibili.com/video/BV19S4y1M7hm/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1460828005077164032)](https://www.zhihu.com/zvideo/1460828005077164032)  <br />[![](https://img.shields.io/youtube/views/1pvxufGRuW4?style=social)](https://www.youtube.com/watch?v=1pvxufGRuW4) |\n| 12/15/21 | [MoCo](https://arxiv.org/pdf/1911.05722.pdf) é€æ®µç²¾è¯» | <img src="imgs/mocov1.jpg" width="200px"/> | 1:24:11 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1C3411s7t9)](https://www.bilibili.com/video/BV1C3411s7t9/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1454723120678936576)](https://www.zhihu.com/zvideo/1454723120678936576)   <br />[![](https://img.shields.io/youtube/views/1pvxufGRuW4?style=social)](https://www.youtube.com/watch?v=1pvxufGRuW4) |\n| 12/9/21 | å¦‚ä½•æ‰¾ç ”ç©¶æƒ³æ³• 1 | <img src="imgs/mae_idea.jpg" width="200px"/> | 5:34 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1qq4y1z7F2)](https://www.bilibili.com/video/BV1qq4y1z7F2/) |\n| 12/8/21 | [MAE](https://arxiv.org/pdf/2111.06377.pdf) é€æ®µç²¾è¯» | <img src="imgs/mae.jpg" width="200px"/> | 47:04 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1sq4y1q77t)](https://www.bilibili.com/video/BV1sq4y1q77t/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1452458167968251904)](https://www.zhihu.com/zvideo/1452458167968251904)  <br />[![](https://img.shields.io/youtube/views/mYlX2dpdHHM?style=social)](https://youtu.be/mYlX2dpdHHM) |\n| 11/29/21 | [ViT](https://arxiv.org/pdf/2010.11929.pdf) é€æ®µç²¾è¯» | <img src="imgs/vit.jpg" width="200px"/> | 1:11:30 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV15P4y137jb)](https://www.bilibili.com/video/BV15P4y137jb/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1449195245754380288)](https://www.zhihu.com/zvideo/1449195245754380288)  <br />[![](https://img.shields.io/youtube/views/FRFt3x0bO94?style=social)](https://youtu.be/FRFt3x0bO94) |\n| 11/18/21 | [BERT](https://arxiv.org/abs/1810.04805) é€æ®µç²¾è¯» | <img src="imgs/bert.jpg" width="200px"/> | 45:49  | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1PL411M7eQ)](https://www.bilibili.com/video/BV1PL411M7eQ/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1445340200976785408)](https://www.zhihu.com/zvideo/1445340200976785408)  <br />[![](https://img.shields.io/youtube/views/ULD3uIb2MHQ?style=social)](https://youtu.be/ULD3uIb2MHQ) |\n| 11/9/21 | [GAN](https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf) é€æ®µç²¾è¯» | <img src="imgs/gan.jpg" width="200px"/> | 46:16  | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1rb4y187vD)](https://www.bilibili.com/video/BV1rb4y187vD/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1442091389241159681)](https://www.zhihu.com/zvideo/1442091389241159681)  <br />[![](https://img.shields.io/youtube/views/g_0HtlrLiDo?style=social)](https://www.youtube.com/watch?v=g_0HtlrLiDo) |\n| 11/3/21 | é›¶åŸºç¡€å¤šå›¾è¯¦è§£ [å›¾ç¥žç»ç½‘ç»œ](https://distill.pub/2021/gnn-intro/)ï¼ˆGNN/GCNï¼‰ | <img src="imgs/gnn.jpg" width="200px"/> | 1:06:19 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1iT4y1d7zP)](https://www.bilibili.com/video/BV1iT4y1d7zP/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1439540657619087360)](https://www.zhihu.com/zvideo/1439540657619087360)  <br />[![](https://img.shields.io/youtube/views/sejA2PtCITw?style=social)](https://youtu.be/sejA2PtCITw) |\n| 10/27/21 | [Transformer](https://arxiv.org/abs/1706.03762) é€æ®µç²¾è¯»<br> ï¼ˆè§†é¢‘ä¸­æåˆ°çš„æ–‡çŒ® [^transformer]) |<img src="imgs/transformer.jpg" width="200px"/> | 1:27:05 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1pu411o7BE)](https://www.bilibili.com/video/BV1pu411o7BE/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1437034536677404672)](https://www.zhihu.com/zvideo/1437034536677404672)  <br />[![](https://img.shields.io/youtube/views/nzqlFIcCSWQ?style=social)](https://youtu.be/nzqlFIcCSWQ) |\n| 10/22/21 | [ResNet](https://arxiv.org/abs/1512.03385) è®ºæ–‡é€æ®µç²¾è¯» | <img src="imgs/resnet-2.jpg" width="200px"/> | 53:46 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1P3411y7nn)](https://www.bilibili.com/video/BV1P3411y7nn/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1434795406001180672)](https://www.zhihu.com/zvideo/1434795406001180672)  <br />[![](https://img.shields.io/youtube/views/pWMnzCX4cwQ?style=social)](https://www.youtube.com/watch?v=pWMnzCX4cwQ) |\n| 10/21/21 | æ’‘èµ·è®¡ç®—æœºè§†è§‰åŠè¾¹å¤©çš„ [ResNet](https://arxiv.org/abs/1512.03385) | <img src="imgs/resnet-1.jpg" width="200px"/> | 11:50 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Fb4y1h73E)](https://www.bilibili.com/video/BV1Fb4y1h73E/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1434787226101751808)](https://www.zhihu.com/zvideo/1434787226101751808)  <br />[![](https://img.shields.io/youtube/views/NnSldWhSqvY?style=social)](https://www.youtube.com/watch?v=NnSldWhSqvY) |\n| 10/15/21 | [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) è®ºæ–‡é€æ®µç²¾è¯» | <img src="imgs/alexnet-2.jpg" width="200px"/> | 55:21 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1hq4y157t1)](https://www.bilibili.com/video/BV1hq4y157t1/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1432354207483871232)](https://www.zhihu.com/zvideo/1432354207483871232)  <br />[![](https://img.shields.io/youtube/views/wYmlILPsLlY?style=social)](https://www.youtube.com/watch?v=wYmlILPsLlY) |\n| 10/14/21 | 9å¹´åŽé‡è¯»æ·±åº¦å­¦ä¹ å¥ åŸºä½œä¹‹ä¸€ï¼š[AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) | <img src="imgs/alexnet-1.jpg" width="200px"/> | 19:59 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1ih411J7Kz)](https://www.bilibili.com/video/BV1ih411J7Kz/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1432155856322920448)](https://www.zhihu.com/zvideo/1432155856322920448)  <br />[![](https://img.shields.io/youtube/views/vdYH0fE6thY?style=social)](https://www.youtube.com/watch?v=vdYH0fE6thY) |\n| 10/06/21 | å¦‚ä½•è¯»è®ºæ–‡ | <img src="imgs/read-paper.jpg" width="200px"/> | 06:39 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1H44y1t75x)](https://www.bilibili.com/video/BV1H44y1t75x/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1428973951632969728)](https://www.zhihu.com/zvideo/1428973951632969728)  <br />[![](https://img.shields.io/youtube/views/txjl_Q4jCyQ?style=social)](https://www.youtube.com/watch?v=txjl_Q4jCyQ&list=PLFXJ6jwg0qW-7UM8iUTj3qKqdhbQULP5I&index=1) |\n\n[^transformer]: 1 [æ–¯å¦ç¦100+ä½œè€…çš„200+é¡µç»¼è¿°](https://arxiv.org/abs/2108.07258)ï¼Œ2 [å¯¹LayerNormçš„æ–°ç ”ç©¶](https://arxiv.org/pdf/1911.07013.pdf)ï¼Œ3 [å¯¹Attentionåœ¨Transformeré‡Œé¢ä½œç”¨çš„ç ”ç©¶](https://arxiv.org/abs/2103.03404)\n\n\n## æ‰€æœ‰è®ºæ–‡\n\nåŒ…æ‹¬å·²ç»å½•åˆ¶å®Œæˆå’Œä¹‹åŽå°†è¦ä»‹ç»çš„è®ºæ–‡ã€‚é€‰å–çš„åŽŸåˆ™æ˜¯10å¹´å†…æ·±åº¦å­¦ä¹ é‡Œæœ‰å½±å“åŠ›æ–‡ç« ï¼ˆå¿…è¯»æ–‡ç« ï¼‰ï¼Œæˆ–è€…è¿‘æœŸæ¯”è¾ƒæœ‰æ„æ€çš„æ–‡ç« ã€‚å½“ç„¶è¿™åå¹´é‡Œé‡è¦çš„å·¥ä½œå¤ªå¤šäº†ï¼Œä¸å¯èƒ½ä¸€ä¸€è¿‡ä¸€éã€‚åœ¨é€‰å–çš„æ—¶å€™æˆ‘ä¼šåå‘ä¸€äº›ä¹‹å‰ [ç›´æ’­è¯¾](https://c.d2l.ai/zh-v2/) ä¸­æ²¡è®²åˆ°è¿‡çš„ã€‚ æ¬¢è¿Žå¤§å®¶åœ¨ [è®¨è®ºåŒº](https://github.com/mli/paper-reading/discussions) é‡Œæä¾›å»ºï¼ˆç‚¹ï¼‰è®®ï¼ˆæ­Œï¼‰ã€‚\n\næ€»è®ºæ–‡æ•° 67ï¼Œå½•åˆ¶å®Œæˆæ•° 32\n\nï¼ˆè¿™é‡Œå¼•ç”¨é‡‡ç”¨çš„æ˜¯ semanticscholarï¼Œæ˜¯å› ä¸ºå®ƒæä¾› [API](https://api.semanticscholar.org/api-docs/graph#operation/get_graph_get_paper) å¯ä»¥è‡ªåŠ¨èŽ·å–ï¼Œä¸ç”¨æ‰‹åŠ¨æ›´æ–°ã€‚ï¼‰\n\n### è®¡ç®—æœºè§†è§‰ - CNN\n\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| âœ…      | 2012 | [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) | æ·±åº¦å­¦ä¹ çƒ­æ½®çš„å¥ åŸºä½œ                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fabd1c342495432171beb7ca8fd9551ef13cbd0ff%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff) |\n| | 2014 | [VGG](https://arxiv.org/pdf/1409.1556.pdf) | ä½¿ç”¨ 3x3 å·ç§¯æž„é€ æ›´æ·±çš„ç½‘ç»œ                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Feb42cf88027de515750f230b23b1a057dc782108%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman/eb42cf88027de515750f230b23b1a057dc782108) |\n| | 2014 | [GoogleNet](https://arxiv.org/pdf/1409.4842.pdf) | ä½¿ç”¨å¹¶è¡Œæž¶æž„æž„é€ æ›´æ·±çš„ç½‘ç»œ                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe15cf50aa89fee8535703b9f9512fca5bfc43327%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Going-deeper-with-convolutions-Szegedy-Liu/e15cf50aa89fee8535703b9f9512fca5bfc43327) |\n|  âœ…  | 2015 |  [ResNet](https://arxiv.org/pdf/1512.03385.pdf) | æž„å»ºæ·±å±‚ç½‘ç»œéƒ½è¦æœ‰çš„æ®‹å·®è¿žæŽ¥ã€‚               |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2c03df8b48bf3fa39054345bafabfeff15bfd11d%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Deep-Residual-Learning-for-Image-Recognition-He-Zhang/2c03df8b48bf3fa39054345bafabfeff15bfd11d)  |\n|  | 2017 | [MobileNet](https://arxiv.org/pdf/1704.04861.pdf) | é€‚åˆç»ˆç«¯è®¾å¤‡çš„å°CNN                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3647d6d0f151dc05626449ee09cc7bce55be497e%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/MobileNets%3A-Efficient-Convolutional-Neural-Networks-Howard-Zhu/3647d6d0f151dc05626449ee09cc7bce55be497e)  |\n| | 2019 | [EfficientNet](https://arxiv.org/pdf/1905.11946.pdf) | é€šè¿‡æž¶æž„æœç´¢å¾—åˆ°çš„CNN                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/EfficientNet%3A-Rethinking-Model-Scaling-for-Neural-Tan-Le/4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9)  |\n| | 2021 |  [Non-deep networks](https://arxiv.org/pdf/2110.07641.pdf) | è®©ä¸æ·±çš„ç½‘ç»œä¹Ÿèƒ½åœ¨ImageNetåˆ·åˆ°SOTA                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0d7f6086772079bc3e243b7b375a9ca1a517ba8b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Non-deep-Networks-Goyal-Bochkovskiy/0d7f6086772079bc3e243b7b375a9ca1a517ba8b) |\n\n### è®¡ç®—æœºè§†è§‰ - Transformer\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| âœ… | 2020 | [ViT](https://arxiv.org/pdf/2010.11929.pdf) | Transformeræ€å…¥CVç•Œ                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F7b15fa1b8d413fbe14ef7a97f651f47f5aff3903%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/An-Image-is-Worth-16x16-Words%3A-Transformers-for-at-Dosovitskiy-Beyer/7b15fa1b8d413fbe14ef7a97f651f47f5aff3903)  |\n| âœ… | 2021 | [Swin Transformer](https://arxiv.org/pdf/2103.14030.pdf) | å¤šå±‚æ¬¡çš„Vision Transformer                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc8b25fab5608c3e033d34b4483ec47e68ba109b7%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Swin-Transformer%3A-Hierarchical-Vision-Transformer-Liu-Lin/c8b25fab5608c3e033d34b4483ec47e68ba109b7) |\n| | 2021 | [MLP-Mixer](https://arxiv.org/pdf/2105.01601.pdf) | ä½¿ç”¨MLPæ›¿æ¢self-attention            |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2def61f556f9a5576ace08911496b7c7e4f970a4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/MLP-Mixer%3A-An-all-MLP-Architecture-for-Vision-Tolstikhin-Houlsby/2def61f556f9a5576ace08911496b7c7e4f970a4)  |\n| âœ… | 2021 | [MAE](https://arxiv.org/pdf/2111.06377.pdf) | BERTçš„CVç‰ˆ             |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc1962a8cf364595ed2838a097e9aa7cd159d3118%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Masked-Autoencoders-Are-Scalable-Vision-Learners-He-Chen/c1962a8cf364595ed2838a097e9aa7cd159d3118)  |\n\n### ç”Ÿæˆæ¨¡åž‹\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                              | ç®€ä»‹         | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------- | ------------ | ------------------------------------------------------------ |\n|  âœ… | 2014 | [GAN](https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf) | ç”Ÿæˆæ¨¡åž‹çš„å¼€åˆ›å·¥ä½œ                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F54e325aee6b2d476bbbb88615ac15e251c6e8214%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Generative-Adversarial-Nets-Goodfellow-Pouget-Abadie/54e325aee6b2d476bbbb88615ac15e251c6e8214)  |\n|  | 2015 | [DCGAN](https://arxiv.org/pdf/1511.06434.pdf) | ä½¿ç”¨CNNçš„GAN          |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8388f1be26329fa45e5807e968a641ce170ea078%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Unsupervised-Representation-Learning-with-Deep-Radford-Metz/8388f1be26329fa45e5807e968a641ce170ea078)  |\n|  | 2016 | [pix2pix](https://arxiv.org/pdf/1611.07004.pdf) |           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8acbe90d5b852dadea7810345451a99608ee54c7%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Image-to-Image-Translation-with-Conditional-Isola-Zhu/8acbe90d5b852dadea7810345451a99608ee54c7)  |\n|  | 2016 | [SRGAN](https://arxiv.org/pdf/1609.04802.pdf) | å›¾ç‰‡è¶…åˆ†è¾¨çŽ‡          |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdf0c54fe61f0ffb9f0e36a17c2038d9a1964cba3%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Photo-Realistic-Single-Image-Super-Resolution-Using-Ledig-Theis/df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3)  |\n|  | 2017 | [WGAN](https://arxiv.org/abs/1701.07875) | è®­ç»ƒæ›´åŠ å®¹æ˜“          |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2f85b7376769473d2bed56f855f115e23d727094%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Wasserstein-GAN-Arjovsky-Chintala/2f85b7376769473d2bed56f855f115e23d727094)  |\n|  | 2017 | [CycleGAN](https://arxiv.org/abs/1703.10593) |           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc43d954cf8133e6254499f3d68e45218067e4941%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Unpaired-Image-to-Image-Translation-Using-Networks-Zhu-Park/c43d954cf8133e6254499f3d68e45218067e4941)  |\n|  | 2018 | [StyleGAN](https://arxiv.org/abs/1812.04948) |           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fceb2ebef0b41e31c1a21b28c2734123900c005e2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/A-Style-Based-Generator-Architecture-for-Generative-Karras-Laine/ceb2ebef0b41e31c1a21b28c2734123900c005e2)  |\n| | 2019 | [StyleGAN2](https://arxiv.org/pdf/1912.04958.pdf) |        |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff3e3d1f86a534a3654d0ee263142e44f4e2c61e9%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Analyzing-and-Improving-the-Image-Quality-of-Karras-Laine/f3e3d1f86a534a3654d0ee263142e44f4e2c61e9)  |\n| | 2020 | [DDPM](https://arxiv.org/pdf/2006.11239.pdf) | Diffusion Models   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F289db3be7bf77e06e75541ba93269de3d604ac72%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Denoising-Diffusion-Probabilistic-Models-Ho-Jain/289db3be7bf77e06e75541ba93269de3d604ac72)  |\n| | 2021 | [Improved DDPM](https://arxiv.org/pdf/2102.09672.pdf) | æ”¹è¿›çš„ DDPM   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fde18baa4964804cf471d85a5a090498242d2e79f%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Improved-Denoising-Diffusion-Probabilistic-Models-Nichol-Dhariwal/de18baa4964804cf471d85a5a090498242d2e79f)  |\n| | 2021 | [Guided Diffusion Models](https://arxiv.org/pdf/2105.05233.pdf) | å·ç§°è¶…è¶Š GAN  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F64ea8f180d0682e6c18d1eb688afdb2027c02794%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Diffusion-Models-Beat-GANs-on-Image-Synthesis-Dhariwal-Nichol/64ea8f180d0682e6c18d1eb688afdb2027c02794)  |\n| | 2021 | [StyleGAN3](https://arxiv.org/pdf/2106.12423.pdf) |        |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc1ff08b59f00c44f34dfdde55cd53370733a2c19%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Alias-Free-Generative-Adversarial-Networks-Karras-Aittala/c1ff08b59f00c44f34dfdde55cd53370733a2c19)  |\n|  âœ…  | 2022 | [DALL.E 2](https://arxiv.org/pdf/2204.06125.pdf) | CLIP + Diffusion modelsï¼Œæ–‡æœ¬ç”Ÿæˆå›¾åƒæ–°é«˜åº¦     |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc57293882b2561e1ba03017902df9fc2f289dea2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Hierarchical-Text-Conditional-Image-Generation-with-Ramesh-Dhariwal/c57293882b2561e1ba03017902df9fc2f289dea2)  |\n|  âœ…  | 2024 | [Sora](https://openai.com/index/video-generation-models-as-world-simulators/) | å¼€å¯è§†é¢‘ç”Ÿæˆçƒ­æ½®     |  |\n|  âœ…  | 2024 | [Movie Gen](https://arxiv.org/pdf/2410.13720) | ç²¾ç¡®çš„æ–‡æœ¬æŒ‡å¯¼è§†é¢‘ç¼–è¾‘ã€ä¸ªæ€§åŒ–è§†é¢‘ç”Ÿæˆ     |  |\n|  âœ…  | 2025 | [HunyuanVideo](https://arxiv.org/pdf/2412.03603) | å¼€æºè§†é¢‘ç”Ÿæˆæ¡†æž¶     |  |\n\n### è®¡ç®—æœºè§†è§‰ - Object Detection\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                              | ç®€ä»‹         | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------- | ------------ | ------------------------------------------------------------ |\n|        | 2014 | [R-CNN](https://arxiv.org/pdf/1311.2524v5.pdf)    | Two-stage             |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2f4df08d9072fc2ac181b7fced6a245315ce05c8%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/2f4df08d9072fc2ac181b7fced6a245315ce05c8)  |\n|        | 2015 | [Fast R-CNN](http://arxiv.org/abs/1504.08083v2)   |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F7ffdbc358b63378f07311e883dddacc9faeeaf4b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/7ffdbc358b63378f07311e883dddacc9faeeaf4b)  |\n|        | 2015 | [Faster R-CNN](http://arxiv.org/abs/1506.01497v3) |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F424561d8585ff8ebce7d5d07de8dbf7aae5e7270%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/424561d8585ff8ebce7d5d07de8dbf7aae5e7270)  |\n|        | 2016 | [SSD](http://arxiv.org/abs/1512.02325v5)          | Single stage          |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0)  |\n|        | 2016 | [YOLO](http://arxiv.org/abs/1506.02640v5)         |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff8e79ac0ea341056ef20f2616628b3e964764cfd%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/f8e79ac0ea341056ef20f2616628b3e964764cfd)  |\n|        | 2017 | [Mask R-CNN](http://arxiv.org/abs/1703.06870v3)   |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fea99a5535388196d0d44be5b4d7dd02029a43bb2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ea99a5535388196d0d44be5b4d7dd02029a43bb2)  |\n|        | 2017 | [YOLOv2](http://arxiv.org/abs/1612.08242v1)       |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F7d39d69b23424446f0400ef603b2e3e22d0309d6%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/7d39d69b23424446f0400ef603b2e3e22d0309d6)  |\n|        | 2018 | [YOLOv3](http://arxiv.org/abs/1804.02767v1)       |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe4845fb1e624965d4f036d7fd32e8dcdd2408148%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/e4845fb1e624965d4f036d7fd32e8dcdd2408148)  |\n|        | 2019 | [CenterNet](https://arxiv.org/pdf/1904.07850.pdf) | Anchor free           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6a2e2fd1b5bb11224daef98b3fb6d029f68a73f2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Objects-as-Points-Zhou-Wang/6a2e2fd1b5bb11224daef98b3fb6d029f68a73f2)  |\n|   âœ…     | 2020 | [DETR](https://arxiv.org/pdf/2005.12872.pdf)      | Transformer           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F962dc29fdc3fbdc5930a10aba114050b82fe5a3e%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/End-to-End-Object-Detection-with-Transformers-Carion-Massa/962dc29fdc3fbdc5930a10aba114050b82fe5a3e)  |\n\n<a name="contrastive_learning"></a>\n\n### è®¡ç®—æœºè§†è§‰ - å¯¹æ¯”å­¦ä¹ \n\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| âœ…      | 2018 | [InstDisc](https://arxiv.org/pdf/1805.01978.pdf) | æå‡ºå®žä¾‹åˆ¤åˆ«å’Œmemory bankåšå¯¹æ¯”å­¦ä¹                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F155b7782dbd713982a4133df3aee7adfd0b6b304%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Unsupervised-Feature-Learning-via-Non-parametric-Wu-Xiong/155b7782dbd713982a4133df3aee7adfd0b6b304)  |\n| âœ…      | 2018 | [CPC](https://arxiv.org/pdf/1807.03748.pdf) | å¯¹æ¯”é¢„æµ‹ç¼–ç ï¼Œå›¾åƒè¯­éŸ³æ–‡æœ¬å¼ºåŒ–å­¦ä¹ å…¨éƒ½èƒ½åš                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb227f3e4c0dc96e5ac5426b85485a70f2175a205%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Representation-Learning-with-Contrastive-Predictive-Oord-Li/b227f3e4c0dc96e5ac5426b85485a70f2175a205) |\n| âœ…      | 2019 | [InvaSpread](https://arxiv.org/pdf/1904.03436.pdf) | ä¸€ä¸ªç¼–ç å™¨çš„ç«¯åˆ°ç«¯å¯¹æ¯”å­¦ä¹                    |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Unsupervised-Embedding-Learning-via-Invariant-and-Ye-Zhang/e4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b)  |\n| âœ…  | 2019 |  [CMC](https://arxiv.org/pdf/1906.05849.pdf) | å¤šè§†è§’ä¸‹çš„å¯¹æ¯”å­¦ä¹                |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F97f4d09175705be4677d675fa27e55defac44800%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Contrastive-Multiview-Coding-Tian-Krishnan/97f4d09175705be4677d675fa27e55defac44800)  |\n| âœ… | 2019 | [MoCov1](https://arxiv.org/pdf/1911.05722.pdf) | æ— ç›‘ç£è®­ç»ƒæ•ˆæžœä¹Ÿå¾ˆå¥½                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fec46830a4b275fd01d4de82bffcabe6da086128f%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Momentum-Contrast-for-Unsupervised-Visual-Learning-He-Fan/ec46830a4b275fd01d4de82bffcabe6da086128f) |\n|  âœ… | 2020 |  [SimCLRv1](https://arxiv.org/pdf/2002.05709.pdf) |  ç®€å•çš„å¯¹æ¯”å­¦ä¹  (æ•°æ®å¢žå¼º + MLP head + å¤§batchè®­ç»ƒä¹…)                  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F34733eaf66007516347a40ad5d9bbe1cc9dacb6b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/A-Simple-Framework-for-Contrastive-Learning-of-Chen-Kornblith/34733eaf66007516347a40ad5d9bbe1cc9dacb6b)  |\n|  âœ… | 2020 | [MoCov2](https://arxiv.org/pdf/2003.04297.pdf) | MoCov1 + improvements from SimCLRv1                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fa1b8a8df281bbaec148a897927a49ea47ea31515%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Improved-Baselines-with-Momentum-Contrastive-Chen-Fan/a1b8a8df281bbaec148a897927a49ea47ea31515)  |\n|  âœ… | 2020 |  [SimCLRv2](https://arxiv.org/pdf/2006.10029.pdf) | å¤§çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ¨¡åž‹å¾ˆé€‚åˆåšåŠç›‘ç£å­¦ä¹                    |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3e7f5f4382ac6f9c4fef6197dd21abf74456acd1%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Big-Self-Supervised-Models-are-Strong-Learners-Chen-Kornblith/3e7f5f4382ac6f9c4fef6197dd21abf74456acd1)  |\n| âœ…  | 2020 |  [BYOL](https://arxiv.org/pdf/2006.07733.pdf) | ä¸éœ€è¦è´Ÿæ ·æœ¬çš„å¯¹æ¯”å­¦ä¹                    | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F38f93092ece8eee9771e61c1edaf11b1293cae1b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Bootstrap-Your-Own-Latent%3A-A-New-Approach-to-Grill-Strub/38f93092ece8eee9771e61c1edaf11b1293cae1b) |\n|  âœ… | 2020 |  [SWaV](https://arxiv.org/pdf/2006.09882.pdf) | èšç±»å¯¹æ¯”å­¦ä¹                    | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F10161d83d29fc968c4612c9e9e2b61a2fc25842e%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Unsupervised-Learning-of-Visual-Features-by-Cluster-Caron-Misra/10161d83d29fc968c4612c9e9e2b61a2fc25842e) |\n|  âœ… | 2020 |  [SimSiam](https://arxiv.org/pdf/2011.10566.pdf) | åŒ–ç¹ä¸ºç®€çš„å­ªç”Ÿè¡¨å¾å­¦ä¹                    |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Exploring-Simple-Siamese-Representation-Learning-Chen-He/0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d)  |\n| âœ… | 2021 | [MoCov3](https://arxiv.org/pdf/2104.02057.pdf) | å¦‚ä½•æ›´ç¨³å®šçš„è‡ªç›‘ç£è®­ç»ƒViT                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F739ceacfafb1c4eaa17509351b647c773270b3ae%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/An-Empirical-Study-of-Training-Self-Supervised-Chen-Xie/739ceacfafb1c4eaa17509351b647c773270b3ae)  |\n|  âœ… | 2021 |  [DINO](https://arxiv.org/pdf/2104.14294.pdf) | transformeråŠ è‡ªç›‘ç£åœ¨è§†è§‰ä¹Ÿå¾ˆé¦™                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fad4a0938c48e61b7827869e4ac3baffd0aefab35%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Emerging-Properties-in-Self-Supervised-Vision-Caron-Touvron/ad4a0938c48e61b7827869e4ac3baffd0aefab35)  |\n\n\n### è®¡ç®—æœºè§†è§‰ - è§†é¢‘ç†è§£\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| âœ… | 2014 |  [DeepVideo](https://cs.stanford.edu/people/karpathy/deepvideo/) | æå‡ºsports1Mæ•°æ®é›†ï¼Œç”¨æ·±åº¦å­¦ä¹ åšè§†é¢‘ç†è§£ |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6d4c9c923e9f145d1c01a2de2afc38ec23c44253%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Large-Scale-Video-Classification-with-Convolutional-Karpathy-Toderici/6d4c9c923e9f145d1c01a2de2afc38ec23c44253)  |\n| âœ… | 2014 |  [Two-stream](https://arxiv.org/pdf/1406.2199.pdf) | å¼•å…¥å…‰æµåšæ—¶åºå»ºæ¨¡ï¼Œç¥žç»ç½‘ç»œé¦–æ¬¡è¶…è¶Šæ‰‹å·¥ç‰¹å¾ |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F67dccc9a856b60bdc4d058d83657a089b8ad4486%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Two-Stream-Convolutional-Networks-for-Action-in-Simonyan-Zisserman/67dccc9a856b60bdc4d058d83657a089b8ad4486)  |\n| âœ… | 2014 |  [C3D](https://arxiv.org/pdf/1412.0767.pdf) |  æ¯”è¾ƒæ·±çš„3D-CNNåšè§†é¢‘ç†è§£ |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fd25c65d261ea0e6a458be4c50c40ffe5bc508f77%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Learning-Spatiotemporal-Features-with-3D-Networks-Tran-Bourdev/d25c65d261ea0e6a458be4c50c40ffe5bc508f77)  |\n| âœ… | 2015 |  [Beyond-short-snippets](https://arxiv.org/pdf/1503.08909.pdf) | å°è¯•ä½¿ç”¨LSTM  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5418b2a482720e013d487a385c26fae0f017c6a6%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Beyond-short-snippets%3A-Deep-networks-for-video-Ng-Hausknecht/5418b2a482720e013d487a385c26fae0f017c6a6)  |\n| âœ… | 2016 |  [Convolutional fusion](https://arxiv.org/pdf/1604.06573.pdf) | åšearly fusionæ¥åŠ å¼ºæ—¶ç©ºé—´å»ºæ¨¡    |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9d9aced120e530484609164c836da64548693484%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Convolutional-Two-Stream-Network-Fusion-for-Video-Feichtenhofer-Pinz/9d9aced120e530484609164c836da64548693484)  |\n| âœ… | 2016 |  [TSN](https://arxiv.org/pdf/1608.00859.pdf) | è¶…çº§æœ‰æ•ˆçš„è§†é¢‘åˆ†æ®µå»ºæ¨¡ï¼Œbag of tricks in video |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fea3d7de6c0880e14455b9acb28f1bc1234321456%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Temporal-Segment-Networks%3A-Towards-Good-Practices-Wang-Xiong/ea3d7de6c0880e14455b9acb28f1bc1234321456)  |\n| âœ… | 2017 |  [I3D](https://arxiv.org/pdf/1705.07750.pdf) | æå‡ºKineticsæ•°æ®é›†ï¼Œè†¨èƒ€2Dç½‘ç»œåˆ°3Dï¼Œå¼€å¯3D-CNNæ—¶ä»£  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb61a3f8b80bbd44f24544dc915f52fd30bbdf485%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Quo-Vadis%2C-Action-Recognition-A-New-Model-and-the-Carreira-Zisserman/b61a3f8b80bbd44f24544dc915f52fd30bbdf485)  |\n| âœ… | 2017 |  [R2+1D](https://arxiv.org/pdf/1711.11248.pdf) | æ‹†åˆ†3Då·ç§¯æ ¸ï¼Œä½¿3Dç½‘ç»œå®¹æ˜“ä¼˜åŒ–  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F89c3050522a0bb9820c32dc7444e003ef0d3e2e4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/A-Closer-Look-at-Spatiotemporal-Convolutions-for-Tran-Wang/89c3050522a0bb9820c32dc7444e003ef0d3e2e4)  |\n| âœ… | 2017 |  [Non-local](https://arxiv.org/pdf/1711.07971.pdf) | å¼•å…¥è‡ªæ³¨æ„åŠ›åšè§†è§‰é—®é¢˜  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8899094797e82c5c185a0893896320ef77f60e64%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Non-local-Neural-Networks-Wang-Girshick/8899094797e82c5c185a0893896320ef77f60e64)  |\n| âœ… | 2018 |  [SlowFast](https://arxiv.org/pdf/1812.03982.pdf) | å¿«æ…¢ä¸¤æ”¯æå‡æ•ˆçŽ‡   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8b47b9c3c35b2b2a78bff7822605b3040f87d699%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/SlowFast-Networks-for-Video-Recognition-Feichtenhofer-Fan/8b47b9c3c35b2b2a78bff7822605b3040f87d699)  |\n| âœ… | 2021 |  [TimeSformer](https://arxiv.org/pdf/2102.05095.pdf) | è§†é¢‘ä¸­ç¬¬ä¸€ä¸ªå¼•å…¥transformerï¼Œå¼€å¯video transformeræ—¶ä»£ |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc143ea9e30b1f2d93a9c060253845423f9e60e1f%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Is-Space-Time-Attention-All-You-Need-for-Video-Bertasius-Wang/c143ea9e30b1f2d93a9c060253845423f9e60e1f)  |\n\n\n### å¤šæ¨¡æ€å­¦ä¹ \n\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| âœ… | 2021 |  [CLIP](https://openai.com/blog/clip/) | å›¾ç‰‡å’Œæ–‡æœ¬ä¹‹é—´çš„å¯¹æ¯”å­¦ä¹                    |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Learning-Transferable-Visual-Models-From-Natural-Radford-Kim/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4)  |\n| âœ… | 2021 |  [ViLT](https://arxiv.org/pdf/2102.03334.pdf) | ç¬¬ä¸€ä¸ªæ‘†è„±äº†ç›®æ ‡æ£€æµ‹çš„è§†è§‰æ–‡æœ¬æ¨¡åž‹      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0839722fb5369c0abaff8515bfc08299efc790a1%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ViLT%3A-Vision-and-Language-Transformer-Without-or-Kim-Son/0839722fb5369c0abaff8515bfc08299efc790a1)  |\n| âœ… | 2021 |  [ViLD](https://arxiv.org/pdf/2104.13921.pdf) | CLIPè’¸é¦å¸®åŠ©å¼€é›†ç›®æ ‡æ£€æµ‹      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcf9b8da26d9b92e75ba49616ed2a1033f59fce14%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Open-vocabulary-Object-Detection-via-Vision-and-Gu-Lin/cf9b8da26d9b92e75ba49616ed2a1033f59fce14)  |\n| âœ… | 2021 |  [GLIP](https://arxiv.org/pdf/2112.03857.pdf) | è”åˆç›®æ ‡æ£€æµ‹å’Œæ–‡æœ¬å®šä½           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5341b412383c43f4a693ad63ec4489e3ec7688c8%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Grounded-Language-Image-Pre-training-Li-Zhang/5341b412383c43f4a693ad63ec4489e3ec7688c8)  |\n| âœ… | 2021 |  [CLIP4Clip](https://arxiv.org/pdf/2104.08860.pdf) | æ‹¿CLIPç›´æŽ¥åšè§†é¢‘æ–‡æœ¬retrieval       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F281ad83e06d731d5d686acf07cd701576f1188c4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/CLIP4Clip%3A-An-Empirical-Study-of-CLIP-for-End-to-Luo-Ji/281ad83e06d731d5d686acf07cd701576f1188c4)  |\n| âœ… | 2021 |  [ActionCLIP](https://arxiv.org/pdf/2109.08472.pdf) | ç”¨å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æœ‰ç›‘ç£çš„åšè§†é¢‘åŠ¨ä½œåˆ†ç±»   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdc05240a06326b5b1664f7e8c95c330b08cd0349%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ActionCLIP%3A-A-New-Paradigm-for-Video-Action-Wang-Xing/dc05240a06326b5b1664f7e8c95c330b08cd0349)  |\n| âœ… | 2021 |  [PointCLIP](https://arxiv.org/pdf/2112.02413.pdf) | 3Då˜2Dï¼Œå·§å¦™åˆ©ç”¨CLIPåšç‚¹äº‘  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff3ce9ba3fcec362b70263a7ed63d9404975496a0%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/PointCLIP%3A-Point-Cloud-Understanding-by-CLIP-Zhang-Guo/f3ce9ba3fcec362b70263a7ed63d9404975496a0)  |\n| âœ… | 2022 |  [LSeg](https://arxiv.org/pdf/2201.03546.pdf) | æœ‰ç›‘ç£çš„å¼€é›†åˆ†å‰²                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcc9826c222ac1e81b4b374dd9e0df130f298b1e8%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Language-driven-Semantic-Segmentation-Li-Weinberger/cc9826c222ac1e81b4b374dd9e0df130f298b1e8)  |\n| âœ… | 2022 |  [GroupViT](https://arxiv.org/pdf/2202.11094.pdf) | åªç”¨å›¾åƒæ–‡æœ¬å¯¹ä¹Ÿèƒ½æ— ç›‘ç£åšåˆ†å‰²        |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0b5f27a5766c5d1394a6282ad94fec21d620bd6b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/GroupViT%3A-Semantic-Segmentation-Emerges-from-Text-Xu-Mello/0b5f27a5766c5d1394a6282ad94fec21d620bd6b)  |\n| âœ… | 2022 |  [CLIPasso](https://arxiv.org/pdf/2202.05822.pdf) | CLIPè·¨ç•Œç”Ÿæˆç®€ç¬”ç”»   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9dec819778bebae4a468c7813f7638534c826f52%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/CLIPasso%3A-Semantically-Aware-Object-Sketching-Vinker-Pajouheshgar/9dec819778bebae4a468c7813f7638534c826f52)  |\n| âœ… | 2022 |  [DepthCLIP](https://arxiv.org/pdf/2207.01077.pdf) | ç”¨æ–‡æœ¬è·¨ç•Œä¼°è®¡æ·±åº¦   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9d0afe58801fe9e5537902e853d6e9e385340a92%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Can-Language-Understand-Depth-Zhang-Zeng/9d0afe58801fe9e5537902e853d6e9e385340a92)  |\n\n\n\n### è‡ªç„¶è¯­è¨€å¤„ç† - Transformer\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| âœ… | 2017 | [Transformer](https://arxiv.org/abs/1706.03762) | ç»§MLPã€CNNã€RNNåŽçš„ç¬¬å››å¤§ç±»æž¶æž„                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F204e3073870fae3d05bcbc2f6a8e263d9b72e776%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776)  |\n| âœ… | 2018 | [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) | ä½¿ç”¨ Transformer è§£ç å™¨æ¥åšé¢„è®­ç»ƒ               |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcd18800a0fe0b668a1cc19f2ec95b5003d0a5035%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)  |\n| âœ… | 2018 | [BERT](https://arxiv.org/abs/1810.04805) | Transformerä¸€ç»ŸNLPçš„å¼€å§‹                  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdf2b0e26d0599ce3e70df8a9da02e51594e0e992%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992)  |\n| âœ… | 2019 | [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  |  æ›´å¤§çš„ GPT æ¨¡åž‹ï¼Œæœç€zero-shot learningè¿ˆäº†ä¸€å¤§æ­¥             |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9405cc0d6169988371b2755e573cc28650d14dfe%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)  |\n| âœ… | 2020 |  [GPT-3](https://arxiv.org/abs/2005.14165) | 100å€æ›´å¤§çš„ GPT-2ï¼Œfew-shot learningæ•ˆæžœæ˜¾è‘—                  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6b85b63579a916f705a8e10a49bd8d849d91b1fc%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Language-Models-are-Few-Shot-Learners-Brown-Mann/6b85b63579a916f705a8e10a49bd8d849d91b1fc)  |\n| âœ… | 2024 |  [Llama 3.1](https://arxiv.org/pdf/2407.21783) | å¼ºå¤§çš„Metaå¼€æºæ¨¡åž‹ - åŠ¨æ€æ‰©å±•ï¼Œå¤šæ¨¡æ€å­¦ä¹ ï¼Œé›¶æ ·æœ¬å­¦ä¹ ï¼Œé«˜æ•ˆè®¡ç®—                  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F4176a4cecfaef26b2c503827493867e703f3411a%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/4176a4cecfaef26b2c503827493867e703f3411a)  |\n\n### ç³»ç»Ÿ\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n|  âœ… |  2014 | [å‚æ•°æœåŠ¡å™¨](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf) | æ”¯æŒåƒäº¿å‚æ•°çš„ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡åž‹       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0de0c3240bda7972bd0a3c8369ebc4b4f2e4f9c2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Scaling-Distributed-Machine-Learning-with-the-Li-Andersen/0de0c3240bda7972bd0a3c8369ebc4b4f2e4f9c2)  |\n| âœ…  | 2018 | [GPipe](https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf) | æµæ°´çº¿ï¼ˆPipelineï¼‰å¹¶è¡Œ      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc18663fea10c8a303d045fd2c1f33cacf9b73ca3%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/GPipe%3A-Efficient-Training-of-Giant-Neural-Networks-Huang-Cheng/c18663fea10c8a303d045fd2c1f33cacf9b73ca3)  |\n| âœ… | 2019 | [Megatron-LM](https://arxiv.org/pdf/1909.08053.pdf) | å¼ é‡ï¼ˆTensorï¼‰å¹¶è¡Œ      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8323c591e119eb09b28b29fd6c7bc76bd889df7a%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/8323c591e119eb09b28b29fd6c7bc76bd889df7a) |\n| âœ… | 2019 | [Zero](https://arxiv.org/pdf/1910.02054.pdf) | å‚æ•°åˆ†ç‰‡      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F00c957711b12468cb38424caccdf5291bb354033%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ZeRO%3A-Memory-optimizations-Toward-Training-Trillion-Rajbhandari-Rasley/00c957711b12468cb38424caccdf5291bb354033)  |\n| âœ… |  2022 | [Pathways](https://arxiv.org/pdf/2203.12533.pdf) |  å°†Jaxæ‹“å±•åˆ°ä¸ŠåƒTPUæ ¸ä¸Š       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F512e9aa873a1cd7eb61ce1f25ca7df6acb7e2352%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Pathways%3A-Asynchronous-Distributed-Dataflow-for-ML-Barham-Chowdhery/512e9aa873a1cd7eb61ce1f25ca7df6acb7e2352)  |\n\n### å›¾ç¥žç»ç½‘ç»œ\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n|  âœ… |  2021 | [å›¾ç¥žç»ç½‘ç»œä»‹ç»](https://distill.pub/2021/gnn-intro/) | GNNçš„å¯è§†åŒ–ä»‹ç»                  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2c0e0440882a42be752268d0b64243243d752a74%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/A-Gentle-Introduction-to-Graph-Neural-Networks-S%C3%A1nchez-Lengeling-Reif/2c0e0440882a42be752268d0b64243243d752a74)  |\n\n### ä¼˜åŒ–ç®—æ³•\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| | 2014 | [Adam](https://arxiv.org/abs/1412.6980) | æ·±åº¦å­¦ä¹ é‡Œæœ€å¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•ä¹‹ä¸€                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fa6cb366736791bcccc5c8639de5a8f9636bf87e8%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Adam%3A-A-Method-for-Stochastic-Optimization-Kingma-Ba/a6cb366736791bcccc5c8639de5a8f9636bf87e8)  |\n| | 2016 |  [ä¸ºä»€ä¹ˆè¶…å¤§çš„æ¨¡åž‹æ³›åŒ–æ€§ä¸é”™](https://arxiv.org/abs/1611.03530)   |               |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F54ddb00fa691728944fd8becea90a373d21597cf%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Understanding-deep-learning-requires-rethinking-Zhang-Bengio/54ddb00fa691728944fd8becea90a373d21597cf)  |\n| | 2017 | [ä¸ºä»€ä¹ˆMomentumæœ‰æ•ˆ](https://distill.pub/2017/momentum/) | Distillçš„å¯è§†åŒ–ä»‹ç»            |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3e8ccf9d3d843c9855c5d76ab66d3e775384da72%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Why-Momentum-Really-Works-Goh/3e8ccf9d3d843c9855c5d76ab66d3e775384da72)  |\n\n\n### æ–°é¢†åŸŸåº”ç”¨\n\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| | 2016 | [AlphaGo](https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf) | å¼ºåŒ–å­¦ä¹ å‡ºåœˆ                 |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F846aedd869a00c09b40f1f1f35673cb22bc87490%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Mastering-the-game-of-Go-with-deep-neural-networks-Silver-Huang/846aedd869a00c09b40f1f1f35673cb22bc87490)  |\n| | 2020 | [AlphaFold](https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf) | èµ¢å¾—æ¯”èµ›çš„çš„è›‹ç™½è´¨3Dç»“æž„é¢„æµ‹ |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3a083d843f891b3574494c385699c21766ce8b7a%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Improved-protein-structure-prediction-using-from-Senior-Evans/3a083d843f891b3574494c385699c21766ce8b7a)  |\n| âœ… | 2021 | [AlphaFold 2](https://www.nature.com/articles/s41586-021-03819-2.pdf) | åŽŸå­çº§åˆ«ç²¾åº¦çš„è›‹ç™½è´¨3Dç»“æž„é¢„æµ‹       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdc32a984b651256a8ec282be52310e6bd33d9815%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Highly-accurate-protein-structure-prediction-with-Jumper-Evans/dc32a984b651256a8ec282be52310e6bd33d9815)  |\n| âœ… | 2021 | [Codex](https://arxiv.org/pdf/2107.03374.pdf) | ä½¿ç”¨æ³¨é‡Šç”Ÿæˆä»£ç        |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Facbdbf49f9bc3f151b93d9ca9a06009f4f6eb269%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Evaluating-Large-Language-Models-Trained-on-Code-Chen-Tworek/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269)  |\n| âœ… | 2021 | [æŒ‡å¯¼æ•°å­¦ç›´è§‰](https://www.nature.com/articles/s41586-021-04086-x.pdf) | åˆ†æžä¸åŒæ•°å­¦ç‰©ä½“ä¹‹å‰çš„è”ç³»æ¥å¸®åŠ©å‘çŽ°æ–°å®šç†         |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff672b8fb430606fee0bb368f16603531ce1e90c4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Advancing-mathematics-by-guiding-human-intuition-AI-Davies-Velickovic/f672b8fb430606fee0bb368f16603531ce1e90c4)  |\n| âœ… | 2022 | [AlphaCode](https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf) | åª²ç¾Žä¸€èˆ¬ç¨‹åºå‘˜çš„ç¼–ç¨‹è§£é¢˜æ°´å¹³       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5cbe278b65a81602a864184bbca37de91448a5f5%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Competition-Level-Code-Generation-with-AlphaCode-Li-Choi/5cbe278b65a81602a864184bbca37de91448a5f5)  |\n\n', '{"language":null,"stars":32098,"forks":2753,"watchers":32098,"open_issues":1,"topics":["deep-learning","paper","reading-list"],"default_branch":"main","size_kb":9001,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:mli:paper-reading","source_url":"https://github.com/mli/paper-reading"}]', NULL, 'Apache-2.0', 'approved', 80, '0ce0c189873cc44bc9aea40a09abb5b9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-mli-paper-reading from https://github.com/mli.png
Image converted to WebP: data/images/github-mli-paper-reading.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-yunjey-pytorch-tutorial', 'github--yunjey--pytorch-tutorial', 'pytorch-tutorial', 'yunjey', '<p align="center"><img width="40%" src="logo/pytorch_logo_2018.svg" /></p> -------------------------------------------------------------------------------- This repository provides tutorial code for deep learning researchers to learn PyTorch. In the tutorial, most of the models were implemented with less than 30 lines of code. Before starting this tutorial, it is recommended to finish Official Pytorch Tutorial. <br/> * PyTorch Basics * Linear Regression * Logistic Regression * Feedforward Neu...', '["deep-learning","neural-networks","pytorch","pytorch-tutorial","python"]', 'other', 31997, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/yunjey/pytorch-tutorial","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center"><img width="40%" src="logo/pytorch_logo_2018.svg" /></p>\n\n--------------------------------------------------------------------------------\n\nThis repository provides tutorial code for deep learning researchers to learn [PyTorch](https://github.com/pytorch/pytorch). In the tutorial, most of the models were implemented with less than 30 lines of code. Before starting this tutorial, it is recommended to finish [Official Pytorch Tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html).\n\n\n<br/>\n\n## Table of Contents\n\n#### 1. Basics\n* [PyTorch Basics](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/pytorch_basics/main.py)\n* [Linear Regression](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/linear_regression/main.py#L22-L23)\n* [Logistic Regression](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/logistic_regression/main.py#L33-L34)\n* [Feedforward Neural Network](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/feedforward_neural_network/main.py#L37-L49)\n\n#### 2. Intermediate\n* [Convolutional Neural Network](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/convolutional_neural_network/main.py#L35-L56)\n* [Deep Residual Network](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/deep_residual_network/main.py#L76-L113)\n* [Recurrent Neural Network](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/recurrent_neural_network/main.py#L39-L58)\n* [Bidirectional Recurrent Neural Network](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/bidirectional_recurrent_neural_network/main.py#L39-L58)\n* [Language Model (RNN-LM)](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/language_model/main.py#L30-L50)\n\n#### 3. Advanced\n* [Generative Adversarial Networks](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/generative_adversarial_network/main.py#L41-L57)\n* [Variational Auto-Encoder](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/variational_autoencoder/main.py#L38-L65)\n* [Neural Style Transfer](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/neural_style_transfer)\n* [Image Captioning (CNN-RNN)](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning)\n\n#### 4. Utilities\n* [TensorBoard in PyTorch](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/04-utils/tensorboard)\n\n\n<br/>\n\n## Getting Started\n```bash\n$ git clone https://github.com/yunjey/pytorch-tutorial.git\n$ cd pytorch-tutorial/tutorials/PATH_TO_PROJECT\n$ python main.py\n```\n\n<br/>\n\n## Dependencies\n* [Python 2.7 or 3.5+](https://www.continuum.io/downloads)\n* [PyTorch 0.4.0+](http://pytorch.org/)\n\n\n\n\n', '{"language":"Python","stars":31997,"forks":8276,"watchers":31997,"open_issues":89,"topics":["deep-learning","neural-networks","pytorch","pytorch-tutorial"],"default_branch":"master","size_kb":13110,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial.git","source_url":"https://github.com/yunjey/pytorch-tutorial.git"}]', NULL, 'MIT', 'approved', 65, '813384de2a7b87f7a91b75b7dda66737', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-yunjey-pytorch-tutorial from https://github.com/yunjey.png
Image converted to WebP: data/images/github-yunjey-pytorch-tutorial.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-huggingface-diffusers', 'github--huggingface--diffusers', 'diffusers', 'huggingface', '<!--- Copyright 2022 - The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the Lice...', '["deep-learning","diffusion","flux","image-generation","image2image","image2video","latent-diffusion-models","pytorch","qwen-image","score-based-generative-modeling","stable-diffusion","stable-diffusion-diffusers","text2image","text2video","video2video","python"]', 'other', 31972, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/huggingface/diffusers","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<!---\nCopyright 2022 - The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the "License");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an "AS IS" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align="center">\n    <br>\n    <img src="https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/en/imgs/diffusers_library.jpg" width="400"/>\n    <br>\n<p>\n<p align="center">\n    <a href="https://github.com/huggingface/diffusers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/datasets.svg?color=blue"></a>\n    <a href="https://github.com/huggingface/diffusers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/diffusers.svg"></a>\n    <a href="https://pepy.tech/project/diffusers"><img alt="GitHub release" src="https://static.pepy.tech/badge/diffusers/month"></a>\n    <a href="CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg"></a>\n    <a href="https://twitter.com/diffuserslib"><img alt="X account" src="https://img.shields.io/twitter/url/https/twitter.com/diffuserslib.svg?style=social&label=Follow%20%40diffuserslib"></a>\n</p>\n\nðŸ¤— Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you''re looking for a simple inference solution or training your own diffusion models, ðŸ¤— Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\nðŸ¤— Diffusers offers three core components:\n\n- State-of-the-art [diffusion pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview) that can be run in inference with just a few lines of code.\n- Interchangeable noise [schedulers](https://huggingface.co/docs/diffusers/api/schedulers/overview) for different diffusion speeds and output quality.\n- Pretrained [models](https://huggingface.co/docs/diffusers/api/models/overview) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.\n\n## Installation\n\nWe recommend installing ðŸ¤— Diffusers in a virtual environment from PyPI or Conda. For more details about installing [PyTorch](https://pytorch.org/get-started/locally/), please refer to their official documentation.\n\n### PyTorch\n\nWith `pip` (official package):\n\n```bash\npip install --upgrade diffusers[torch]\n```\n\nWith `conda` (maintained by the community):\n\n```sh\nconda install -c conda-forge diffusers\n```\n\n### Apple Silicon (M1/M2) support\n\nPlease refer to the [How to use Stable Diffusion in Apple Silicon](https://huggingface.co/docs/diffusers/optimization/mps) guide.\n\n## Quickstart\n\nGenerating outputs is super easy with ðŸ¤— Diffusers. To generate an image from text, use the `from_pretrained` method to load any pretrained diffusion model (browse the [Hub](https://huggingface.co/models?library=diffusers&sort=downloads) for 30,000+ checkpoints):\n\n```python\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained("stable-diffusion-v1-5/stable-diffusion-v1-5", torch_dtype=torch.float16)\npipeline.to("cuda")\npipeline("An image of a squirrel in Picasso style").images[0]\n```\n\nYou can also dig into the models and schedulers toolbox to build your own diffusion system:\n\n```python\nfrom diffusers import DDPMScheduler, UNet2DModel\nfrom PIL import Image\nimport torch\n\nscheduler = DDPMScheduler.from_pretrained("google/ddpm-cat-256")\nmodel = UNet2DModel.from_pretrained("google/ddpm-cat-256").to("cuda")\nscheduler.set_timesteps(50)\n\nsample_size = model.config.sample_size\nnoise = torch.randn((1, 3, sample_size, sample_size), device="cuda")\ninput = noise\n\nfor t in scheduler.timesteps:\n    with torch.no_grad():\n        noisy_residual = model(input, t).sample\n        prev_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample\n        input = prev_noisy_sample\n\nimage = (input / 2 + 0.5).clamp(0, 1)\nimage = image.cpu().permute(0, 2, 3, 1).numpy()[0]\nimage = Image.fromarray((image * 255).round().astype("uint8"))\nimage\n```\n\nCheck out the [Quickstart](https://huggingface.co/docs/diffusers/quicktour) to launch your diffusion journey today!\n\n## How to navigate the documentation\n\n| **Documentation**                                                   | **What can I learn?**                                                                                                                                                                           |\n|---------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [Tutorial](https://huggingface.co/docs/diffusers/tutorials/tutorial_overview)                                                            | A basic crash course for learning how to use the library''s most important features like using models and schedulers to build your own diffusion system, and training your own diffusion model.  |\n| [Loading](https://huggingface.co/docs/diffusers/using-diffusers/loading)                                                             | Guides for how to load and configure all the components (pipelines, models, and schedulers) of the library, as well as how to use different schedulers.                                         |\n| [Pipelines for inference](https://huggingface.co/docs/diffusers/using-diffusers/overview_techniques)                                             | Guides for how to use pipelines for different inference tasks, batched generation, controlling generated outputs and randomness, and how to contribute a pipeline to the library.               |\n| [Optimization](https://huggingface.co/docs/diffusers/optimization/fp16)                                                        | Guides for how to optimize your diffusion model to run faster and consume less memory.                                                                                                          |\n| [Training](https://huggingface.co/docs/diffusers/training/overview) | Guides for how to train a diffusion model for different tasks with different training techniques.                                                                                               |\n## Contribution\n\nWe â¤ï¸  contributions from the open-source community!\nIf you want to contribute to this library, please check out our [Contribution guide](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md).\nYou can look out for [issues](https://github.com/huggingface/diffusers/issues) you''d like to tackle to contribute to the library.\n- See [Good first issues](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22) for general opportunities to contribute\n- See [New model/pipeline](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+pipeline%2Fmodel%22) to contribute exciting new diffusion models / diffusion pipelines\n- See [New scheduler](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+scheduler%22)\n\nAlso, say ðŸ‘‹ in our public Discord channel <a href="https://discord.gg/G7tWnz98XR"><img alt="Join us on Discord" src="https://img.shields.io/discord/823813159592001537?color=5865F2&logo=discord&logoColor=white"></a>. We discuss the hottest trends about diffusion models, help each other with contributions, personal projects or just hang out â˜•.\n\n\n## Popular Tasks & Pipelines\n\n<table>\n  <tr>\n    <th>Task</th>\n    <th>Pipeline</th>\n    <th>ðŸ¤— Hub</th>\n  </tr>\n  <tr style="border-top: 2px solid black">\n    <td>Unconditional Image Generation</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/ddpm"> DDPM </a></td>\n    <td><a href="https://huggingface.co/google/ddpm-ema-church-256"> google/ddpm-ema-church-256 </a></td>\n  </tr>\n  <tr style="border-top: 2px solid black">\n    <td>Text-to-Image</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img">Stable Diffusion Text-to-Image</a></td>\n      <td><a href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5"> stable-diffusion-v1-5/stable-diffusion-v1-5 </a></td>\n  </tr>\n  <tr>\n    <td>Text-to-Image</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/unclip">unCLIP</a></td>\n      <td><a href="https://huggingface.co/kakaobrain/karlo-v1-alpha"> kakaobrain/karlo-v1-alpha </a></td>\n  </tr>\n  <tr>\n    <td>Text-to-Image</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if">DeepFloyd IF</a></td>\n      <td><a href="https://huggingface.co/DeepFloyd/IF-I-XL-v1.0"> DeepFloyd/IF-I-XL-v1.0 </a></td>\n  </tr>\n  <tr>\n    <td>Text-to-Image</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/kandinsky">Kandinsky</a></td>\n      <td><a href="https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder"> kandinsky-community/kandinsky-2-2-decoder </a></td>\n  </tr>\n  <tr style="border-top: 2px solid black">\n    <td>Text-guided Image-to-Image</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/controlnet">ControlNet</a></td>\n      <td><a href="https://huggingface.co/lllyasviel/sd-controlnet-canny"> lllyasviel/sd-controlnet-canny </a></td>\n  </tr>\n  <tr>\n    <td>Text-guided Image-to-Image</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/pix2pix">InstructPix2Pix</a></td>\n      <td><a href="https://huggingface.co/timbrooks/instruct-pix2pix"> timbrooks/instruct-pix2pix </a></td>\n  </tr>\n  <tr>\n    <td>Text-guided Image-to-Image</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img">Stable Diffusion Image-to-Image</a></td>\n      <td><a href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5"> stable-diffusion-v1-5/stable-diffusion-v1-5 </a></td>\n  </tr>\n  <tr style="border-top: 2px solid black">\n    <td>Text-guided Image Inpainting</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/inpaint">Stable Diffusion Inpainting</a></td>\n      <td><a href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-inpainting"> stable-diffusion-v1-5/stable-diffusion-inpainting </a></td>\n  </tr>\n  <tr style="border-top: 2px solid black">\n    <td>Image Variation</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation">Stable Diffusion Image Variation</a></td>\n      <td><a href="https://huggingface.co/lambdalabs/sd-image-variations-diffusers"> lambdalabs/sd-image-variations-diffusers </a></td>\n  </tr>\n  <tr style="border-top: 2px solid black">\n    <td>Super Resolution</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/upscale">Stable Diffusion Upscale</a></td>\n      <td><a href="https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler"> stabilityai/stable-diffusion-x4-upscaler </a></td>\n  </tr>\n  <tr>\n    <td>Super Resolution</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/latent_upscale">Stable Diffusion Latent Upscale</a></td>\n      <td><a href="https://huggingface.co/stabilityai/sd-x2-latent-upscaler"> stabilityai/sd-x2-latent-upscaler </a></td>\n  </tr>\n</table>\n\n## Popular libraries using ðŸ§¨ Diffusers\n\n- https://github.com/microsoft/TaskMatrix\n- https://github.com/invoke-ai/InvokeAI\n- https://github.com/InstantID/InstantID\n- https://github.com/apple/ml-stable-diffusion\n- https://github.com/Sanster/lama-cleaner\n- https://github.com/IDEA-Research/Grounded-Segment-Anything\n- https://github.com/ashawkey/stable-dreamfusion\n- https://github.com/deep-floyd/IF\n- https://github.com/bentoml/BentoML\n- https://github.com/bmaltais/kohya_ss\n- +14,000 other amazing GitHub repositories ðŸ’ª\n\nThank you for using us â¤ï¸.\n\n## Credits\n\nThis library concretizes previous work by many different authors and would not have been possible without their great research and implementations. We''d like to thank, in particular, the following implementations which have helped us in our development and without which the API could not have been as polished today:\n\n- @CompVis'' latent diffusion models library, available [here](https://github.com/CompVis/latent-diffusion)\n- @hojonathanho original DDPM implementation, available [here](https://github.com/hojonathanho/diffusion) as well as the extremely useful translation into PyTorch by @pesser, available [here](https://github.com/pesser/pytorch_diffusion)\n- @ermongroup''s DDIM implementation, available [here](https://github.com/ermongroup/ddim)\n- @yang-song''s Score-VE and Score-VP implementations, available [here](https://github.com/yang-song/score_sde_pytorch)\n\nWe also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, available [here](https://github.com/heejkoo/Awesome-Diffusion-Models) as well as @crowsonkb and @rromb for useful discussions and insights.\n\n## Citation\n\n```bibtex\n@misc{von-platen-etal-2022-diffusers,\n  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Dhruv Nair and Sayak Paul and William Berman and Yiyi Xu and Steven Liu and Thomas Wolf},\n  title = {Diffusers: State-of-the-art diffusion models},\n  year = {2022},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\url{https://github.com/huggingface/diffusers}}\n}\n```\n', '{"language":"Python","stars":31972,"forks":6575,"watchers":31972,"open_issues":948,"topics":["deep-learning","diffusion","flux","image-generation","image2image","image2video","latent-diffusion-models","pytorch","qwen-image","score-based-generative-modeling","stable-diffusion","stable-diffusion-diffusers","text2image","text2video","video2video"],"default_branch":"main","size_kb":89556,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:microsoft:TaskMatrix","source_url":"https://github.com/microsoft/TaskMatrix"},{"type":"has_code","target_id":"github:invoke-ai:InvokeAI","source_url":"https://github.com/invoke-ai/InvokeAI"},{"type":"has_code","target_id":"github:InstantID:InstantID","source_url":"https://github.com/InstantID/InstantID"},{"type":"has_code","target_id":"github:apple:ml-stable-diffusion","source_url":"https://github.com/apple/ml-stable-diffusion"},{"type":"has_code","target_id":"github:Sanster:lama-cleaner","source_url":"https://github.com/Sanster/lama-cleaner"},{"type":"has_code","target_id":"github:IDEA-Research:Grounded-Segment-Anything","source_url":"https://github.com/IDEA-Research/Grounded-Segment-Anything"},{"type":"has_code","target_id":"github:ashawkey:stable-dreamfusion","source_url":"https://github.com/ashawkey/stable-dreamfusion"},{"type":"has_code","target_id":"github:deep-floyd:IF","source_url":"https://github.com/deep-floyd/IF"},{"type":"has_code","target_id":"github:bentoml:BentoML","source_url":"https://github.com/bentoml/BentoML"},{"type":"has_code","target_id":"github:bmaltais:kohya_ss","source_url":"https://github.com/bmaltais/kohya_ss"},{"type":"has_code","target_id":"github:CompVis:latent-diffusion","source_url":"https://github.com/CompVis/latent-diffusion"},{"type":"has_code","target_id":"github:hojonathanho:diffusion","source_url":"https://github.com/hojonathanho/diffusion"},{"type":"has_code","target_id":"github:pesser:pytorch_diffusion","source_url":"https://github.com/pesser/pytorch_diffusion"},{"type":"has_code","target_id":"github:ermongroup:ddim","source_url":"https://github.com/ermongroup/ddim"},{"type":"has_code","target_id":"github:yang-song:score_sde_pytorch","source_url":"https://github.com/yang-song/score_sde_pytorch"},{"type":"has_code","target_id":"github:heejkoo:Awesome-Diffusion-Models","source_url":"https://github.com/heejkoo/Awesome-Diffusion-Models"},{"type":"has_code","target_id":"github:huggingface:diffusers}}","source_url":"https://github.com/huggingface/diffusers}}"}]', NULL, 'Apache-2.0', 'approved', 80, '2d38181d2c984df1b26b2e0abfdda324', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-huggingface-diffusers from https://github.com/huggingface.png
Image converted to WebP: data/images/github-huggingface-diffusers.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-tatsu-lab-stanford-alpaca', 'github--tatsu-lab--stanford-alpaca', 'stanford_alpaca', 'tatsu-lab', '<p align="center" width="100%"> <img src="assets/logo.png" alt="Stanford-Alpaca" style="width: 50%; min-width: 300px; display: block; margin: auto;"> </p> This is the repo for the Stanford Alpaca project, which aims to build and share an instruction-following LLaMA model. The repo contains: - The 52K data used for fine-tuning the model. - The code for generating the data. - The code for fine-tuning the model. - The code for recovering Alpaca-7B weights from our released weight diff. Note: We ...', '["deep-learning","instruction-following","language-model","python"]', 'other', 30243, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/tatsu-lab/stanford_alpaca","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '\n<p align="center" width="100%">\n<img src="assets/logo.png" alt="Stanford-Alpaca" style="width: 50%; min-width: 300px; display: block; margin: auto;">\n</p>\n\n# Stanford Alpaca: An Instruction-following LLaMA Model\n\n[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/LICENSE)\n[![Data License](https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/DATA_LICENSE)\n[![Weight Diff License](https://img.shields.io/badge/Weight%20Diff%20License-CC%20By%20NC%204.0-yellow)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/WEIGHT_DIFF_LICENSE)\n[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\nThis is the repo for the Stanford Alpaca project, which aims to build and share an instruction-following LLaMA model. The repo contains:\n\n- The [52K data](#data-release) used for fine-tuning the model.\n- The code for [generating the data](#data-generation-process).\n- The code for [fine-tuning the model](#fine-tuning).\n- The code for [recovering Alpaca-7B weights from our released weight diff](#recovering-alpaca-weights).\n\nNote: We thank the community for feedback on Stanford-Alpaca and supporting our research. Our live demo is suspended until further notice.\n\n**Usage and License Notices**: Alpaca is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes. \nThe weight diff is also CC BY NC 4.0 (allowing only non-commercial use).\n\n## Overview\n\nThe current Alpaca model is fine-tuned from a 7B LLaMA model [1] on 52K instruction-following data generated by the techniques in the Self-Instruct [2] paper, with some modifications that we discuss in the next section.\nIn a preliminary human evaluation, we found that the Alpaca 7B model behaves similarly to the `text-davinci-003` model on the Self-Instruct instruction-following evaluation suite [2].\n\nAlpaca is still under development, and there are many limitations that have to be addressed.\nImportantly, we have not yet fine-tuned the Alpaca model to be safe and harmless.\nWe thus encourage users to be cautious when interacting with Alpaca, and to report any concerning behavior to help improve the safety and ethical considerations of the model.\n\nOur initial release contains the data generation procedure, dataset, and training recipe. We intend to release the model weights if we are given permission to do so by the creators of LLaMA. For now, we have chosen to host a live demo to help readers better understand the capabilities and limits of Alpaca, as well as a way to help us better evaluate Alpaca''s performance on a broader audience.\n\n**Please read our release [blog post](https://crfm.stanford.edu/2023/03/13/alpaca.html) for more details about the model, our discussion of the potential harm and limitations of Alpaca models, and our thought process for releasing a reproducible model.**\n\n[1]: LLaMA: Open and Efficient Foundation Language Models. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. https://arxiv.org/abs/2302.13971v1\n\n[2]: Self-Instruct: Aligning Language Model with Self Generated Instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi. https://arxiv.org/abs/2212.10560\n\n## Data Release\n\n[`alpaca_data.json`](./alpaca_data.json) contains 52K instruction-following data we used for fine-tuning the Alpaca model.\nThis JSON file is a list of dictionaries, each dictionary contains the following fields:\n\n- `instruction`: `str`, describes the task the model should perform. Each of the 52K instructions is unique.\n- `input`: `str`, optional context or input for the task. For example, when the instruction is "Summarize the following article", the input is the article. Around 40% of the examples have an input.\n- `output`: `str`, the answer to the instruction as generated by `text-davinci-003`.\n\nWe used the following prompts for fine-tuning the Alpaca model:\n\n- for examples with a non-empty input field:\n\n ```\n Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n \n ### Instruction:\n {instruction}\n \n ### Input:\n {input}\n \n ### Response:\n ```\n\n- for examples with an empty input field:\n\n ```\n Below is an instruction that describes a task. Write a response that appropriately completes the request.\n \n ### Instruction:\n {instruction}\n \n ### Response:\n ```\n\n During inference (eg for the web demo), we use the user instruction with an empty input field (second option).\n\n## Data Generation Process\n\n<details>\n<summary> <strong> Running the code </strong> </summary>\n\n1. Set environment variables `OPENAI_API_KEY` to your OpenAI API key.\n2. Install the dependencies with `pip install -r requirements.txt`.\n3. Run `python -m generate_instruction generate_instruction_following_data` to generate the data.\n\n</details>\n\nWe built on the data generation pipeline from [self-instruct](https://github.com/yizhongw/self-instruct) and made the following modifications:\n\n- We used `text-davinci-003` to generate the instruction data instead of `davinci`.\n- We wrote a new prompt (`prompt.txt`) that explicitly gave the requirement of instruction generation to `text-davinci-003`. Note: there is a slight error in the prompt we used, and future users should incorporate the edit in <https://github.com/tatsu-lab/stanford_alpaca/pull/24>\n- We adopted much more aggressive batch decoding, i.e., generating 20 instructions at once, which significantly reduced the cost of data generation.\n- We simplified the data generation pipeline by discarding the difference between classification and non-classification instructions.\n- We only generated a single instance for each instruction, instead of 2 to 3 instances as in [1].\n\nThis produced an instruction-following dataset with 52K examples obtained at a much lower cost (less than $500).\nIn a preliminary study, we also find our 52K generated data to be much more diverse than the data released by [self-instruct](https://github.com/yizhongw/self-instruct/blob/main/data/seed_tasks.jsonl).\nWe plot the below figure (in the style of Figure 2 in the [self-instruct paper](https://arxiv.org/abs/2212.10560) to demonstrate the diversity of our data.\nThe inner circle of the plot represents the root verb of the instructions, and the outer circle represents the direct objects.\n\n[//]: # (![parse_analysis]&#40;assert/parse_analysis.png | width=100&#41;)\n[<img src="assets/parse_analysis.png" width="750" />](./assets/parse_analysis.png)\n\n## Fine-tuning\n\nWe fine-tune our models using standard Hugging Face training code.\nWe fine-tune LLaMA-7B and LLaMA-13B with the following hyperparameters:\n\n| Hyperparameter | LLaMA-7B | LLaMA-13B |\n|----------------|----------|-----------|\n| Batch size     | 128      | 128       |\n| Learning rate  | 2e-5     | 1e-5      |\n| Epochs         | 3        | 5         |\n| Max length     | 512      | 512       |\n| Weight decay   | 0        | 0         |\n\nTo reproduce our fine-tuning runs for LLaMA, first install the requirements\n\n```bash\npip install -r requirements.txt\n```\n\nBelow is a command that fine-tunes LLaMA-7B with our dataset on a machine with 4 A100 80G GPUs in FSDP `full_shard` mode.\nWe were able to reproduce a model of similar quality as the one we hosted in our demo with the following command using **Python 3.10**.\nReplace `<your_random_port>` with a port of your own, `<your_path_to_hf_converted_llama_ckpt_and_tokenizer>` with the\npath to your converted checkpoint and tokenizer (following instructions in the PR), and `<your_output_dir>` with where you want to store your outputs.\n\n```bash\ntorchrun --nproc_per_node=4 --master_port=<your_random_port> train.py \\n    --model_name_or_path <your_path_to_hf_converted_llama_ckpt_and_tokenizer> \\n    --data_path ./alpaca_data.json \\n    --bf16 True \\n    --output_dir <your_output_dir> \\n    --num_train_epochs 3 \\n    --per_device_train_batch_size 4 \\n    --per_device_eval_batch_size 4 \\n    --gradient_accumulation_steps 8 \\n    --evaluation_strategy "no" \\n    --save_strategy "steps" \\n    --save_steps 2000 \\n    --save_total_limit 1 \\n    --learning_rate 2e-5 \\n    --weight_decay 0. \\n    --warmup_ratio 0.03 \\n    --lr_scheduler_type "cosine" \\n    --logging_steps 1 \\n    --fsdp "full_shard auto_wrap" \\n    --fsdp_transformer_layer_cls_to_wrap ''LlamaDecoderLayer'' \\n    --tf32 True\n```\n\nThe same script also works for OPT fine-tuning. Here''s an example for fine-tuning OPT-6.7B\n\n```bash\ntorchrun --nproc_per_node=4 --master_port=<your_random_port> train.py \\n    --model_name_or_path "facebook/opt-6.7b" \\n    --data_path ./alpaca_data.json \\n    --bf16 True \\n    --output_dir <your_output_dir> \\n    --num_train_epochs 3 \\n    --per_device_train_batch_size 4 \\n    --per_device_eval_batch_size 4 \\n    --gradient_accumulation_steps 8 \\n    --evaluation_strategy "no" \\n    --save_strategy "steps" \\n    --save_steps 2000 \\n    --save_total_limit 1 \\n    --learning_rate 2e-5 \\n    --weight_decay 0. \\n    --warmup_ratio 0.03 \\n    --lr_scheduler_type "cosine" \\n    --logging_steps 1 \\n    --fsdp "full_shard auto_wrap" \\n    --fsdp_transformer_layer_cls_to_wrap ''OPTDecoderLayer'' \\n    --tf32 True\n```\n\nNote the given training script is meant to be simple and easy to use, and is not particularly optimized.\nTo run on more gpus, you may prefer to turn down `gradient_accumulation_steps` to keep a global batch size of 128. Global batch size has not been tested for optimality.\n\n### Addressing OOM\n\nNaively, fine-tuning a 7B model requires about 7 x 4 x 4 = 112 GB of VRAM. Commands given above enable parameter sharding, so no redundant model copy is stored on any GPU.\nIf you''d like to further reduce the memory footprint, here are some options:\n\n- Turn on CPU offload for FSDP with `--fsdp "full_shard auto_wrap offload"`. This saves VRAM at the cost of longer runtime.\n- In our experience, DeepSpeed stage-3 (with offload) can at times be more memory efficient than FSDP with offload. Here''s an example to use DeepSpeed stage-3 with 4 GPUs with both parameter and optimizer offload:\n    ```bash\n    pip install deepspeed\n    torchrun --nproc_per_node=4 --master_port=<your_random_port> train.py \\n        --model_name_or_path <your_path_to_hf_converted_llama_ckpt_and_tokenizer> \\n        --data_path ./alpaca_data.json \\n        --bf16 True \\n        --output_dir <your_output_dir> \\n        --num_train_epochs 3 \\n        --per_device_train_batch_size 4 \\n        --per_device_eval_batch_size 4 \\n        --gradient_accumulation_steps 8 \\n        --evaluation_strategy "no" \\n        --save_strategy "steps" \\n        --save_steps 2000 \\n        --save_total_limit 1 \\n        --learning_rate 2e-5 \\n        --weight_decay 0. \\n        --warmup_ratio 0.03 \\n        --deepspeed "./configs/default_offload_opt_param.json" \\n        --tf32 True\n    ```\n  - The DeepSpeed library also provides some [helpful functions](https://deepspeed.readthedocs.io/en/latest/memory.html) to estimate memory usage. \n- [LoRA](https://arxiv.org/abs/2106.09685) fine-tunes low-rank slices of the query, key, and value embedding heads. This can reduce the total memory footprint from 112GB to about 7x4=28GB. We may release our re-implemention of this in the future, but for now the [peft](https://github.com/huggingface/peft) codebase can be a useful resource.\n\n## Recovering Alpaca Weights\n\nThe weight diff between Alpaca-7B and LLaMA-7B is located [here](https://huggingface.co/tatsu-lab/alpaca-7b-wdiff/tree/main).\nTo recover the original Alpaca-7B weights, follow these steps:\n```text\n1. Convert Meta''s released weights into huggingface format. Follow this guide:\n    https://huggingface.co/docs/transformers/main/model_doc/llama\n2. Make sure you cloned the released weight diff into your local machine. The weight diff is located at:\n    https://huggingface.co/tatsu-lab/alpaca-7b/tree/main\n3. Run this function with the correct paths. E.g.,\n    python weight_diff.py recover --path_raw <path_to_step_1_dir> --path_diff <path_to_step_2_dir> --path_tuned <path_to_store_recovered_weights>\n```\n\nOnce step 3 completes, you should have a directory with the recovered weights, from which you can load the model like the following\n\n```python\nimport transformers\nalpaca_model = transformers.AutoModelForCausalLM.from_pretrained("<path_to_store_recovered_weights>")\nalpaca_tokenizer = transformers.AutoTokenizer.from_pretrained("<path_to_store_recovered_weights>")\n```\n\n### Authors\n\nAll grad students below contributed equally and the order is determined by random draw.\n\n- [Rohan Taori](https://www.rohantaori.com/)\n- [Ishaan Gulrajani](https://ishaan.io/)\n- [Tianyi Zhang](https://tiiiger.github.io/)\n- [Yann Dubois](https://yanndubs.github.io/)\n- [Xuechen Li](https://www.lxuechen.com/)\n\nAll advised by [Tatsunori B. Hashimoto](https://thashim.github.io/). Yann is also advised by [Percy Liang](https://cs.stanford.edu/~pliang/) and Xuechen is also advised by [Carlos Guestrin](https://guestrin.su.domains/).\n\n### Citation\n\nPlease cite the repo if you use the data or code in this repo.\n\n```\n@misc{alpaca,\n  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },\n  title = {Stanford Alpaca: An Instruction-following LLaMA model},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},\n}\n```\n\nNaturally, you should also cite the original LLaMA paper [1] and the Self-Instruct paper [2].\n\n### Acknowledgements\n\nWe thank Yizhong Wang for his help in explaining the data generation pipeline in Self-Instruct and providing the code for the parse analysis plot.\nWe thank Yifan Mai for helpful support, and members of the Stanford NLP Group as well as the Center for Research on Foundation Models (CRFM) for their helpful feedback.\n', '{"language":"Python","stars":30243,"forks":4031,"watchers":30243,"open_issues":188,"topics":["deep-learning","instruction-following","language-model"],"default_branch":"main","size_kb":8654,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:tatsu-lab:stanford_alpaca","source_url":"https://github.com/tatsu-lab/stanford_alpaca"},{"type":"has_code","target_id":"github:tatsu-lab:stanford_alpaca","source_url":"https://github.com/tatsu-lab/stanford_alpaca"},{"type":"has_code","target_id":"github:tatsu-lab:stanford_alpaca","source_url":"https://github.com/tatsu-lab/stanford_alpaca"},{"type":"has_code","target_id":"github:psf:black","source_url":"https://github.com/psf/black"},{"type":"has_code","target_id":"github:yizhongw:self-instruct","source_url":"https://github.com/yizhongw/self-instruct"},{"type":"has_code","target_id":"github:tatsu-lab:stanford_alpaca","source_url":"https://github.com/tatsu-lab/stanford_alpaca"},{"type":"has_code","target_id":"github:yizhongw:self-instruct","source_url":"https://github.com/yizhongw/self-instruct"},{"type":"has_code","target_id":"github:huggingface:peft","source_url":"https://github.com/huggingface/peft"},{"type":"has_code","target_id":"github:tatsu-lab:stanford_alpaca}},","source_url":"https://github.com/tatsu-lab/stanford_alpaca}},"}]', NULL, 'Apache-2.0', 'approved', 80, '8d49d6bde35a2cbfe80a7c368952f807', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-tatsu-lab-stanford-alpaca from https://github.com/tatsu-lab.png
Image converted to WebP: data/images/github-tatsu-lab-stanford-alpaca.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-deezer-spleeter', 'github--deezer--spleeter', 'spleeter', 'deezer', '<img src="https://github.com/deezer/spleeter/raw/master/images/spleeter_logo.png" height="80" /> !PyPI - Python Version > :warning: Spleeter 2.1.0 release introduces some breaking changes, including new CLI option naming for input, and the drop > of dedicated GPU package. Please read CHANGELOG for more details. **Spleeter** is Deezer source separation library with pretrained models written in Python and uses Tensorflow. It makes it easy to train source separation model (assuming you have a da...', '["audio-processing","bass","deep-learning","deezer","drums","model","pretrained-models","python","tensorflow","vocals","python"]', 'other', 27852, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/deezer/spleeter","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<img src="https://github.com/deezer/spleeter/raw/master/images/spleeter_logo.png" height="80" />\n\n[![Github actions](https://github.com/deezer/spleeter/workflows/pytest/badge.svg)](https://github.com/deezer/spleeter/actions) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/spleeter) [![PyPI version](https://badge.fury.io/py/spleeter.svg)](https://badge.fury.io/py/spleeter) [![Conda](https://img.shields.io/conda/vn/deezer-research/spleeter)](https://anaconda.org/deezer-research/spleeter) [![Docker Pulls](https://img.shields.io/docker/pulls/deezer/spleeter)](https://hub.docker.com/r/deezer/spleeter) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deezer/spleeter/blob/master/spleeter.ipynb) [![Gitter chat](https://badges.gitter.im/gitterHQ/gitter.png)](https://gitter.im/spleeter/community) [![status](https://joss.theoj.org/papers/259e5efe669945a343bad6eccb89018b/status.svg)](https://joss.theoj.org/papers/259e5efe669945a343bad6eccb89018b)\n\n> :warning: [Spleeter 2.1.0](https://pypi.org/project/spleeter/) release introduces some breaking changes, including new CLI option naming for input, and the drop\n> of dedicated GPU package. Please read [CHANGELOG](CHANGELOG.md) for more details.\n\n## About\n\n**Spleeter** is [Deezer](https://www.deezer.com/) source separation library with pretrained models\nwritten in [Python](https://www.python.org/) and uses [Tensorflow](https://tensorflow.org/). It makes it easy\nto train source separation model (assuming you have a dataset of isolated sources), and provides\nalready trained state of the art model for performing various flavour of separation :\n\n* Vocals (singing voice) / accompaniment separation ([2 stems](https://github.com/deezer/spleeter/wiki/2.-Getting-started#using-2stems-model))\n* Vocals / drums / bass / other separation ([4 stems](https://github.com/deezer/spleeter/wiki/2.-Getting-started#using-4stems-model))\n* Vocals / drums / bass / piano / other separation ([5 stems](https://github.com/deezer/spleeter/wiki/2.-Getting-started#using-5stems-model))\n\n2 stems and 4 stems models have [high performances](https://github.com/deezer/spleeter/wiki/Separation-Performances) on the [musdb](https://sigsep.github.io/datasets/musdb.html) dataset. **Spleeter** is also very fast as it can perform separation of audio files to 4 stems 100x faster than real-time when run on a GPU.\n\nWe designed **Spleeter** so you can use it straight from [command line](https://github.com/deezer/spleeter/wiki/2.-Getting-started#usage)\nas well as directly in your own development pipeline as a [Python library](https://github.com/deezer/spleeter/wiki/4.-API-Reference#separator). It can be installed with [pip](https://github.com/deezer/spleeter/wiki/1.-Installation#using-pip) or be used with\n[Docker](https://github.com/deezer/spleeter/wiki/2.-Getting-started#using-docker-image).\n\n### Projects and Softwares using **Spleeter**\n\nSince it''s been released, there are multiple forks exposing **Spleeter** through either a Guided User Interface (GUI) or a standalone free or paying website. Please note that we do not host, maintain or directly support any of these initiatives.\n\nThat being said, many cool projects have been built on top of ours. Notably the porting to the *Ableton Live* ecosystem through the [Spleeter 4 Max](https://github.com/diracdeltas/spleeter4max#spleeter-for-max) project.\n\n**Spleeter** pre-trained models have also been used by professionnal audio softwares. Here''s a non-exhaustive list:\n\n* [iZotope](https://www.izotope.com/en/shop/rx-8-standard.html) in its *Music Rebalance* feature within **RX 8**\n* [SpectralLayers](https://new.steinberg.net/spectralayers/) in its *Unmix* feature in **SpectralLayers 7**\n* [Acon Digital](https://acondigital.com/products/acoustica-audio-editor/) within **Acoustica 7**\n* [VirtualDJ](https://www.virtualdj.com/stems/) in their stem isolation feature\n* [Algoriddim](https://www.algoriddim.com/apps) in their **NeuralMix** and **djayPRO** app suite\n\nðŸ†• **Spleeter** is a baseline in the ongoing [Music Demixing Challenge](https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021)!\n\n## Spleeter Pro (Commercial version)\n\nCheck out our commercial version : [Spleeter Pro](https://www.deezer-techservices.com/solutions/spleeter/). Benefit from our expertise for precise audio separation, faster processing speeds, and dedicated professional support. \n\n## Quick start\n\nWant to try it out but don''t want to install anything ? We have set up a [Google Colab](https://colab.research.google.com/github/deezer/spleeter/blob/master/spleeter.ipynb).\n\nReady to dig into it ? In a few lines you can install **Spleeter**  and separate the vocal and accompaniment parts from an example audio file.\nYou need first to install `ffmpeg` and `libsndfile`. It can be done on most platform using [Conda](https://github.com/deezer/spleeter/wiki/1.-Installation#using-conda):\n\n```bash\n# install dependencies using conda\nconda install -c conda-forge ffmpeg libsndfile\n# install spleeter with pip\npip install spleeter\n# download an example audio file (if you don''t have wget, use another tool for downloading)\nwget https://github.com/deezer/spleeter/raw/master/audio_example.mp3\n# separate the example audio into two components\nspleeter separate -p spleeter:2stems -o output audio_example.mp3\n```\n\n> :warning: Note that we no longer recommend using `conda` for installing spleeter.\n\n> âš ï¸ There are known issues with Apple M1 chips, mostly due to TensorFlow compatibility. Until these are fixed, you can use [this workaround](https://github.com/deezer/spleeter/issues/607#issuecomment-1021669444).\n\nYou should get two separated audio files (`vocals.wav` and `accompaniment.wav`) in the `output/audio_example` folder.\n\nFor a detailed documentation, please check the [repository wiki](https://github.com/deezer/spleeter/wiki/1.-Installation)\n\n## Development and Testing\n\nThis project is managed using [Poetry](https://python-poetry.org/docs/basic-usage/), to run test suite you\ncan execute the following set of commands:\n\n```bash\n# Clone spleeter repository\ngit clone https://github.com/Deezer/spleeter && cd spleeter\n# Install poetry\npip install poetry\n# Install spleeter dependencies\npoetry install\n# Run unit test suite\npoetry run pytest tests/\n```\n\n## Reference\n\n* Deezer Research - Source Separation Engine Story - deezer.io blog post:\n  * [English version](https://deezer.io/releasing-spleeter-deezer-r-d-source-separation-engine-2b88985e797e)\n  * [Japanese version](http://dzr.fm/splitterjp)\n* [Music Source Separation tool with pre-trained models / ISMIR2019 extended abstract](http://archives.ismir.net/ismir2019/latebreaking/000036.pdf)\n\nIf you use **Spleeter** in your work, please cite:\n\n```BibTeX\n@article{spleeter2020,\n  doi = {10.21105/joss.02154},\n  url = {https://doi.org/10.21105/joss.02154},\n  year = {2020},\n  publisher = {The Open Journal},\n  volume = {5},\n  number = {50},\n  pages = {2154},\n  author = {Romain Hennequin and Anis Khlif and Felix Voituret and Manuel Moussallam},\n  title = {Spleeter: a fast and efficient music source separation tool with pre-trained models},\n  journal = {Journal of Open Source Software},\n  note = {Deezer Research}\n}\n```\n\n## License\n\nThe code of **Spleeter** is [MIT-licensed](LICENSE).\n\n## Disclaimer\n\nIf you plan to use **Spleeter** on copyrighted material, make sure you get proper authorization from right owners beforehand.\n\n## Troubleshooting\n\n**Spleeter** is a complex piece of software and although we continously try to improve and test it you may encounter unexpected issues running it. If that''s the case please check the [FAQ page](https://github.com/deezer/spleeter/wiki/5.-FAQ) first as well as the list of [currently open issues](https://github.com/deezer/spleeter/issues)\n\n### Windows users\n\n   It appears that sometimes the shortcut command `spleeter` does not work properly on windows. This is a known issue that we will hopefully fix soon. In the meantime replace `spleeter separate` by `python -m spleeter separate` in command line and it should work.\n\n## Contributing\n\nIf you would like to participate in the development of **Spleeter** you are more than welcome to do so. Don''t hesitate to throw us a pull request and we''ll do our best to examine it quickly. Please check out our [guidelines](.github/CONTRIBUTING.md) first.\n\n## Note\n\nThis repository include a demo audio file `audio_example.mp3` which is an excerpt\nfrom Slow Motion Dream by Steven M Bryant (c) copyright 2011 Licensed under a Creative\nCommons Attribution (3.0) [license](http://dig.ccmixter.org/files/stevieb357/34740)\nFt: CSoul,Alex Beroza & Robert Siekawitch\n', '{"language":"Python","stars":27852,"forks":3054,"watchers":27852,"open_issues":272,"topics":["audio-processing","bass","deep-learning","deezer","drums","model","pretrained-models","python","tensorflow","vocals"],"default_branch":"master","size_kb":9630,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:diracdeltas:spleeter4max","source_url":"https://github.com/diracdeltas/spleeter4max#spleeter-for-max"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:Deezer:spleeter","source_url":"https://github.com/Deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"}]', NULL, 'MIT', 'approved', 65, '592da5b210e457129c7145904a825b2f', NULL, NULL, CURRENT_TIMESTAMP);
