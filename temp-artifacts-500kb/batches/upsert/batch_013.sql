/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-Janus-1.3B', 'huggingface--deepseek-ai--janus-1.3b', 'Janus-1.3B', 'deepseek-ai', '--- license: mit license_name: deepseek license_link: LICENSE pipeline_tag: any-to-any library_name: transformers tags: - muiltimodal - text-to-image - unified-model --- **2024.10.20**: We have uploaded the correct . The previous file was missing the , which caused poor visual generation results. Janus is a novel autoregressive framework that unifies multimodal understanding and generation. It addresses the limitations of previous approaches by decoupling visual encoding into separate pathway...', '["transformers","safetensors","multi_modality","muiltimodal","text-to-image","unified-model","any-to-any","arxiv:2410.13848","license:mit","endpoints_compatible","region:us"]', 'any-to-any', 592, 9758, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/Janus-1.3B","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlicense_name: deepseek\nlicense_link: LICENSE\npipeline_tag: any-to-any\nlibrary_name: transformers\ntags:\n- muiltimodal\n- text-to-image\n- unified-model\n---\n\n## 0. Update\n**2024.10.20**: We have uploaded the correct `tokenizer_config.json`. The previous file was missing the `pad_token`, which caused poor visual generation results.\n\n\n## 1. Introduction\n\nJanus is a novel autoregressive framework that unifies multimodal understanding and generation. \nIt addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in understanding and generation, but also enhances the framework‚Äôs flexibility. \nJanus surpasses previous unified model and matches or exceeds the performance of task-specific models. \nThe simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models.\n\n[Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2410.13848)\n\n[**Github Repository**](https://github.com/deepseek-ai/Janus)\n\n<div align="center">\n<img alt="image" src="teaser.png" style="width:90%;">\n</div>\n\n\n### 2. Model Summary\n\nJanus is a unified understanding and generation MLLM, which decouples visual encoding for multimodal understanding and generation. \nJanus is constructed based on the DeepSeek-LLM-1.3b-base which is trained on an approximate corpus of 500B text tokens.\nFor multimodal understanding, it uses the [SigLIP-L](https://huggingface.co/timm/ViT-L-16-SigLIP-384) as the vision encoder, which supports 384 x 384 image input. For image generation, Janus uses the tokenizer from [here](https://github.com/FoundationVision/LlamaGen) with a downsample rate of 16.\n\n<div align="center">\n<img alt="image" src="arch.jpg" style="width:90%;">\n</div>\n\n## 3. Quick Start\n\nPlease refer to [**Github Repository**](https://github.com/deepseek-ai/Janus)\n\n\n## 4. License\n\nThis code repository is licensed under [the MIT License](https://github.com/deepseek-ai/DeepSeek-LLM/blob/HEAD/LICENSE-CODE). The use of Janus models is subject to [DeepSeek Model License](https://github.com/deepseek-ai/DeepSeek-LLM/blob/HEAD/LICENSE-MODEL).\n## 5. Citation\n\n```\n@misc{wu2024janus,\n      title={Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation}, \n      author={Chengyue Wu and Xiaokang Chen and Zhiyu Wu and Yiyang Ma and Xingchao Liu and Zizheng Pan and Wen Liu and Zhenda Xie and Xingkai Yu and Chong Ruan and Ping Luo},\n      year={2024},\n      eprint={2410.13848},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2410.13848}, \n}\n```\n\n## 6. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](mailto:service@deepseek.com).', '{"pipeline_tag":"any-to-any","library_name":"transformers","framework":"transformers","params":2089297547,"storage_bytes":4178706382,"files_count":11,"spaces_count":19,"gated":false,"private":false,"config":{"model_type":"multi_modality","tokenizer_config":{"bos_token":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","eos_token":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","pad_token":"<ÔΩú‚ñÅpad‚ñÅÔΩú>","unk_token":null,"use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:Janus","source_url":"https://github.com/deepseek-ai/Janus"},{"type":"has_code","target_id":"github:FoundationVision:LlamaGen","source_url":"https://github.com/FoundationVision/LlamaGen"},{"type":"has_code","target_id":"github:deepseek-ai:Janus","source_url":"https://github.com/deepseek-ai/Janus"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-LLM","source_url":"https://github.com/deepseek-ai/DeepSeek-LLM"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-LLM","source_url":"https://github.com/deepseek-ai/DeepSeek-LLM"},{"type":"based_on_paper","target_id":"arxiv:2410.13848","source_url":"https://arxiv.org/abs/2410.13848"}]', NULL, 'MIT', 'approved', 62.7, '4b9a18cd67c138da1976f08e22ae3d68', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-vrgamedevgirl84-Wan14BT2VFusioniX', 'huggingface--vrgamedevgirl84--wan14bt2vfusionix', 'Wan14BT2VFusioniX', 'vrgamedevgirl84', '--- tags: - text-to-video - diffusion - merged-model - video-generation - wan2.1 widget: - text: >- Prompt: A gritty close-up of an elven princess kneeling in a rocky ravine, calming a wounded, desert dragon. Its scales are cracked, dry, She wears a crimson sash over bone-colored armor, her auburn hair half-tied back. The camera dollies in rapidly as she reaches for its eye ridge. Lighting comes from golden sunlight reflecting off surrounding rock, casting a warm, earthy hue with no artificia...', '["text-to-video","diffusion","merged-model","video-generation","wan2.1","base_model:wan-ai/wan2.1-t2v-14b","base_model:finetune:wan-ai/wan2.1-t2v-14b","license:apache-2.0","region:us"]', 'text-to-video', 592, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- text-to-video\n- diffusion\n- merged-model\n- video-generation\n- wan2.1\n\nwidget:\n- text: >-\n    Prompt: A gritty close-up of an elven princess kneeling in a rocky ravine, calming a wounded, desert dragon. Its scales are cracked, dry, She wears a crimson sash over bone-colored armor, her auburn hair half-tied back. The camera dollies in rapidly as she reaches for its eye ridge. Lighting comes from golden sunlight reflecting off surrounding rock, casting a warm, earthy hue with no artificial glow.\n  output:\n    url: videos/Video_00063.mp4\n    \n- text: >-\n    Prompt: Tight close-up of her smiling lips and sparkling eyes, catching golden hour sunlight. She wears a white sundress with floral prints and a wide-brimmed straw hat. Camera pulls back in a dolly motion, revealing her twirling under a cherry blossom tree. Petals flutter in the air, casting playful shadows. Soft lens flares enhance the euphoric, dreamlike vibe. (Before vs After ‚Äî Left: Wan2.1 | Right: Merged model Wan14BT2V_MasterModel)\n  output:\n    url: videos/AnimateDiff_00001.mp4\n\n- text: >-\n    Prompt: A gritty close-up of a dwarven beastmaster‚Äôs face, his grey beard braided tightly, brows furrowed as he looks just off-camera. The camera dollies out over his shoulder, revealing a perched gryphon watching him from a boulder, its feathers rustling slightly in the breeze. The moment holds stillness and mutual trust. Lighting is early daylight, clean and sharp with strong environmental clarity.\n  output:\n    url: videos/FusionX_00012.mp4\n\n- text: >-\n    Prompt: A gritty close-up of a jungle tracker crouching low, face flushed with focus as she watches a perched macaw a few feet ahead. Her cheek twitches as she shifts forward, beads of sweat visible on her brow. The camera slowly dollies in from below her line of sight, capturing the moment her eyes widen in fascination. Lighting is rich and directional from above, creating a warm glow over her face with minimal shadows.\n  output:\n    url: videos/FusionX_00005.mp4\n\n- text: >-\n    Prompt: A gritty close-up of a battle-worn ranger kneeling in a scorched clearing, calming a wounded gryphon whose wing is torn and bloodied. Its feathers are dusky bronze with streaks of ash-gray. She wears soot-covered hunter green armor, her blonde hair pulled into a loose braid. The camera dollies in as her hand brushes the creature''s sharp beak. Lighting comes from late afternoon sun filtering through smoke, casting a burnt-orange haze across the frame.\n  output:\n    url: videos/Video_00069.mp4\n\n\n\nbase_model:\n- Wan-AI/Wan2.1-T2V-14B\nlicense: apache-2.0\n---\n\n# üåÄ Wan2.1_14B_FusionX\n\n**High-Performance Merged Text-to-Video Model**  \nBuilt on WAN 2.1 and fused with research-grade components for cinematic motion, detail, and speed ‚Äî optimized for ComfyUI and rapid iteration in as few as 6 steps.\n\nMerged models for faster, richer motion & detail ‚Äî high performance even at just 8 steps.\n\n> üìå Important: To match the quality shown here, use the linked workflows or make sure to follow the recommended settings outlined below.\n\n---\n\n## üöÄ Overview\n\nA powerful text-to-video model built on top of **WAN 2.1 14B**, merged with several research-grade models to boost:\n\n- Motion quality\n- Scene consistency\n- Visual detail\n\nComparable with closed-source solutions, but open and optimized for **ComfyUI** workflows.\n\n---\n\n## üí° Inside the Fusion\n\nThis model is made up of the following which is on TOP of Wan 2.1 14B 720p(FusionX would not be what it is without these Models):\n\n- **CausVid** ‚Äì [Causal motion modeling for better flow and dynamics](https://github.com/tianweiy/CausVid)\n- **AccVideo** ‚Äì [Better temporal alignment and speed boost](https://github.com/aejion/AccVideo)\n- **MoviiGen1.1** ‚Äì [Cinematic smoothness and lighting](https://huggingface.co/ZuluVision/MoviiGen1.1)\n- **MPS Reward LoRA** ‚Äì [Tuned for motion and detail](https://huggingface.co/alibaba-pai/Wan2.1-Fun-Reward-LoRAs)\n- **Custom LoRAs** ‚Äì For texture, clarity, and small detail enhancements (Set at a very low level)\n\nAll merged models are provided for research and non-commercial use only.\nSome components are subject to licenses such as CC BY-NC-SA 4.0, and do not fall under permissive licenses like Apache 2.0 or MIT.\nPlease refer to each model‚Äôs original license for full usage terms.\n\n---\n\n## üö®‚ú®**Hey guys! Just a quick update!**\n\nWe finally cooked up **FusionX LoRAs**!! üß†üí•  \nThis is huge ‚Äì now you can plug FusionX into your favorite workflows as a LoRA on top of the Wan base models and SkyReels models!üîåüí´ \nYou can still stick with the base FusionX Model if you already use it, but if you would rather have more control over the "FusionX" strength and a speed boost, then this might be for you.\n\nOh, and there‚Äôs a **nice speed boost** too! ‚ö°  \n**Example:** *(RTX 5090)*  \n- FusionX as a full base model: **8 steps = 160s** ‚è±Ô∏è  \n- FusionX as a **LoRA on Wan 2.1 14B fp8 T2V**: **8 steps = 120s** üöÄ\n\n**Bonus:** You can bump up the FusionX LoRA strength and lower your steps for a **huge speed boost** while testing/drafting.  \nExample: strength `2.00` with `3 steps` takes `72 seconds`.  \nOr lower the strength to experiment with a **less ‚ÄúFusionX‚Äù look**. ‚ö°üîç\n\nWe‚Äôve got:\n- **T2V (Text to Video)** üé¨ ‚Äì works perfectly with **VACE** ‚öôÔ∏è  \n- **I2V (Image to Video)** üñºÔ∏è‚û°Ô∏èüìΩÔ∏è  \n- A dedicated **Phantom LoRA** üëª  \nThe new LoRA''s are [HERE](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/tree/main/FusionX_LoRa)\nNote: The LoRa''s are not meant to be put on top of the FusionX main models and instead you would use them with the Wan base models.\n**New workflows**  are [HERE](https://civitai.com/models/1681541)  üõ†Ô∏èüöÄ\n\n---\n\nAfter lots of testing üß™, the video quality with the LoRA is **just as good** (and sometimes **even better**! üíØ)  \nThat‚Äôs thanks to it being trained on the **fp16 version** of FusionX üß¨üíé\n\n---\n\n### üåÄ Preview Gallery  \n*These are compressed GIF previews for quick viewing ‚Äî final video outputs are higher quality.*\n\n![FusionX_00020](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00020.gif)  \n![FusionX_00021](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00021.gif)  \n![FusionX_00022](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00022.gif)  \n![FusionX_00023](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00023.gif)  \n![FusionX_00024](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00024.gif)  \n![FusionX_00025](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00025.gif)  \n![FusionX_00026](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00026.gif)  \n![FusionX_00027](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00027.gif)  \n![FusionX_00028](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00028.gif)  \n![FusionX_00029](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00029.gif)  \n![FusionX_00030](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00030.gif)  \n![FusionX_00031](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/resolve/main/videos/FusionX_00031.gif)\n\n---\n\n\n## üìÇ Workflows & Model Downloads\n\n- üí° **ComfyUI workflows** can be found here:  \n  üëâ [Workflow Collection (WIP)](https://civitai.com/models/1663553)\n\n- üì¶ **Model files (T2V, I2V, Phantom, VACE)**:  \n  üëâ [Main Hugging Face Repo](https://huggingface.co/vrgamedevgirl84/Wan14BT2VFusioniX/tree/main)\n\n### üß† GGUF Variants:\n- üñºÔ∏è [FusionX Image-to-Video (GGUF)](https://huggingface.co/QuantStack/Wan2.1_I2V_14B_FusionX-GGUF/tree/main)  \n- üé• [FusionX Text-to-Video (GGUF)](https://huggingface.co/QuantStack/Wan2.1_T2V_14B_FusionX-GGUF/tree/main)  \n- üéûÔ∏è [FusionX T2V VACE (for native)](https://huggingface.co/QuantStack/Wan2.1_T2V_14B_FusionX_VACE-GGUF/tree/main)  \n- üëª [FusionX Phantom](https://huggingface.co/QuantStack/Phantom_Wan_14B_FusionX-GGUF/tree/main)\n\n---\n## üé¨ Example Videos\n\nWant to see what FusionX can do? Check out these real outputs generated using the latest workflows and settings:\n\n- **Text-to-Video**  \n  üëâ [Watch Examples](https://civitai.com/posts/17874424)\n\n- **Image-to-Video**  \n  üëâ [Watch Examples](https://civitai.com/posts/18029174)\n\n- **Phantom Mode**  \n  üëâ [Watch Examples](https://civitai.com/posts/17986906)\n\n- **VACE Integration**  \n  üëâ [Watch Examples](https://civitai.com/posts/18080876)\n\n---\n\n## üîß Usage Details\n\n### Text-to-Video\n\n- **CGF**: Must be set to `1`  \n- **Shift**:  \n  - `1024x576`: Start at `1`  \n  - `1080x720`: Start at `2`  \n  - For realism ‚Üí lower values  \n  - For stylized ‚Üí test `3‚Äì9`\n- **Scheduler**:  \n  - Recommended: `uni_pc`  \n  - Alternative: `flowmatch_causvid` (better for some details)\n\n### Image-to-Video\n\n- **CGF**: `1`\n- **Shift**: `2` works best in most cases\n- **Scheduler**:  \n  - Recommended: `dmp++_sde/beta`  \n- To boost motion and reduce slow-mo effect:\n  - Frame count: `121`\n  - FPS: `24`\n\n---\n\n## üõ† Technical Notes\n\n- Works in as few as **6 steps**\n- Best quality at **8‚Äì10 steps**\n- Drop-in replacement for `Wan2.1-T2V-14B`\n- Up to **50% faster rendering**, especially with **SageAttn**\n- Works natively and with **Kaji Wan Wrapper**  \n  [Wrapper GitHub](https://github.com/kijai/ComfyUI-WanVideoWrapper)\n- Do **not** re-add merged LoRAs (CausVid, AccVideo, MPS)\n- Feel free to add **other LoRAs** for style/variation\n- Native WAN workflows also supported (slightly slower)\n\n---\n\n## üß™ Performance Tips\n\n- RTX 5090 ‚Üí ~138 sec/video at 1024x576 / 81 frames\n- If VRAM is limited:\n  - Enable block swapping\n  - Start with `5` blocks and adjust as needed\n- Use **SageAttn** for ~30% speedup (wrapper only)\n- Do **not** use `teacache`\n- "Enhance a video" (tested): Adds vibrance (try values 2‚Äì4)\n- "SLG" not tested ‚Äî feel free to explore\n\n---\n\n## üß† Prompt Help\n\nWant better cinematic prompts? Try the **WAN Cinematic Video Prompt Generator GPT** ‚Äî it adds visual richness and makes a big difference in quality. [Download Here](https://chatgpt.com/g/g-67c3a6d6d19c81919b3247d2bfd01d0b-wan-cinematic-video-prompt-generator)\n\n---\n\n## üì£ Join The Community\n\nWe‚Äôre building a friendly space to chat, share outputs, and get help.\n\n- Motion LoRAs coming soon\n- Tips, updates, and support from other users\n\nüëâ [Join the Discord](https://discord.com/invite/hxPmmXmRW3)\n\n---\n\n## ‚öñÔ∏è License\n\nSome merged components use permissive licenses (Apache 2.0 / MIT),  \n**but others** ‚Äî such as those from research models like *CausVid* ‚Äî may be released under **non-commercial licenses** (e.g., [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)).\n\n- ‚úÖ You **can** use, modify, and redistribute **under original license terms**  \n- ‚ùó You **must** retain and respect the license of each component  \n- ‚ö†Ô∏è **Commercial use is not permitted** for models or components under non-commercial licenses  \n- üìå Outputs are **not automatically licensed** ‚Äî do your own due diligence\n\nThis model is intended for **research, education, and personal use only**.  \nFor commercial use or monetization, please consult a legal advisor and verify all component licenses.\n\n---\n\n## üôè Credits\n\n- WAN Team (base model)\n- aejion (AccVideo)\n- Tianwei Yin (CausVid)\n- ZuluVision (MoviiGen)\n- Alibaba PAI (MPS LoRA)\n- Kijai (ComfyUI Wrapper)\n\nAnd thanks to the open-source community!\n\n---\n', '{"pipeline_tag":"text-to-video","library_name":null,"framework":null,"params":null,"storage_bytes":122604036605,"files_count":41,"spaces_count":41,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:tianweiy:CausVid","source_url":"https://github.com/tianweiy/CausVid"},{"type":"has_code","target_id":"github:aejion:AccVideo","source_url":"https://github.com/aejion/AccVideo"},{"type":"has_code","target_id":"github:kijai:ComfyUI-WanVideoWrapper","source_url":"https://github.com/kijai/ComfyUI-WanVideoWrapper"}]', NULL, 'Apache-2.0', 'approved', 77.7, '38ccaf5a6343ef203638d51865cfd56e', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tencent-HunyuanWorld-1', 'huggingface--tencent--hunyuanworld-1', 'HunyuanWorld-1', 'tencent', '--- library_name: diffusion-single-file license: other license_name: tencent-hunyuanworld-1.0-community license_link: https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0/blob/main/LICENSE language: - en - zh tags: - hunyuan3d - worldmodel - 3d-aigc - 3d-generation - 3d - scene-generation pipeline_tag: image-to-3d extra_gated_eu_disallowed: true --- <p align="center"> <img src="assets/teaser.png"> </p> <div align="center"> <a href=https://3d.hunyuan.tencent.com/sceneTo3D target="_blank"><img s...', '["diffusion-single-file","hunyuan3d","worldmodel","3d-aigc","3d-generation","3d","scene-generation","image-to-3d","en","zh","arxiv:2507.21809","license:other","region:us"]', 'image-to-3d', 589, 13662, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tencent/HunyuanWorld-1","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: diffusion-single-file\nlicense: other\nlicense_name: tencent-hunyuanworld-1.0-community\nlicense_link: https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0/blob/main/LICENSE\nlanguage:\n- en\n- zh\ntags:\n- hunyuan3d\n- worldmodel\n- 3d-aigc\n- 3d-generation\n- 3d\n- scene-generation\npipeline_tag: image-to-3d\nextra_gated_eu_disallowed: true\n---\n<p align="center">\n  <img src="assets/teaser.png">\n</p>\n\n<div align="center">\n  <a href=https://3d.hunyuan.tencent.com/sceneTo3D target="_blank"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanWorld-1 target="_blank"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://3d-models.hunyuan.tencent.com/world/ target="_blank"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href=https://arxiv.org/abs/2507.21809 target="_blank"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://discord.gg/dNBrdrGGMa target="_blank"><img src= https://img.shields.io/badge/Discord-white.svg?logo=discord height=22px></a>\n  <a href=https://x.com/TencentHunyuan target="_blank"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n <a href="#community-resources" target="_blank"><img src=https://img.shields.io/badge/Community-lavender.svg?logo=homeassistantcommunitystore height=22px></a>\n</div>\n\n[//]: # (  <a href=# target="_blank"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>)\n\n[//]: # (  <a href=# target="_blank"><img src= https://img.shields.io/badge/Colab-8f2628.svg?logo=googlecolab height=22px></a>)\n\n[//]: # (  <a href="#"><img alt="PyPI - Downloads" src="https://img.shields.io/pypi/v/mulankit?logo=pypi"  height=22px></a>)\n\n<br>\n\n<p align="center">\n  "To see a World in a Grain of Sand, and a Heaven in a Wild Flower"\n</p>\n\n## üîó BibTeX\n```\n@misc{hunyuanworld2025tencent,\n    title={HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D Worlds from Words or Pixels},\n    author={Tencent Hunyuan3D Team},\n    year={2025},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n```\n\n## Acknowledgements\nWe would like to thank the contributors to the [Stable Diffusion](https://github.com/Stability-AI/stablediffusion), [FLUX](https://github.com/black-forest-labs/flux), [diffusers](https://github.com/huggingface/diffusers), [HuggingFace](https://huggingface.co), [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN), [ZIM](https://github.com/naver-ai/ZIM), [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO), [MoGe](https://github.com/microsoft/moge), [Worldsheet](https://worldsheet.github.io/), [WorldGen](https://github.com/ZiYang-xie/WorldGen) repositories, for their open research.', '{"pipeline_tag":"image-to-3d","library_name":"diffusion-single-file","framework":"diffusion-single-file","params":null,"storage_bytes":1081660589,"files_count":12,"spaces_count":9,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanWorld-1.0","source_url":"https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0"},{"type":"has_code","target_id":"github:Stability-AI:stablediffusion","source_url":"https://github.com/Stability-AI/stablediffusion"},{"type":"has_code","target_id":"github:black-forest-labs:flux","source_url":"https://github.com/black-forest-labs/flux"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:xinntao:Real-ESRGAN","source_url":"https://github.com/xinntao/Real-ESRGAN"},{"type":"has_code","target_id":"github:naver-ai:ZIM","source_url":"https://github.com/naver-ai/ZIM"},{"type":"has_code","target_id":"github:IDEA-Research:GroundingDINO","source_url":"https://github.com/IDEA-Research/GroundingDINO"},{"type":"has_code","target_id":"github:microsoft:moge","source_url":"https://github.com/microsoft/moge"},{"type":"has_code","target_id":"github:ZiYang-xie:WorldGen","source_url":"https://github.com/ZiYang-xie/WorldGen"},{"type":"based_on_paper","target_id":"arxiv:2507.21809","source_url":"https://arxiv.org/abs/2507.21809"}]', NULL, 'Other', 'approved', 82.7, '783c2f341a77cfe37ac939ce0c7cb59a', NULL, 'https://huggingface.co/tencent/HunyuanWorld-1/resolve/main/assets/qrcode/discord.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-tencent-HunyuanWorld-1 from https://huggingface.co/tencent/HunyuanWorld-1/resolve/main/assets/qrcode/discord.png
Image converted to WebP: data/images/huggingface-tencent-HunyuanWorld-1.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-3.1-405B-Instruct', 'huggingface--meta-llama--llama-3.1-405b-instruct', 'Llama-3.1-405B-Instruct', 'meta-llama', '', '["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","conversational","en","de","fr","it","pt","hi","es","th","arxiv:2204.05149","base_model:meta-llama/llama-3.1-405b","base_model:finetune:meta-llama/llama-3.1-405b","license:llama3.1","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 587, 127249, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":405853388800,"storage_bytes":3035854528726,"files_count":584,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","chat_template":"{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][''role''] == ''system'' %}\n    {%- set system_message = messages[0][''content'']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject(''equalto'', ''code_interpreter'') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][''content'']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there''s no first user message!\") }}\n{%- endif %}\n    {{- ''<|start_header_id|>user<|end_header_id|>\\n\\n'' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == ''ipython'' or message.role == ''tool'' or ''tool_calls'' in message) %}\n        {{- ''<|start_header_id|>'' + message[''role''] + ''<|end_header_id|>\\n\\n''+ message[''content''] | trim + ''<|eot_id|>'' }}\n    {%- elif ''tool_calls'' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + ''=\"'' + arg_val + ''\"'' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' -}}\n            {{- ''{\"name\": \"'' + tool_call.name + ''\", '' }}\n            {{- ''\"parameters\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we''re in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' }}\n{%- endif %}\n","eos_token":"<|eot_id|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2204.05149","source_url":"https://arxiv.org/abs/2204.05149"}]', NULL, 'llama3.1', 'approved', 37.7, 'af82932cd58a9adb6f0f2f7ef1e1ed7e', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-R1-Distill-Qwen-14B', 'huggingface--deepseek-ai--deepseek-r1-distill-qwen-14b', 'DeepSeek-R1-Distill-Qwen-14B', 'deepseek-ai', '--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/d...', '["transformers","safetensors","qwen2","text-generation","conversational","arxiv:2501.12948","license:mit","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 585, 325119, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n\n<p align="center">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Paper Link</b>üëÅÔ∏è</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align="center">\n  <img width="80%" src="figures/benchmark.jpg">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model''s reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align="center">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align="center">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align="center">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align="center">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek''s official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button "DeepThink"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face''s Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: "Please reason step by step, and put your final answer within \boxed{}."\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting "\<think\>\n\n\</think\>") when responding to certain queries, which can adversely affect the model''s performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with "\<think\>\n" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":14770033664,"storage_bytes":29540133872,"files_count":13,"spaces_count":91,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''') %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{% set ns.system_prompt = message[''content''] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{{''<ÔΩúUserÔΩú>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is none %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls'']%}{%- if not ns.is_first %}{{''<ÔΩúAssistantÔΩú><ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- set ns.is_first = true -%}{%- else %}{{''\\n'' + ''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endfor %}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is not none %}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'' + message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message[''content''] %}{% if ''</think>'' in content %}{% set content = content.split(''</think>'')[-1] %}{% endif %}{{''<ÔΩúAssistantÔΩú>'' + content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- set ns.is_output_first = false %}{%- else %}{{''\\n<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{''<ÔΩúAssistantÔΩú><think>\\n''}}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2501.12948","source_url":"https://arxiv.org/abs/2501.12948"}]', NULL, 'MIT', 'approved', 97.7, '69bdf980e4334c092c4b8cb68aaa707d', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B/resolve/main/figures/benchmark.jpg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-R1-Distill-Qwen-14B from https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B/resolve/main/figures/benchmark.jpg
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-R1-Distill-Qwen-14B.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-3-12b-it', 'huggingface--google--gemma-3-12b-it', 'gemma-3-12b-it', 'google', '', '["transformers","safetensors","gemma3","any-to-any","image-text-to-text","conversational","arxiv:1905.07830","arxiv:1905.10044","arxiv:1911.11641","arxiv:1904.09728","arxiv:1705.03551","arxiv:1911.01547","arxiv:1907.10641","arxiv:1903.00161","arxiv:2009.03300","arxiv:2304.06364","arxiv:2103.03874","arxiv:2110.14168","arxiv:2311.12022","arxiv:2108.07732","arxiv:2107.03374","arxiv:2210.03057","arxiv:2106.03193","arxiv:1910.11856","arxiv:2502.12404","arxiv:2502.21228","arxiv:2404.16816","arxiv:2104.12756","arxiv:2311.16502","arxiv:2203.10244","arxiv:2404.12390","arxiv:1810.12440","arxiv:1908.02660","arxiv:2312.11805","base_model:google/gemma-3-12b-pt","base_model:finetune:google/gemma-3-12b-pt","license:gemma","text-generation-inference","endpoints_compatible","region:us"]', 'image-text-to-text', 584, 1532251, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-3-12b-it","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":12187325040,"storage_bytes":111273492196,"files_count":18,"spaces_count":76,"gated":"manual","private":false,"config":{"architectures":["Gemma3ForConditionalGeneration"],"model_type":"gemma3","processor_config":{"chat_template":"{{ bos_token }}\n{%- if messages[0][''role''] == ''system'' -%}\n    {%- if messages[0][''content''] is string -%}\n        {%- set first_user_prefix = messages[0][''content''] + ''\n\n'' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0][''content''][0][''text''] + ''\n\n'' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message[''role''] == ''assistant'') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message[''role''] -%}\n    {%- endif -%}\n    {{ ''<start_of_turn>'' + role + ''\n'' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message[''content''] is string -%}\n        {{ message[''content''] | trim }}\n    {%- elif message[''content''] is iterable -%}\n        {%- for item in message[''content''] -%}\n            {%- if item[''type''] == ''image'' -%}\n                {{ ''<start_of_image>'' }}\n            {%- elif item[''type''] == ''text'' -%}\n                {{ item[''text''] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ ''<end_of_turn>\n'' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{''<start_of_turn>model\n''}}\n{%- endif -%}\n"},"tokenizer_config":{"bos_token":"<bos>","chat_template":"{{ bos_token }}\n{%- if messages[0][''role''] == ''system'' -%}\n    {%- if messages[0][''content''] is string -%}\n        {%- set first_user_prefix = messages[0][''content''] + ''\n\n'' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0][''content''][0][''text''] + ''\n\n'' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message[''role''] == ''assistant'') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message[''role''] -%}\n    {%- endif -%}\n    {{ ''<start_of_turn>'' + role + ''\n'' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message[''content''] is string -%}\n        {{ message[''content''] | trim }}\n    {%- elif message[''content''] is iterable -%}\n        {%- for item in message[''content''] -%}\n            {%- if item[''type''] == ''image'' -%}\n                {{ ''<start_of_image>'' }}\n            {%- elif item[''type''] == ''text'' -%}\n                {{ item[''text''] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ ''<end_of_turn>\n'' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{''<start_of_turn>model\n''}}\n{%- endif -%}\n","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1903.00161","source_url":"https://arxiv.org/abs/1903.00161"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:2304.06364","source_url":"https://arxiv.org/abs/2304.06364"},{"type":"based_on_paper","target_id":"arxiv:2103.03874","source_url":"https://arxiv.org/abs/2103.03874"},{"type":"based_on_paper","target_id":"arxiv:2110.14168","source_url":"https://arxiv.org/abs/2110.14168"},{"type":"based_on_paper","target_id":"arxiv:2311.12022","source_url":"https://arxiv.org/abs/2311.12022"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"},{"type":"based_on_paper","target_id":"arxiv:2210.03057","source_url":"https://arxiv.org/abs/2210.03057"},{"type":"based_on_paper","target_id":"arxiv:2106.03193","source_url":"https://arxiv.org/abs/2106.03193"},{"type":"based_on_paper","target_id":"arxiv:1910.11856","source_url":"https://arxiv.org/abs/1910.11856"},{"type":"based_on_paper","target_id":"arxiv:2502.12404","source_url":"https://arxiv.org/abs/2502.12404"},{"type":"based_on_paper","target_id":"arxiv:2502.21228","source_url":"https://arxiv.org/abs/2502.21228"},{"type":"based_on_paper","target_id":"arxiv:2404.16816","source_url":"https://arxiv.org/abs/2404.16816"},{"type":"based_on_paper","target_id":"arxiv:2104.12756","source_url":"https://arxiv.org/abs/2104.12756"},{"type":"based_on_paper","target_id":"arxiv:2311.16502","source_url":"https://arxiv.org/abs/2311.16502"},{"type":"based_on_paper","target_id":"arxiv:2203.10244","source_url":"https://arxiv.org/abs/2203.10244"},{"type":"based_on_paper","target_id":"arxiv:2404.12390","source_url":"https://arxiv.org/abs/2404.12390"},{"type":"based_on_paper","target_id":"arxiv:1810.12440","source_url":"https://arxiv.org/abs/1810.12440"},{"type":"based_on_paper","target_id":"arxiv:1908.02660","source_url":"https://arxiv.org/abs/1908.02660"},{"type":"based_on_paper","target_id":"arxiv:2312.11805","source_url":"https://arxiv.org/abs/2312.11805"}]', NULL, 'Gemma', 'approved', 37.7, '26860ff7b85e55e1fce52bbcb346e218', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Comfy-Org-flux1-dev', 'huggingface--comfy-org--flux1-dev', 'flux1-dev', 'Comfy-Org', '--- license: other license_name: flux-1-dev-non-commercial-license license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/LICENSE.md tags: - diffusion-single-file - comfyui --- This is a smaller checkpoint for flux1-dev that will work better for ComfyUI users with less VRAM (under 24gb). The two text encoders used by Flux are already included in this one safetensor. Use it with the node in ComfyUI.', '["diffusion-single-file","comfyui","license:other","region:us"]', 'other', 583, 367176, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Comfy-Org/flux1-dev","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: flux-1-dev-non-commercial-license\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/LICENSE.md\ntags:\n- diffusion-single-file\n- comfyui\n---\n\nThis is a smaller checkpoint for flux1-dev that will work better for ComfyUI users with less VRAM (under 24gb). \n\nThe two text encoders used by Flux are already included in this one safetensor.\n\nUse it with the `Load Checkpoint` node in ComfyUI.', '{"pipeline_tag":null,"library_name":"diffusion-single-file","framework":"diffusion-single-file","params":null,"storage_bytes":89902171980,"files_count":7,"spaces_count":5,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'Other', 'approved', 37.7, '477e732582d74ec61baf3550ceef9c78', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-intfloat-multilingual-e5-large-instruct', 'huggingface--intfloat--multilingual-e5-large-instruct', 'multilingual-e5-large-instruct', 'intfloat', '--- tags: - mteb - sentence-transformers - transformers model-index: - name: multilingual-e5-large-instruct results: - task: type: Classification dataset: type: mteb/amazon_counterfactual name: MTEB AmazonCounterfactualClassification (en) config: en split: test revision: e8379541af4e31359cca9fbcf4b00f2671dba205 metrics: - type: accuracy value: 76.23880597014924 - type: ap value: 39.07351965022687 - type: f1 value: 70.04836733862683 - task: type: Classification dataset: type: mteb/amazon_count...', '["sentence-transformers","onnx","safetensors","xlm-roberta","feature-extraction","mteb","transformers","multilingual","af","am","ar","as","az","be","bg","bn","br","bs","ca","cs","cy","da","de","el","en","eo","es","et","eu","fa","fi","fr","fy","ga","gd","gl","gu","ha","he","hi","hr","hu","hy","id","is","it","ja","jv","ka","kk","km","kn","ko","ku","ky","la","lo","lt","lv","mg","mk","ml","mn","mr","ms","my","ne","nl","no","om","or","pa","pl","ps","pt","ro","ru","sa","sd","si","sk","sl","so","sq","sr","su","sv","sw","ta","te","th","tl","tr","ug","uk","ur","uz","vi","xh","yi","zh","arxiv:2402.05672","arxiv:2401.00368","arxiv:2104.08663","arxiv:2210.07316","license:mit","model-index","text-embeddings-inference","endpoints_compatible","region:us"]', 'feature-extraction', 582, 1257156, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/intfloat/multilingual-e5-large-instruct","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- mteb\n- sentence-transformers\n- transformers\nmodel-index:\n- name: multilingual-e5-large-instruct\n  results:\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en)\n      config: en\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 76.23880597014924\n    - type: ap\n      value: 39.07351965022687\n    - type: f1\n      value: 70.04836733862683\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (de)\n      config: de\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 66.71306209850107\n    - type: ap\n      value: 79.01499914759529\n    - type: f1\n      value: 64.81951817560703\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en-ext)\n      config: en-ext\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 73.85307346326837\n    - type: ap\n      value: 22.447519885878737\n    - type: f1\n      value: 61.0162730745633\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (ja)\n      config: ja\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 76.04925053533191\n    - type: ap\n      value: 23.44983217128922\n    - type: f1\n      value: 62.5723230907759\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_polarity\n      name: MTEB AmazonPolarityClassification\n      config: default\n      split: test\n      revision: e2d317d38cd51312af73b3d32a06d1a08b442046\n    metrics:\n    - type: accuracy\n      value: 96.28742500000001\n    - type: ap\n      value: 94.8449918887462\n    - type: f1\n      value: 96.28680923610432\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (en)\n      config: en\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 56.716\n    - type: f1\n      value: 55.76510398266401\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (de)\n      config: de\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 52.99999999999999\n    - type: f1\n      value: 52.00829994765178\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (es)\n      config: es\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 48.806000000000004\n    - type: f1\n      value: 48.082345914983634\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (fr)\n      config: fr\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 48.507999999999996\n    - type: f1\n      value: 47.68752844642045\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (ja)\n      config: ja\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 47.709999999999994\n    - type: f1\n      value: 47.05870376637181\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (zh)\n      config: zh\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 44.662000000000006\n    - type: f1\n      value: 43.42371965372771\n  - task:\n      type: Retrieval\n    dataset:\n      type: arguana\n      name: MTEB ArguAna\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 31.721\n    - type: map_at_10\n      value: 49.221\n    - type: map_at_100\n      value: 49.884\n    - type: map_at_1000\n      value: 49.888\n    - type: map_at_3\n      value: 44.31\n    - type: map_at_5\n      value: 47.276\n    - type: mrr_at_1\n      value: 32.432\n    - type: mrr_at_10\n      value: 49.5\n    - type: mrr_at_100\n      value: 50.163000000000004\n    - type: mrr_at_1000\n      value: 50.166\n    - type: mrr_at_3\n      value: 44.618\n    - type: mrr_at_5\n      value: 47.541\n    - type: ndcg_at_1\n      value: 31.721\n    - type: ndcg_at_10\n      value: 58.384\n    - type: ndcg_at_100\n      value: 61.111000000000004\n    - type: ndcg_at_1000\n      value: 61.187999999999995\n    - type: ndcg_at_3\n      value: 48.386\n    - type: ndcg_at_5\n      value: 53.708999999999996\n    - type: precision_at_1\n      value: 31.721\n    - type: precision_at_10\n      value: 8.741\n    - type: precision_at_100\n      value: 0.991\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_3\n      value: 20.057\n    - type: precision_at_5\n      value: 14.609\n    - type: recall_at_1\n      value: 31.721\n    - type: recall_at_10\n      value: 87.411\n    - type: recall_at_100\n      value: 99.075\n    - type: recall_at_1000\n      value: 99.644\n    - type: recall_at_3\n      value: 60.171\n    - type: recall_at_5\n      value: 73.044\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-p2p\n      name: MTEB ArxivClusteringP2P\n      config: default\n      split: test\n      revision: a122ad7f3f0291bf49cc6f4d32aa80929df69d5d\n    metrics:\n    - type: v_measure\n      value: 46.40419580759799\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-s2s\n      name: MTEB ArxivClusteringS2S\n      config: default\n      split: test\n      revision: f910caf1a6075f7329cdf8c1a6135696f37dbd53\n    metrics:\n    - type: v_measure\n      value: 40.48593255007969\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/askubuntudupquestions-reranking\n      name: MTEB AskUbuntuDupQuestions\n      config: default\n      split: test\n      revision: 2000358ca161889fa9c082cb41daa8dcfb161a54\n    metrics:\n    - type: map\n      value: 63.889179122289995\n    - type: mrr\n      value: 77.61146286769556\n  - task:\n      type: STS\n    dataset:\n      type: mteb/biosses-sts\n      name: MTEB BIOSSES\n      config: default\n      split: test\n      revision: d3fb88f8f02e40887cd149695127462bbcf29b4a\n    metrics:\n    - type: cos_sim_pearson\n      value: 88.15075203727929\n    - type: cos_sim_spearman\n      value: 86.9622224570873\n    - type: euclidean_pearson\n      value: 86.70473853624121\n    - type: euclidean_spearman\n      value: 86.9622224570873\n    - type: manhattan_pearson\n      value: 86.21089380980065\n    - type: manhattan_spearman\n      value: 86.75318154937008\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (de-en)\n      config: de-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 99.65553235908142\n    - type: f1\n      value: 99.60681976339595\n    - type: precision\n      value: 99.58246346555325\n    - type: recall\n      value: 99.65553235908142\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (fr-en)\n      config: fr-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 99.26260180497468\n    - type: f1\n      value: 99.14520507740848\n    - type: precision\n      value: 99.08650671362535\n    - type: recall\n      value: 99.26260180497468\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (ru-en)\n      config: ru-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 98.07412538967787\n    - type: f1\n      value: 97.86629719431936\n    - type: precision\n      value: 97.76238309664012\n    - type: recall\n      value: 98.07412538967787\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (zh-en)\n      config: zh-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 99.42074776197998\n    - type: f1\n      value: 99.38564156573635\n    - type: precision\n      value: 99.36808846761454\n    - type: recall\n      value: 99.42074776197998\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/banking77\n      name: MTEB Banking77Classification\n      config: default\n      split: test\n      revision: 0fd18e25b25c072e09e0d92ab615fda904d66300\n    metrics:\n    - type: accuracy\n      value: 85.73376623376623\n    - type: f1\n      value: 85.68480707214599\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-p2p\n      name: MTEB BiorxivClusteringP2P\n      config: default\n      split: test\n      revision: 65b79d1d13f80053f67aca9498d9402c2d9f1f40\n    metrics:\n    - type: v_measure\n      value: 40.935218072113855\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-s2s\n      name: MTEB BiorxivClusteringS2S\n      config: default\n      split: test\n      revision: 258694dd0231531bc1fd9de6ceb52a0853c6d908\n    metrics:\n    - type: v_measure\n      value: 36.276389017675264\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 27.764166666666668\n    - type: map_at_10\n      value: 37.298166666666674\n    - type: map_at_100\n      value: 38.530166666666666\n    - type: map_at_1000\n      value: 38.64416666666667\n    - type: map_at_3\n      value: 34.484833333333334\n    - type: map_at_5\n      value: 36.0385\n    - type: mrr_at_1\n      value: 32.93558333333333\n    - type: mrr_at_10\n      value: 41.589749999999995\n    - type: mrr_at_100\n      value: 42.425333333333334\n    - type: mrr_at_1000\n      value: 42.476333333333336\n    - type: mrr_at_3\n      value: 39.26825\n    - type: mrr_at_5\n      value: 40.567083333333336\n    - type: ndcg_at_1\n      value: 32.93558333333333\n    - type: ndcg_at_10\n      value: 42.706583333333334\n    - type: ndcg_at_100\n      value: 47.82483333333333\n    - type: ndcg_at_1000\n      value: 49.95733333333334\n    - type: ndcg_at_3\n      value: 38.064750000000004\n    - type: ndcg_at_5\n      value: 40.18158333333333\n    - type: precision_at_1\n      value: 32.93558333333333\n    - type: precision_at_10\n      value: 7.459833333333334\n    - type: precision_at_100\n      value: 1.1830833333333335\n    - type: precision_at_1000\n      value: 0.15608333333333332\n    - type: precision_at_3\n      value: 17.5235\n    - type: precision_at_5\n      value: 12.349833333333333\n    - type: recall_at_1\n      value: 27.764166666666668\n    - type: recall_at_10\n      value: 54.31775\n    - type: recall_at_100\n      value: 76.74350000000001\n    - type: recall_at_1000\n      value: 91.45208333333332\n    - type: recall_at_3\n      value: 41.23425\n    - type: recall_at_5\n      value: 46.73983333333334\n  - task:\n      type: Retrieval\n    dataset:\n      type: climate-fever\n      name: MTEB ClimateFEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 12.969\n    - type: map_at_10\n      value: 21.584999999999997\n    - type: map_at_100\n      value: 23.3\n    - type: map_at_1000\n      value: 23.5\n    - type: map_at_3\n      value: 18.218999999999998\n    - type: map_at_5\n      value: 19.983\n    - type: mrr_at_1\n      value: 29.316\n    - type: mrr_at_10\n      value: 40.033\n    - type: mrr_at_100\n      value: 40.96\n    - type: mrr_at_1000\n      value: 41.001\n    - type: mrr_at_3\n      value: 37.123\n    - type: mrr_at_5\n      value: 38.757999999999996\n    - type: ndcg_at_1\n      value: 29.316\n    - type: ndcg_at_10\n      value: 29.858\n    - type: ndcg_at_100\n      value: 36.756\n    - type: ndcg_at_1000\n      value: 40.245999999999995\n    - type: ndcg_at_3\n      value: 24.822\n    - type: ndcg_at_5\n      value: 26.565\n    - type: precision_at_1\n      value: 29.316\n    - type: precision_at_10\n      value: 9.186\n    - type: precision_at_100\n      value: 1.6549999999999998\n    - type: precision_at_1000\n      value: 0.22999999999999998\n    - type: precision_at_3\n      value: 18.436\n    - type: precision_at_5\n      value: 13.876\n    - type: recall_at_1\n      value: 12.969\n    - type: recall_at_10\n      value: 35.142\n    - type: recall_at_100\n      value: 59.143\n    - type: recall_at_1000\n      value: 78.594\n    - type: recall_at_3\n      value: 22.604\n    - type: recall_at_5\n      value: 27.883000000000003\n  - task:\n      type: Retrieval\n    dataset:\n      type: dbpedia-entity\n      name: MTEB DBPedia\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 8.527999999999999\n    - type: map_at_10\n      value: 17.974999999999998\n    - type: map_at_100\n      value: 25.665\n    - type: map_at_1000\n      value: 27.406000000000002\n    - type: map_at_3\n      value: 13.017999999999999\n    - type: map_at_5\n      value: 15.137\n    - type: mrr_at_1\n      value: 62.5\n    - type: mrr_at_10\n      value: 71.891\n    - type: mrr_at_100\n      value: 72.294\n    - type: mrr_at_1000\n      value: 72.296\n    - type: mrr_at_3\n      value: 69.958\n    - type: mrr_at_5\n      value: 71.121\n    - type: ndcg_at_1\n      value: 50.875\n    - type: ndcg_at_10\n      value: 38.36\n    - type: ndcg_at_100\n      value: 44.235\n    - type: ndcg_at_1000\n      value: 52.154\n    - type: ndcg_at_3\n      value: 43.008\n    - type: ndcg_at_5\n      value: 40.083999999999996\n    - type: precision_at_1\n      value: 62.5\n    - type: precision_at_10\n      value: 30.0\n    - type: precision_at_100\n      value: 10.038\n    - type: precision_at_1000\n      value: 2.0869999999999997\n    - type: precision_at_3\n      value: 46.833000000000006\n    - type: precision_at_5\n      value: 38.800000000000004\n    - type: recall_at_1\n      value: 8.527999999999999\n    - type: recall_at_10\n      value: 23.828\n    - type: recall_at_100\n      value: 52.322\n    - type: recall_at_1000\n      value: 77.143\n    - type: recall_at_3\n      value: 14.136000000000001\n    - type: recall_at_5\n      value: 17.761\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/emotion\n      name: MTEB EmotionClassification\n      config: default\n      split: test\n      revision: 4f58c6b202a23cf9a4da393831edf4f9183cad37\n    metrics:\n    - type: accuracy\n      value: 51.51\n    - type: f1\n      value: 47.632159862049896\n  - task:\n      type: Retrieval\n    dataset:\n      type: fever\n      name: MTEB FEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 60.734\n    - type: map_at_10\n      value: 72.442\n    - type: map_at_100\n      value: 72.735\n    - type: map_at_1000\n      value: 72.75\n    - type: map_at_3\n      value: 70.41199999999999\n    - type: map_at_5\n      value: 71.80499999999999\n    - type: mrr_at_1\n      value: 65.212\n    - type: mrr_at_10\n      value: 76.613\n    - type: mrr_at_100\n      value: 76.79899999999999\n    - type: mrr_at_1000\n      value: 76.801\n    - type: mrr_at_3\n      value: 74.8\n    - type: mrr_at_5\n      value: 76.12400000000001\n    - type: ndcg_at_1\n      value: 65.212\n    - type: ndcg_at_10\n      value: 77.988\n    - type: ndcg_at_100\n      value: 79.167\n    - type: ndcg_at_1000\n      value: 79.452\n    - type: ndcg_at_3\n      value: 74.362\n    - type: ndcg_at_5\n      value: 76.666\n    - type: precision_at_1\n      value: 65.212\n    - type: precision_at_10\n      value: 10.003\n    - type: precision_at_100\n      value: 1.077\n    - type: precision_at_1000\n      value: 0.11199999999999999\n    - type: precision_at_3\n      value: 29.518\n    - type: precision_at_5\n      value: 19.016\n    - type: recall_at_1\n      value: 60.734\n    - type: recall_at_10\n      value: 90.824\n    - type: recall_at_100\n      value: 95.71600000000001\n    - type: recall_at_1000\n      value: 97.577\n    - type: recall_at_3\n      value: 81.243\n    - type: recall_at_5\n      value: 86.90299999999999\n  - task:\n      type: Retrieval\n    dataset:\n      type: fiqa\n      name: MTEB FiQA2018\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 23.845\n    - type: map_at_10\n      value: 39.281\n    - type: map_at_100\n      value: 41.422\n    - type: map_at_1000\n      value: 41.593\n    - type: map_at_3\n      value: 34.467\n    - type: map_at_5\n      value: 37.017\n    - type: mrr_at_1\n      value: 47.531\n    - type: mrr_at_10\n      value: 56.204\n    - type: mrr_at_100\n      value: 56.928999999999995\n    - type: mrr_at_1000\n      value: 56.962999999999994\n    - type: mrr_at_3\n      value: 54.115\n    - type: mrr_at_5\n      value: 55.373000000000005\n    - type: ndcg_at_1\n      value: 47.531\n    - type: ndcg_at_10\n      value: 47.711999999999996\n    - type: ndcg_at_100\n      value: 54.510999999999996\n    - type: ndcg_at_1000\n      value: 57.103\n    - type: ndcg_at_3\n      value: 44.145\n    - type: ndcg_at_5\n      value: 45.032\n    - type: precision_at_1\n      value: 47.531\n    - type: precision_at_10\n      value: 13.194\n    - type: precision_at_100\n      value: 2.045\n    - type: precision_at_1000\n      value: 0.249\n    - type: precision_at_3\n      value: 29.424\n    - type: precision_at_5\n      value: 21.451\n    - type: recall_at_1\n      value: 23.845\n    - type: recall_at_10\n      value: 54.967\n    - type: recall_at_100\n      value: 79.11399999999999\n    - type: recall_at_1000\n      value: 94.56700000000001\n    - type: recall_at_3\n      value: 40.256\n    - type: recall_at_5\n      value: 46.215\n  - task:\n      type: Retrieval\n    dataset:\n      type: hotpotqa\n      name: MTEB HotpotQA\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 37.819\n    - type: map_at_10\n      value: 60.889\n    - type: map_at_100\n      value: 61.717999999999996\n    - type: map_at_1000\n      value: 61.778\n    - type: map_at_3\n      value: 57.254000000000005\n    - type: map_at_5\n      value: 59.541\n    - type: mrr_at_1\n      value: 75.638\n    - type: mrr_at_10\n      value: 82.173\n    - type: mrr_at_100\n      value: 82.362\n    - type: mrr_at_1000\n      value: 82.37\n    - type: mrr_at_3\n      value: 81.089\n    - type: mrr_at_5\n      value: 81.827\n    - type: ndcg_at_1\n      value: 75.638\n    - type: ndcg_at_10\n      value: 69.317\n    - type: ndcg_at_100\n      value: 72.221\n    - type: ndcg_at_1000\n      value: 73.382\n    - type: ndcg_at_3\n      value: 64.14\n    - type: ndcg_at_5\n      value: 67.07600000000001\n    - type: precision_at_1\n      value: 75.638\n    - type: precision_at_10\n      value: 14.704999999999998\n    - type: precision_at_100\n      value: 1.698\n    - type: precision_at_1000\n      value: 0.185\n    - type: precision_at_3\n      value: 41.394999999999996\n    - type: precision_at_5\n      value: 27.162999999999997\n    - type: recall_at_1\n      value: 37.819\n    - type: recall_at_10\n      value: 73.52499999999999\n    - type: recall_at_100\n      value: 84.875\n    - type: recall_at_1000\n      value: 92.559\n    - type: recall_at_3\n      value: 62.092999999999996\n    - type: recall_at_5\n      value: 67.907\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/imdb\n      name: MTEB ImdbClassification\n      config: default\n      split: test\n      revision: 3d86128a09e091d6018b6d26cad27f2739fc2db7\n    metrics:\n    - type: accuracy\n      value: 94.60079999999999\n    - type: ap\n      value: 92.67396345347356\n    - type: f1\n      value: 94.5988098167121\n  - task:\n      type: Retrieval\n    dataset:\n      type: msmarco\n      name: MTEB MSMARCO\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 21.285\n    - type: map_at_10\n      value: 33.436\n    - type: map_at_100\n      value: 34.63\n    - type: map_at_1000\n      value: 34.681\n    - type: map_at_3\n      value: 29.412\n    - type: map_at_5\n      value: 31.715\n    - type: mrr_at_1\n      value: 21.848\n    - type: mrr_at_10\n      value: 33.979\n    - type: mrr_at_100\n      value: 35.118\n    - type: mrr_at_1000\n      value: 35.162\n    - type: mrr_at_3\n      value: 30.036\n    - type: mrr_at_5\n      value: 32.298\n    - type: ndcg_at_1\n      value: 21.862000000000002\n    - type: ndcg_at_10\n      value: 40.43\n    - type: ndcg_at_100\n      value: 46.17\n    - type: ndcg_at_1000\n      value: 47.412\n    - type: ndcg_at_3\n      value: 32.221\n    - type: ndcg_at_5\n      value: 36.332\n    - type: precision_at_1\n      value: 21.862000000000002\n    - type: precision_at_10\n      value: 6.491\n    - type: precision_at_100\n      value: 0.935\n    - type: precision_at_1000\n      value: 0.104\n    - type: precision_at_3\n      value: 13.744\n    - type: precision_at_5\n      value: 10.331999999999999\n    - type: recall_at_1\n      value: 21.285\n    - type: recall_at_10\n      value: 62.083\n    - type: recall_at_100\n      value: 88.576\n    - type: recall_at_1000\n      value: 98.006\n    - type: recall_at_3\n      value: 39.729\n    - type: recall_at_5\n      value: 49.608000000000004\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (en)\n      config: en\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 93.92612859097127\n    - type: f1\n      value: 93.82370333372853\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (de)\n      config: de\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 92.67681036911807\n    - type: f1\n      value: 92.14191382411472\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (es)\n      config: es\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 92.26817878585723\n    - type: f1\n      value: 91.92824250337878\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (fr)\n      config: fr\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 89.96554963983714\n    - type: f1\n      value: 90.02859329630792\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (hi)\n      config: hi\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 90.02509860164935\n    - type: f1\n      value: 89.30665159182062\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (th)\n      config: th\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 87.55515370705244\n    - type: f1\n      value: 87.94449232331907\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (en)\n      config: en\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 82.4623803009576\n    - type: f1\n      value: 66.06738378772725\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (de)\n      config: de\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 79.3716539870386\n    - type: f1\n      value: 60.37614033396853\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (es)\n      config: es\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 80.34022681787857\n    - type: f1\n      value: 58.302008026952\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (fr)\n      config: fr\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 76.72095208268087\n    - type: f1\n      value: 59.64524724009049\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (hi)\n      config: hi\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 77.87020437432773\n    - type: f1\n      value: 57.80202694670567\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (th)\n      config: th\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 77.73598553345387\n    - type: f1\n      value: 58.19628250675031\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (af)\n      config: af\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.6630800268998\n    - type: f1\n      value: 65.00996668051691\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (am)\n      config: am\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 60.7128446536651\n    - type: f1\n      value: 57.95860594874963\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ar)\n      config: ar\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 63.61129791526563\n    - type: f1\n      value: 59.75328290206483\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (az)\n      config: az\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.00134498991257\n    - type: f1\n      value: 67.0230483991802\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (bn)\n      config: bn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 68.54068594485541\n    - type: f1\n      value: 65.54604628946976\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (cy)\n      config: cy\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 63.032952252858095\n    - type: f1\n      value: 58.715741857057104\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (da)\n      config: da\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.80901143241427\n    - type: f1\n      value: 68.33963989243877\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (de)\n      config: de\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.47141896435777\n    - type: f1\n      value: 69.56765020308262\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (el)\n      config: el\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.2373907195696\n    - type: f1\n      value: 69.04529836036467\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (en)\n      config: en\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 77.05783456624076\n    - type: f1\n      value: 74.69430584708174\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (es)\n      config: es\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.82111634162744\n    - type: f1\n      value: 70.77228952803762\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fa)\n      config: fa\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.25353059852051\n    - type: f1\n      value: 71.05310103416411\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fi)\n      config: fi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.28648285137861\n    - type: f1\n      value: 69.08020473732226\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fr)\n      config: fr\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 73.31540013449899\n    - type: f1\n      value: 70.9426355465791\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (he)\n      config: he\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.2151983860121\n    - type: f1\n      value: 67.52541755908858\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hi)\n      config: hi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.58372562205784\n    - type: f1\n      value: 69.49769064229827\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hu)\n      config: hu\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.9233355749832\n    - type: f1\n      value: 69.36311548259593\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hy)\n      config: hy\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 68.07330195023538\n    - type: f1\n      value: 64.99882022345572\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (id)\n      config: id\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.62273032952253\n    - type: f1\n      value: 70.6394885471001\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (is)\n      config: is\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 65.77000672494957\n    - type: f1\n      value: 62.9368944815065\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (it)\n      config: it\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 73.453261600538\n    - type: f1\n      value: 70.85069934666681\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ja)\n      config: ja\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.6906523201076\n    - type: f1\n      value: 72.03249740074217\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (jv)\n      config: jv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 63.03631472763953\n    - type: f1\n      value: 59.3165215571852\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ka)\n      config: ka\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 58.913920645595155\n    - type: f1\n      value: 57.367337711611285\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (km)\n      config: km\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 54.42837928715535\n    - type: f1\n      value: 52.60527294970906\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (kn)\n      config: kn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 66.33490248823135\n    - type: f1\n      value: 63.213340969404065\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ko)\n      config: ko\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.58507061197041\n    - type: f1\n      value: 68.40256628040486\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (lv)\n      config: lv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.11230665770006\n    - type: f1\n      value: 66.44863577842305\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ml)\n      config: ml\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.70073974445192\n    - type: f1\n      value: 67.21291337273702\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (mn)\n      config: mn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 66.43913920645595\n    - type: f1\n      value: 64.09838087422806\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ms)\n      config: ms\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.80026899798251\n    - type: f1\n      value: 68.76986742962444\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (my)\n      config: my\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 64.78816408876934\n    - type: f1\n      value: 62.18781873428972\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (nb)\n      config: nb\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.6577000672495\n    - type: f1\n      value: 68.75171511133003\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (nl)\n      config: nl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.42501681237391\n    - type: f1\n      value: 71.18434963451544\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (pl)\n      config: pl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 73.64828513786146\n    - type: f1\n      value: 70.67741914007422\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (pt)\n      config: pt\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 73.62811028917284\n    - type: f1\n      value: 71.36402039740959\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ro)\n      config: ro\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.88634835238736\n    - type: f1\n      value: 69.23701923480677\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ru)\n      config: ru\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.15938130464022\n    - type: f1\n      value: 71.87792218993388\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sl)\n      config: sl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.96301277740416\n    - type: f1\n      value: 67.29584200202983\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sq)\n      config: sq\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.49562878278412\n    - type: f1\n      value: 66.91716685679431\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sv)\n      config: sv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.6805648957633\n    - type: f1\n      value: 72.02723592594374\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sw)\n      config: sw\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 63.00605245460659\n    - type: f1\n      value: 60.16716669482932\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ta)\n      config: ta\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 66.90988567585742\n    - type: f1\n      value: 63.99405488777784\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (te)\n      config: te\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.62273032952253\n    - type: f1\n      value: 65.17213906909481\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (th)\n      config: th\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.50907868190988\n    - type: f1\n      value: 69.15165697194853\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (tl)\n      config: tl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.30733019502352\n    - type: f1\n      value: 66.69024007380474\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (tr)\n      config: tr\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.24277067921989\n    - type: f1\n      value: 68.80515408492947\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ur)\n      config: ur\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.49831876260929\n    - type: f1\n      value: 64.83778567111116\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (vi)\n      config: vi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.28782784129119\n    - type: f1\n      value: 69.3294186700733\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (zh-CN)\n      config: zh-CN\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 73.315400134499\n    - type: f1\n      value: 71.22674385243207\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (zh-TW)\n      config: zh-TW\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.37794216543377\n    - type: f1\n      value: 68.96962492838232\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (af)\n      config: af\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.33557498318764\n    - type: f1\n      value: 72.28949738478356\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (am)\n      config: am\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 65.84398117014123\n    - type: f1\n      value: 64.71026362091463\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ar)\n      config: ar\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 69.76462676529925\n    - type: f1\n      value: 69.8229667407667\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (az)\n      config: az\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 72.02420981842636\n    - type: f1\n      value: 71.76576384895898\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (bn)\n      config: bn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 72.7572293207801\n    - type: f1\n      value: 72.76840765295256\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (cy)\n      config: cy\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 68.02286482851379\n    - type: f1\n      value: 66.17237947327872\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (da)\n      config: da\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.60928043039678\n    - type: f1\n      value: 77.27094731234773\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (de)\n      config: de\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.68325487558843\n    - type: f1\n      value: 77.97530399082261\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (el)\n      config: el\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.13315400134498\n    - type: f1\n      value: 75.97558584796424\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (en)\n      config: en\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 80.47410894418292\n    - type: f1\n      value: 80.52244841473792\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (es)\n      config: es\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.9670477471419\n    - type: f1\n      value: 77.37318805793146\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fa)\n      config: fa\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 78.09683927370544\n    - type: f1\n      value: 77.69773737430847\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fi)\n      config: fi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.20847343644922\n    - type: f1\n      value: 75.17071738727348\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fr)\n      config: fr\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.07464694014796\n    - type: f1\n      value: 77.16136207698571\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (he)\n      config: he\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.53396099529255\n    - type: f1\n      value: 73.58296404484122\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hi)\n      config: hi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.75319435104237\n    - type: f1\n      value: 75.24674707850833\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hu)\n      config: hu\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.0948217888366\n    - type: f1\n      value: 76.47559490205028\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hy)\n      config: hy\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.07599193006052\n    - type: f1\n      value: 70.76028043093511\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (id)\n      config: id\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.10490921318089\n    - type: f1\n      value: 77.01215275283272\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (is)\n      config: is\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.25756556825824\n    - type: f1\n      value: 70.20605314648762\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (it)\n      config: it\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.08137188971082\n    - type: f1\n      value: 77.3899269057439\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ja)\n      config: ja\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 79.35440484196369\n    - type: f1\n      value: 79.58964690002772\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (jv)\n      config: jv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 68.42299932750504\n    - type: f1\n      value: 68.07844356925413\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ka)\n      config: ka\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.15669132481507\n    - type: f1\n      value: 65.89383352608513\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (km)\n      config: km\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 60.11432414256894\n    - type: f1\n      value: 57.69910594559806\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (kn)\n      config: kn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.24747814391392\n    - type: f1\n      value: 70.42455553830918\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ko)\n      config: ko\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.46267652992603\n    - type: f1\n      value: 76.8854559308316\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (lv)\n      config: lv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.24815063887021\n    - type: f1\n      value: 72.77805034658074\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ml)\n      config: ml\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.11566913248151\n    - type: f1\n      value: 73.86147988001356\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (mn)\n      config: mn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 70.0168123739072\n    - type: f1\n      value: 69.38515920054571\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ms)\n      config: ms\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.41156691324814\n    - type: f1\n      value: 73.43474953408237\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (my)\n      config: my\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 68.39609952925353\n    - type: f1\n      value: 67.29731681109291\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (nb)\n      config: nb\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.20914593140552\n    - type: f1\n      value: 77.07066497935367\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (nl)\n      config: nl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 78.52387357094821\n    - type: f1\n      value: 78.5259569473291\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (pl)\n      config: pl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.6913248150639\n    - type: f1\n      value: 76.91201656350455\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (pt)\n      config: pt\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.1217215870881\n    - type: f1\n      value: 77.41179937912504\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ro)\n      config: ro\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.25891055817083\n    - type: f1\n      value: 75.8089244542887\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ru)\n      config: ru\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.70679219905851\n    - type: f1\n      value: 78.21459594517711\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sl)\n      config: sl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.83523873570948\n    - type: f1\n      value: 74.86847028401978\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sq)\n      config: sq\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.71755211835911\n    - type: f1\n      value: 74.0214326485662\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sv)\n      config: sv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 79.06523201075991\n    - type: f1\n      value: 79.10545620325138\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sw)\n      config: sw\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 67.91862811028918\n    - type: f1\n      value: 66.50386121217983\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ta)\n      config: ta\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 70.93140551445865\n    - type: f1\n      value: 70.755435928495\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (te)\n      config: te\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 72.40753194351042\n    - type: f1\n      value: 71.61816115782923\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (th)\n      config: th\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.1815736381977\n    - type: f1\n      value: 75.08016717887205\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (tl)\n      config: tl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 72.86482851378614\n    - type: f1\n      value: 72.39521180006291\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (tr)\n      config: tr\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.46940147948891\n    - type: f1\n      value: 76.70044085362349\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ur)\n      config: ur\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.89307330195024\n    - type: f1\n      value: 71.5721825332298\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (vi)\n      config: vi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.7511768661735\n    - type: f1\n      value: 75.17918654541515\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (zh-CN)\n      config: zh-CN\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 78.69535978480162\n    - type: f1\n      value: 78.90019070153316\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (zh-TW)\n      config: zh-TW\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.45729657027572\n    - type: f1\n      value: 76.19578371794672\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-p2p\n      name: MTEB MedrxivClusteringP2P\n      config: default\n      split: test\n      revision: e7a26af6f3ae46b30dde8737f02c07b1505bcc73\n    metrics:\n    - type: v_measure\n      value: 36.92715354123554\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-s2s\n      name: MTEB MedrxivClusteringS2S\n      config: default\n      split: test\n      revision: 35191c8c0dca72d8ff3efcd72aa802307d469663\n    metrics:\n    - type: v_measure\n      value: 35.53536244162518\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/mind_small\n      name: MTEB MindSmallReranking\n      config: default\n      split: test\n      revision: 3bdac13927fdc888b903db93b2ffdbd90b295a69\n    metrics:\n    - type: map\n      value: 33.08507884504006\n    - type: mrr\n      value: 34.32436977159129\n  - task:\n      type: Retrieval\n    dataset:\n      type: nfcorpus\n      name: MTEB NFCorpus\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 5.935\n    - type: map_at_10\n      value: 13.297\n    - type: map_at_100\n      value: 16.907\n    - type: map_at_1000\n      value: 18.391\n    - type: map_at_3\n      value: 9.626999999999999\n    - type: map_at_5\n      value: 11.190999999999999\n    - type: mrr_at_1\n      value: 46.129999999999995\n    - type: mrr_at_10\n      value: 54.346000000000004\n    - type: mrr_at_100\n      value: 55.067\n    - type: mrr_at_1000\n      value: 55.1\n    - type: mrr_at_3\n      value: 51.961\n    - type: mrr_at_5\n      value: 53.246\n    - type: ndcg_at_1\n      value: 44.118\n    - type: ndcg_at_10\n      value: 35.534\n    - type: ndcg_at_100\n      value: 32.946999999999996\n    - type: ndcg_at_1000\n      value: 41.599000000000004\n    - type: ndcg_at_3\n      value: 40.25\n    - type: ndcg_at_5\n      value: 37.978\n    - type: precision_at_1\n      value: 46.129999999999995\n    - type: precision_at_10\n      value: 26.842\n    - type: precision_at_100\n      value: 8.427\n    - type: precision_at_1000\n      value: 2.128\n    - type: precision_at_3\n      value: 37.977\n    - type: precision_at_5\n      value: 32.879000000000005\n    - type: recall_at_1\n      value: 5.935\n    - type: recall_at_10\n      value: 17.211000000000002\n    - type: recall_at_100\n      value: 34.33\n    - type: recall_at_1000\n      value: 65.551\n    - type: recall_at_3\n      value: 10.483\n    - type: recall_at_5\n      value: 13.078999999999999\n  - task:\n      type: Retrieval\n    dataset:\n      type: nq\n      name: MTEB NQ\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 35.231\n    - type: map_at_10\n      value: 50.202000000000005\n    - type: map_at_100\n      value: 51.154999999999994\n    - type: map_at_1000\n      value: 51.181\n    - type: map_at_3\n      value: 45.774\n    - type: map_at_5\n      value: 48.522\n    - type: mrr_at_1\n      value: 39.687\n    - type: mrr_at_10\n      value: 52.88\n    - type: mrr_at_100\n      value: 53.569\n    - type: mrr_at_1000\n      value: 53.58500000000001\n    - type: mrr_at_3\n      value: 49.228\n    - type: mrr_at_5\n      value: 51.525\n    - type: ndcg_at_1\n      value: 39.687\n    - type: ndcg_at_10\n      value: 57.754000000000005\n    - type: ndcg_at_100\n      value: 61.597\n    - type: ndcg_at_1000\n      value: 62.18900000000001\n    - type: ndcg_at_3\n      value: 49.55\n    - type: ndcg_at_5\n      value: 54.11899999999999\n    - type: precision_at_1\n      value: 39.687\n    - type: precision_at_10\n      value: 9.313\n    - type: precision_at_100\n      value: 1.146\n    - type: precision_at_1000\n      value: 0.12\n    - type: precision_at_3\n      value: 22.229\n    - type: precision_at_5\n      value: 15.939\n    - type: recall_at_1\n      value: 35.231\n    - type: recall_at_10\n      value: 78.083\n    - type: recall_at_100\n      value: 94.42099999999999\n    - type: recall_at_1000\n      value: 98.81\n    - type: recall_at_3\n      value: 57.047000000000004\n    - type: recall_at_5\n      value: 67.637\n  - task:\n      type: Retrieval\n    dataset:\n      type: quora\n      name: MTEB QuoraRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 71.241\n    - type: map_at_10\n      value: 85.462\n    - type: map_at_100\n      value: 86.083\n    - type: map_at_1000\n      value: 86.09700000000001\n    - type: map_at_3\n      value: 82.49499999999999\n    - type: map_at_5\n      value: 84.392\n    - type: mrr_at_1\n      value: 82.09\n    - type: mrr_at_10\n      value: 88.301\n    - type: mrr_at_100\n      value: 88.383\n    - type: mrr_at_1000\n      value: 88.384\n    - type: mrr_at_3\n      value: 87.37\n    - type: mrr_at_5\n      value: 88.035\n    - type: ndcg_at_1\n      value: 82.12\n    - type: ndcg_at_10\n      value: 89.149\n    - type: ndcg_at_100\n      value: 90.235\n    - type: ndcg_at_1000\n      value: 90.307\n    - type: ndcg_at_3\n      value: 86.37599999999999\n    - type: ndcg_at_5\n      value: 87.964\n    - type: precision_at_1\n      value: 82.12\n    - type: precision_at_10\n      value: 13.56\n    - type: precision_at_100\n      value: 1.539\n    - type: precision_at_1000\n      value: 0.157\n    - type: precision_at_3\n      value: 37.88\n    - type: precision_at_5\n      value: 24.92\n    - type: recall_at_1\n      value: 71.241\n    - type: recall_at_10\n      value: 96.128\n    - type: recall_at_100\n      value: 99.696\n    - type: recall_at_1000\n      value: 99.994\n    - type: recall_at_3\n      value: 88.181\n    - type: recall_at_5\n      value: 92.694\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering\n      name: MTEB RedditClustering\n      config: default\n      split: test\n      revision: 24640382cdbf8abc73003fb0fa6d111a705499eb\n    metrics:\n    - type: v_measure\n      value: 56.59757799655151\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering-p2p\n      name: MTEB RedditClusteringP2P\n      config: default\n      split: test\n      revision: 282350215ef01743dc01b456c7f5241fa8937f16\n    metrics:\n    - type: v_measure\n      value: 64.27391998854624\n  - task:\n      type: Retrieval\n    dataset:\n      type: scidocs\n      name: MTEB SCIDOCS\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 4.243\n    - type: map_at_10\n      value: 10.965\n    - type: map_at_100\n      value: 12.934999999999999\n    - type: map_at_1000\n      value: 13.256\n    - type: map_at_3\n      value: 7.907\n    - type: map_at_5\n      value: 9.435\n    - type: mrr_at_1\n      value: 20.9\n    - type: mrr_at_10\n      value: 31.849\n    - type: mrr_at_100\n      value: 32.964\n    - type: mrr_at_1000\n      value: 33.024\n    - type: mrr_at_3\n      value: 28.517\n    - type: mrr_at_5\n      value: 30.381999999999998\n    - type: ndcg_at_1\n      value: 20.9\n    - type: ndcg_at_10\n      value: 18.723\n    - type: ndcg_at_100\n      value: 26.384999999999998\n    - type: ndcg_at_1000\n      value: 32.114\n    - type: ndcg_at_3\n      value: 17.753\n    - type: ndcg_at_5\n      value: 15.558\n    - type: precision_at_1\n      value: 20.9\n    - type: precision_at_10\n      value: 9.8\n    - type: precision_at_100\n      value: 2.078\n    - type: precision_at_1000\n      value: 0.345\n    - type: precision_at_3\n      value: 16.900000000000002\n    - type: precision_at_5\n      value: 13.88\n    - type: recall_at_1\n      value: 4.243\n    - type: recall_at_10\n      value: 19.885\n    - type: recall_at_100\n      value: 42.17\n    - type: recall_at_1000\n      value: 70.12\n    - type: recall_at_3\n      value: 10.288\n    - type: recall_at_5\n      value: 14.072000000000001\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sickr-sts\n      name: MTEB SICK-R\n      config: default\n      split: test\n      revision: a6ea5a8cab320b040a23452cc28066d9beae2cee\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.84209174935282\n    - type: cos_sim_spearman\n      value: 81.73248048438833\n    - type: euclidean_pearson\n      value: 83.02810070308149\n    - type: euclidean_spearman\n      value: 81.73248295679514\n    - type: manhattan_pearson\n      value: 82.95368060376002\n    - type: manhattan_spearman\n      value: 81.60277910998718\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts12-sts\n      name: MTEB STS12\n      config: default\n      split: test\n      revision: a0d554a64d88156834ff5ae9920b964011b16384\n    metrics:\n    - type: cos_sim_pearson\n      value: 88.52628804556943\n    - type: cos_sim_spearman\n      value: 82.5713913555672\n    - type: euclidean_pearson\n      value: 85.8796774746988\n    - type: euclidean_spearman\n      value: 82.57137506803424\n    - type: manhattan_pearson\n      value: 85.79671002960058\n    - type: manhattan_spearman\n      value: 82.49445981618027\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts13-sts\n      name: MTEB STS13\n      config: default\n      split: test\n      revision: 7e90230a92c190f1bf69ae9002b8cea547a64cca\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.23682503505542\n    - type: cos_sim_spearman\n      value: 87.15008956711806\n    - type: euclidean_pearson\n      value: 86.79805401524959\n    - type: euclidean_spearman\n      value: 87.15008956711806\n    - type: manhattan_pearson\n      value: 86.65298502699244\n    - type: manhattan_spearman\n      value: 86.97677821948562\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts14-sts\n      name: MTEB STS14\n      config: default\n      split: test\n      revision: 6031580fec1f6af667f0bd2da0a551cf4f0b2375\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.63370304677802\n    - type: cos_sim_spearman\n      value: 84.97105553540318\n    - type: euclidean_pearson\n      value: 85.28896108687721\n    - type: euclidean_spearman\n      value: 84.97105553540318\n    - type: manhattan_pearson\n      value: 85.09663190337331\n    - type: manhattan_spearman\n      value: 84.79126831644619\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts15-sts\n      name: MTEB STS15\n      config: default\n      split: test\n      revision: ae752c7c21bf194d8b67fd573edf7ae58183cbe3\n    metrics:\n    - type: cos_sim_pearson\n      value: 90.2614838800733\n    - type: cos_sim_spearman\n      value: 91.0509162991835\n    - type: euclidean_pearson\n      value: 90.33098317533373\n    - type: euclidean_spearman\n      value: 91.05091625871644\n    - type: manhattan_pearson\n      value: 90.26250435151107\n    - type: manhattan_spearman\n      value: 90.97999594417519\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts16-sts\n      name: MTEB STS16\n      config: default\n      split: test\n      revision: 4d8694f8f0e0100860b497b999b3dbed754a0513\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.80480973335091\n    - type: cos_sim_spearman\n      value: 87.313695492969\n    - type: euclidean_pearson\n      value: 86.49267251576939\n    - type: euclidean_spearman\n      value: 87.313695492969\n    - type: manhattan_pearson\n      value: 86.44019901831935\n    - type: manhattan_spearman\n      value: 87.24205395460392\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-en)\n      config: en-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 90.05662789380672\n    - type: cos_sim_spearman\n      value: 90.02759424426651\n    - type: euclidean_pearson\n      value: 90.4042483422981\n    - type: euclidean_spearman\n      value: 90.02759424426651\n    - type: manhattan_pearson\n      value: 90.51446975000226\n    - type: manhattan_spearman\n      value: 90.08832889933616\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (en)\n      config: en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 67.5975528273532\n    - type: cos_sim_spearman\n      value: 67.62969861411354\n    - type: euclidean_pearson\n      value: 69.224275734323\n    - type: euclidean_spearman\n      value: 67.62969861411354\n    - type: manhattan_pearson\n      value: 69.3761447059927\n    - type: manhattan_spearman\n      value: 67.90921005611467\n  - task:\n      type: STS\n    dataset:\n      type: mteb/stsbenchmark-sts\n      name: MTEB STSBenchmark\n      config: default\n      split: test\n      revision: b0fddb56ed78048fa8b90373c8a3cfc37b684831\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.11244327231684\n    - type: cos_sim_spearman\n      value: 88.37902438979035\n    - type: euclidean_pearson\n      value: 87.86054279847336\n    - type: euclidean_spearman\n      value: 88.37902438979035\n    - type: manhattan_pearson\n      value: 87.77257757320378\n    - type: manhattan_spearman\n      value: 88.25208966098123\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/scidocs-reranking\n      name: MTEB SciDocsRR\n      config: default\n      split: test\n      revision: d3c5e1fc0b855ab6097bf1cda04dd73947d7caab\n    metrics:\n    - type: map\n      value: 85.87174608143563\n    - type: mrr\n      value: 96.12836872640794\n  - task:\n      type: Retrieval\n    dataset:\n      type: scifact\n      name: MTEB SciFact\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 57.760999999999996\n    - type: map_at_10\n      value: 67.258\n    - type: map_at_100\n      value: 67.757\n    - type: map_at_1000\n      value: 67.78800000000001\n    - type: map_at_3\n      value: 64.602\n    - type: map_at_5\n      value: 65.64\n    - type: mrr_at_1\n      value: 60.667\n    - type: mrr_at_10\n      value: 68.441\n    - type: mrr_at_100\n      value: 68.825\n    - type: mrr_at_1000\n      value: 68.853\n    - type: mrr_at_3\n      value: 66.444\n    - type: mrr_at_5\n      value: 67.26100000000001\n    - type: ndcg_at_1\n      value: 60.667\n    - type: ndcg_at_10\n      value: 71.852\n    - type: ndcg_at_100\n      value: 73.9\n    - type: ndcg_at_1000\n      value: 74.628\n    - type: ndcg_at_3\n      value: 67.093\n    - type: ndcg_at_5\n      value: 68.58\n    - type: precision_at_1\n      value: 60.667\n    - type: precision_at_10\n      value: 9.6\n    - type: precision_at_100\n      value: 1.0670000000000002\n    - type: precision_at_1000\n      value: 0.11199999999999999\n    - type: precision_at_3\n      value: 26.111\n    - type: precision_at_5\n      value: 16.733\n    - type: recall_at_1\n      value: 57.760999999999996\n    - type: recall_at_10\n      value: 84.967\n    - type: recall_at_100\n      value: 93.833\n    - type: recall_at_1000\n      value: 99.333\n    - type: recall_at_3\n      value: 71.589\n    - type: recall_at_5\n      value: 75.483\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/sprintduplicatequestions-pairclassification\n      name: MTEB SprintDuplicateQuestions\n      config: default\n      split: test\n      revision: d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46\n    metrics:\n    - type: cos_sim_accuracy\n      value: 99.66633663366336\n    - type: cos_sim_ap\n      value: 91.17685358899108\n    - type: cos_sim_f1\n      value: 82.16818642350559\n    - type: cos_sim_precision\n      value: 83.26488706365504\n    - type: cos_sim_recall\n      value: 81.10000000000001\n    - type: dot_accuracy\n      value: 99.66633663366336\n    - type: dot_ap\n      value: 91.17663411119032\n    - type: dot_f1\n      value: 82.16818642350559\n    - type: dot_precision\n      value: 83.26488706365504\n    - type: dot_recall\n      value: 81.10000000000001\n    - type: euclidean_accuracy\n      value: 99.66633663366336\n    - type: euclidean_ap\n      value: 91.17685189882275\n    - type: euclidean_f1\n      value: 82.16818642350559\n    - type: euclidean_precision\n      value: 83.26488706365504\n    - type: euclidean_recall\n      value: 81.10000000000001\n    - type: manhattan_accuracy\n      value: 99.66633663366336\n    - type: manhattan_ap\n      value: 91.2241619496737\n    - type: manhattan_f1\n      value: 82.20472440944883\n    - type: manhattan_precision\n      value: 86.51933701657458\n    - type: manhattan_recall\n      value: 78.3\n    - type: max_accuracy\n      value: 99.66633663366336\n    - type: max_ap\n      value: 91.2241619496737\n    - type: max_f1\n      value: 82.20472440944883\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering\n      name: MTEB StackExchangeClustering\n      config: default\n      split: test\n      revision: 6cbc1f7b2bc0622f2e39d2c77fa502909748c259\n    metrics:\n    - type: v_measure\n      value: 66.85101268897951\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering-p2p\n      name: MTEB StackExchangeClusteringP2P\n      config: default\n      split: test\n      revision: 815ca46b2622cec33ccafc3735d572c266efdb44\n    metrics:\n    - type: v_measure\n      value: 42.461184054706905\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/stackoverflowdupquestions-reranking\n      name: MTEB StackOverflowDupQuestions\n      config: default\n      split: test\n      revision: e185fbe320c72810689fc5848eb6114e1ef5ec69\n    metrics:\n    - type: map\n      value: 51.44542568873886\n    - type: mrr\n      value: 52.33656151854681\n  - task:\n      type: Summarization\n    dataset:\n      type: mteb/summeval\n      name: MTEB SummEval\n      config: default\n      split: test\n      revision: cda12ad7615edc362dbf25a00fdd61d3b1eaf93c\n    metrics:\n    - type: cos_sim_pearson\n      value: 30.75982974997539\n    - type: cos_sim_spearman\n      value: 30.385405026539914\n    - type: dot_pearson\n      value: 30.75982433546523\n    - type: dot_spearman\n      value: 30.385405026539914\n  - task:\n      type: Retrieval\n    dataset:\n      type: trec-covid\n      name: MTEB TRECCOVID\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 0.22799999999999998\n    - type: map_at_10\n      value: 2.064\n    - type: map_at_100\n      value: 13.056000000000001\n    - type: map_at_1000\n      value: 31.747999999999998\n    - type: map_at_3\n      value: 0.67\n    - type: map_at_5\n      value: 1.097\n    - type: mrr_at_1\n      value: 90.0\n    - type: mrr_at_10\n      value: 94.667\n    - type: mrr_at_100\n      value: 94.667\n    - type: mrr_at_1000\n      value: 94.667\n    - type: mrr_at_3\n      value: 94.667\n    - type: mrr_at_5\n      value: 94.667\n    - type: ndcg_at_1\n      value: 86.0\n    - type: ndcg_at_10\n      value: 82.0\n    - type: ndcg_at_100\n      value: 64.307\n    - type: ndcg_at_1000\n      value: 57.023999999999994\n    - type: ndcg_at_3\n      value: 85.816\n    - type: ndcg_at_5\n      value: 84.904\n    - type: precision_at_1\n      value: 90.0\n    - type: precision_at_10\n      value: 85.8\n    - type: precision_at_100\n      value: 66.46\n    - type: precision_at_1000\n      value: 25.202\n    - type: precision_at_3\n      value: 90.0\n    - type: precision_at_5\n      value: 89.2\n    - type: recall_at_1\n      value: 0.22799999999999998\n    - type: recall_at_10\n      value: 2.235\n    - type: recall_at_100\n      value: 16.185\n    - type: recall_at_1000\n      value: 53.620999999999995\n    - type: recall_at_3\n      value: 0.7040000000000001\n    - type: recall_at_5\n      value: 1.172\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (sqi-eng)\n      config: sqi-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.39999999999999\n    - type: f1\n      value: 96.75\n    - type: precision\n      value: 96.45\n    - type: recall\n      value: 97.39999999999999\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (fry-eng)\n      config: fry-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 85.54913294797689\n    - type: f1\n      value: 82.46628131021194\n    - type: precision\n      value: 81.1175337186898\n    - type: recall\n      value: 85.54913294797689\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (kur-eng)\n      config: kur-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 81.21951219512195\n    - type: f1\n      value: 77.33333333333334\n    - type: precision\n      value: 75.54878048780488\n    - type: recall\n      value: 81.21951219512195\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (tur-eng)\n      config: tur-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 98.6\n    - type: f1\n      value: 98.26666666666665\n    - type: precision\n      value: 98.1\n    - type: recall\n      value: 98.6\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (deu-eng)\n      config: deu-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 99.5\n    - type: f1\n      value: 99.33333333333333\n    - type: precision\n      value: 99.25\n    - type: recall\n      value: 99.5\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (nld-eng)\n      config: nld-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.8\n    - type: f1\n      value: 97.2\n    - type: precision\n      value: 96.89999999999999\n    - type: recall\n      value: 97.8\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ron-eng)\n      config: ron-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.8\n    - type: f1\n      value: 97.18333333333334\n    - type: precision\n      value: 96.88333333333333\n    - type: recall\n      value: 97.8\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ang-eng)\n      config: ang-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 77.61194029850746\n    - type: f1\n      value: 72.81094527363183\n    - type: precision\n      value: 70.83333333333333\n    - type: recall\n      value: 77.61194029850746\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ido-eng)\n      config: ido-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 93.7\n    - type: f1\n      value: 91.91666666666667\n    - type: precision\n      value: 91.08333333333334\n    - type: recall\n      value: 93.7\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (jav-eng)\n      config: jav-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 88.29268292682927\n    - type: f1\n      value: 85.27642276422765\n    - type: precision\n      value: 84.01277584204414\n    - type: recall\n      value: 88.29268292682927\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (isl-eng)\n      config: isl-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 96.1\n    - type: f1\n      value: 95.0\n    - type: precision\n      value: 94.46666666666668\n    - type: recall\n      value: 96.1\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (slv-eng)\n      config: slv-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 93.681652490887\n    - type: f1\n      value: 91.90765492102065\n    - type: precision\n      value: 91.05913325232888\n    - type: recall\n      value: 93.681652490887\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (cym-eng)\n      config: cym-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 92.17391304347827\n    - type: f1\n      value: 89.97101449275361\n    - type: precision\n      value: 88.96811594202899\n    - type: recall\n      value: 92.17391304347827\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (kaz-eng)\n      config: kaz-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 90.43478260869566\n    - type: f1\n      value: 87.72173913043478\n    - type: precision\n      value: 86.42028985507245\n    - type: recall\n      value: 90.43478260869566\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (est-eng)\n      config: est-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 90.4\n    - type: f1\n      value: 88.03\n    - type: precision\n      value: 86.95\n    - type: recall\n      value: 90.4\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (heb-eng)\n      config: heb-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 93.4\n    - type: f1\n      value: 91.45666666666666\n    - type: precision\n      value: 90.525\n    - type: recall\n      value: 93.4\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (gla-eng)\n      config: gla-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 81.9059107358263\n    - type: f1\n      value: 78.32557872364869\n    - type: precision\n      value: 76.78260286824823\n    - type: recall\n      value: 81.9059107358263\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (mar-eng)\n      config: mar-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 94.3\n    - type: f1\n      value: 92.58333333333333\n    - type: precision\n      value: 91.73333333333332\n    - type: recall\n      value: 94.3\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (lat-eng)\n      config: lat-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 79.10000000000001\n    - type: f1\n      value: 74.50500000000001\n    - type: precision\n      value: 72.58928571428571\n    - type: recall\n      value: 79.10000000000001\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (bel-eng)\n      config: bel-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 96.6\n    - type: f1\n      value: 95.55\n    - type: precision\n      value: 95.05\n    - type: recall\n      value: 96.6\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (pms-eng)\n      config: pms-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 82.0952380952381\n    - type: f1\n      value: 77.98458049886621\n    - type: precision\n      value: 76.1968253968254\n    - type: recall\n      value: 82.0952380952381\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (gle-eng)\n      config: gle-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 87.9\n    - type: f1\n      value: 84.99190476190476\n    - type: precision\n      value: 83.65\n    - type: recall\n      value: 87.9\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (pes-eng)\n      config: pes-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 95.7\n    - type: f1\n      value: 94.56666666666666\n    - type: precision\n      value: 94.01666666666667\n    - type: recall\n      value: 95.7\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (nob-eng)\n      config: nob-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 98.6\n    - type: f1\n      value: 98.2\n    - type: precision\n      value: 98.0\n    - type: recall\n      value: 98.6\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (bul-eng)\n      config: bul-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 95.6\n    - type: f1\n      value: 94.38333333333334\n    - type: precision\n      value: 93.78333333333335\n    - type: recall\n      value: 95.6\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (cbk-eng)\n      config: cbk-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 87.4\n    - type: f1\n      value: 84.10380952380952\n    - type: precision\n      value: 82.67\n    - type: recall\n      value: 87.4\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (hun-eng)\n      config: hun-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 95.5\n    - type: f1\n      value: 94.33333333333334\n    - type: precision\n      value: 93.78333333333333\n    - type: recall\n      value: 95.5\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (uig-eng)\n      config: uig-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 89.4\n    - type: f1\n      value: 86.82000000000001\n    - type: precision\n      value: 85.64500000000001\n    - type: recall\n      value: 89.4\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (rus-eng)\n      config: rus-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 95.1\n    - type: f1\n      value: 93.56666666666668\n    - type: precision\n      value: 92.81666666666666\n    - type: recall\n      value: 95.1\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (spa-eng)\n      config: spa-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 98.9\n    - type: f1\n      value: 98.6\n    - type: precision\n      value: 98.45\n    - type: recall\n      value: 98.9\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (hye-eng)\n      config: hye-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 95.01347708894879\n    - type: f1\n      value: 93.51752021563343\n    - type: precision\n      value: 92.82794249775381\n    - type: recall\n      value: 95.01347708894879\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (tel-eng)\n      config: tel-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.00854700854701\n    - type: f1\n      value: 96.08262108262107\n    - type: precision\n      value: 95.65527065527067\n    - type: recall\n      value: 97.00854700854701\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (afr-eng)\n      config: afr-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 96.5\n    - type: f1\n      value: 95.39999999999999\n    - type: precision\n      value: 94.88333333333333\n    - type: recall\n      value: 96.5\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (mon-eng)\n      config: mon-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 96.5909090909091\n    - type: f1\n      value: 95.49242424242425\n    - type: precision\n      value: 94.9621212121212\n    - type: recall\n      value: 96.5909090909091\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (arz-eng)\n      config: arz-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 84.90566037735849\n    - type: f1\n      value: 81.85883997204752\n    - type: precision\n      value: 80.54507337526205\n    - type: recall\n      value: 84.90566037735849\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (hrv-eng)\n      config: hrv-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.5\n    - type: f1\n      value: 96.75\n    - type: precision\n      value: 96.38333333333333\n    - type: recall\n      value: 97.5\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (nov-eng)\n      config: nov-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 86.7704280155642\n    - type: f1\n      value: 82.99610894941635\n    - type: precision\n      value: 81.32295719844358\n    - type: recall\n      value: 86.7704280155642\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (gsw-eng)\n      config: gsw-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 67.52136752136752\n    - type: f1\n      value: 61.89662189662191\n    - type: precision\n      value: 59.68660968660969\n    - type: recall\n      value: 67.52136752136752\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (nds-eng)\n      config: nds-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 89.2\n    - type: f1\n      value: 86.32\n    - type: precision\n      value: 85.015\n    - type: recall\n      value: 89.2\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ukr-eng)\n      config: ukr-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 96.0\n    - type: f1\n      value: 94.78333333333333\n    - type: precision\n      value: 94.18333333333334\n    - type: recall\n      value: 96.0\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (uzb-eng)\n      config: uzb-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 83.8785046728972\n    - type: f1\n      value: 80.54517133956385\n    - type: precision\n      value: 79.154984423676\n    - type: recall\n      value: 83.8785046728972\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (lit-eng)\n      config: lit-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 93.60000000000001\n    - type: f1\n      value: 92.01333333333334\n    - type: precision\n      value: 91.28333333333333\n    - type: recall\n      value: 93.60000000000001\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ina-eng)\n      config: ina-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.1\n    - type: f1\n      value: 96.26666666666667\n    - type: precision\n      value: 95.85000000000001\n    - type: recall\n      value: 97.1\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (lfn-eng)\n      config: lfn-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 84.3\n    - type: f1\n      value: 80.67833333333333\n    - type: precision\n      value: 79.03928571428571\n    - type: recall\n      value: 84.3\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (zsm-eng)\n      config: zsm-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.3\n    - type: f1\n      value: 96.48333333333332\n    - type: precision\n      value: 96.08333333333331\n    - type: recall\n      value: 97.3\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ita-eng)\n      config: ita-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 95.7\n    - type: f1\n      value: 94.66666666666667\n    - type: precision\n      value: 94.16666666666667\n    - type: recall\n      value: 95.7\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (cmn-eng)\n      config: cmn-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.2\n    - type: f1\n      value: 96.36666666666667\n    - type: precision\n      value: 95.96666666666668\n    - type: recall\n      value: 97.2\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (lvs-eng)\n      config: lvs-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 94.3\n    - type: f1\n      value: 92.80666666666667\n    - type: precision\n      value: 92.12833333333333\n    - type: recall\n      value: 94.3\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (glg-eng)\n      config: glg-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.0\n    - type: f1\n      value: 96.22333333333334\n    - type: precision\n      value: 95.875\n    - type: recall\n      value: 97.0\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ceb-eng)', '{"pipeline_tag":"feature-extraction","library_name":"sentence-transformers","framework":"sentence-transformers","params":559890432,"storage_bytes":8885644361,"files_count":19,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["XLMRobertaModel"],"model_type":"xlm-roberta","tokenizer_config":{"bos_token":"<s>","cls_token":"<s>","eos_token":"</s>","mask_token":"<mask>","pad_token":"<pad>","sep_token":"</s>","unk_token":"<unk>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2402.05672","source_url":"https://arxiv.org/abs/2402.05672"},{"type":"based_on_paper","target_id":"arxiv:2401.00368","source_url":"https://arxiv.org/abs/2401.00368"},{"type":"based_on_paper","target_id":"arxiv:2104.08663","source_url":"https://arxiv.org/abs/2104.08663"},{"type":"based_on_paper","target_id":"arxiv:2210.07316","source_url":"https://arxiv.org/abs/2210.07316"}]', NULL, 'MIT', 'approved', 77.7, 'fa56d9053fb6495cc5b4aec326b0ef0f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-hkunlp-instructor-xl', 'huggingface--hkunlp--instructor-xl', 'instructor-xl', 'hkunlp', '--- pipeline_tag: sentence-similarity tags: - text-embedding - embeddings - information-retrieval - beir - text-classification - language-model - text-clustering - text-semantic-similarity - text-evaluation - prompt-retrieval - text-reranking - sentence-transformers - feature-extraction - sentence-similarity - transformers - t5 - English - Sentence Similarity - natural_questions - ms_marco - fever - hotpot_qa - mteb language: en inference: false license: apache-2.0 model-index: - name: final_...', '["sentence-transformers","pytorch","t5","text-embedding","embeddings","information-retrieval","beir","text-classification","language-model","text-clustering","text-semantic-similarity","text-evaluation","prompt-retrieval","text-reranking","feature-extraction","sentence-similarity","transformers","english","sentence similarity","natural_questions","ms_marco","fever","hotpot_qa","mteb","en","arxiv:2212.09741","license:apache-2.0","model-index","text-generation-inference","deploy:azure","region:us"]', 'sentence-similarity', 580, 461991, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/hkunlp/instructor-xl","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\npipeline_tag: sentence-similarity\ntags:\n- text-embedding\n- embeddings\n- information-retrieval\n- beir\n- text-classification\n- language-model\n- text-clustering\n- text-semantic-similarity\n- text-evaluation\n- prompt-retrieval\n- text-reranking\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\n- t5\n- English\n- Sentence Similarity\n- natural_questions\n- ms_marco\n- fever\n- hotpot_qa\n- mteb\nlanguage: en\ninference: false\nlicense: apache-2.0\nmodel-index:\n- name: final_xl_results\n  results:\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en)\n      config: en\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 85.08955223880596\n    - type: ap\n      value: 52.66066378722476\n    - type: f1\n      value: 79.63340218960269\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_polarity\n      name: MTEB AmazonPolarityClassification\n      config: default\n      split: test\n      revision: e2d317d38cd51312af73b3d32a06d1a08b442046\n    metrics:\n    - type: accuracy\n      value: 86.542\n    - type: ap\n      value: 81.92695193008987\n    - type: f1\n      value: 86.51466132573681\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (en)\n      config: en\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 42.964\n    - type: f1\n      value: 41.43146249774862\n  - task:\n      type: Retrieval\n    dataset:\n      type: arguana\n      name: MTEB ArguAna\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 29.872\n    - type: map_at_10\n      value: 46.342\n    - type: map_at_100\n      value: 47.152\n    - type: map_at_1000\n      value: 47.154\n    - type: map_at_3\n      value: 41.216\n    - type: map_at_5\n      value: 44.035999999999994\n    - type: mrr_at_1\n      value: 30.939\n    - type: mrr_at_10\n      value: 46.756\n    - type: mrr_at_100\n      value: 47.573\n    - type: mrr_at_1000\n      value: 47.575\n    - type: mrr_at_3\n      value: 41.548\n    - type: mrr_at_5\n      value: 44.425\n    - type: ndcg_at_1\n      value: 29.872\n    - type: ndcg_at_10\n      value: 55.65\n    - type: ndcg_at_100\n      value: 58.88099999999999\n    - type: ndcg_at_1000\n      value: 58.951\n    - type: ndcg_at_3\n      value: 45.0\n    - type: ndcg_at_5\n      value: 50.09\n    - type: precision_at_1\n      value: 29.872\n    - type: precision_at_10\n      value: 8.549\n    - type: precision_at_100\n      value: 0.991\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_3\n      value: 18.658\n    - type: precision_at_5\n      value: 13.669999999999998\n    - type: recall_at_1\n      value: 29.872\n    - type: recall_at_10\n      value: 85.491\n    - type: recall_at_100\n      value: 99.075\n    - type: recall_at_1000\n      value: 99.644\n    - type: recall_at_3\n      value: 55.974000000000004\n    - type: recall_at_5\n      value: 68.35\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-p2p\n      name: MTEB ArxivClusteringP2P\n      config: default\n      split: test\n      revision: a122ad7f3f0291bf49cc6f4d32aa80929df69d5d\n    metrics:\n    - type: v_measure\n      value: 42.452729850641276\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-s2s\n      name: MTEB ArxivClusteringS2S\n      config: default\n      split: test\n      revision: f910caf1a6075f7329cdf8c1a6135696f37dbd53\n    metrics:\n    - type: v_measure\n      value: 32.21141846480423\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/askubuntudupquestions-reranking\n      name: MTEB AskUbuntuDupQuestions\n      config: default\n      split: test\n      revision: 2000358ca161889fa9c082cb41daa8dcfb161a54\n    metrics:\n    - type: map\n      value: 65.34710928952622\n    - type: mrr\n      value: 77.61124301983028\n  - task:\n      type: STS\n    dataset:\n      type: mteb/biosses-sts\n      name: MTEB BIOSSES\n      config: default\n      split: test\n      revision: d3fb88f8f02e40887cd149695127462bbcf29b4a\n    metrics:\n    - type: cos_sim_spearman\n      value: 84.15312230525639\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/banking77\n      name: MTEB Banking77Classification\n      config: default\n      split: test\n      revision: 0fd18e25b25c072e09e0d92ab615fda904d66300\n    metrics:\n    - type: accuracy\n      value: 82.66233766233766\n    - type: f1\n      value: 82.04175284777669\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-p2p\n      name: MTEB BiorxivClusteringP2P\n      config: default\n      split: test\n      revision: 65b79d1d13f80053f67aca9498d9402c2d9f1f40\n    metrics:\n    - type: v_measure\n      value: 37.36697339826455\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-s2s\n      name: MTEB BiorxivClusteringS2S\n      config: default\n      split: test\n      revision: 258694dd0231531bc1fd9de6ceb52a0853c6d908\n    metrics:\n    - type: v_measure\n      value: 30.551241447593092\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackAndroidRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 36.797000000000004\n    - type: map_at_10\n      value: 48.46\n    - type: map_at_100\n      value: 49.968\n    - type: map_at_1000\n      value: 50.080000000000005\n    - type: map_at_3\n      value: 44.71\n    - type: map_at_5\n      value: 46.592\n    - type: mrr_at_1\n      value: 45.494\n    - type: mrr_at_10\n      value: 54.747\n    - type: mrr_at_100\n      value: 55.43599999999999\n    - type: mrr_at_1000\n      value: 55.464999999999996\n    - type: mrr_at_3\n      value: 52.361000000000004\n    - type: mrr_at_5\n      value: 53.727000000000004\n    - type: ndcg_at_1\n      value: 45.494\n    - type: ndcg_at_10\n      value: 54.989\n    - type: ndcg_at_100\n      value: 60.096000000000004\n    - type: ndcg_at_1000\n      value: 61.58\n    - type: ndcg_at_3\n      value: 49.977\n    - type: ndcg_at_5\n      value: 51.964999999999996\n    - type: precision_at_1\n      value: 45.494\n    - type: precision_at_10\n      value: 10.558\n    - type: precision_at_100\n      value: 1.6049999999999998\n    - type: precision_at_1000\n      value: 0.203\n    - type: precision_at_3\n      value: 23.796\n    - type: precision_at_5\n      value: 16.881\n    - type: recall_at_1\n      value: 36.797000000000004\n    - type: recall_at_10\n      value: 66.83\n    - type: recall_at_100\n      value: 88.34100000000001\n    - type: recall_at_1000\n      value: 97.202\n    - type: recall_at_3\n      value: 51.961999999999996\n    - type: recall_at_5\n      value: 57.940000000000005\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackEnglishRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 32.597\n    - type: map_at_10\n      value: 43.424\n    - type: map_at_100\n      value: 44.78\n    - type: map_at_1000\n      value: 44.913\n    - type: map_at_3\n      value: 40.315\n    - type: map_at_5\n      value: 41.987\n    - type: mrr_at_1\n      value: 40.382\n    - type: mrr_at_10\n      value: 49.219\n    - type: mrr_at_100\n      value: 49.895\n    - type: mrr_at_1000\n      value: 49.936\n    - type: mrr_at_3\n      value: 46.996\n    - type: mrr_at_5\n      value: 48.231\n    - type: ndcg_at_1\n      value: 40.382\n    - type: ndcg_at_10\n      value: 49.318\n    - type: ndcg_at_100\n      value: 53.839999999999996\n    - type: ndcg_at_1000\n      value: 55.82899999999999\n    - type: ndcg_at_3\n      value: 44.914\n    - type: ndcg_at_5\n      value: 46.798\n    - type: precision_at_1\n      value: 40.382\n    - type: precision_at_10\n      value: 9.274000000000001\n    - type: precision_at_100\n      value: 1.497\n    - type: precision_at_1000\n      value: 0.198\n    - type: precision_at_3\n      value: 21.592\n    - type: precision_at_5\n      value: 15.159\n    - type: recall_at_1\n      value: 32.597\n    - type: recall_at_10\n      value: 59.882000000000005\n    - type: recall_at_100\n      value: 78.446\n    - type: recall_at_1000\n      value: 90.88000000000001\n    - type: recall_at_3\n      value: 46.9\n    - type: recall_at_5\n      value: 52.222\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackGamingRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 43.8\n    - type: map_at_10\n      value: 57.293000000000006\n    - type: map_at_100\n      value: 58.321\n    - type: map_at_1000\n      value: 58.361\n    - type: map_at_3\n      value: 53.839999999999996\n    - type: map_at_5\n      value: 55.838\n    - type: mrr_at_1\n      value: 49.592000000000006\n    - type: mrr_at_10\n      value: 60.643\n    - type: mrr_at_100\n      value: 61.23499999999999\n    - type: mrr_at_1000\n      value: 61.251999999999995\n    - type: mrr_at_3\n      value: 58.265\n    - type: mrr_at_5\n      value: 59.717\n    - type: ndcg_at_1\n      value: 49.592000000000006\n    - type: ndcg_at_10\n      value: 63.364\n    - type: ndcg_at_100\n      value: 67.167\n    - type: ndcg_at_1000\n      value: 67.867\n    - type: ndcg_at_3\n      value: 57.912\n    - type: ndcg_at_5\n      value: 60.697\n    - type: precision_at_1\n      value: 49.592000000000006\n    - type: precision_at_10\n      value: 10.088\n    - type: precision_at_100\n      value: 1.2930000000000001\n    - type: precision_at_1000\n      value: 0.13899999999999998\n    - type: precision_at_3\n      value: 25.789\n    - type: precision_at_5\n      value: 17.541999999999998\n    - type: recall_at_1\n      value: 43.8\n    - type: recall_at_10\n      value: 77.635\n    - type: recall_at_100\n      value: 93.748\n    - type: recall_at_1000\n      value: 98.468\n    - type: recall_at_3\n      value: 63.223\n    - type: recall_at_5\n      value: 70.122\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackGisRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 27.721\n    - type: map_at_10\n      value: 35.626999999999995\n    - type: map_at_100\n      value: 36.719\n    - type: map_at_1000\n      value: 36.8\n    - type: map_at_3\n      value: 32.781\n    - type: map_at_5\n      value: 34.333999999999996\n    - type: mrr_at_1\n      value: 29.604999999999997\n    - type: mrr_at_10\n      value: 37.564\n    - type: mrr_at_100\n      value: 38.505\n    - type: mrr_at_1000\n      value: 38.565\n    - type: mrr_at_3\n      value: 34.727000000000004\n    - type: mrr_at_5\n      value: 36.207\n    - type: ndcg_at_1\n      value: 29.604999999999997\n    - type: ndcg_at_10\n      value: 40.575\n    - type: ndcg_at_100\n      value: 45.613\n    - type: ndcg_at_1000\n      value: 47.676\n    - type: ndcg_at_3\n      value: 34.811\n    - type: ndcg_at_5\n      value: 37.491\n    - type: precision_at_1\n      value: 29.604999999999997\n    - type: precision_at_10\n      value: 6.1690000000000005\n    - type: precision_at_100\n      value: 0.906\n    - type: precision_at_1000\n      value: 0.11199999999999999\n    - type: precision_at_3\n      value: 14.237\n    - type: precision_at_5\n      value: 10.056\n    - type: recall_at_1\n      value: 27.721\n    - type: recall_at_10\n      value: 54.041\n    - type: recall_at_100\n      value: 76.62299999999999\n    - type: recall_at_1000\n      value: 92.134\n    - type: recall_at_3\n      value: 38.582\n    - type: recall_at_5\n      value: 44.989000000000004\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackMathematicaRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 16.553\n    - type: map_at_10\n      value: 25.384\n    - type: map_at_100\n      value: 26.655\n    - type: map_at_1000\n      value: 26.778000000000002\n    - type: map_at_3\n      value: 22.733\n    - type: map_at_5\n      value: 24.119\n    - type: mrr_at_1\n      value: 20.149\n    - type: mrr_at_10\n      value: 29.705\n    - type: mrr_at_100\n      value: 30.672\n    - type: mrr_at_1000\n      value: 30.737\n    - type: mrr_at_3\n      value: 27.032\n    - type: mrr_at_5\n      value: 28.369\n    - type: ndcg_at_1\n      value: 20.149\n    - type: ndcg_at_10\n      value: 30.843999999999998\n    - type: ndcg_at_100\n      value: 36.716\n    - type: ndcg_at_1000\n      value: 39.495000000000005\n    - type: ndcg_at_3\n      value: 25.918999999999997\n    - type: ndcg_at_5\n      value: 27.992\n    - type: precision_at_1\n      value: 20.149\n    - type: precision_at_10\n      value: 5.858\n    - type: precision_at_100\n      value: 1.009\n    - type: precision_at_1000\n      value: 0.13799999999999998\n    - type: precision_at_3\n      value: 12.645000000000001\n    - type: precision_at_5\n      value: 9.179\n    - type: recall_at_1\n      value: 16.553\n    - type: recall_at_10\n      value: 43.136\n    - type: recall_at_100\n      value: 68.562\n    - type: recall_at_1000\n      value: 88.208\n    - type: recall_at_3\n      value: 29.493000000000002\n    - type: recall_at_5\n      value: 34.751\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackPhysicsRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 28.000999999999998\n    - type: map_at_10\n      value: 39.004\n    - type: map_at_100\n      value: 40.461999999999996\n    - type: map_at_1000\n      value: 40.566\n    - type: map_at_3\n      value: 35.805\n    - type: map_at_5\n      value: 37.672\n    - type: mrr_at_1\n      value: 33.782000000000004\n    - type: mrr_at_10\n      value: 44.702\n    - type: mrr_at_100\n      value: 45.528\n    - type: mrr_at_1000\n      value: 45.576\n    - type: mrr_at_3\n      value: 42.14\n    - type: mrr_at_5\n      value: 43.651\n    - type: ndcg_at_1\n      value: 33.782000000000004\n    - type: ndcg_at_10\n      value: 45.275999999999996\n    - type: ndcg_at_100\n      value: 50.888\n    - type: ndcg_at_1000\n      value: 52.879\n    - type: ndcg_at_3\n      value: 40.191\n    - type: ndcg_at_5\n      value: 42.731\n    - type: precision_at_1\n      value: 33.782000000000004\n    - type: precision_at_10\n      value: 8.200000000000001\n    - type: precision_at_100\n      value: 1.287\n    - type: precision_at_1000\n      value: 0.16199999999999998\n    - type: precision_at_3\n      value: 19.185\n    - type: precision_at_5\n      value: 13.667000000000002\n    - type: recall_at_1\n      value: 28.000999999999998\n    - type: recall_at_10\n      value: 58.131\n    - type: recall_at_100\n      value: 80.869\n    - type: recall_at_1000\n      value: 93.931\n    - type: recall_at_3\n      value: 44.161\n    - type: recall_at_5\n      value: 50.592000000000006\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackProgrammersRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 28.047\n    - type: map_at_10\n      value: 38.596000000000004\n    - type: map_at_100\n      value: 40.116\n    - type: map_at_1000\n      value: 40.232\n    - type: map_at_3\n      value: 35.205\n    - type: map_at_5\n      value: 37.076\n    - type: mrr_at_1\n      value: 34.932\n    - type: mrr_at_10\n      value: 44.496\n    - type: mrr_at_100\n      value: 45.47\n    - type: mrr_at_1000\n      value: 45.519999999999996\n    - type: mrr_at_3\n      value: 41.743\n    - type: mrr_at_5\n      value: 43.352000000000004\n    - type: ndcg_at_1\n      value: 34.932\n    - type: ndcg_at_10\n      value: 44.901\n    - type: ndcg_at_100\n      value: 50.788999999999994\n    - type: ndcg_at_1000\n      value: 52.867\n    - type: ndcg_at_3\n      value: 39.449\n    - type: ndcg_at_5\n      value: 41.929\n    - type: precision_at_1\n      value: 34.932\n    - type: precision_at_10\n      value: 8.311\n    - type: precision_at_100\n      value: 1.3050000000000002\n    - type: precision_at_1000\n      value: 0.166\n    - type: precision_at_3\n      value: 18.836\n    - type: precision_at_5\n      value: 13.447000000000001\n    - type: recall_at_1\n      value: 28.047\n    - type: recall_at_10\n      value: 57.717\n    - type: recall_at_100\n      value: 82.182\n    - type: recall_at_1000\n      value: 95.82000000000001\n    - type: recall_at_3\n      value: 42.448\n    - type: recall_at_5\n      value: 49.071\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 27.861250000000005\n    - type: map_at_10\n      value: 37.529583333333335\n    - type: map_at_100\n      value: 38.7915\n    - type: map_at_1000\n      value: 38.90558333333335\n    - type: map_at_3\n      value: 34.57333333333333\n    - type: map_at_5\n      value: 36.187166666666656\n    - type: mrr_at_1\n      value: 32.88291666666666\n    - type: mrr_at_10\n      value: 41.79750000000001\n    - type: mrr_at_100\n      value: 42.63183333333333\n    - type: mrr_at_1000\n      value: 42.68483333333333\n    - type: mrr_at_3\n      value: 39.313750000000006\n    - type: mrr_at_5\n      value: 40.70483333333333\n    - type: ndcg_at_1\n      value: 32.88291666666666\n    - type: ndcg_at_10\n      value: 43.09408333333333\n    - type: ndcg_at_100\n      value: 48.22158333333333\n    - type: ndcg_at_1000\n      value: 50.358000000000004\n    - type: ndcg_at_3\n      value: 38.129583333333336\n    - type: ndcg_at_5\n      value: 40.39266666666666\n    - type: precision_at_1\n      value: 32.88291666666666\n    - type: precision_at_10\n      value: 7.5584999999999996\n    - type: precision_at_100\n      value: 1.1903333333333332\n    - type: precision_at_1000\n      value: 0.15658333333333332\n    - type: precision_at_3\n      value: 17.495916666666666\n    - type: precision_at_5\n      value: 12.373833333333332\n    - type: recall_at_1\n      value: 27.861250000000005\n    - type: recall_at_10\n      value: 55.215916666666665\n    - type: recall_at_100\n      value: 77.392\n    - type: recall_at_1000\n      value: 92.04908333333334\n    - type: recall_at_3\n      value: 41.37475\n    - type: recall_at_5\n      value: 47.22908333333333\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackStatsRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 25.064999999999998\n    - type: map_at_10\n      value: 31.635999999999996\n    - type: map_at_100\n      value: 32.596000000000004\n    - type: map_at_1000\n      value: 32.695\n    - type: map_at_3\n      value: 29.612\n    - type: map_at_5\n      value: 30.768\n    - type: mrr_at_1\n      value: 28.528\n    - type: mrr_at_10\n      value: 34.717\n    - type: mrr_at_100\n      value: 35.558\n    - type: mrr_at_1000\n      value: 35.626000000000005\n    - type: mrr_at_3\n      value: 32.745000000000005\n    - type: mrr_at_5\n      value: 33.819\n    - type: ndcg_at_1\n      value: 28.528\n    - type: ndcg_at_10\n      value: 35.647\n    - type: ndcg_at_100\n      value: 40.207\n    - type: ndcg_at_1000\n      value: 42.695\n    - type: ndcg_at_3\n      value: 31.878\n    - type: ndcg_at_5\n      value: 33.634\n    - type: precision_at_1\n      value: 28.528\n    - type: precision_at_10\n      value: 5.46\n    - type: precision_at_100\n      value: 0.84\n    - type: precision_at_1000\n      value: 0.11399999999999999\n    - type: precision_at_3\n      value: 13.547999999999998\n    - type: precision_at_5\n      value: 9.325\n    - type: recall_at_1\n      value: 25.064999999999998\n    - type: recall_at_10\n      value: 45.096000000000004\n    - type: recall_at_100\n      value: 65.658\n    - type: recall_at_1000\n      value: 84.128\n    - type: recall_at_3\n      value: 34.337\n    - type: recall_at_5\n      value: 38.849000000000004\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackTexRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 17.276\n    - type: map_at_10\n      value: 24.535\n    - type: map_at_100\n      value: 25.655\n    - type: map_at_1000\n      value: 25.782\n    - type: map_at_3\n      value: 22.228\n    - type: map_at_5\n      value: 23.612\n    - type: mrr_at_1\n      value: 21.266\n    - type: mrr_at_10\n      value: 28.474\n    - type: mrr_at_100\n      value: 29.398000000000003\n    - type: mrr_at_1000\n      value: 29.482000000000003\n    - type: mrr_at_3\n      value: 26.245\n    - type: mrr_at_5\n      value: 27.624\n    - type: ndcg_at_1\n      value: 21.266\n    - type: ndcg_at_10\n      value: 29.087000000000003\n    - type: ndcg_at_100\n      value: 34.374\n    - type: ndcg_at_1000\n      value: 37.433\n    - type: ndcg_at_3\n      value: 25.040000000000003\n    - type: ndcg_at_5\n      value: 27.116\n    - type: precision_at_1\n      value: 21.266\n    - type: precision_at_10\n      value: 5.258\n    - type: precision_at_100\n      value: 0.9299999999999999\n    - type: precision_at_1000\n      value: 0.13699999999999998\n    - type: precision_at_3\n      value: 11.849\n    - type: precision_at_5\n      value: 8.699\n    - type: recall_at_1\n      value: 17.276\n    - type: recall_at_10\n      value: 38.928000000000004\n    - type: recall_at_100\n      value: 62.529\n    - type: recall_at_1000\n      value: 84.44800000000001\n    - type: recall_at_3\n      value: 27.554000000000002\n    - type: recall_at_5\n      value: 32.915\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackUnixRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 27.297\n    - type: map_at_10\n      value: 36.957\n    - type: map_at_100\n      value: 38.252\n    - type: map_at_1000\n      value: 38.356\n    - type: map_at_3\n      value: 34.121\n    - type: map_at_5\n      value: 35.782000000000004\n    - type: mrr_at_1\n      value: 32.275999999999996\n    - type: mrr_at_10\n      value: 41.198\n    - type: mrr_at_100\n      value: 42.131\n    - type: mrr_at_1000\n      value: 42.186\n    - type: mrr_at_3\n      value: 38.557\n    - type: mrr_at_5\n      value: 40.12\n    - type: ndcg_at_1\n      value: 32.275999999999996\n    - type: ndcg_at_10\n      value: 42.516\n    - type: ndcg_at_100\n      value: 48.15\n    - type: ndcg_at_1000\n      value: 50.344\n    - type: ndcg_at_3\n      value: 37.423\n    - type: ndcg_at_5\n      value: 39.919\n    - type: precision_at_1\n      value: 32.275999999999996\n    - type: precision_at_10\n      value: 7.155\n    - type: precision_at_100\n      value: 1.123\n    - type: precision_at_1000\n      value: 0.14200000000000002\n    - type: precision_at_3\n      value: 17.163999999999998\n    - type: precision_at_5\n      value: 12.127\n    - type: recall_at_1\n      value: 27.297\n    - type: recall_at_10\n      value: 55.238\n    - type: recall_at_100\n      value: 79.2\n    - type: recall_at_1000\n      value: 94.258\n    - type: recall_at_3\n      value: 41.327000000000005\n    - type: recall_at_5\n      value: 47.588\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackWebmastersRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 29.142000000000003\n    - type: map_at_10\n      value: 38.769\n    - type: map_at_100\n      value: 40.292\n    - type: map_at_1000\n      value: 40.510000000000005\n    - type: map_at_3\n      value: 35.39\n    - type: map_at_5\n      value: 37.009\n    - type: mrr_at_1\n      value: 34.19\n    - type: mrr_at_10\n      value: 43.418\n    - type: mrr_at_100\n      value: 44.132\n    - type: mrr_at_1000\n      value: 44.175\n    - type: mrr_at_3\n      value: 40.547\n    - type: mrr_at_5\n      value: 42.088\n    - type: ndcg_at_1\n      value: 34.19\n    - type: ndcg_at_10\n      value: 45.14\n    - type: ndcg_at_100\n      value: 50.364\n    - type: ndcg_at_1000\n      value: 52.481\n    - type: ndcg_at_3\n      value: 39.466\n    - type: ndcg_at_5\n      value: 41.772\n    - type: precision_at_1\n      value: 34.19\n    - type: precision_at_10\n      value: 8.715\n    - type: precision_at_100\n      value: 1.6150000000000002\n    - type: precision_at_1000\n      value: 0.247\n    - type: precision_at_3\n      value: 18.248\n    - type: precision_at_5\n      value: 13.161999999999999\n    - type: recall_at_1\n      value: 29.142000000000003\n    - type: recall_at_10\n      value: 57.577999999999996\n    - type: recall_at_100\n      value: 81.428\n    - type: recall_at_1000\n      value: 94.017\n    - type: recall_at_3\n      value: 41.402\n    - type: recall_at_5\n      value: 47.695\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackWordpressRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 22.039\n    - type: map_at_10\n      value: 30.669999999999998\n    - type: map_at_100\n      value: 31.682\n    - type: map_at_1000\n      value: 31.794\n    - type: map_at_3\n      value: 28.139999999999997\n    - type: map_at_5\n      value: 29.457\n    - type: mrr_at_1\n      value: 24.399\n    - type: mrr_at_10\n      value: 32.687\n    - type: mrr_at_100\n      value: 33.622\n    - type: mrr_at_1000\n      value: 33.698\n    - type: mrr_at_3\n      value: 30.407\n    - type: mrr_at_5\n      value: 31.552999999999997\n    - type: ndcg_at_1\n      value: 24.399\n    - type: ndcg_at_10\n      value: 35.472\n    - type: ndcg_at_100\n      value: 40.455000000000005\n    - type: ndcg_at_1000\n      value: 43.15\n    - type: ndcg_at_3\n      value: 30.575000000000003\n    - type: ndcg_at_5\n      value: 32.668\n    - type: precision_at_1\n      value: 24.399\n    - type: precision_at_10\n      value: 5.656\n    - type: precision_at_100\n      value: 0.874\n    - type: precision_at_1000\n      value: 0.121\n    - type: precision_at_3\n      value: 13.062000000000001\n    - type: precision_at_5\n      value: 9.242\n    - type: recall_at_1\n      value: 22.039\n    - type: recall_at_10\n      value: 48.379\n    - type: recall_at_100\n      value: 71.11800000000001\n    - type: recall_at_1000\n      value: 91.095\n    - type: recall_at_3\n      value: 35.108\n    - type: recall_at_5\n      value: 40.015\n  - task:\n      type: Retrieval\n    dataset:\n      type: climate-fever\n      name: MTEB ClimateFEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 10.144\n    - type: map_at_10\n      value: 18.238\n    - type: map_at_100\n      value: 20.143\n    - type: map_at_1000\n      value: 20.346\n    - type: map_at_3\n      value: 14.809\n    - type: map_at_5\n      value: 16.567999999999998\n    - type: mrr_at_1\n      value: 22.671\n    - type: mrr_at_10\n      value: 34.906\n    - type: mrr_at_100\n      value: 35.858000000000004\n    - type: mrr_at_1000\n      value: 35.898\n    - type: mrr_at_3\n      value: 31.238\n    - type: mrr_at_5\n      value: 33.342\n    - type: ndcg_at_1\n      value: 22.671\n    - type: ndcg_at_10\n      value: 26.540000000000003\n    - type: ndcg_at_100\n      value: 34.138000000000005\n    - type: ndcg_at_1000\n      value: 37.72\n    - type: ndcg_at_3\n      value: 20.766000000000002\n    - type: ndcg_at_5\n      value: 22.927\n    - type: precision_at_1\n      value: 22.671\n    - type: precision_at_10\n      value: 8.619\n    - type: precision_at_100\n      value: 1.678\n    - type: precision_at_1000\n      value: 0.23500000000000001\n    - type: precision_at_3\n      value: 15.592\n    - type: precision_at_5\n      value: 12.43\n    - type: recall_at_1\n      value: 10.144\n    - type: recall_at_10\n      value: 33.46\n    - type: recall_at_100\n      value: 59.758\n    - type: recall_at_1000\n      value: 79.704\n    - type: recall_at_3\n      value: 19.604\n    - type: recall_at_5\n      value: 25.367\n  - task:\n      type: Retrieval\n    dataset:\n      type: dbpedia-entity\n      name: MTEB DBPedia\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 8.654\n    - type: map_at_10\n      value: 18.506\n    - type: map_at_100\n      value: 26.412999999999997\n    - type: map_at_1000\n      value: 28.13\n    - type: map_at_3\n      value: 13.379\n    - type: map_at_5\n      value: 15.529000000000002\n    - type: mrr_at_1\n      value: 66.0\n    - type: mrr_at_10\n      value: 74.13\n    - type: mrr_at_100\n      value: 74.48700000000001\n    - type: mrr_at_1000\n      value: 74.49799999999999\n    - type: mrr_at_3\n      value: 72.75\n    - type: mrr_at_5\n      value: 73.762\n    - type: ndcg_at_1\n      value: 54.50000000000001\n    - type: ndcg_at_10\n      value: 40.236\n    - type: ndcg_at_100\n      value: 44.690999999999995\n    - type: ndcg_at_1000\n      value: 52.195\n    - type: ndcg_at_3\n      value: 45.632\n    - type: ndcg_at_5\n      value: 42.952\n    - type: precision_at_1\n      value: 66.0\n    - type: precision_at_10\n      value: 31.724999999999998\n    - type: precision_at_100\n      value: 10.299999999999999\n    - type: precision_at_1000\n      value: 2.194\n    - type: precision_at_3\n      value: 48.75\n    - type: precision_at_5\n      value: 41.6\n    - type: recall_at_1\n      value: 8.654\n    - type: recall_at_10\n      value: 23.74\n    - type: recall_at_100\n      value: 50.346999999999994\n    - type: recall_at_1000\n      value: 74.376\n    - type: recall_at_3\n      value: 14.636\n    - type: recall_at_5\n      value: 18.009\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/emotion\n      name: MTEB EmotionClassification\n      config: default\n      split: test\n      revision: 4f58c6b202a23cf9a4da393831edf4f9183cad37\n    metrics:\n    - type: accuracy\n      value: 53.245\n    - type: f1\n      value: 48.74520523753552\n  - task:\n      type: Retrieval\n    dataset:\n      type: fever\n      name: MTEB FEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 51.729\n    - type: map_at_10\n      value: 63.904\n    - type: map_at_100\n      value: 64.363\n    - type: map_at_1000\n      value: 64.38199999999999\n    - type: map_at_3\n      value: 61.393\n    - type: map_at_5\n      value: 63.02100000000001\n    - type: mrr_at_1\n      value: 55.686\n    - type: mrr_at_10\n      value: 67.804\n    - type: mrr_at_100\n      value: 68.15299999999999\n    - type: mrr_at_1000\n      value: 68.161\n    - type: mrr_at_3\n      value: 65.494\n    - type: mrr_at_5\n      value: 67.01599999999999\n    - type: ndcg_at_1\n      value: 55.686\n    - type: ndcg_at_10\n      value: 70.025\n    - type: ndcg_at_100\n      value: 72.011\n    - type: ndcg_at_1000\n      value: 72.443\n    - type: ndcg_at_3\n      value: 65.32900000000001\n    - type: ndcg_at_5\n      value: 68.05600000000001\n    - type: precision_at_1\n      value: 55.686\n    - type: precision_at_10\n      value: 9.358\n    - type: precision_at_100\n      value: 1.05\n    - type: precision_at_1000\n      value: 0.11\n    - type: precision_at_3\n      value: 26.318\n    - type: precision_at_5\n      value: 17.321\n    - type: recall_at_1\n      value: 51.729\n    - type: recall_at_10\n      value: 85.04\n    - type: recall_at_100\n      value: 93.777\n    - type: recall_at_1000\n      value: 96.824\n    - type: recall_at_3\n      value: 72.521\n    - type: recall_at_5\n      value: 79.148\n  - task:\n      type: Retrieval\n    dataset:\n      type: fiqa\n      name: MTEB FiQA2018\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 23.765\n    - type: map_at_10\n      value: 39.114\n    - type: map_at_100\n      value: 40.987\n    - type: map_at_1000\n      value: 41.155\n    - type: map_at_3\n      value: 34.028000000000006\n    - type: map_at_5\n      value: 36.925000000000004\n    - type: mrr_at_1\n      value: 46.451\n    - type: mrr_at_10\n      value: 54.711\n    - type: mrr_at_100\n      value: 55.509\n    - type: mrr_at_1000\n      value: 55.535000000000004\n    - type: mrr_at_3\n      value: 52.649\n    - type: mrr_at_5\n      value: 53.729000000000006\n    - type: ndcg_at_1\n      value: 46.451\n    - type: ndcg_at_10\n      value: 46.955999999999996\n    - type: ndcg_at_100\n      value: 53.686\n    - type: ndcg_at_1000\n      value: 56.230000000000004\n    - type: ndcg_at_3\n      value: 43.374\n    - type: ndcg_at_5\n      value: 44.372\n    - type: precision_at_1\n      value: 46.451\n    - type: precision_at_10\n      value: 13.256\n    - type: precision_at_100\n      value: 2.019\n    - type: precision_at_1000\n      value: 0.247\n    - type: precision_at_3\n      value: 29.115000000000002\n    - type: precision_at_5\n      value: 21.389\n    - type: recall_at_1\n      value: 23.765\n    - type: recall_at_10\n      value: 53.452999999999996\n    - type: recall_at_100\n      value: 78.828\n    - type: recall_at_1000\n      value: 93.938\n    - type: recall_at_3\n      value: 39.023\n    - type: recall_at_5\n      value: 45.18\n  - task:\n      type: Retrieval\n    dataset:\n      type: hotpotqa\n      name: MTEB HotpotQA\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 31.918000000000003\n    - type: map_at_10\n      value: 46.741\n    - type: map_at_100\n      value: 47.762\n    - type: map_at_1000\n      value: 47.849000000000004\n    - type: map_at_3\n      value: 43.578\n    - type: map_at_5\n      value: 45.395\n    - type: mrr_at_1\n      value: 63.834999999999994\n    - type: mrr_at_10\n      value: 71.312\n    - type: mrr_at_100\n      value: 71.695\n    - type: mrr_at_1000\n      value: 71.714\n    - type: mrr_at_3\n      value: 69.82000000000001\n    - type: mrr_at_5\n      value: 70.726\n    - type: ndcg_at_1\n      value: 63.834999999999994\n    - type: ndcg_at_10\n      value: 55.879999999999995\n    - type: ndcg_at_100\n      value: 59.723000000000006\n    - type: ndcg_at_1000\n      value: 61.49400000000001\n    - type: ndcg_at_3\n      value: 50.964\n    - type: ndcg_at_5\n      value: 53.47\n    - type: precision_at_1\n      value: 63.834999999999994\n    - type: precision_at_10\n      value: 11.845\n    - type: precision_at_100\n      value: 1.4869999999999999\n    - type: precision_at_1000\n      value: 0.172\n    - type: precision_at_3\n      value: 32.158\n    - type: precision_at_5\n      value: 21.278\n    - type: recall_at_1\n      value: 31.918000000000003\n    - type: recall_at_10\n      value: 59.223000000000006\n    - type: recall_at_100\n      value: 74.328\n    - type: recall_at_1000\n      value: 86.05000000000001\n    - type: recall_at_3\n      value: 48.238\n    - type: recall_at_5\n      value: 53.193999999999996\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/imdb\n      name: MTEB ImdbClassification\n      config: default\n      split: test\n      revision: 3d86128a09e091d6018b6d26cad27f2739fc2db7\n    metrics:\n    - type: accuracy\n      value: 79.7896\n    - type: ap\n      value: 73.65166029460288\n    - type: f1\n      value: 79.71794693711813\n  - task:\n      type: Retrieval\n    dataset:\n      type: msmarco\n      name: MTEB MSMARCO\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 22.239\n    - type: map_at_10\n      value: 34.542\n    - type: map_at_100\n      value: 35.717999999999996\n    - type: map_at_1000\n      value: 35.764\n    - type: map_at_3\n      value: 30.432\n    - type: map_at_5\n      value: 32.81\n    - type: mrr_at_1\n      value: 22.908\n    - type: mrr_at_10\n      value: 35.127\n    - type: mrr_at_100\n      value: 36.238\n    - type: mrr_at_1000\n      value: 36.278\n    - type: mrr_at_3\n      value: 31.076999999999998\n    - type: mrr_at_5\n      value: 33.419\n    - type: ndcg_at_1\n      value: 22.908\n    - type: ndcg_at_10\n      value: 41.607\n    - type: ndcg_at_100\n      value: 47.28\n    - type: ndcg_at_1000\n      value: 48.414\n    - type: ndcg_at_3\n      value: 33.253\n    - type: ndcg_at_5\n      value: 37.486000000000004\n    - type: precision_at_1\n      value: 22.908\n    - type: precision_at_10\n      value: 6.645\n    - type: precision_at_100\n      value: 0.9490000000000001\n    - type: precision_at_1000\n      value: 0.105\n    - type: precision_at_3\n      value: 14.130999999999998\n    - type: precision_at_5\n      value: 10.616\n    - type: recall_at_1\n      value: 22.239\n    - type: recall_at_10\n      value: 63.42\n    - type: recall_at_100\n      value: 89.696\n    - type: recall_at_1000\n      value: 98.351\n    - type: recall_at_3\n      value: 40.77\n    - type: recall_at_5\n      value: 50.93\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (en)\n      config: en\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 95.06839945280439\n    - type: f1\n      value: 94.74276398224072\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (en)\n      config: en\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 72.25718194254446\n    - type: f1\n      value: 53.91164489161391\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (en)\n      config: en\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.47948890383323\n    - type: f1\n      value: 69.98520247230257\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (en)\n      config: en\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.46603900470748\n    - type: f1\n      value: 76.44111526065399\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-p2p\n      name: MTEB MedrxivClusteringP2P\n      config: default\n      split: test\n      revision: e7a26af6f3ae46b30dde8737f02c07b1505bcc73\n    metrics:\n    - type: v_measure\n      value: 33.19106070798198\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-s2s\n      name: MTEB MedrxivClusteringS2S\n      config: default\n      split: test\n      revision: 35191c8c0dca72d8ff3efcd72aa802307d469663\n    metrics:\n    - type: v_measure\n      value: 30.78772205248094\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/mind_small\n      name: MTEB MindSmallReranking\n      config: default\n      split: test\n      revision: 3bdac13927fdc888b903db93b2ffdbd90b295a69\n    metrics:\n    - type: map\n      value: 31.811231631488507\n    - type: mrr\n      value: 32.98200485378021\n  - task:\n      type: Retrieval\n    dataset:\n      type: nfcorpus\n      name: MTEB NFCorpus\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 6.9\n    - type: map_at_10\n      value: 13.703000000000001\n    - type: map_at_100\n      value: 17.251\n    - type: map_at_1000\n      value: 18.795\n    - type: map_at_3\n      value: 10.366999999999999\n    - type: map_at_5\n      value: 11.675\n    - type: mrr_at_1\n      value: 47.059\n    - type: mrr_at_10\n      value: 55.816\n    - type: mrr_at_100\n      value: 56.434\n    - type: mrr_at_1000\n      value: 56.467\n    - type: mrr_at_3\n      value: 53.973000000000006\n    - type: mrr_at_5\n      value: 55.257999999999996\n    - type: ndcg_at_1\n      value: 44.737\n    - type: ndcg_at_10\n      value: 35.997\n    - type: ndcg_at_100\n      value: 33.487\n    - type: ndcg_at_1000\n      value: 41.897\n    - type: ndcg_at_3\n      value: 41.18\n    - type: ndcg_at_5\n      value: 38.721\n    - type: precision_at_1\n      value: 46.129999999999995\n    - type: precision_at_10\n      value: 26.533\n    - type: precision_at_100\n      value: 8.706\n    - type: precision_at_1000\n      value: 2.16\n    - type: precision_at_3\n      value: 38.493\n    - type: precision_at_5\n      value: 33.189\n    - type: recall_at_1\n      value: 6.9\n    - type: recall_at_10\n      value: 17.488999999999997\n    - type: recall_at_100\n      value: 34.583000000000006\n    - type: recall_at_1000\n      value: 64.942\n    - type: recall_at_3\n      value: 11.494\n    - type: recall_at_5\n      value: 13.496\n  - task:\n      type: Retrieval\n    dataset:\n      type: nq\n      name: MTEB NQ\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 33.028999999999996\n    - type: map_at_10\n      value: 49.307\n    - type: map_at_100\n      value: 50.205\n    - type: map_at_1000\n      value: 50.23\n    - type: map_at_3\n      value: 44.782\n    - type: map_at_5\n      value: 47.599999999999994\n    - type: mrr_at_1\n      value: 37.108999999999995\n    - type: mrr_at_10\n      value: 51.742999999999995\n    - type: mrr_at_100\n      value: 52.405\n    - type: mrr_at_1000\n      value: 52.422000000000004\n    - type: mrr_at_3\n      value: 48.087999999999994\n    - type: mrr_at_5\n      value: 50.414\n    - type: ndcg_at_1\n      value: 37.08\n    - type: ndcg_at_10\n      value: 57.236\n    - type: ndcg_at_100\n      value: 60.931999999999995\n    - type: ndcg_at_1000\n      value: 61.522\n    - type: ndcg_at_3\n      value: 48.93\n    - type: ndcg_at_5\n      value: 53.561\n    - type: precision_at_1\n      value: 37.08\n    - type: precision_at_10\n      value: 9.386\n    - type: precision_at_100\n      value: 1.1480000000000001\n    - type: precision_at_1000\n      value: 0.12\n    - type: precision_at_3\n      value: 22.258\n    - type: precision_at_5\n      value: 16.025\n    - type: recall_at_1\n      value: 33.028999999999996\n    - type: recall_at_10\n      value: 78.805\n    - type: recall_at_100\n      value: 94.643\n    - type: recall_at_1000\n      value: 99.039\n    - type: recall_at_3\n      value: 57.602\n    - type: recall_at_5\n      value: 68.253\n  - task:\n      type: Retrieval\n    dataset:\n      type: quora\n      name: MTEB QuoraRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 71.122\n    - type: map_at_10\n      value: 85.237\n    - type: map_at_100\n      value: 85.872\n    - type: map_at_1000\n      value: 85.885\n    - type: map_at_3\n      value: 82.27499999999999\n    - type: map_at_5\n      value: 84.13199999999999\n    - type: mrr_at_1\n      value: 81.73\n    - type: mrr_at_10\n      value: 87.834\n    - type: mrr_at_100\n      value: 87.92\n    - type: mrr_at_1000\n      value: 87.921\n    - type: mrr_at_3\n      value: 86.878\n    - type: mrr_at_5\n      value: 87.512\n    - type: ndcg_at_1\n      value: 81.73\n    - type: ndcg_at_10\n      value: 88.85499999999999\n    - type: ndcg_at_100\n      value: 89.992\n    - type: ndcg_at_1000\n      value: 90.07\n    - type: ndcg_at_3\n      value: 85.997\n    - type: ndcg_at_5\n      value: 87.55199999999999\n    - type: precision_at_1\n      value: 81.73\n    - type: precision_at_10\n      value: 13.491\n    - type: precision_at_100\n      value: 1.536\n    - type: precision_at_1000\n      value: 0.157\n    - type: precision_at_3\n      value: 37.623\n    - type: precision_at_5\n      value: 24.742\n    - type: recall_at_1\n      value: 71.122\n    - type: recall_at_10\n      value: 95.935\n    - type: recall_at_100\n      value: 99.657\n    - type: recall_at_1000\n      value: 99.996\n    - type: recall_at_3\n      value: 87.80799999999999\n    - type: recall_at_5\n      value: 92.161\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering\n      name: MTEB RedditClustering\n      config: default\n      split: test\n      revision: 24640382cdbf8abc73003fb0fa6d111a705499eb\n    metrics:\n    - type: v_measure\n      value: 63.490029238193756\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering-p2p\n      name: MTEB RedditClusteringP2P\n      config: default\n      split: test\n      revision: 282350215ef01743dc01b456c7f5241fa8937f16\n    metrics:\n    - type: v_measure\n      value: 65.13153408508836\n  - task:\n      type: Retrieval\n    dataset:\n      type: scidocs\n      name: MTEB SCIDOCS\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 4.202999999999999\n    - type: map_at_10\n      value: 10.174\n    - type: map_at_100\n      value: 12.138\n    - type: map_at_1000\n      value: 12.418\n    - type: map_at_3\n      value: 7.379\n    - type: map_at_5\n      value: 8.727\n    - type: mrr_at_1\n      value: 20.7\n    - type: mrr_at_10\n      value: 30.389\n    - type: mrr_at_100\n      value: 31.566\n    - type: mrr_at_1000\n      value: 31.637999999999998\n    - type: mrr_at_3\n      value: 27.133000000000003\n    - type: mrr_at_5\n      value: 29.078\n    - type: ndcg_at_1\n      value: 20.7\n    - type: ndcg_at_10\n      value: 17.355999999999998\n    - type: ndcg_at_100\n      value: 25.151\n    - type: ndcg_at_1000\n      value: 30.37\n    - type: ndcg_at_3\n      value: 16.528000000000002\n    - type: ndcg_at_5\n      value: 14.396999999999998\n    - type: precision_at_1\n      value: 20.7\n    - type: precision_at_10\n      value: 8.98\n    - type: precision_at_100\n      value: 2.015\n    - type: precision_at_1000\n      value: 0.327\n    - type: precision_at_3\n      value: 15.367\n    - type: precision_at_5\n      value: 12.559999999999999\n    - type: recall_at_1\n      value: 4.202999999999999\n    - type: recall_at_10\n      value: 18.197\n    - type: recall_at_100\n      value: 40.903\n    - type: recall_at_1000\n      value: 66.427\n    - type: recall_at_3\n      value: 9.362\n    - type: recall_at_5\n      value: 12.747\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sickr-sts\n      name: MTEB SICK-R\n      config: default\n      split: test\n      revision: a6ea5a8cab320b040a23452cc28066d9beae2cee\n    metrics:\n    - type: cos_sim_spearman\n      value: 81.69890989765257\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts12-sts\n      name: MTEB STS12\n      config: default\n      split: test\n      revision: a0d554a64d88156834ff5ae9920b964011b16384\n    metrics:\n    - type: cos_sim_spearman\n      value: 75.31953790551489\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts13-sts\n      name: MTEB STS13\n      config: default\n      split: test\n      revision: 7e90230a92c190f1bf69ae9002b8cea547a64cca\n    metrics:\n    - type: cos_sim_spearman\n      value: 87.44050861280759\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts14-sts\n      name: MTEB STS14\n      config: default\n      split: test\n      revision: 6031580fec1f6af667f0bd2da0a551cf4f0b2375\n    metrics:\n    - type: cos_sim_spearman\n      value: 81.86922869270393\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts15-sts\n      name: MTEB STS15\n      config: default\n      split: test\n      revision: ae752c7c21bf194d8b67fd573edf7ae58183cbe3\n    metrics:\n    - type: cos_sim_spearman\n      value: 88.9399170304284\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts16-sts\n      name: MTEB STS16\n      config: default\n      split: test\n      revision: 4d8694f8f0e0100860b497b999b3dbed754a0513\n    metrics:\n    - type: cos_sim_spearman\n      value: 85.38015314088582\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-en)\n      config: en-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_spearman\n      value: 90.53653527788835\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (en)\n      config: en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_spearman\n      value: 68.64526474250209\n  - task:\n      type: STS\n    dataset:\n      type: mteb/stsbenchmark-sts\n      name: MTEB STSBenchmark\n      config: default\n      split: test\n      revision: b0fddb56ed78048fa8b90373c8a3cfc37b684831\n    metrics:\n    - type: cos_sim_spearman\n      value: 86.56156983963042\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/scidocs-reranking\n      name: MTEB SciDocsRR\n      config: default\n      split: test\n      revision: d3c5e1fc0b855ab6097bf1cda04dd73947d7caab\n    metrics:\n    - type: map\n      value: 79.48610254648003\n    - type: mrr\n      value: 94.02481505422682\n  - task:\n      type: Retrieval\n    dataset:\n      type: scifact\n      name: MTEB SciFact\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 48.983\n    - type: map_at_10\n      value: 59.077999999999996\n    - type: map_at_100\n      value: 59.536\n    - type: map_at_1000\n      value: 59.575\n    - type: map_at_3\n      value: 55.691\n    - type: map_at_5\n      value: 57.410000000000004\n    - type: mrr_at_1\n      value: 51.666999999999994\n    - type: mrr_at_10\n      value: 60.427\n    - type: mrr_at_100\n      value: 60.763\n    - type: mrr_at_1000\n      value: 60.79900000000001\n    - type: mrr_at_3\n      value: 57.556\n    - type: mrr_at_5\n      value: 59.089000000000006\n    - type: ndcg_at_1\n      value: 51.666999999999994\n    - type: ndcg_at_10\n      value: 64.559\n    - type: ndcg_at_100\n      value: 66.58\n    - type: ndcg_at_1000\n      value: 67.64\n    - type: ndcg_at_3\n      value: 58.287\n    - type: ndcg_at_5\n      value: 61.001000000000005\n    - type: precision_at_1\n      value: 51.666999999999994\n    - type: precision_at_10\n      value: 9.067\n    - type: precision_at_100\n      value: 1.0170000000000001\n    - type: precision_at_1000\n      value: 0.11100000000000002\n    - type: precision_at_3\n      value: 23.0\n    - type: precision_at_5\n      value: 15.6\n    - type: recall_at_1\n      value: 48.983\n    - type: recall_at_10\n      value: 80.289\n    - type: recall_at_100\n      value: 89.43299999999999\n    - type: recall_at_1000\n      value: 97.667\n    - type: recall_at_3\n      value: 62.978\n    - type: recall_at_5\n      value: 69.872\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/sprintduplicatequestions-pairclassification\n      name: MTEB SprintDuplicateQuestions\n      config: default\n      split: test\n      revision: d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46\n    metrics:\n    - type: cos_sim_accuracy\n      value: 99.79009900990098\n    - type: cos_sim_ap\n      value: 94.94115052608419\n    - type: cos_sim_f1\n      value: 89.1260162601626\n    - type: cos_sim_precision\n      value: 90.599173553719\n    - type: cos_sim_recall\n      value: 87.7\n    - type: dot_accuracy\n      value: 99.79009900990098\n    - type: dot_ap\n      value: 94.94115052608419\n    - type: dot_f1\n      value: 89.1260162601626\n    - type: dot_precision\n      value: 90.599173553719\n    - type: dot_recall\n      value: 87.7\n    - type: euclidean_accuracy\n      value: 99.79009900990098\n    - type: euclidean_ap\n      value: 94.94115052608419\n    - type: euclidean_f1\n      value: 89.1260162601626\n    - type: euclidean_precision\n      value: 90.599173553719\n    - type: euclidean_recall\n      value: 87.7\n    - type: manhattan_accuracy\n      value: 99.7940594059406\n    - type: manhattan_ap\n      value: 94.95271414642431\n    - type: manhattan_f1\n      value: 89.24508790072387\n    - type: manhattan_precision\n      value: 92.3982869379015\n    - type: manhattan_recall\n      value: 86.3\n    - type: max_accuracy\n      value: 99.7940594059406\n    - type: max_ap\n      value: 94.95271414642431\n    - type: max_f1\n      value: 89.24508790072387\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering\n      name: MTEB StackExchangeClustering\n      config: default\n      split: test\n      revision: 6cbc1f7b2bc0622f2e39d2c77fa502909748c259\n    metrics:\n    - type: v_measure\n      value: 68.43866571935851\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering-p2p\n      name: MTEB StackExchangeClusteringP2P\n      config: default\n      split: test\n      revision: 815ca46b2622cec33ccafc3735d572c266efdb44\n    metrics:\n    - type: v_measure\n      value: 35.16579026551532\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/stackoverflowdupquestions-reranking\n      name: MTEB StackOverflowDupQuestions\n      config: default\n      split: test\n      revision: e185fbe320c72810689fc5848eb6114e1ef5ec69\n    metrics:\n    - type: map\n      value: 52.518952473513934\n    - type: mrr\n      value: 53.292457134368895\n  - task:\n      type: Summarization\n    dataset:\n      type: mteb/summeval\n      name: MTEB SummEval\n      config: default\n      split: test\n      revision: cda12ad7615edc362dbf25a00fdd61d3b1eaf93c\n    metrics:\n    - type: cos_sim_pearson\n      value: 31.12529588316604\n    - type: cos_sim_spearman\n      value: 32.31662126895294\n    - type: dot_pearson\n      value: 31.125303796647056\n    - type: dot_spearman\n      value: 32.31662126895294\n  - task:\n      type: Retrieval\n    dataset:\n      type: trec-covid\n      name: MTEB TRECCOVID\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 0.219\n    - type: map_at_10\n      value: 1.7469999999999999\n    - type: map_at_100\n      value: 10.177999999999999\n    - type: map_at_1000\n      value: 26.108999999999998\n    - type: map_at_3\n      value: 0.64\n    - type: map_at_5\n      value: 0.968\n    - type: mrr_at_1\n      value: 82.0\n    - type: mrr_at_10\n      value: 89.067\n    - type: mrr_at_100\n      value: 89.067\n    - type: mrr_at_1000\n      value: 89.067\n    - type: mrr_at_3\n      value: 88.333\n    - type: mrr_at_5\n      value: 88.73299999999999\n    - type: ndcg_at_1\n      value: 78.0\n    - type: ndcg_at_10\n      value: 71.398\n    - type: ndcg_at_100\n      value: 55.574999999999996\n    - type: ndcg_at_1000\n      value: 51.771\n    - type: ndcg_at_3\n      value: 77.765\n    - type: ndcg_at_5\n      value: 73.614\n    - type: precision_at_1\n      value: 82.0\n    - type: precision_at_10\n      value: 75.4\n    - type: precision_at_100\n      value: 58.040000000000006\n    - type: precision_at_1000\n      value: 23.516000000000002\n    - type: precision_at_3\n      value: 84.0\n    - type: precision_at_5\n      value: 78.4\n    - type: recall_at_1\n      value: 0.219\n    - type: recall_at_10\n      value: 1.958\n    - type: recall_at_100\n      value: 13.797999999999998\n    - type: recall_at_1000\n      value: 49.881\n    - type: recall_at_3\n      value: 0.672\n    - type: recall_at_5\n      value: 1.0370000000000001\n  - task:\n      type: Retrieval\n    dataset:\n      type: webis-touche2020\n      name: MTEB Touche2020\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 1.8610000000000002\n    - type: map_at_10\n      value: 8.705\n    - type: map_at_100\n      value: 15.164\n    - type: map_at_1000\n      value: 16.78\n    - type: map_at_3\n      value: 4.346\n    - type: map_at_5\n      value: 6.151\n    - type: mrr_at_1\n      value: 22.448999999999998\n    - type: mrr_at_10\n      value: 41.556\n    - type: mrr_at_100\n      value: 42.484\n    - type: mrr_at_1000\n      value: 42.494\n    - type: mrr_at_3\n      value: 37.755\n    - type: mrr_at_5\n      value: 40.102\n    - type: ndcg_at_1\n      value: 21.429000000000002\n    - type: ndcg_at_10\n      value: 23.439\n    - type: ndcg_at_100\n      value: 36.948\n    - type: ndcg_at_1000\n      value: 48.408\n    - type: ndcg_at_3\n      value: 22.261\n    - type: ndcg_at_5\n      value: 23.085\n    - type: precision_at_1\n      value: 22.448999999999998\n    - type: precision_at_10\n      value: 21.633\n    - type: precision_at_100\n      value: 8.02\n    - type: precision_at_1000\n      value: 1.5939999999999999\n    - type: precision_at_3\n      value: 23.810000000000002\n    - type: precision_at_5\n      value: 24.490000000000002\n    - type: recall_at_1\n      value: 1.8610000000000002\n    - type: recall_at_10\n      value: 15.876000000000001\n    - type: recall_at_100\n      value: 50.300999999999995\n    - type: recall_at_1000\n      value: 86.098\n    - type: recall_at_3\n      value: 5.892\n    - type: recall_at_5\n      value: 9.443\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/toxic_conversations_50k\n      name: MTEB ToxicConversationsClassification\n      config: default\n      split: test\n      revision: d7c0de2777da35d6aae2200a62c6e0e5af397c4c\n    metrics:\n    - type: accuracy\n      value: 70.3264\n    - type: ap\n      value: 13.249577616243794\n    - type: f1\n      value: 53.621518367695685\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/tweet_sentiment_extraction\n      name: MTEB TweetSentimentExtractionClassification\n      config: default\n      split: test\n      revision: d604517c81ca91fe16a244d1248fc021f9ecee7a\n    metrics:\n    - type: accuracy\n      value: 61.57611771363894\n    - type: f1\n      value: 61.79797478568639\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/twentynewsgroups-clustering\n      name: MTEB TwentyNewsgroupsClustering\n      config: default\n      split: test\n      revision: 6125ec4e24fa026cec8a478383ee943acfbd5449\n    metrics:\n    - type: v_measure\n      value: 53.38315344479284\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/twittersemeval2015-pairclassification\n      name: MTEB TwitterSemEval2015\n      config: default\n      split: test\n      revision: 70970daeab8776df92f5ea462b6173c0b46fd2d1\n    metrics:\n    - type: cos_sim_accuracy\n      value: 87.55438993860642\n    - type: cos_sim_ap\n      value: 77.98702600017738\n    - type: cos_sim_f1\n      value: 71.94971653931476\n    - type: cos_sim_precision\n      value: 67.50693802035153\n    - type: cos_sim_recall\n      value: 77.01846965699208\n    - type: dot_accuracy\n      value: 87.55438993860642\n    - type: dot_ap\n      value: 77.98702925907986\n    - type: dot_f1\n      value: 71.94971653931476\n    - type: dot_precision\n      value: 67.50693802035153\n    - type: dot_recall\n      value: 77.01846965699208\n    - type: euclidean_accuracy\n      value: 87.55438993860642\n    - type: euclidean_ap\n      value: 77.98702951957925\n    - type: euclidean_f1\n      value: 71.94971653931476\n    - type: euclidean_precision\n      value: 67.50693802035153\n    - type: euclidean_recall\n      value: 77.01846965699208\n    - type: manhattan_accuracy\n      value: 87.54246885617214\n    - type: manhattan_ap\n      value: 77.95531413902947\n    - type: manhattan_f1\n      value: 71.93605683836589\n    - type: manhattan_precision\n      value: 69.28152492668622\n    - type: manhattan_recall\n      value: 74.80211081794195\n    - type: max_accuracy\n      value: 87.55438993860642\n    - type: max_ap\n      value: 77.98702951957925\n    - type: max_f1\n      value: 71.94971653931476\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/twitterurlcorpus-pairclassification\n      name: MTEB TwitterURLCorpus\n      config: default\n      split: test\n      revision: 8b6510b0b1fa4e4c4f879467980e9be563ec1cdf\n    metrics:\n    - type: cos_sim_accuracy\n      value: 89.47296930182016\n    - type: cos_sim_ap\n      value: 86.92853616302108\n    - type: cos_sim_f1\n      value: 79.35138351681047\n    - type: cos_sim_precision\n      value: 76.74820143884892\n    - type: cos_sim_recall\n      value: 82.13735756082538\n    - type: dot_accuracy\n      value: 89.47296930182016\n    - type: dot_ap\n      value: 86.92854339601595\n    - type: dot_f1\n      value: 79.35138351681047\n    - type: dot_precision\n      value: 76.74820143884892\n    - type: dot_recall\n      value: 82.13735756082538\n    - type: euclidean_accuracy\n      value: 89.47296930182016\n    - type: euclidean_ap\n      value: 86.92854191061649\n    - type: euclidean_f1\n      value: 79.35138351681047\n    - type: euclidean_precision\n      value: 76.74820143884892\n    - type: euclidean_recall\n      value: 82.13735756082538\n    - type: manhattan_accuracy\n      value: 89.47685023479644\n    - type: manhattan_ap\n      value: 86.90063722679578\n    - type: manhattan_f1\n      value: 79.30753865502702\n    - type: manhattan_precision\n      value: 76.32066068631639\n    - type: manhattan_recall\n      value: 82.53772713273791\n    - type: max_accuracy\n      value: 89.47685023479644\n    - type: max_ap\n      value: 86.92854339601595\n    - type: max_f1\n      value: 79.35138351681047\n---\n\n# hkunlp/instructor-xl\nWe introduce **Instructor**üë®‚Äçüè´, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) ***by simply providing the task instruction, without any finetuning***. Instructorüë®‚Äç achieves sota on 70 diverse embedding tasks!\nThe model is easy to use with **our customized** `sentence-transformer` library. For more details, check out [our paper](https://arxiv.org/abs/2212.09741) and [project page](https://instructor-embedding.github.io/)! \n\n**************************** **Updates** ****************************\n\n* 01/21: We released a new [checkpoint](https://huggingface.co/hkunlp/instructor-xl) trained with hard negatives, which gives better performance.\n* 12/21: We released our [paper](https://arxiv.org/abs/2212.09741), [code](https://github.com/HKUNLP/instructor-embedding), [checkpoint](https://huggingface.co/hkunlp/instructor-xl) and [project page](https://instructor-embedding.github.io/)! Check them out!\n\n## Quick start\n<hr />\n\n## Installation\n```bash\npip install InstructorEmbedding\n```\n\n## Compute your customized embeddings\nThen you can use the model like this to calculate domain-specific and task-aware embeddings:\n```python\nfrom InstructorEmbedding import INSTRUCTOR\nmodel = INSTRUCTOR(''hkunlp/instructor-xl'')\nsentence = "3D ActionSLAM: wearable person tracking in multi-floor environments"\ninstruction = "Represent the Science title:"\nembeddings = model.encode([[instruction,sentence]])\nprint(embeddings)\n```\n\n## Use cases\n<hr />\n\n## Calculate embeddings for your customized texts\nIf you want to calculate customized embeddings for specific sentences, you may follow the unified template to write instructions: \n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Represent the `domain` `text_type` for `task_objective`:\n* `domain` is optional, and it specifies the domain of the text, e.g., science, finance, medicine, etc.\n* `text_type` is required, and it specifies the encoding unit, e.g., sentence, document, paragraph, etc.\n* `task_objective` is optional, and it specifies the objective of embedding, e.g., retrieve a document, classify the sentence, etc.\n\n## Calculate Sentence similarities\nYou can further use the model to compute similarities between two groups of sentences, with **customized embeddings**.\n```python\nfrom sklearn.metrics.pairwise import cosine_similarity\nsentences_a = [[''Represent the Science sentence: '',''Parton energy loss in QCD matter''], \n               [''Represent the Financial statement: '',''The Federal Reserve on Wednesday raised its benchmark interest rate.'']]\nsentences_b = [[''Represent the Science sentence: '',''The Chiral Phase Transition in Dissipative Dynamics''],\n               [''Represent the Financial statement: '',''The funds rose less than 0.5 per cent on Friday'']]\nembeddings_a = model.encode(sentences_a)\nembeddings_b = model.encode(sentences_b)\nsimilarities = cosine_similarity(embeddings_a,embeddings_b)\nprint(similarities)\n```\n\n## Information Retrieval\nYou can also use **customized embeddings** for information retrieval.\n```python\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nquery  = [[''Represent the Wikipedia question for retrieving supporting documents: '',''where is the food stored in a yam plant'']]\ncorpus = [[''Represent the Wikipedia document for retrieval: '',''Capitalism has been dominant in the Western world since the end of feudalism, but most feel[who?] that the term "mixed economies" more precisely describes most contemporary economies, due to their containing both private-owned and state-owned enterprises. In capitalism, prices determine the demand-supply scale. For example, higher demand for certain goods and services lead to higher prices and lower demand for certain goods lead to lower prices.''],\n          [''Represent the Wikipedia document for retrieval: '',"The disparate impact theory is especially controversial under the Fair Housing Act because the Act regulates many activities relating to housing, insurance, and mortgage loans√¢‚Ç¨‚Äùand some scholars have argued that the theory''s use under the Fair Housing Act, combined with extensions of the Community Reinvestment Act, contributed to rise of sub-prime lending and the crash of the U.S. housing market and ensuing global economic recession"],\n          [''Represent the Wikipedia document for retrieval: '',''Disparate impact in United States labor law refers to practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another, even though rules applied by employers or landlords are formally neutral. Although the protected classes vary by statute, most federal civil rights laws protect based on race, color, religion, national origin, and sex as protected traits, and some laws include disability status and other traits as well.'']]\nquery_embeddings = model.encode(query)\ncorpus_embeddings = model.encode(corpus)\nsimilarities = cosine_similarity(query_embeddings,corpus_embeddings)\nretrieved_doc_id = np.argmax(similarities)\nprint(retrieved_doc_id)\n```\n\n## Clustering\nUse **customized embeddings** for clustering texts in groups.\n```python\nimport sklearn.cluster\nsentences = [[''Represent the Medicine sentence for clustering: '',''Dynamical Scalar Degree of Freedom in Horava-Lifshitz Gravity''],\n             [''Represent the Medicine sentence for clustering: '',''Comparison of Atmospheric Neutrino Flux Calculations at Low Energies''],\n             [''Represent the Medicine sentence for clustering: '',''Fermion Bags in the Massive Gross-Neveu Model''],\n             [''Represent the Medicine sentence for clustering: '',"QCD corrections to Associated t-tbar-H production at the Tevatron"],\n             [''Represent the Medicine sentence for clustering: '',''A New Analysis of the R Measurements: Resonance Parameters of the Higher,  Vector States of Charmonium'']]\nembeddings = model.encode(sentences)\nclustering_model = sklearn.cluster.MiniBatchKMeans(n_clusters=2)\nclustering_model.fit(embeddings)\ncluster_assignment = clustering_model.labels_\nprint(cluster_assignment)\n```', '{"pipeline_tag":"sentence-similarity","library_name":"sentence-transformers","framework":"sentence-transformers","params":null,"storage_bytes":19868112990,"files_count":14,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["T5EncoderModel"],"model_type":"t5","tokenizer_config":{"eos_token":"</s>","pad_token":"<pad>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:HKUNLP:instructor-embedding","source_url":"https://github.com/HKUNLP/instructor-embedding"},{"type":"based_on_paper","target_id":"arxiv:2212.09741","source_url":"https://arxiv.org/abs/2212.09741"}]', NULL, 'Apache-2.0', 'approved', 77.6, 'f335b9f3ecd3ab122b4247dcad40c0e1', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-XLabs-AI-flux-lora-collection', 'huggingface--xlabs-ai--flux-lora-collection', 'flux-lora-collection', 'XLabs-AI', '--- license: other license_name: flux-1-dev-non-commercial-license license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE. language: - en pipeline_tag: text-to-image tags: - LoRA - Stable Diffusion - image-generation - Flux --- !FLUX LoRA Collections This repository provides a checkpoint with trained LoRAs for FLUX.1-dev model by Black Forest Labs <img src="https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/light/join-our-discord-rev1.png?raw=true"> !Examp...', '["lora","stable diffusion","image-generation","flux","text-to-image","en","license:other","region:us"]', 'text-to-image', 579, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/XLabs-AI/flux-lora-collection","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: flux-1-dev-non-commercial-license\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.\nlanguage:\n- en\npipeline_tag: text-to-image\ntags:\n- LoRA\n- Stable Diffusion\n- image-generation\n- Flux\n---\n![FLUX LoRA Collections](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/light/flux-lora-collection-rev1.png?raw=true)\nThis repository provides a checkpoint with trained LoRAs for\n[FLUX.1-dev model](https://huggingface.co/black-forest-labs/FLUX.1-dev) by Black Forest Labs\n[<img src="https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/light/join-our-discord-rev1.png?raw=true">](https://discord.gg/FHY2guThfy)\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/furry4.png?raw=true)\n# ComfyUI\n\n[See our github](https://github.com/XLabs-AI/x-flux-comfyui) for comfy ui workflows.\n![Example Picture 1](https://github.com/XLabs-AI/x-flux-comfyui/blob/main/assets/image1.png?raw=true)\n# Training details\n[XLabs AI](https://github.com/XLabs-AI) team is happy to publish fune-tuning Flux scripts, including:\n\n- **LoRA** üî•\n- **ControlNet** üî•\n\n[See our github](https://github.com/XLabs-AI/x-flux) for train script and train configs.\n\n# Training Dataset\nDataset has the following format for the training process:\n\n```\n‚îú‚îÄ‚îÄ images/\n‚îÇ    ‚îú‚îÄ‚îÄ 1.png\n‚îÇ    ‚îú‚îÄ‚îÄ 1.json\n‚îÇ    ‚îú‚îÄ‚îÄ 2.png\n‚îÇ    ‚îú‚îÄ‚îÄ 2.json\n‚îÇ    ‚îú‚îÄ‚îÄ ...\n```\nA .json file contains "caption" field with a text prompt.\n\nThank https://civitai.com/user/dobrosketchkun and https://civitai.com/user/sadxzero for datasets for loras\n\n# Inference\n## furry_lora\n```bash\npython3 main.py \\n --prompt "Female furry Pixie with text ''hello world''" \\n --lora_repo_id XLabs-AI/flux-furry-lora --lora_name furry_lora.safetensors --device cuda --offload --use_lora \\n --model_type flux-dev-fp8 --width 1024 --height 1024 \\n --timestep_to_start_cfg 1 --num_steps 25 --true_gs 3.5 --guidance 4\n\n```\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/furry4.png?raw=true)\n```bash\npython3 main.py \\n --prompt "Male furry Lycanthrope with fur-covered body in ancient ruins, howling at the full moon, surrounded by eerie mist, werewolf transformation, elder scrolls, eslweyr, glitch aesthetic, anime-inspired, digital illustration, artstation, furry" \\n --lora_repo_id XLabs-AI/flux-furry-lora --lora_name furry_lora.safetensors --device cuda --offload --use_lora \\n --model_type flux-dev-fp8 --width 1024 --height 1024 \\n --timestep_to_start_cfg 1 --num_steps 25 --true_gs 3.5\n\n```\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/furry2.png?raw=true)\n## mjv6_lora\n```bash\npython3 main.py \\n--prompt "A handsome man in a suit, 25 years old, cool, futuristic" \\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name mjv6_lora.safetensors \\n--device cuda:4 --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_13.png?raw=true)\n\n```bash\npython3 main.py \\n--prompt "A girl in a suit covered with bold tattoos and holding a vest pistol, beautiful woman, 25 years old, cool, future fantasy, turquoise & light orange ping curl hair" \\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name mjv6_lora.safetensors \\n--device cuda --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_12.png?raw=true)\n## anime_lora\n```bash\npython3 main.py \\n--prompt "A cute corgi lives in a house made out of sushi, anime" \\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name anime_lora.safetensors \\n--device cuda --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_14.png?raw=true)\n```bash\npython3 main.py \\n--prompt "a girl with orange hair, standing in a room with a window, looking out at a cityscape, anime" \\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name anime_lora.safetensors \\n--device cuda --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_15.png?raw=true)\n\n## disney_lora\n```bash\npython3 main.py \\n--prompt "An aerial view of beach with people on it, disney style" \\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name disney_lora.safetensors \\n--device cuda --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_19.png?raw=true)\n\n```bash\npython3 main.py \\n--prompt "A blue jay standing on a large basket of rainbow macarons, disney style" \\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name disney_lora.safetensors \\n--device cuda --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_18.png?raw=true)\n\n## scenery_lora\n```bash\npython3 main.py \\n--prompt "A fantasy cityscape with multiple buildings and skyscrapers all of which are covered in snow and ice, scenery style" \\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name scenery_lora.safetensors \\n--device cuda --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_21.png?raw=true)\n\n```bash\npython3 main.py \\n--prompt "A large ornate building with multiple levels and arches surrounded by trees and greenery. In front of it there are several statues and sculptures on pedestals with fire burning brightly in front of them, scenery style" \\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name scenery_lora.safetensors \\n--device cuda --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_22.png?raw=true)\n## art_lora\n```bash\npython3 main.py \\n--prompt "white rabbit in blue dress and hat holding bow and arrow, art" \\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name art_lora.safetensors \\n--device cuda --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_23.png?raw=true)\n\n```bash\npython3 main.py \\n--prompt "castle in the middle of forest at night, art" \\n--lora_repo_id XLabs-AI/flux-lora-collection --lora_name art_lora.safetensors \\n--device cuda --offload --use_lora --model_type flux-dev-fp8 --width 1024 --height 1024\n```\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/result_24.png?raw=true)\n# License\n\nlora.safetensors falls under the [FLUX.1 [dev]](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md) Non-Commercial License<br/>', '{"pipeline_tag":"text-to-image","library_name":null,"framework":null,"params":null,"storage_bytes":1479610132,"files_count":15,"spaces_count":49,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux-comfyui","source_url":"https://github.com/XLabs-AI/x-flux-comfyui"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux-comfyui","source_url":"https://github.com/XLabs-AI/x-flux-comfyui"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"}]', NULL, 'Other', 'approved', 62.6, '6421016c9eefc9d3503342452db44f8b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-black-forest-labs-FLUX.1-Redux-dev', 'huggingface--black-forest-labs--flux.1-redux-dev', 'FLUX.1-Redux-dev', 'black-forest-labs', '', '["diffusers","safetensors","image-generation","flux","diffusion-single-file","en","license:other","diffusers:fluxpriorreduxpipeline","region:us"]', 'other', 577, 26988, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":null,"library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":1243640584,"files_count":11,"spaces_count":39,"gated":"auto","private":false,"config":{"diffusers":{"_class_name":"FluxPriorReduxPipeline"}}}', '[]', '[]', NULL, 'Other', 'approved', 37.6, 'c3a3298d41feda8abc1460a344ed14e4', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-agentica-org-DeepScaleR-1.5B-Preview', 'huggingface--agentica-org--deepscaler-1.5b-preview', 'DeepScaleR-1.5B-Preview', 'agentica-org', '--- license: mit library_name: transformers datasets: - AI-MO/NuminaMath-CoT - KbsdJames/Omni-MATH - RUC-AIBOX/STILL-3-Preview-RL-Data - hendrycks/competition_math language: - en base_model: - deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B pipeline_tag: text-generation --- <div align="center"> <span style="font-family: default; font-size: 1.5em;">DeepScaleR-1.5B-Preview</span> <div> üöÄ Democratizing Reinforcement Learning for LLMs üåü </div> </div> <br> <div align="center" style="line-height: 1;"> ...', '["transformers","safetensors","qwen2","text-generation","en","dataset:ai-mo/numinamath-cot","dataset:kbsdjames/omni-math","dataset:ruc-aibox/still-3-preview-rl-data","dataset:hendrycks/competition_math","license:mit","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 577, 79700, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/agentica-org/DeepScaleR-1.5B-Preview","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: mit\nlibrary_name: transformers\ndatasets:\n- AI-MO/NuminaMath-CoT\n- KbsdJames/Omni-MATH\n- RUC-AIBOX/STILL-3-Preview-RL-Data\n- hendrycks/competition_math\nlanguage:\n- en\nbase_model:\n- deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\npipeline_tag: text-generation\n---\n\n<div align="center">\n<span style="font-family: default; font-size: 1.5em;">DeepScaleR-1.5B-Preview</span>\n<div>\nüöÄ Democratizing Reinforcement Learning for LLMs üåü\n</div>\n</div>\n<br>\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/agentica-project/rllm" style="margin: 2px;">\n    <img alt="Code" src="https://img.shields.io/badge/DeepScaleR-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2" target="_blank" style="margin: 2px;">\n    <img alt="Blog" src="https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://x.com/Agentica_/status/1889006266661617779" style="margin: 2px;">\n    <img alt="X.ai" src="https://img.shields.io/badge/Agentica-white?style=for-the-badge&logo=X&logoColor=000&color=000&labelColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/agentica-org" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/Agentica-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n</div>\n</div>\n\n## DeepScaleR Overview\nDeepScaleR-1.5B-Preview is a language model fine-tuned from DeepSeek-R1-Distilled-Qwen-1.5B using distributed reinforcement learning (RL) to scale up to long context lengths. The model achieves 43.1% Pass@1 accuracy on AIME 2024, representing a 15% improvement over the base model (28.8%) and surpassing OpenAI''s O1-Preview performance with just 1.5B parameters.\n\n## Data\nOur training dataset consists of approximately 40,000 unique problem-answer pairs compiled from:\n- AIME problems (1984-2023)\n- AMC problems (prior to 2023)\n- Omni-MATH dataset\n- Still dataset\n\n## Training Recipe\nWe employ Deepseek''s Group Relative Policy Optimization (GRPO), a simplified RL algorithm that extends PPO by:\n- Normalizing advantage function over all samples generated from the same prompt.\n- Applying KL divergence regularization on top of PPO''s surrogate loss to prevent significant policy drift.\n\n**Reward Function**: Our reward function is simple but effective:\n- 1 for correct answers passing LaTeX/Sympy checks\n- 0 for incorrect or improperly formatted answers\n- Note: No partial rewards (such as PRMs) or intermediate feedback.\n\n**Iterative Context Lengthening**: A key challenge in scaling RL for reasoning is compute cost. Our approach trains models with progressively longer contexts as the model improves, thus saving monetary costs and end2end training time: \n- Initial 8K Context (0-1040 steps):\n    - 22.9% -> 33% Pass@1 on AIME 2024\n    - Trained on 8 A100-80GB GPUs, BS= (Prompts) * (Samples/Prompt) = 128 * 8 = 1024\n- Extended to 16K (steps 1040-1520):\n    - 33% -> 43% Pass@1 on AIME 2024\n    - Trained on 32 A100-80GB GPUs, BS= (Prompts) * (Samples/Prompt) = 128 * 16 = 2048\n- Further extended to 24K (step 1520+):\n    - 38% -> 43% Pass@1 on AIME 2024\n    - Trained on 32 A100-80GB GPUs, BS= (Prompts) * (Samples/Prompt) = 128 * 16 = 2048\n    - Significant improvements within <200 steps\n\nA more detailed description of the training recipe can be found in our [blog post](https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2).\n\n## Evaluation\nWe report Pass@1 accuracy averaged over 16 samples for each problem.\n| Model | AIME 2024 | MATH 500 | AMC 2023 | Minerva Math | OlympiadBench | Avg. |\n|-------|-----------|-----------|-----------|--------------|---------------|------|\n| Qwen-2.5-7B-Instruct | 13.3 | 79.8 | 50.6 | 34.6 | 40.7 | 43.8 |\n| rStar-Math-7B | 26.7 | 78.4 | 47.5 | - | 47.1 | - |\n| Eurus-2-7B-PRIME | 26.7 | 79.2 | 57.8 | 38.6 | 42.1 | 48.9 |\n| Qwen2.5-7B-SimpleRL | 26.7 | 82.4 | 62.5 | <strong>39.7</strong> | 43.3 | 50.9 |\n| DeepSeek-R1-Distill-Qwen-1.5B | 28.8 | 82.8 | 62.9 | 26.5 | 43.3 | 48.9 |\n| Still-1.5B | 32.5 | 84.4 | 66.7 | 29.0 | 45.4 | 51.6 |\n| <strong>DeepScaleR-1.5B-Preview</strong> | <strong>43.1</strong> | <strong>87.8</strong> | <strong>73.6</strong> | 30.2 | <strong>50.0</strong> | <strong>57.0</strong> |\n| O1-Preview | 40.0 | 81.4 | - | - | - | - |\n\n## Serving DeepScaleR\nOur model can be served using popular high-performance inference systems:\n- vLLM\n- Hugging Face Text Generation Inference (TGI)\n- SGLang\n- TensorRT-LLM\n\nAll these systems support the OpenAI Chat Completions API format.\n\n## License\nThis project is released under the MIT License, reflecting our commitment to open and accessible AI development.\nWe believe in democratizing AI technology by making our work freely available for anyone to use, modify, and build upon.\nThis permissive license ensures that researchers, developers, and enthusiasts worldwide can leverage and extend our work without restrictions, fostering innovation and collaboration in the AI community.\n\n## Acknowledgement\n- Our training experiments are powered by our heavily modified fork of [Verl](https://github.com/agentica-project/verl), an open-source RLHF library.\n- Our model is trained on top of [`DeepSeek-R1-Distill-Qwen-1.5B`](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B).\n- Our work is done as part of  [Berkeley Sky Computing Lab](https://skycomputing.berkeley.edu/) and [Berkeley AI Research](https://bair.berkeley.edu/).\n\n## Citation \n```bibtex\n@misc{deepscaler2025,\n  title={DeepScaleR: Surpassing O1-Preview with a 1.5B Model by Scaling RL},\n  author={Michael Luo and Sijun Tan and Justin Wong and Xiaoxiang Shi and William Y. Tang and Manan Roongta and Colin Cai and Jeffrey Luo and Li Erran Li and Raluca Ada Popa and Ion Stoica},\n  year={2025},\n  howpublished={\url{https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2}},\n  note={Notion Blog}\n  year={2025}\n}', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":1777088000,"storage_bytes":7119849176,"files_count":11,"spaces_count":11,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2"}}', '[]', '[{"type":"has_code","target_id":"github:agentica-project:rllm\"","source_url":"https://github.com/agentica-project/rllm\""},{"type":"has_code","target_id":"github:agentica-project:verl","source_url":"https://github.com/agentica-project/verl"}]', NULL, 'MIT', 'approved', 62.6, '90323b784eefe9ae2d6185745ec451cf', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-EleutherAI-gpt-neox-20b', 'huggingface--eleutherai--gpt-neox-20b', 'gpt-neox-20b', 'EleutherAI', '--- language: - en tags: - pytorch - causal-lm license: apache-2.0 datasets: - EleutherAI/pile --- GPT-NeoX-20B is a 20 billion parameter autoregressive language model trained on the Pile using the GPT-NeoX library. Its architecture intentionally resembles that of GPT-3, and is almost identical to that of GPT-J- 6B. Its training dataset contains a multitude of English-language texts, reflecting the general-purpose nature of this model. See the accompanying paper for details about model archit...', '["transformers","pytorch","safetensors","gpt_neox","text-generation","causal-lm","en","dataset:eleutherai/pile","arxiv:2204.06745","arxiv:2101.00027","arxiv:2201.07311","arxiv:2104.09864","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 574, 19852, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/EleutherAI/gpt-neox-20b","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage:\n- en\ntags:\n- pytorch\n- causal-lm\nlicense: apache-2.0\ndatasets:\n- EleutherAI/pile\n---\n\nGPT-NeoX-20B is a 20 billion parameter autoregressive language model trained \non [the Pile](https://pile.eleuther.ai/) using the [GPT-NeoX \nlibrary](https://github.com/EleutherAI/gpt-neox). Its architecture intentionally \nresembles that of GPT-3, and is almost identical to that of [GPT-J-\n6B](https://huggingface.co/EleutherAI/gpt-j-6B). Its training dataset contains \na multitude of English-language texts, reflecting the general-purpose nature \nof this model. See the [accompanying paper](https://arxiv.org/abs/2204.06745) \nfor details about model architecture (including how it differs from GPT-3), \ntraining procedure, and additional evaluations.\n\n### Model details\n\n- Developed by: [EleutherAI](http://eleuther.ai)\n- Model type: Transformer-based Language Model\n- Language: English\n- Learn more: [GPT-NeoX-20B: An Open-Source Autoregressive Language \nModel](https://arxiv.org/abs/2204.06745). For details about the training dataset, \nsee [the Pile paper](https://arxiv.org/abs/2101.00027), and [its data\nsheet](https://arxiv.org/abs/2201.07311).\n- License: Apache 2.0\n- Contact: to ask questions about this model, join the [EleutherAI \nDiscord](https://discord.gg/zBGx3azzUn), and post them in `#release-discussion`. \nPlease read the existing GPT-NeoX-20B documentation before asking about the model \non Discord. For general correspondence: [contact@eleuther.\nai](mailto:contact@eleuther.ai).\n\n<figure style="width:30em">\n\n| Hyperparameter         | Value       |\n| ---------------------- | ----------- |\n| n<sub>parameters</sub> | 20554567680 |\n| n<sub>layers</sub>     | 44          |\n| d<sub>model</sub>      | 6144        |\n| n<sub>heads</sub>      | 64          |\n| d<sub>head</sub>       | 96          |\n| n<sub>vocab</sub>      | 50257       |\n| Sequence Length        | 2048        |\n| Learning Rate          | 0.97 x 10<sup>-5</sup> |\n| Positional Encoding    | [Rotary Position Embedding (RoPE)](https://arxiv.org/abs/2104.09864) |\n</figure>\n\n### Uses and limitations\n\n#### Intended use\n\nGPT-NeoX-20B was developed primarily for research purposes. It learns an inner \nrepresentation of the English language that can be used to extract features \nuseful for downstream tasks.\n\nIn addition to scientific uses, you may also further fine-tune and adapt \nGPT-NeoX-20B for deployment, as long as your use is in accordance with the \nApache 2.0 license. This model works with the [Transformers \nLibrary](https://huggingface.co/docs/transformers/index). If you decide to use \npre-trained GPT-NeoX-20B as a basis for your fine-tuned model, please note that \nyou need to conduct your own risk and bias assessment. \n\n#### Out-of-scope use\n\nGPT-NeoX-20B is **not** intended for deployment as-is. It is not a product \nand cannot be used for human-facing interactions without supervision.\n\nGPT-NeoX-20B has not been fine-tuned for downstream tasks for which language \nmodels are commonly deployed, such as writing genre prose, or commercial \nchatbots. This means GPT-NeoX-20B will likely **not** respond to a given prompt \nthe way products such as ChatGPT do. This is because, unlike GPT-NeoX-20B, \nChatGPT was fine-tuned using methods such as Reinforcement Learning from Human \nFeedback (RLHF) to better ‚Äúunderstand‚Äù human instructions and dialogue.\n\nThis model is English-language only, and thus cannot be used for translation\nor generating text in other languages.\n\n#### Limitations and biases\n\nThe core functionality of GPT-NeoX-20B is to take a string of text and predict \nthe next token. Remember that the statistically most likely next token need \nnot result in the most ‚Äúaccurate‚Äù text. Never rely on GPT-NeoX-20B to produce \nfactually accurate output.\n\nThis model was trained on [the Pile](https://pile.eleuther.ai/), a dataset \nknown to contain profanity and texts that are lewd or otherwise offensive. \nSee [Section 6 of the Pile paper](https://arxiv.org/abs/2101.00027) for a \ndiscussion of documented biases with regards to gender, religion, and race. \nGPT-NeoX-20B may produce socially unacceptable or undesirable text, *even if*\n the prompt itself does not include anything explicitly offensive. \n\nWe recommend curating the outputs of this model before presenting it to a human \nreader. Please inform your audience that you are using artificially generated \ntext. \n\n#### How to use\n If you simply want to try out some prompts, check out [this \n playground](https://20b.eleuther.ai/).\n \n GPT-NeoX-20B can be loaded using the `AutoModelForCausalLM` functionality:\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neox-20b")\nmodel = AutoModelForCausalLM.from_pretrained("EleutherAI/gpt-neox-20b")\n```\n\n### Training\n\n#### Training dataset\n\nThe Pile is a 825GiB general-purpose dataset in English. It was created by \nEleutherAI specifically for training large language models. It contains texts \nfrom 22 diverse sources, roughly broken down into five categories: academic \nwriting (e.g. arXiv), internet (e.g. CommonCrawl), prose (e.g. Project \nGutenberg), dialogue (e.g. YouTube subtitles), and miscellaneous (e.g. GitHub, \nEnron Emails). See [the Pile paper](https://arxiv.org/abs/2101.00027) for \na breakdown of all data sources, methodology, and a discussion of ethical \nimplications. Consult [the datasheet](https://arxiv.org/abs/2201.07311) for \nmore detailed documentation about the Pile and its component datasets. The \nPile can be downloaded from the [official website](https://pile.eleuther.ai/), \nor from a [community mirror](https://the-eye.eu/public/AI/pile/).\n\nThe Pile was **not** deduplicated before being used to train GPT-NeoX-20B.\n\n#### Training procedure\n\nGPT-NeoX-20B was trained with a batch size of approximately 3.15M tokens \n(1538 sequences of 2048 tokens each), for a total of 150,000 steps. Tensor \nparallelism and pipeline parallelism were used to distribute the model across \nGPUs. Additional details about the training procedure are in [Section 3 of \nthe accompanying paper](https://arxiv.org/abs/2204.06745).\n\n\n### Evaluations\n\n<figure style="width:55em">\n\n| Model         | OpenAI‚Äôs LAMBADA | SciQ          | PIQA          | TriviaQA      | ARC (Challenge) |\n| ------------- | :--------------: | :-----------: | :-----------: | :-----------: | :-------------: |\n| GPT-J-6B      | 0.683 ¬± 0.006    | 0.910 ¬± 0.009 | 0.752 ¬± 0.010 | 0.170 ¬± 0.004 | 0.340 ¬± 0.014   |\n| FairSeq 6.7B  | 0.673 ¬± 0.007    | 0.895 ¬± 0.010 | 0.762 ¬± 0.010 | 0.221 ¬± 0.004 | 0.329 ¬± 0.014   |\n| GPT-3 Curie   | 0.693 ¬± 0.006    | 0.918 ¬± 0.009 | 0.767 ¬± 0.010 | 0.196 ¬± 0.004 | 0.334 ¬± 0.014   |\n| FairSeq 13B   | 0.709 ¬± 0.006    | 0.910 ¬± 0.009 | 0.769 ¬± 0.010 | 0.270 ¬± 0.004 | 0.345 ¬± 0.014   |\n| GPT-NeoX-20B  | 0.720 ¬± 0.006    | 0.928 ¬± 0.008 | 0.779 ¬± 0.010 | 0.259 ¬± 0.004 | 0.380 ¬± 0.014   |\n| GPT-3 DaVinci | 0.752 ¬± 0.006    | 0.949 ¬± 0.007 | 0.791 ¬± 0.009 | 0.409 ¬± 0.005 | 0.435 ¬± 0.014   |\n<figcaption>Zero-shot performance on selected natural language tasks.</figcaption>\n</figure>\n\nThis is a heavily abridged version of the evaluation results. Appendix D of the\n [GPT-NeoX-20B paper](https://arxiv.org/abs/2204.06745) compares more model \nsizes, and contains additional evaluations, including on: zero and five-shot \nnatural language tasks, zero and five-shot Basic Arithmetic and MATH, \nand zero-shot Hendrycks tasks.\n\n### BibTeX\n\nTo cite the GPT-NeoX-20B paper:\n\n```\n@misc{https://doi.org/10.48550/arxiv.2204.06745,\n  doi = {10.48550/ARXIV.2204.06745},\n  \n  url = {https://arxiv.org/abs/2204.06745},\n  \n  author = {Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},\n  \n  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {GPT-NeoX-20B: An Open-Source Autoregressive Language Model},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```\n# [Open LLM Leaderboard Evaluation Results](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\nDetailed results can be found [here](https://huggingface.co/datasets/open-llm-leaderboard/details_EleutherAI__gpt-neox-20b)\n\n| Metric                | Value                     |\n|-----------------------|---------------------------|\n| Avg.                  | 36.02   |\n| ARC (25-shot)         | 45.73          |\n| HellaSwag (10-shot)   | 73.45    |\n| MMLU (5-shot)         | 25.0         |\n| TruthfulQA (0-shot)   | 31.61   |\n| Winogrande (5-shot)   | 68.9   |\n| GSM8K (5-shot)        | 2.43        |\n| DROP (3-shot)         | 5.04         |\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":20739117584,"storage_bytes":123696860531,"files_count":102,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["GPTNeoXForCausalLM"],"model_type":"gpt_neox","tokenizer_config":{"unk_token":"<|endoftext|>","bos_token":"<|endoftext|>","eos_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:EleutherAI:gpt-neox","source_url":"https://github.com/EleutherAI/gpt-neox"},{"type":"based_on_paper","target_id":"arxiv:2204.06745","source_url":"https://arxiv.org/abs/2204.06745"},{"type":"based_on_paper","target_id":"arxiv:2101.00027","source_url":"https://arxiv.org/abs/2101.00027"},{"type":"based_on_paper","target_id":"arxiv:2201.07311","source_url":"https://arxiv.org/abs/2201.07311"},{"type":"based_on_paper","target_id":"arxiv:2104.09864","source_url":"https://arxiv.org/abs/2104.09864"}]', NULL, 'Apache-2.0', 'approved', 62.6, '55f472100ec68d7e52ca1c24d0a91256', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-CrucibleAI-ControlNetMediaPipeFace', 'huggingface--crucibleai--controlnetmediapipeface', 'ControlNetMediaPipeFace', 'CrucibleAI', '--- language: - en thumbnail: '''' tags: - controlnet - laion - face - mediapipe - image-to-image license: openrail base_model: stabilityai/stable-diffusion-2-1-base datasets: - LAION-Face - LAION pipeline_tag: image-to-image --- - Overview: Samples, Contents, and Construction - Usage: Downloading, Training, and Inference - License - Credits and Thanks This dataset is designed to train a ControlNet with human facial expressions. It includes keypoints for pupils to allow gaze direction. Training...', '["diffusers","safetensors","controlnet","laion","face","mediapipe","image-to-image","en","dataset:laion-face","dataset:laion","arxiv:2302.05543","arxiv:2112.10752","arxiv:2210.08402","base_model:stabilityai/stable-diffusion-2-1-base","license:openrail","region:us"]', 'image-to-image', 574, 1187, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage:\n- en\nthumbnail: ''''\ntags:\n- controlnet\n- laion\n- face\n- mediapipe\n- image-to-image\nlicense: openrail\nbase_model: stabilityai/stable-diffusion-2-1-base\ndatasets:\n- LAION-Face\n- LAION\npipeline_tag: image-to-image\n---\n\n# ControlNet LAION Face Dataset\n\n## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n\n## Samples:\n\nCherry-picked from ControlNet + Stable Diffusion v2.1 Base\n\n|Input|Face Detection|Output|\n|:---:|:---:|:---:|\n|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/happy_source.jpg">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/happy_annotation.png">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/happy_result.png">|\n|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/neutral_source.jpg">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/neutral_annotation.png">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/neutral_result.png">|\n|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/sad_source.jpg">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/sad_annotation.png">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/sad_result.png">|\n|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/screaming_source.jpg">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/screaming_annotation.png">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/screaming_result.png">|\n|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/sideways_source.jpg">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/sideways_annotation.png">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/sideways_result.png">|\n|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/surprised_source.jpg">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/surprised_annotation.png">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/surprised_result.png">|\n\nImages with multiple faces are also supported:\n\n<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/family_source.jpg">\n\n<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/family_annotation.png">\n\n<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/family_result.png">\n\n\n## Dataset Contents:\n\n- train_laion_face.py - Entrypoint for ControlNet training.\n- laion_face_dataset.py - Code for performing dataset iteration.  Cropping and resizing happens here.\n- tool_download_face_targets.py - A tool to read metadata.json and populate the target folder.\n- tool_generate_face_poses.py - The original file used to generate the source images.  Included for reproducibility, but not required for training.\n- training/laion-face-processed/prompt.jsonl - Read by laion_face_dataset.  Includes prompts for the images.\n- training/laion-face-processed/metadata.json - Excerpts from LAION for the relevant data.  Also used for downloading the target dataset.\n- training/laion-face-processed/source/xxxxxxxxx.jpg - Images with detections performed.  Generated from the target images.\n- training/laion-face-processed/target/xxxxxxxxx.jpg - Selected images from LAION Face.\n\n## Dataset Construction:\n\nSource images were generated by pulling slice 00000 from LAION Face and passing them through MediaPipe''s face detector with special configuration parameters.  \n\nThe colors and line thicknesses used for MediaPipe are as follows:\n\n```\nf_thick = 2\nf_rad = 1\nright_iris_draw = DrawingSpec(color=(10, 200, 250), thickness=f_thick, circle_radius=f_rad)\nright_eye_draw = DrawingSpec(color=(10, 200, 180), thickness=f_thick, circle_radius=f_rad)\nright_eyebrow_draw = DrawingSpec(color=(10, 220, 180), thickness=f_thick, circle_radius=f_rad)\nleft_iris_draw = DrawingSpec(color=(250, 200, 10), thickness=f_thick, circle_radius=f_rad)\nleft_eye_draw = DrawingSpec(color=(180, 200, 10), thickness=f_thick, circle_radius=f_rad)\nleft_eyebrow_draw = DrawingSpec(color=(180, 220, 10), thickness=f_thick, circle_radius=f_rad)\nmouth_draw = DrawingSpec(color=(10, 180, 10), thickness=f_thick, circle_radius=f_rad)\nhead_draw = DrawingSpec(color=(10, 200, 10), thickness=f_thick, circle_radius=f_rad)\n\niris_landmark_spec = {468: right_iris_draw, 473: left_iris_draw}\n```\n\nWe have implemented a method named `draw_pupils` which modifies some functionality from MediaPipe.  It exists as a stopgap until some pending changes are merged.\n\n\n# Usage:\n\nThe containing ZIP file should be decompressed into the root of the ControlNet directory.  The `train_laion_face.py`, `laion_face_dataset.py`, and other `.py` files should sit adjacent to `tutorial_train.py` and `tutorial_train_sd21.py`.  We are assuming a checkout of the ControlNet repo at 0acb7e5, but there is no direct dependency on the repository.\n\n## Downloading:\n\nFor copyright reasons, we cannot include the original target files.  We have provided a script (tool_download_face_targets.py) which will read from training/laion-face-processed/metadata.json and populate the target folder.  This file has no requirements, but will use tqdm if it is installed.\n\n## Training:\n\nWhen the targets folder is fully populated, training can be run on a machine with at least 24 gigabytes of VRAM.  Our model was trained for 200 hours (four epochs) on an A6000.\n\n```bash\npython tool_add_control.py ./models/v1-5-pruned-emaonly.ckpt ./models/controlnet_sd15_laion_face.ckpt\npython ./train_laion_face_sd15.py\n```\n\n## Inference:\n\nWe have provided `gradio_face2image.py`.  Update the following two lines to point them to your trained model.\n\n```\nmodel = create_model(''./models/cldm_v21.yaml'').cpu()  # If you fine-tune on SD2.1 base, this does not need to change.\nmodel.load_state_dict(load_state_dict(''./models/control_sd21_openpose.pth'', location=''cuda''))\n```\n\nThe model has some limitations: while it is empirically better at tracking gaze and mouth poses than previous attempts, it may still ignore controls.  Adding details to the prompt like, "looking right" can abate bad behavior. \n\n## üß® Diffusers\n\nIt is recommended to use the checkpoint with [Stable Diffusion 2.1 - Base](stabilityai/stable-diffusion-2-1-base) as the checkpoint has been trained on it.\nExperimentally, the checkpoint can be used with other diffusion models such as dreamboothed stable diffusion.\n\nTo use with Stable Diffusion 1.5, insert `subfolder="diffusion_sd15"` into the from_pretrained arguments.  A v1.5 half-precision variant is provided but untested.\n\n1. Install `diffusers` and related packages:\n```\n$ pip install diffusers transformers accelerate\n```\n\n2. Run code:\n```py\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nfrom diffusers.utils import load_image\n\nimage = load_image(\n    "https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/family_annotation.png"\n)\n\n# Stable Diffusion 2.1-base:\ncontrolnet = ControlNetModel.from_pretrained("CrucibleAI/ControlNetMediaPipeFace", torch_dtype=torch.float16, variant="fp16")\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n	"stabilityai/stable-diffusion-2-1-base", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n)\n# OR\n# Stable Diffusion 1.5:\ncontrolnet = ControlNetModel.from_pretrained("CrucibleAI/ControlNetMediaPipeFace", subfolder="diffusion_sd15")\npipe = StableDiffusionControlNetPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", controlnet=controlnet, safety_checker=None)\n\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n\n# Remove if you do not have xformers installed\n# see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers\n# for installation instructions\npipe.enable_xformers_memory_efficient_attention()\npipe.enable_model_cpu_offload()\n\nimage = pipe("a happy family at a dentist advertisement", image=image, num_inference_steps=30).images[0]\nimage.save(''./images.png'')\n```\n\n\n# License:\n\n### Source Images: (/training/laion-face-processed/source/)\nThis work is marked with CC0 1.0. To view a copy of this license, visit http://creativecommons.org/publicdomain/zero/1.0\n\n### Trained Models:\nOur trained ControlNet checkpoints are released under CreativeML Open RAIL-M.\n\n### Source Code:\nlllyasviel/ControlNet is licensed under the Apache License 2.0\n\nOur modifications are released under the same license.\n\n\n# Credits and Thanks:\n\nGreatest thanks to Zhang et al. for ControlNet, Rombach et al. (StabilityAI) for Stable Diffusion, and Schuhmann et al. for LAION.\n\nSample images for this document were obtained from Unsplash and are CC0.\n\n```\n@misc{zhang2023adding,\n  title={Adding Conditional Control to Text-to-Image Diffusion Models}, \n  author={Lvmin Zhang and Maneesh Agrawala},\n  year={2023},\n  eprint={2302.05543},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n\n@misc{rombach2021highresolution,\n      title={High-Resolution Image Synthesis with Latent Diffusion Models}, \n      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Bj√∂rn Ommer},\n      year={2021},\n      eprint={2112.10752},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\n@misc{schuhmann2022laion5b,\n      title={LAION-5B: An open large-scale dataset for training next generation image-text models}, \n      author={Christoph Schuhmann and Romain Beaumont and Richard Vencu and Cade Gordon and Ross Wightman and Mehdi Cherti and Theo Coombes and Aarush Katta and Clayton Mullis and Mitchell Wortsman and Patrick Schramowski and Srivatsa Kundurthy and Katherine Crowson and Ludwig Schmidt and Robert Kaczmarczyk and Jenia Jitsev},\n      year={2022},\n      eprint={2210.08402},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\nThis project was made possible by Crucible AI.', '{"pipeline_tag":"image-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":34147689491,"files_count":47,"spaces_count":20,"gated":false,"private":false,"config":{}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2302.05543","source_url":"https://arxiv.org/abs/2302.05543"},{"type":"based_on_paper","target_id":"arxiv:2112.10752","source_url":"https://arxiv.org/abs/2112.10752"},{"type":"based_on_paper","target_id":"arxiv:2210.08402","source_url":"https://arxiv.org/abs/2210.08402"}]', NULL, 'OpenRAIL', 'approved', 77.6, '414bff0a823c910296b264b53da5cd50', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-TripoSR', 'huggingface--stabilityai--triposr', 'TripoSR', 'stabilityai', '--- datasets: - allenai/objaverse tags: - 3d extra_gated_fields: Name: text Email: text Country: text Organization or Affiliation: text I ALLOW Stability AI to email me about new model releases: checkbox license: mit pipeline_tag: image-to-3d --- > Try our new model: **SF3D** with several improvements such as faster generation and more game-ready assets. > > The model is available here and we also have a demo. TripoSR is a fast and feed-forward 3D generative model developed in collaboration b...', '["3d","image-to-3d","dataset:allenai/objaverse","arxiv:2311.04400","arxiv:2403.02151","license:mit","region:us"]', 'image-to-3d', 572, 28366, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/TripoSR","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\ndatasets:\n- allenai/objaverse\ntags:\n- 3d\nextra_gated_fields:\n  Name: text\n  Email: text\n  Country: text\n  Organization or Affiliation: text\n  I ALLOW Stability AI to email me about new model releases: checkbox\nlicense: mit\npipeline_tag: image-to-3d\n---\n\n> Try our new model: **SF3D** with several improvements such as faster generation and more game-ready assets.\n> \n> The model is available [here](https://huggingface.co/stabilityai/stable-fast-3d) and we also have a [demo](https://huggingface.co/spaces/stabilityai/stable-fast-3d).\n \n# TripoSR\n![](figures/input800.mp4)\nTripoSR is a fast and feed-forward 3D generative model developed in collaboration between Stability AI and Tripo AI.\n\n\n## Model Details\n\n### Model Description\n\nWe closely follow [LRM](https://arxiv.org/abs/2311.04400) network architecture for the model design, where TripoSR incorporates a series of technical advancements over the LRM model in terms of both data curation as well as model and training improvements. For more technical details and evaluations, please refer to [our tech report](https://arxiv.org/abs/2403.02151).\n\n* **Developed by**: [Stability AI](https://stability.ai/), [Tripo AI](https://tripo3d.ai/)\n* **Model type**: Feed-forward 3D reconstruction from a single image\n* **License**: MIT\n* **Hardware**: We train `TripoSR` for 5 days on 22 GPU nodes each with 8 A100 40GB GPUs\n\n### Model Sources\n\n* **Repository**: https://github.com/VAST-AI-Research/TripoSR\n* **Tech report**: https://arxiv.org/abs/2403.02151\n* **Demo**: https://huggingface.co/spaces/stabilityai/TripoSR\n\n### Training Dataset\n\nWe use renders from the [Objaverse](https://objaverse.allenai.org/objaverse-1.0) dataset, utilizing our enhanced rendering method that more closely replicate the distribution of images found in the real world, significantly improving our model‚Äôs ability to generalize. We selected a carefully curated subset of the Objaverse dataset for the training data, which is available under the CC-BY license. \n\n\n## Usage\n\n* For usage instructions, please refer to our [TripoSR GitHub repository](https://github.com/VAST-AI-Research/TripoSR)\n\n* You can also try it in [our gradio demo](https://huggingface.co/spaces/stabilityai/TripoSR)\n\n\n### Misuse, Malicious Use, and Out-of-Scope Use\n\nThe model should not be used to intentionally create or disseminate 3D models that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.', '{"pipeline_tag":"image-to-3d","library_name":null,"framework":null,"params":null,"storage_bytes":5262702691,"files_count":6,"spaces_count":60,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:VAST-AI-Research:TripoSR","source_url":"https://github.com/VAST-AI-Research/TripoSR"},{"type":"has_code","target_id":"github:VAST-AI-Research:TripoSR","source_url":"https://github.com/VAST-AI-Research/TripoSR"},{"type":"based_on_paper","target_id":"arxiv:2311.04400","source_url":"https://arxiv.org/abs/2311.04400"},{"type":"based_on_paper","target_id":"arxiv:2403.02151","source_url":"https://arxiv.org/abs/2403.02151"}]', NULL, 'MIT', 'approved', 62.6, 'c47993efce76945342e2749d12c3e5c7', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-NousResearch-Yarn-Mistral-7b-128k', 'huggingface--nousresearch--yarn-mistral-7b-128k', 'Yarn-Mistral-7b-128k', 'NousResearch', '--- datasets: - emozilla/yarn-train-tokenized-16k-mistral metrics: - perplexity library_name: transformers license: apache-2.0 language: - en --- Preprint (arXiv) GitHub !yarn Nous-Yarn-Mistral-7b-128k is a state-of-the-art language model for long context, further pretrained on long context data for 1500 steps using the YaRN extension method. It is an extension of Mistral-7B-v0.1 and supports a 128k token context window. To use, pass when loading the model, for example In addition you will ne...', '["transformers","pytorch","mistral","text-generation","custom_code","en","dataset:emozilla/yarn-train-tokenized-16k-mistral","arxiv:2309.00071","license:apache-2.0","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 571, 2201, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\ndatasets:\n- emozilla/yarn-train-tokenized-16k-mistral\nmetrics:\n- perplexity\nlibrary_name: transformers\nlicense: apache-2.0\nlanguage:\n- en\n---\n\n# Model Card: Nous-Yarn-Mistral-7b-128k\n\n[Preprint (arXiv)](https://arxiv.org/abs/2309.00071)  \n[GitHub](https://github.com/jquesnelle/yarn)\n![yarn](https://raw.githubusercontent.com/jquesnelle/yarn/mistral/data/proofpile-long-small-mistral.csv.png)\n\n## Model Description\n\nNous-Yarn-Mistral-7b-128k is a state-of-the-art language model for long context, further pretrained on long context data for 1500 steps using the YaRN extension method.\nIt is an extension of [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) and supports a 128k token context window.\n\nTo use, pass `trust_remote_code=True` when loading the model, for example\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained("NousResearch/Yarn-Mistral-7b-128k",\n  use_flash_attention_2=True,\n  torch_dtype=torch.bfloat16,\n  device_map="auto",\n  trust_remote_code=True)\n```\n\nIn addition you will need to use the latest version of `transformers` (until 4.35 comes out)\n```sh\npip install git+https://github.com/huggingface/transformers\n```\n\n## Benchmarks\n\nLong context benchmarks:\n| Model | Context Window | 8k PPL | 16k PPL | 32k PPL | 64k PPL | 128k PPL |\n|-------|---------------:|------:|----------:|-----:|-----:|------------:|\n| [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) | 8k | 2.96 | - | - | - | - |\n| [Yarn-Mistral-7b-64k](https://huggingface.co/NousResearch/Yarn-Mistral-7b-64k) | 64k | 3.04 | 2.65 | 2.44 | 2.20 | - |\n| [Yarn-Mistral-7b-128k](https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k) | 128k | 3.08 | 2.68 | 2.47 | 2.24 | 2.19 |\n\nShort context benchmarks showing that quality degradation is minimal:\n| Model | Context Window | ARC-c | Hellaswag | MMLU | Truthful QA |\n|-------|---------------:|------:|----------:|-----:|------------:|\n| [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) | 8k | 59.98 | 83.31 | 64.16 | 42.15 |\n| [Yarn-Mistral-7b-64k](https://huggingface.co/NousResearch/Yarn-Mistral-7b-64k) | 64k | 59.38 | 81.21 | 61.32 | 42.50 |\n| [Yarn-Mistral-7b-128k](https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k) | 128k | 58.87 | 80.58 | 60.64 | 42.46 |\n\n## Collaborators\n\n - [bloc97](https://github.com/bloc97): Methods, paper and evals\n - [@theemozilla](https://twitter.com/theemozilla): Methods, paper, model training, and evals\n - [@EnricoShippole](https://twitter.com/EnricoShippole): Model training\n - [honglu2875](https://github.com/honglu2875): Paper and evals\n\nThe authors would like to thank LAION AI for their support of compute for this model.\nIt was trained on the [JUWELS](https://www.fz-juelich.de/en/ias/jsc/systems/supercomputers/juwels) supercomputer.', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":28967555174,"files_count":14,"spaces_count":34,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"auto_map":{"AutoConfig":"configuration_mistral.MistralConfig","AutoModelForCausalLM":"modeling_mistral_yarn.MistralForCausalLM"},"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","eos_token":"</s>","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":true}}}', '[]', '[{"type":"has_code","target_id":"github:jquesnelle:yarn","source_url":"https://github.com/jquesnelle/yarn"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"}]', NULL, 'Apache-2.0', 'approved', 62.6, '74a499e02b21468d0871814eebe88ff5', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-fofr-sdxl-emoji', 'huggingface--fofr--sdxl-emoji', 'sdxl-emoji', 'fofr', '--- license: creativeml-openrail-m tags: - text-to-image - stable-diffusion - lora - diffusers base_model: stabilityai/stable-diffusion-xl-base-1.0 pivotal_tuning: true textual_embeddings: embeddings.pti instance_prompt: an <s0><s1> emoji inference: false --- !lora_image > Grab your replicate token here You may also do inference via the API with Node.js or curl, and locally with COG and Docker, check out the Replicate API page for this model Replicate SDXL LoRAs are trained with Pivotal Tunin...', '["diffusers","text-to-image","stable-diffusion","lora","license:creativeml-openrail-m","region:us"]', 'text-to-image', 571, 122, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/fofr/sdxl-emoji","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n  - text-to-image\n  - stable-diffusion\n  - lora\n  - diffusers\nbase_model: stabilityai/stable-diffusion-xl-base-1.0\npivotal_tuning: true\ntextual_embeddings: embeddings.pti\ninstance_prompt: an <s0><s1> emoji\ninference: false\n---\n# sdxl-emoji LoRA by [fofr](https://replicate.com/fofr)\n### An SDXL fine-tune based on Apple Emojis\n\n![lora_image](https://replicate.delivery/pbxt/a3z81v5vwlKfLq1H5uBqpVmkHalOVup0jSLma9E2UaF3tawIA/out-0.png)\n>\n\n## Inference with Replicate API\nGrab your replicate token [here](https://replicate.com/account)\n```bash\npip install replicate\nexport REPLICATE_API_TOKEN=r8_*************************************\n```\n\n```py\nimport replicate\n\noutput = replicate.run(\n    "sdxl-emoji@sha256:dee76b5afde21b0f01ed7925f0665b7e879c50ee718c5f78a9d38e04d523cc5e",\n    input={"prompt": "A TOK emoji of a man"}\n)\nprint(output)\n```\nYou may also do inference via the API with Node.js or curl, and locally with COG and Docker, [check out the Replicate API page for this model](https://replicate.com/fofr/sdxl-emoji/api)\n\n## Inference with üß® diffusers\nReplicate SDXL LoRAs are trained with Pivotal Tuning, which combines training a concept via Dreambooth LoRA with training a new token with Textual Inversion.\nAs `diffusers` doesn''t yet support textual inversion for SDXL, we will use cog-sdxl `TokenEmbeddingsHandler` class.\n\nThe trigger tokens for your prompt will be `<s0><s1>`\n\n```shell\npip install diffusers transformers accelerate safetensors huggingface_hub\ngit clone https://github.com/replicate/cog-sdxl cog_sdxl\n```\n\n```py\nimport torch\nfrom huggingface_hub import hf_hub_download\nfrom diffusers import DiffusionPipeline\nfrom cog_sdxl.dataset_and_utils import TokenEmbeddingsHandler\nfrom diffusers.models import AutoencoderKL\n\npipe = DiffusionPipeline.from_pretrained(\n        "stabilityai/stable-diffusion-xl-base-1.0",\n        torch_dtype=torch.float16,\n        variant="fp16",\n).to("cuda")\n\npipe.load_lora_weights("fofr/sdxl-emoji", weight_name="lora.safetensors")\n\ntext_encoders = [pipe.text_encoder, pipe.text_encoder_2]\ntokenizers = [pipe.tokenizer, pipe.tokenizer_2]\n\nembedding_path = hf_hub_download(repo_id="fofr/sdxl-emoji", filename="embeddings.pti", repo_type="model")\nembhandler = TokenEmbeddingsHandler(text_encoders, tokenizers)\nembhandler.load_embeddings(embedding_path)\nprompt="A <s0><s1> emoji of a man"\nimages = pipe(\n    prompt,\n    cross_attention_kwargs={"scale": 0.8},\n).images\n#your output image\nimages[0]\n```\n', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":185968776,"files_count":5,"spaces_count":20,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:replicate:cog-sdxl","source_url":"https://github.com/replicate/cog-sdxl"}]', NULL, 'creativeml-openrail-m', 'approved', 62.6, '72a1c3c596db84c658f91b72c117ee39', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-3.2-11B-Vision', 'huggingface--meta-llama--llama-3.2-11b-vision', 'Llama-3.2-11B-Vision', 'meta-llama', '', '["transformers","safetensors","mllama","image-to-text","facebook","meta","pytorch","llama","llama-3","image-text-to-text","en","de","fr","it","pt","hi","es","th","arxiv:2204.05149","license:llama3.2","text-generation-inference","endpoints_compatible","region:us"]', 'image-text-to-text', 570, 11604, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-3.2-11B-Vision","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":10642941475,"storage_bytes":52480505184,"files_count":20,"spaces_count":70,"gated":"manual","private":false,"config":{"architectures":["MllamaForConditionalGeneration"],"model_type":"mllama","tokenizer_config":{"bos_token":"<|begin_of_text|>","eos_token":"<|end_of_text|>","pad_token":"<|finetune_right_pad_id|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2204.05149","source_url":"https://arxiv.org/abs/2204.05149"}]', NULL, 'llama3.2', 'approved', 37.6, 'd97adc0f3aeb21aa05d38d7b4820e00c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-3n-E2B-it-litert-preview', 'huggingface--google--gemma-3n-e2b-it-litert-preview', 'gemma-3n-E2B-it-litert-preview', 'google', '', '["image-text-to-text","arxiv:1905.07830","arxiv:1905.10044","arxiv:1911.11641","arxiv:1904.09728","arxiv:1705.03551","arxiv:1911.01547","arxiv:1907.10641","arxiv:1903.00161","arxiv:2210.03057","arxiv:2502.12404","arxiv:2411.19799","arxiv:2009.03300","arxiv:2502.21228","arxiv:2311.12022","arxiv:2403.07974","arxiv:2108.07732","arxiv:2107.03374","license:gemma","region:us"]', 'image-text-to-text', 570, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-3n-E2B-it-litert-preview","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-text-to-text","library_name":null,"framework":null,"params":null,"storage_bytes":3136926777,"files_count":3,"spaces_count":0,"gated":"manual","private":false,"config":null}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1903.00161","source_url":"https://arxiv.org/abs/1903.00161"},{"type":"based_on_paper","target_id":"arxiv:2210.03057","source_url":"https://arxiv.org/abs/2210.03057"},{"type":"based_on_paper","target_id":"arxiv:2502.12404","source_url":"https://arxiv.org/abs/2502.12404"},{"type":"based_on_paper","target_id":"arxiv:2411.19799","source_url":"https://arxiv.org/abs/2411.19799"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:2502.21228","source_url":"https://arxiv.org/abs/2502.21228"},{"type":"based_on_paper","target_id":"arxiv:2311.12022","source_url":"https://arxiv.org/abs/2311.12022"},{"type":"based_on_paper","target_id":"arxiv:2403.07974","source_url":"https://arxiv.org/abs/2403.07974"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"}]', NULL, 'Gemma', 'approved', 37.6, '581a1d8c55441f56830427924f113f38', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen2.5-VL-3B-Instruct', 'huggingface--qwen--qwen2.5-vl-3b-instruct', 'Qwen2.5-VL-3B-Instruct', 'Qwen', '--- license_name: qwen-research license_link: https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/blob/main/LICENSE language: - en pipeline_tag: image-text-to-text tags: - multimodal library_name: transformers --- <a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> In the past five months since Qwen2-VL‚Äôs release, num...', '["transformers","safetensors","qwen2_5_vl","image-to-text","multimodal","image-text-to-text","conversational","en","arxiv:2309.00071","arxiv:2409.12191","arxiv:2308.12966","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'image-text-to-text', 569, 7688461, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '\n---\nlicense_name: qwen-research\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: image-text-to-text\ntags:\n- multimodal\nlibrary_name: transformers\n---\n\n# Qwen2.5-VL-3B-Instruct\n<a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Introduction\n\nIn the past five months since Qwen2-VL‚Äôs release, numerous developers have built new models on the Qwen2-VL vision-language models, providing us with valuable feedback. During this period, we focused on building more useful vision-language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5-VL.\n\n#### Key Enhancements:\n* **Understand things visually**: Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images.\n\n* **Being agentic**: Qwen2.5-VL directly plays as a visual agent that can reason and dynamically direct tools, which is capable of computer use and phone use.\n\n* **Understanding long videos and capturing events**: Qwen2.5-VL can comprehend videos of over 1 hour, and this time it has a new ability of cpaturing event by pinpointing the relevant video segments.\n\n* **Capable of visual localization in different formats**: Qwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes.\n\n* **Generating structured outputs**: for data like scans of invoices, forms, tables, etc. Qwen2.5-VL supports structured outputs of their contents, benefiting usages in finance, commerce, etc.\n\n\n#### Model Architecture Updates:\n\n* **Dynamic Resolution and Frame Rate Training for Video Understanding**:\n\nWe extend dynamic resolution to the temporal dimension by adopting dynamic FPS sampling, enabling the model to comprehend videos at various sampling rates. Accordingly, we update mRoPE in the time dimension with IDs and absolute time alignment, enabling the model to learn temporal sequence and speed, and ultimately acquire the ability to pinpoint specific moments.\n\n<p align="center">\n    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL/qwen2.5vl_arc.jpeg" width="80%"/>\n<p>\n\n\n* **Streamlined and Efficient Vision Encoder**\n\nWe enhance both training and inference speeds by strategically implementing window attention into the ViT. The ViT architecture is further optimized with SwiGLU and RMSNorm, aligning it with the structure of the Qwen2.5 LLM.\n\n\nWe have three models with 3, 7 and 72 billion parameters. This repo contains the instruction-tuned 3B Qwen2.5-VL model. For more information, visit our [Blog](https://qwenlm.github.io/blog/qwen2.5-vl/) and [GitHub](https://github.com/QwenLM/Qwen2.5-VL).\n\n\n\n## Evaluation\n\n### Image benchmark\n\n| Benchmark | InternVL2.5-4B |Qwen2-VL-7B |Qwen2.5-VL-3B |\n| :--- | :---:  | :---: | :---: |\n| MMMU<sub>val</sub>  | 52.3 | 54.1 | 53.1| \n| MMMU-Pro<sub>val</sub>  | **32.7** | 30.5 | 31.6|\n| AI2D<sub>test</sub> | 81.4 | **83.0** | 81.5 |\n| DocVQA<sub>test</sub>  | 91.6 | 94.5 | **93.9** | \n| InfoVQA<sub>test</sub>  | 72.1 | 76.5 | **77.1** |\n| TextVQA<sub>val</sub>  | 76.8 | **84.3** | 79.3|\n| MMBench-V1.1<sub>test</sub>  | 79.3 | **80.7** | 77.6 | \n| MMStar | 58.3 | **60.7** | 55.9 | \n| MathVista<sub>testmini</sub>  | 60.5 | 58.2 | **62.3** |\n| MathVision<sub>full</sub>  | 20.9 | 16.3  | **21.2** |\n\n\n### Video benchmark\n| Benchmark | InternVL2.5-4B | Qwen2-VL-7B | Qwen2.5-VL-3B |\n| :--- | :---:  | :---: | :---: |\n| MVBench | 71.6 | 67.0 | 67.0 |\n| VideoMME | 63.6/62.3 | 69.0/63.3 | 67.6/61.5 |\n| MLVU | 48.3 | - | 68.2 |\n| LVBench | - | - | 43.3 |\n| MMBench-Video | 1.73 | 1.44 | 1.63 |\n| EgoSchema | - | - | 64.8 |\n| PerceptionTest | - | - | 66.9 |\n| TempCompass | - | - | 64.4 |\n| LongVideoBench | 55.2 | 55.6 | 54.2 |\n| CharadesSTA/mIoU | - | - | 38.8 |\n\n\n### Agent benchmark\n| Benchmarks              | Qwen2.5-VL-3B |\n|-------------------------|---------------|\n| ScreenSpot              |     55.5    |\n| ScreenSpot Pro          |     23.9    |\n| AITZ_EM                 |  	76.9    |\n| Android Control High_EM |    	63.7    |\n| Android Control Low_EM  |  	22.2    |\n| AndroidWorld_SR         | 	90.8  	|\n| MobileMiniWob++_SR      | 	67.9    |\n\n## Requirements\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers accelerate\n```\nor you might encounter the following error:\n```\nKeyError: ''qwen2_5_vl''\n```\n\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen2.5-VL with ü§ñ ModelScope and ü§ó Transformers.\n\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers accelerate\n```\nor you might encounter the following error:\n```\nKeyError: ''qwen2_5_vl''\n```\n\n\nWe offer a toolkit to help you handle various types of visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved images and videos. You can install it using the following command:\n\n```bash\n# It''s highly recommanded to use `[decord]` feature for faster video loading.\npip install qwen-vl-utils[decord]==0.0.8\n```\n\nIf you are not using Linux, you might not be able to install `decord` from PyPI. In that case, you can use `pip install qwen-vl-utils` which will fall back to using torchvision for video processing. However, you can still [install decord from source](https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source) to get decord used when loading video.\n\n### Using ü§ó  Transformers to Chat\n\nHere we show a code snippet to show you how to use the chat model with `transformers` and `qwen_vl_utils`:\n\n```python\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    "Qwen/Qwen2.5-VL-3B-Instruct", torch_dtype="auto", device_map="auto"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n#     "Qwen/Qwen2.5-VL-3B-Instruct",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation="flash_attention_2",\n#     device_map="auto",\n# )\n\n# default processer\nprocessor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-3B-Instruct")\n\n# The default range for the number of visual tokens per image in the model is 4-16384.\n# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-3B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels)\n\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "image",\n                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",\n            },\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors="pt",\n)\ninputs = inputs.to("cuda")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n<details>\n<summary>Multi image inference</summary>\n\n```python\n# Messages containing multiple images and a text query\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "file:///path/to/image1.jpg"},\n            {"type": "image", "image": "file:///path/to/image2.jpg"},\n            {"type": "text", "text": "Identify the similarities between these images."},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors="pt",\n)\ninputs = inputs.to("cuda")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n</details>\n\n<details>\n<summary>Video inference</summary>\n\n```python\n# Messages containing a images list as a video and a text query\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "video",\n                "video": [\n                    "file:///path/to/frame1.jpg",\n                    "file:///path/to/frame2.jpg",\n                    "file:///path/to/frame3.jpg",\n                    "file:///path/to/frame4.jpg",\n                ],\n            },\n            {"type": "text", "text": "Describe this video."},\n        ],\n    }\n]\n\n# Messages containing a local video path and a text query\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "video",\n                "video": "file:///path/to/video1.mp4",\n                "max_pixels": 360 * 420,\n                "fps": 1.0,\n            },\n            {"type": "text", "text": "Describe this video."},\n        ],\n    }\n]\n\n# Messages containing a video url and a text query\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "video",\n                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",\n            },\n            {"type": "text", "text": "Describe this video."},\n        ],\n    }\n]\n\n#In Qwen 2.5 VL, frame rate information is also input into the model to align with absolute time.\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    fps=fps,\n    padding=True,\n    return_tensors="pt",\n    **video_kwargs,\n)\ninputs = inputs.to("cuda")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\nVideo URL compatibility largely depends on the third-party library version. The details are in the table below. change the backend by `FORCE_QWENVL_VIDEO_READER=torchvision` or `FORCE_QWENVL_VIDEO_READER=decord` if you prefer not to use the default one.\n\n| Backend     | HTTP | HTTPS |\n|-------------|------|-------|\n| torchvision >= 0.19.0 | ‚úÖ  | ‚úÖ   |\n| torchvision < 0.19.0  | ‚ùå  | ‚ùå   |\n| decord      | ‚úÖ  | ‚ùå   |\n</details>\n\n<details>\n<summary>Batch inference</summary>\n\n```python\n# Sample messages for batch inference\nmessages1 = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "file:///path/to/image1.jpg"},\n            {"type": "image", "image": "file:///path/to/image2.jpg"},\n            {"type": "text", "text": "What are the common elements in these pictures?"},\n        ],\n    }\n]\nmessages2 = [\n    {"role": "system", "content": "You are a helpful assistant."},\n    {"role": "user", "content": "Who are you?"},\n]\n# Combine messages for batch processing\nmessages = [messages1, messages2]\n\n# Preparation for batch inference\ntexts = [\n    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n    for msg in messages\n]\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=texts,\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors="pt",\n)\ninputs = inputs.to("cuda")\n\n# Batch Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_texts = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_texts)\n```\n</details>\n\n### ü§ñ ModelScope\nWe strongly advise users especially those in mainland China to use ModelScope. `snapshot_download` can help you solve issues concerning downloading checkpoints.\n\n\n### More Usage Tips\n\nFor input images, we support local files, base64, and URLs. For videos, we currently only support local files.\n\n```python\n# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.\n## Local file path\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "file:///path/to/your/image.jpg"},\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n## Image URL\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "http://path/to/your/image.jpg"},\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n## Base64 encoded image\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "data:image;base64,/9j/..."},\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n```\n#### Image Resolution for performance boost\n\nThe model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs, such as a token count range of 256-1280, to balance speed and memory usage.\n\n```python\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n    "Qwen/Qwen2.5-VL-3B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels\n)\n```\n\nBesides, We provide two methods for fine-grained control over the image size input to the model:\n\n1. Define min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels.\n   \n2. Specify exact dimensions: Directly set `resized_height` and `resized_width`. These values will be rounded to the nearest multiple of 28.\n\n```python\n# min_pixels and max_pixels\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "image",\n                "image": "file:///path/to/your/image.jpg",\n                "resized_height": 280,\n                "resized_width": 420,\n            },\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n# resized_height and resized_width\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "image",\n                "image": "file:///path/to/your/image.jpg",\n                "min_pixels": 50176,\n                "max_pixels": 50176,\n            },\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n\n```\n{\n	...,\n    "type": "yarn",\n    "mrope_section": [\n        16,\n        24,\n        24\n    ],\n    "factor": 4,\n    "original_max_position_embeddings": 32768\n}\n```\n\nHowever, it should be noted that this method has a significant impact on the performance of temporal and spatial localization tasks, and is therefore not recommended for use.\n\nAt the same time, for long video inputs, since MRoPE itself is more economical with ids, the max_position_embeddings can be directly modified to a larger value, such as 64k.\n\n\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5-VL,\n    title = {Qwen2.5-VL},\n    url = {https://qwenlm.github.io/blog/qwen2.5-vl/},\n    author = {Qwen Team},\n    month = {January},\n    year = {2025}\n}\n\n@article{Qwen2VL,\n  title={Qwen2-VL: Enhancing Vision-Language Model''s Perception of the World at Any Resolution},\n  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\n  journal={arXiv preprint arXiv:2409.12191},\n  year={2024}\n}\n\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```\n', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":3754622976,"storage_bytes":7509337976,"files_count":14,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2_5_VLForConditionalGeneration"],"model_type":"qwen2_5_vl","processor_config":{"chat_template":"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message[''role''] != ''system'' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message[''role''] }}\n{% if message[''content''] is string %}{{ message[''content''] }}<|im_end|>\n{% else %}{% for content in message[''content''] %}{% if content[''type''] == ''image'' or ''image'' in content or ''image_url'' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content[''type''] == ''video'' or ''video'' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif ''text'' in content %}{{ content[''text''] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"},"tokenizer_config":{"bos_token":null,"chat_template":"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message[''role''] != ''system'' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message[''role''] }}\n{% if message[''content''] is string %}{{ message[''content''] }}<|im_end|>\n{% else %}{% for content in message[''content''] %}{% if content[''type''] == ''image'' or ''image'' in content or ''image_url'' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content[''type''] == ''video'' or ''video'' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif ''text'' in content %}{{ content[''text''] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen2.5-VL","source_url":"https://github.com/QwenLM/Qwen2.5-VL"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:dmlc:decord","source_url":"https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2409.12191","source_url":"https://arxiv.org/abs/2409.12191"},{"type":"based_on_paper","target_id":"arxiv:2308.12966","source_url":"https://arxiv.org/abs/2308.12966"}]', NULL, NULL, 'pending', 67.6, '6e2dd9e5e50a2e2c613a779533661cd7', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen2.5-VL-72B-Instruct', 'huggingface--qwen--qwen2.5-vl-72b-instruct', 'Qwen2.5-VL-72B-Instruct', 'Qwen', '--- license: other license_name: qwen license_link: https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct/blob/main/LICENSE language: - en pipeline_tag: image-text-to-text tags: - multimodal library_name: transformers --- <a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> In the past five months since Qwen2-VL‚Äôs relea...', '["transformers","safetensors","qwen2_5_vl","image-to-text","multimodal","image-text-to-text","conversational","en","arxiv:2309.00071","arxiv:2409.12191","arxiv:2308.12966","license:other","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'image-text-to-text', 569, 135334, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '\n---\nlicense: other\nlicense_name: qwen\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: image-text-to-text\ntags:\n- multimodal\nlibrary_name: transformers\n---\n\n# Qwen2.5-VL-72B-Instruct\n<a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Introduction\n\nIn the past five months since Qwen2-VL‚Äôs release, numerous developers have built new models on the Qwen2-VL vision-language models, providing us with valuable feedback. During this period, we focused on building more useful vision-language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5-VL.\n\n#### Key Enhancements:\n* **Understand things visually**: Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images.\n\n* **Being agentic**: Qwen2.5-VL directly plays as a visual agent that can reason and dynamically direct tools, which is capable of computer use and phone use.\n\n* **Understanding long videos and capturing events**: Qwen2.5-VL can comprehend videos of over 1 hour, and this time it has a new ability of cpaturing event by pinpointing the relevant video segments.\n\n* **Capable of visual localization in different formats**: Qwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes.\n\n* **Generating structured outputs**: for data like scans of invoices, forms, tables, etc. Qwen2.5-VL supports structured outputs of their contents, benefiting usages in finance, commerce, etc.\n\n\n#### Model Architecture Updates:\n\n* **Dynamic Resolution and Frame Rate Training for Video Understanding**:\n\nWe extend dynamic resolution to the temporal dimension by adopting dynamic FPS sampling, enabling the model to comprehend videos at various sampling rates. Accordingly, we update mRoPE in the time dimension with IDs and absolute time alignment, enabling the model to learn temporal sequence and speed, and ultimately acquire the ability to pinpoint specific moments.\n\n<p align="center">\n    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL/qwen2.5vl_arc.jpeg" width="80%"/>\n<p>\n\n* **Streamlined and Efficient Vision Encoder**\n\nWe enhance both training and inference speeds by strategically implementing window attention into the ViT. The ViT architecture is further optimized with SwiGLU and RMSNorm, aligning it with the structure of the Qwen2.5 LLM.\n\n\nWe have three models with 3, 7 and 72 billion parameters. This repo contains the instruction-tuned 72B Qwen2.5-VL model. For more information, visit our [Blog](https://qwenlm.github.io/blog/qwen2.5-vl/) and [GitHub](https://github.com/QwenLM/Qwen2.5-VL).\n\n\n\n## Evaluation\n\n### Image benchmark\n\n|      Benchmarks        | GPT4o     | Claude3.5 Sonnet  | Gemini-2-flash  | InternVL2.5-78B | Qwen2-VL-72B | Qwen2.5-VL-72B |\n|-----------------------|-----------|-------------------|-----------------|-----------------|--------------|----------------|\n| MMMU<sub>val</sub>    | 70.3      | 70.4              | 70.7            | 70.1                 | 64.5         | 70.2          |\n| MMMU_Pro              | 54.5      | 54.7              | 57.0            | 48.6              | 46.2         | 51.1           |\n| MathVista_MINI        | 63.8      | 65.4              | 73.1            | 76.6                | 70.5         | 74.8           |\n| MathVision_FULL       | 30.4      | 38.3              | 41.3            | 32.2               | 25.9         | 38.1           |\n| Hallusion Bench       | 55.0      | 55.16             |                | 57.4             | 58.1         | 55.16           |\n| MMBench_DEV_EN_V11    | 82.1      | 83.4              | 83.0            | 88.5             | 86.6         | 88           |\n| AI2D_TEST             | 84.6      | 81.2              |                 | 89.1           | 88.1         | 88.4           |\n| ChartQA_TEST          | 86.7      | 90.8              | 85.2            | 88.3               | 88.3         | 89.5           |\n| DocVQA_VAL            | 91.1      | 95.2              | 92.1            | 96.5             | 96.1         |      96.4      |\n| MMStar                | 64.7      | 65.1              | 69.4            | 69.5             | 68.3         |       70.8         |\n| MMVet_turbo           | 69.1      |  70.1              |                 | 72.3           | 74.0         |       76.19         |\n| OCRBench              | 736       | 788               |                 | 854               | 877          |         885       |\n| OCRBench-V2(en/zh)    |  46.5/32.3 |  45.2/39.6         | 51.9/43.1       | 45/46.2     | 47.8/46.1    | 61.5/63.7    |\n| CC-OCR                | 66.6     | 62.7              | 73.0            | 64.7          | 68.7       |79.8           |\n\n\n### Video benchmark\n| Benchmarks          | GPT4o | Gemini-1.5-Pro | InternVL2.5-78B | Qwen2VL-72B | Qwen2.5VL-72B |\n|---------------------|-------|----------------|-----------------|-------------|---------------|\n| VideoMME w/o sub.   | 71.9  | 75.0           | 72.1            | 71.2        | 73.3          |\n| VideoMME w sub.     | 77.2  | 81.3           | 74.0            | 77.8        | 79.1          |\n| MVBench             | 64.6  | 60.5           | 76.4            | 73.6        | 70.4          |\n| MMBench-Video       | 1.63  | 1.30           | 1.97            | 1.70        | 2.02          |\n| LVBench             | 30.8  | 33.1           | -               | 41.3        | 47.3          |\n| EgoSchema           | 72.2  | 71.2           | -               | 77.9        | 76.2          |\n| PerceptionTest_test | -     | -              | -               | 68.0        | 73.2          |\n| MLVU_M-Avg_dev      | 64.6  | -              | 75.7            |             | 74.6          |\n| TempCompass_overall | 73.8  | -              | -               |             | 74.8          |\n\n\n### Agent benchmark\n\n| Benchmarks              | GPT4o       | Gemini 2.0 | Claude | Aguvis-72B | Qwen2VL-72B | Qwen2.5VL-72B |\n|-------------------------|-------------|------------|--------|------------|-------------|---------------|\n| ScreenSpot              | 18.1        | 84.0       | 83.0   |            |             | 87.1          |\n| ScreenSpot Pro          |             |            | 17.1   |            | 1.6         | 43.6          |\n| AITZ_EM                 | 35.3        |            |        |            | 72.8        | 83.2          |\n| Android Control High_EM |             |            |        | 66.4       | 59.1        | 67.36         |\n| Android Control Low_EM  |             |            |        | 84.4       | 59.2        | 93.7          |\n| AndroidWorld_SR         | 34.5% (SoM) |            | 27.9%  | 26.1%      |             | 35%           |\n| MobileMiniWob++_SR      |             |            |        | 66%        |             | 68%           |\n| OSWorld                 |             |            | 14.90  | 10.26      |             | 8.83          |\n\n\n## Requirements\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers accelerate\n```\nor you might encounter the following error:\n```\nKeyError: ''qwen2_5_vl''\n```\n\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen2.5-VL with ü§ñ ModelScope and ü§ó Transformers.\n\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers accelerate\n```\nor you might encounter the following error:\n```\nKeyError: ''qwen2_5_vl''\n```\n\n\nWe offer a toolkit to help you handle various types of visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved images and videos. You can install it using the following command:\n\n```bash\n# It''s highly recommanded to use `[decord]` feature for faster video loading.\npip install qwen-vl-utils[decord]==0.0.8\n```\n\nIf you are not using Linux, you might not be able to install `decord` from PyPI. In that case, you can use `pip install qwen-vl-utils` which will fall back to using torchvision for video processing. However, you can still [install decord from source](https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source) to get decord used when loading video.\n\n### Using ü§ó  Transformers to Chat\n\nHere we show a code snippet to show you how to use the chat model with `transformers` and `qwen_vl_utils`:\n\n```python\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    "Qwen/Qwen2.5-VL-72B-Instruct", torch_dtype="auto", device_map="auto"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n#     "Qwen/Qwen2.5-VL-72B-Instruct",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation="flash_attention_2",\n#     device_map="auto",\n# )\n\n# default processer\nprocessor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-72B-Instruct")\n\n# The default range for the number of visual tokens per image in the model is 4-16384.\n# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-72B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels)\n\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "image",\n                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",\n            },\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors="pt",\n)\ninputs = inputs.to("cuda")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n<details>\n<summary>Multi image inference</summary>\n\n```python\n# Messages containing multiple images and a text query\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "file:///path/to/image1.jpg"},\n            {"type": "image", "image": "file:///path/to/image2.jpg"},\n            {"type": "text", "text": "Identify the similarities between these images."},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors="pt",\n)\ninputs = inputs.to("cuda")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n</details>\n\n<details>\n<summary>Video inference</summary>\n\n```python\n# Messages containing a images list as a video and a text query\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "video",\n                "video": [\n                    "file:///path/to/frame1.jpg",\n                    "file:///path/to/frame2.jpg",\n                    "file:///path/to/frame3.jpg",\n                    "file:///path/to/frame4.jpg",\n                ],\n            },\n            {"type": "text", "text": "Describe this video."},\n        ],\n    }\n]\n\n# Messages containing a local video path and a text query\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "video",\n                "video": "file:///path/to/video1.mp4",\n                "max_pixels": 360 * 420,\n                "fps": 1.0,\n            },\n            {"type": "text", "text": "Describe this video."},\n        ],\n    }\n]\n\n# Messages containing a video url and a text query\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "video",\n                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",\n            },\n            {"type": "text", "text": "Describe this video."},\n        ],\n    }\n]\n\n#In Qwen 2.5 VL, frame rate information is also input into the model to align with absolute time.\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    fps=fps,\n    padding=True,\n    return_tensors="pt",\n    **video_kwargs,\n)\ninputs = inputs.to("cuda")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\nVideo URL compatibility largely depends on the third-party library version. The details are in the table below. change the backend by `FORCE_QWENVL_VIDEO_READER=torchvision` or `FORCE_QWENVL_VIDEO_READER=decord` if you prefer not to use the default one.\n\n| Backend     | HTTP | HTTPS |\n|-------------|------|-------|\n| torchvision >= 0.19.0 | ‚úÖ  | ‚úÖ   |\n| torchvision < 0.19.0  | ‚ùå  | ‚ùå   |\n| decord      | ‚úÖ  | ‚ùå   |\n</details>\n\n<details>\n<summary>Batch inference</summary>\n\n```python\n# Sample messages for batch inference\nmessages1 = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "file:///path/to/image1.jpg"},\n            {"type": "image", "image": "file:///path/to/image2.jpg"},\n            {"type": "text", "text": "What are the common elements in these pictures?"},\n        ],\n    }\n]\nmessages2 = [\n    {"role": "system", "content": "You are a helpful assistant."},\n    {"role": "user", "content": "Who are you?"},\n]\n# Combine messages for batch processing\nmessages = [messages1, messages2]\n\n# Preparation for batch inference\ntexts = [\n    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n    for msg in messages\n]\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=texts,\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors="pt",\n)\ninputs = inputs.to("cuda")\n\n# Batch Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_texts = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_texts)\n```\n</details>\n\n### ü§ñ ModelScope\nWe strongly advise users especially those in mainland China to use ModelScope. `snapshot_download` can help you solve issues concerning downloading checkpoints.\n\n\n### More Usage Tips\n\nFor input images, we support local files, base64, and URLs. For videos, we currently only support local files.\n\n```python\n# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.\n## Local file path\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "file:///path/to/your/image.jpg"},\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n## Image URL\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "http://path/to/your/image.jpg"},\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n## Base64 encoded image\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "data:image;base64,/9j/..."},\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n```\n#### Image Resolution for performance boost\n\nThe model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs, such as a token count range of 256-1280, to balance speed and memory usage.\n\n```python\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n    "Qwen/Qwen2.5-VL-72B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels\n)\n```\n\nBesides, We provide two methods for fine-grained control over the image size input to the model:\n\n1. Define min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels.\n   \n2. Specify exact dimensions: Directly set `resized_height` and `resized_width`. These values will be rounded to the nearest multiple of 28.\n\n```python\n# min_pixels and max_pixels\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "image",\n                "image": "file:///path/to/your/image.jpg",\n                "resized_height": 280,\n                "resized_width": 420,\n            },\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n# resized_height and resized_width\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "image",\n                "image": "file:///path/to/your/image.jpg",\n                "min_pixels": 50176,\n                "max_pixels": 50176,\n            },\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n\n```json\n{\n	...,\n    "type": "yarn",\n    "mrope_section": [\n        16,\n        24,\n        24\n    ],\n    "factor": 4,\n    "original_max_position_embeddings": 32768\n}\n```\n\nHowever, it should be noted that this method has a significant impact on the performance of temporal and spatial localization tasks, and is therefore not recommended for use.\n\nAt the same time, for long video inputs, since MRoPE itself is more economical with ids, the max_position_embeddings can be directly modified to a larger value, such as 64k.\n\n\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5-VL,\n    title = {Qwen2.5-VL},\n    url = {https://qwenlm.github.io/blog/qwen2.5-vl/},\n    author = {Qwen Team},\n    month = {January},\n    year = {2025}\n}\n\n@article{Qwen2VL,\n  title={Qwen2-VL: Enhancing Vision-Language Model''s Perception of the World at Any Resolution},\n  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\n  journal={arXiv preprint arXiv:2409.12191},\n  year={2024}\n}\n\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```\n', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":73410777344,"storage_bytes":146821823583,"files_count":50,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2_5_VLForConditionalGeneration"],"model_type":"qwen2_5_vl","processor_config":{"chat_template":"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message[''role''] != ''system'' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message[''role''] }}\n{% if message[''content''] is string %}{{ message[''content''] }}<|im_end|>\n{% else %}{% for content in message[''content''] %}{% if content[''type''] == ''image'' or ''image'' in content or ''image_url'' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content[''type''] == ''video'' or ''video'' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif ''text'' in content %}{{ content[''text''] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"},"tokenizer_config":{"bos_token":null,"chat_template":"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message[''role''] != ''system'' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message[''role''] }}\n{% if message[''content''] is string %}{{ message[''content''] }}<|im_end|>\n{% else %}{% for content in message[''content''] %}{% if content[''type''] == ''image'' or ''image'' in content or ''image_url'' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content[''type''] == ''video'' or ''video'' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif ''text'' in content %}{{ content[''text''] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen2.5-VL","source_url":"https://github.com/QwenLM/Qwen2.5-VL"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:dmlc:decord","source_url":"https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2409.12191","source_url":"https://arxiv.org/abs/2409.12191"},{"type":"based_on_paper","target_id":"arxiv:2308.12966","source_url":"https://arxiv.org/abs/2308.12966"}]', NULL, 'Other', 'approved', 77.6, '99c98e784da12fdad3d0720fe721606c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nerijs-pixel-art-xl', 'huggingface--nerijs--pixel-art-xl', 'pixel-art-xl', 'nerijs', '--- license: creativeml-openrail-m tags: - text-to-image - stable-diffusion - lora - diffusers base_model: stabilityai/stable-diffusion-xl-base-1.0 instance_prompt: pixel art widget: - text: pixel art, a cute corgi, simple, flat colors --- !F1hS8XHXwAQrMEW.jpeg !F1hS489X0AE-PK5.jpeg Downscale 8 times to get pixel perfect images (use Nearest Neighbors) Use a fixed VAE to avoid artifacts (0.9 or fp16 fix) Use it with a LCM Lora! Use 8 steps and guidance scale of 1.5 1.2 Lora strength for the Pi...', '["diffusers","text-to-image","stable-diffusion","lora","license:creativeml-openrail-m","region:us"]', 'text-to-image', 568, 4708, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nerijs/pixel-art-xl","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n  - text-to-image\n  - stable-diffusion\n  - lora\n  - diffusers\nbase_model: stabilityai/stable-diffusion-xl-base-1.0\ninstance_prompt: pixel art\nwidget:\n  - text: pixel art, a cute corgi, simple, flat colors\n---\n# Pixel Art XL\n## Consider supporting further research on [Patreon](https://www.patreon.com/user?u=29466374) or [Twitter](https://twitter.com/nerijs)\n\n![F1hS8XHXwAQrMEW.jpeg](https://cdn-uploads.huggingface.co/production/uploads/6303f37c3926de1f7ec42d3e/SSOQ9lfB1PVhXVWJiL7Mx.jpeg)\n![F1hS489X0AE-PK5.jpeg](https://cdn-uploads.huggingface.co/production/uploads/6303f37c3926de1f7ec42d3e/tY19J3xWDlSY2hhTTHySc.jpeg)\n\n\nDownscale 8 times to get pixel perfect images (use Nearest Neighbors)\nUse a fixed VAE to avoid artifacts (0.9 or fp16 fix)\n\n### Need more performance?\nUse it with a LCM Lora!\n\nUse 8 steps and guidance scale of 1.5\n1.2 Lora strength for the Pixel Art XL works better\n\n```python\nfrom diffusers import DiffusionPipeline, LCMScheduler\nimport torch\n\nmodel_id = "stabilityai/stable-diffusion-xl-base-1.0"\nlcm_lora_id = "latent-consistency/lcm-lora-sdxl"\npipe = DiffusionPipeline.from_pretrained(model_id, variant="fp16")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\npipe.load_lora_weights(lcm_lora_id, adapter_name="lora")\npipe.load_lora_weights("./pixel-art-xl.safetensors", adapter_name="pixel")\n\npipe.set_adapters(["lora", "pixel"], adapter_weights=[1.0, 1.2])\npipe.to(device="cuda", dtype=torch.float16)\n\nprompt = "pixel, a cute corgi"\nnegative_prompt = "3d render, realistic"\n\nnum_images = 9\n\nfor i in range(num_images):\n    img = pipe(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=8,\n        guidance_scale=1.5,\n    ).images[0]\n    \n    img.save(f"lcm_lora_{i}.png")\n```\n\n### Tips:\nDon''t use refiner\n\nWorks great with only 1 text encoder\n\nNo style prompt required\n\nNo trigger keyword require\n\nWorks great with isometric and non-isometric\n\nWorks with 0.9 and 1.0\n\n#### Changelog\nv1: Initial release', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":170543052,"files_count":3,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'creativeml-openrail-m', 'approved', 62.6, 'cab9fcaaf900e659744f77ad0ea36286', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen2.5-Coder-7B-Instruct', 'huggingface--qwen--qwen2.5-coder-7b-instruct', 'Qwen2.5-Coder-7B-Instruct', 'Qwen', '--- license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE language: - en base_model: - Qwen/Qwen2.5-Coder-7B pipeline_tag: text-generation library_name: transformers tags: - code - codeqwen - chat - qwen - qwen-coder --- <a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/...', '["transformers","safetensors","qwen2","text-generation","code","codeqwen","chat","qwen","qwen-coder","conversational","en","arxiv:2409.12186","arxiv:2309.00071","arxiv:2407.10671","base_model:qwen/qwen2.5-coder-7b","base_model:finetune:qwen/qwen2.5-coder-7b","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 568, 663875, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE\nlanguage:\n- en\nbase_model:\n- Qwen/Qwen2.5-Coder-7B\npipeline_tag: text-generation\nlibrary_name: transformers\ntags:\n- code\n- codeqwen\n- chat\n- qwen\n- qwen-coder\n---\n\n\n# Qwen2.5-Coder-7B-Instruct\n<a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Introduction\n\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\n\n- Significantly improvements in **code generation**, **code reasoning** and **code fixing**. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.\n- A more comprehensive foundation for real-world applications such as **Code Agents**. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\n- **Long-context Support** up to 128K tokens.\n\n**This repo contains the instruction-tuned 7B Qwen2.5-Coder model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 7.61B\n- Number of Paramaters (Non-Embedding): 6.53B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 28 for Q and 4 for KV\n- Context Length: Full 131,072 tokens\n  - Please refer to [this section](#processing-long-texts) for detailed instructions on how to deploy Qwen2.5 for handling long texts.\n  \nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/), [GitHub](https://github.com/QwenLM/Qwen2.5-Coder), [Documentation](https://qwen.readthedocs.io/en/latest/), [Arxiv](https://arxiv.org/abs/2409.12186).\n\n## Requirements\n\nThe code of Qwen2.5-Coder has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: ''qwen2''\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen2.5-Coder-7B-Instruct"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = "write a quick sort algorithm."\nmessages = [\n    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n```json\n{\n  ...,\n  "rope_scaling": {\n    "factor": 4.0,\n    "original_max_position_embeddings": 32768,\n    "type": "yarn"\n  }\n}\n```\n\nFor deployment, we recommend using vLLM. \nPlease refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \nWe advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [üìë blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@article{hui2024qwen2,\n      title={Qwen2. 5-Coder Technical Report},\n      author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\n      journal={arXiv preprint arXiv:2409.12186},\n      year={2024}\n}\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7615616512,"storage_bytes":30462543728,"files_count":14,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- messages[0][''content''] }}\n    {%- else %}\n        {{- ''You are Qwen, created by Alibaba Cloud. You are a helpful assistant.'' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0][''content''] + ''<|im_end|>\\n'' }}\n    {%- else %}\n        {{- ''<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content %}\n            {{- ''\\n'' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen2.5-Coder","source_url":"https://github.com/QwenLM/Qwen2.5-Coder"},{"type":"based_on_paper","target_id":"arxiv:2409.12186","source_url":"https://arxiv.org/abs/2409.12186"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2407.10671","source_url":"https://arxiv.org/abs/2407.10671"}]', NULL, 'Apache-2.0', 'approved', 62.6, '135938a1f3d2efb2a2da64876a40f0b2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nvidia-Llama-3.1-Nemotron-70B-Instruct', 'huggingface--nvidia--llama-3.1-nemotron-70b-instruct', 'Llama-3.1-Nemotron-70B-Instruct', 'nvidia', '--- license: llama3.1 language: - en inference: false fine-tuning: false tags: - nvidia - llama3.1 datasets: - nvidia/HelpSteer2 base_model: meta-llama/Llama-3.1-70B-Instruct library_name: nemo --- Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries. This model reaches Arena Hard of 85.0, AlpacaEval 2 LC of 57.6 and GPT-4-Turbo MT-Bench of 8.98, which are known to be predictive of LMSys Chatbot Ar...', '["nemo","nvidia","llama3.1","en","dataset:nvidia/helpsteer2","arxiv:2410.01257","arxiv:2310.05344","arxiv:2311.09528","arxiv:2406.08673","base_model:meta-llama/llama-3.1-70b-instruct","license:llama3.1","region:us"]', 'other', 567, 69, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: llama3.1\nlanguage:\n- en\ninference: false\nfine-tuning: false\ntags:\n- nvidia\n- llama3.1\ndatasets:\n- nvidia/HelpSteer2\nbase_model: meta-llama/Llama-3.1-70B-Instruct\nlibrary_name: nemo\n---\n# Model Overview\n\n## Description:\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries.\n\n\nThis model reaches [Arena Hard](https://github.com/lmarena/arena-hard-auto) of 85.0, [AlpacaEval 2 LC](https://tatsu-lab.github.io/alpaca_eval/) of 57.6 and [GPT-4-Turbo MT-Bench](https://github.com/lm-sys/FastChat/pull/3158) of 8.98, which are known to be predictive of [LMSys Chatbot Arena Elo](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)\n\nAs of 1 Oct 2024, this model is #1 on all three automatic alignment benchmarks (verified tab for AlpacaEval 2 LC), edging out strong frontier models such as GPT-4o and Claude 3.5 Sonnet.\n\nAs of Oct 24th, 2024 the model has Elo Score of 1267(+-7), rank 9 and style controlled rank of 26 on [ChatBot Arena leaderboard](https://lmarena.ai/?leaderboard).\n\nThis model was trained using RLHF (specifically, REINFORCE), [Llama-3.1-Nemotron-70B-Reward](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward) and [HelpSteer2-Preference prompts](https://huggingface.co/datasets/nvidia/HelpSteer2) on a Llama-3.1-70B-Instruct model as the initial policy.\n\nIf you prefer to use the model in the HuggingFace Transformers codebase, we have done a model conversion format into [Llama-3.1-Nemotron-70B-Instruct-HF](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct-HF) .\n\nTry hosted inference for free at [build.nvidia.com](https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct) - it comes with an OpenAI-compatible API interface.\n\n\nSee details on our paper at [https://arxiv.org/abs/2410.01257](https://arxiv.org/abs/2410.01257) - as a preview, this model can correctly the question ```How many r in strawberry?``` without specialized prompting or additional reasoning tokens:\n\n```\nA sweet question!\nLet‚Äôs count the ‚ÄúR‚Äùs in ‚Äústrawberry‚Äù:\n1. S\n2. T\n3. R\n4. A\n5. W\n6. B\n7. E\n8. R\n9. R\n10. Y\nThere are **3 ‚ÄúR‚Äùs** in the word ‚Äústrawberry‚Äù.\n```\n\nNote: This model is a demonstration of our techniques for improving helpfulness in general-domain instruction following. It has not been tuned for performance in specialized domains such as math.\n\n## License\nYour use of this model is governed by the [NVIDIA Open Model License](https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf).\nAdditional Information: [Llama 3.1 Community License Agreement](https://www.llama.com/llama3_1/license/). Built with Llama.\n\n\n## Evaluation Metrics\n\nAs of 1 Oct 2024, Llama-3.1-Nemotron-70B-Instruct performs best on Arena Hard, AlpacaEval 2 LC (verified tab) and MT Bench (GPT-4-Turbo)\n\n | Model  | Arena Hard | AlpacaEval | MT-Bench | Mean Response Length |\n|:-----------------------------|:----------------|:-----|:----------|:-------|\n|Details | (95% CI) | 2 LC (SE) | (GPT-4-Turbo) | (# of Characters for MT-Bench)| \n| _**Llama-3.1-Nemotron-70B-Instruct**_ | **85.0** (-1.5, 1.5) | **57.6** (1.65) | **8.98** | 2199.8 | \n| Llama-3.1-70B-Instruct | 55.7 (-2.9, 2.7) | 38.1 (0.90)  | 8.22 | 1728.6 |\n| Llama-3.1-405B-Instruct | 69.3 (-2.4, 2.2) | 39.3 (1.43) | 8.49 | 1664.7 |\n| Claude-3-5-Sonnet-20240620 | 79.2 (-1.9, 1.7) | 52.4 (1.47) | 8.81 | 1619.9 |\n| GPT-4o-2024-05-13 | 79.3 (-2.1, 2.0) | 57.5 (1.47) | 8.74 | 1752.2 |\n         \n## Usage:\n\nWe demonstrate inference using NVIDIA NeMo Framework, which allows hassle-free model deployment based on [NVIDIA TRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), a highly optimized inference solution focussing on high throughput and low latency.\n\nPre-requisite: You would need at least a machine with 4 40GB or 2 80GB NVIDIA GPUs, and 150GB of free disk space. \n\n1. Please sign up to get **free and immediate** access to [NVIDIA NeMo Framework container](https://developer.nvidia.com/nemo-framework). If you don‚Äôt have an NVIDIA NGC account, you will be prompted to sign up for an account before proceeding.\n2. If you don‚Äôt have an NVIDIA NGC API key, sign into [NVIDIA NGC](https://ngc.nvidia.com/setup), selecting organization/team: ea-bignlp/ga-participants and click Generate API key. Save this key for the next step. Else, skip this step. \n3. On your machine, docker login to nvcr.io using\n   ```\n   docker login nvcr.io\n   Username: $oauthtoken\n   Password: <Your Saved NGC API Key>\n   ```\n4. Download the required container\n   ```\n   docker pull nvcr.io/nvidia/nemo:24.05.llama3.1\n   ```\n   \n5. Download the checkpoint\n   ```\n   git lfs install\n   git clone https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct\n   ```\n\n6. Run Docker container\n   (In addition, to use Llama3.1 tokenizer, you need to ```export HF_HOME=<YOUR_HF_HOME_CONTAINING_TOKEN_WITH_LLAMA3.1_70B_ACCESS>```)\n   ```\n   docker run --gpus all -it --rm --shm-size=150g -p 8000:8000 -v ${PWD}/Llama-3.1-Nemotron-70B-Instruct:/opt/checkpoints/Llama-3.1-Nemotron-70B-Instruct,${HF_HOME}:/hf_home -w /opt/NeMo nvcr.io/nvidia/nemo:24.05.llama3.1\n   ```\n   \n7. Within the container, start the server in the background. This step does both conversion of the nemo checkpoint to TRT-LLM and then deployment using TRT-LLM. For an explanation of each argument and advanced usage, please refer to [NeMo FW Deployment Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/deployment/llm/in_framework.html)\n   \n   ```\n   HF_HOME=/hf_home python scripts/deploy/nlp/deploy_inframework_triton.py --nemo_checkpoint /opt/checkpoints/Llama-3.1-Nemotron-70B-Instruct --model_type="llama" --triton_model_name nemotron --triton_http_address 0.0.0.0 --triton_port 8000 --num_gpus 2 --max_input_len 3072 --max_output_len 1024 --max_batch_size 1 &\n   ```\n   \n8. Once the server is ready (i.e. when you see this messages below), you are ready to launch your client code\n\n    ```\n    Started HTTPService at 0.0.0.0:8000\n    Started GRPCInferenceService at 0.0.0.0:8001\n    Started Metrics Service at 0.0.0.0:8002\n    ```\n\n    ```\n    python scripts/deploy/nlp/query_inframework.py -mn nemotron -p "How many r in strawberry?" -mol 1024\n    ```\n    \n \n\n## References(s):\n\n* [HelpSteer2-Preference](https://arxiv.org/abs/2410.01257)\n* [SteerLM method](https://arxiv.org/abs/2310.05344)\n* [HelpSteer](https://arxiv.org/abs/2311.09528)\n* [HelpSteer2](https://arxiv.org/abs/2406.08673)\n* [Introducing Llama 3.1: Our most capable models to date](https://ai.meta.com/blog/meta-llama-3-1/) \n* [Meta''s Llama 3.1 Webpage](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1) \n* [Meta''s Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md)\n\n\n## Model Architecture: \n**Architecture Type:** Transformer <br>\n**Network Architecture:** Llama 3.1 <br>\n\n## Input:\n**Input Type(s):** Text <br>\n**Input Format:** String <br>\n**Input Parameters:** One Dimensional (1D) <br>\n**Other Properties Related to Input:** Max of 128k tokens<br>\n\n## Output:\n**Output Type(s):** Text <br>\n**Output Format:** String <br>\n**Output Parameters:** One Dimensional (1D) <br>\n**Other Properties Related to Output:**  Max of 4k tokens <br>\n\n\n## Software Integration:\n**Supported Hardware Microarchitecture Compatibility:** <br>\n* NVIDIA Ampere <br>\n* NVIDIA Hopper <br>\n* NVIDIA Turing <br>\n**Supported Operating System(s):** Linux <br>\n\n## Model Version: \nv1.0\n\n# Training & Evaluation: \n\n* REINFORCE implemented in NeMo Aligner\n\n## Datasets:\n\n**Data Collection Method by dataset** <br>\n* [Hybrid: Human, Synthetic] <br>\n\n**Labeling Method by dataset** <br>\n* [Human] <br>\n\n**Link:** \n* [HelpSteer2](https://huggingface.co/datasets/nvidia/HelpSteer2)\n\n**Properties (Quantity, Dataset Descriptions, Sensor(s)):** <br>\n* 21, 362 prompt-responses built to make more models more aligned with human preference - specifically more helpful, factually-correct, coherent, and customizable based on complexity and verbosity.\n* 20, 324 prompt-responses used for training and 1, 038 used for validation.\n\n\n# Inference:\n**Engine:** [Triton](https://developer.nvidia.com/triton-inference-server) <br>\n**Test Hardware:** H100, A100 80GB, A100 40GB <br>\n\n\n## Ethical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards.  Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\nPlease report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\n## Citation\n\nIf you find this model useful, please cite the following works\n\n```bibtex\n@misc{wang2024helpsteer2preferencecomplementingratingspreferences,\n      title={HelpSteer2-Preference: Complementing Ratings with Preferences}, \n      author={Zhilin Wang and Alexander Bukharin and Olivier Delalleau and Daniel Egert and Gerald Shen and Jiaqi Zeng and Oleksii Kuchaiev and Yi Dong},\n      year={2024},\n      eprint={2410.01257},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2410.01257}, \n}\n```', '{"pipeline_tag":null,"library_name":"nemo","framework":"nemo","params":null,"storage_bytes":141130115749,"files_count":3711,"spaces_count":54,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:lmarena:arena-hard-auto","source_url":"https://github.com/lmarena/arena-hard-auto"},{"type":"has_code","target_id":"github:lm-sys:FastChat","source_url":"https://github.com/lm-sys/FastChat"},{"type":"has_code","target_id":"github:NVIDIA:TensorRT-LLM","source_url":"https://github.com/NVIDIA/TensorRT-LLM"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"based_on_paper","target_id":"arxiv:2410.01257","source_url":"https://arxiv.org/abs/2410.01257"},{"type":"based_on_paper","target_id":"arxiv:2310.05344","source_url":"https://arxiv.org/abs/2310.05344"},{"type":"based_on_paper","target_id":"arxiv:2311.09528","source_url":"https://arxiv.org/abs/2311.09528"},{"type":"based_on_paper","target_id":"arxiv:2406.08673","source_url":"https://arxiv.org/abs/2406.08673"}]', NULL, 'llama3.1', 'approved', 62.5, 'ad788aab833f2f9a96c70c06fd39f866', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-reducto-RolmOCR', 'huggingface--reducto--rolmocr', 'RolmOCR', 'reducto', '--- library_name: transformers license: apache-2.0 datasets: - allenai/olmOCR-mix-0225 base_model: Qwen/Qwen2.5-VL-7B-Instruct --- Earlier this year, the Allen Institute for AI released olmOCR, an open-source tool that performs document OCR using the Qwen2-VL-7B vision language model (VLM). We were excited to see a high-quality, openly available approach to parsing PDFs and other complex documents ‚Äî and curious to explore what else might be possible using newer foundation models and some ligh...', '["transformers","safetensors","qwen2_5_vl","image-to-text","dataset:allenai/olmocr-mix-0225","base_model:qwen/qwen2.5-vl-7b-instruct","base_model:finetune:qwen/qwen2.5-vl-7b-instruct","license:apache-2.0","text-generation-inference","endpoints_compatible","region:us"]', 'image-to-text', 567, 3843, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/reducto/RolmOCR","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlibrary_name: transformers\nlicense: apache-2.0\ndatasets:\n- allenai/olmOCR-mix-0225\nbase_model: Qwen/Qwen2.5-VL-7B-Instruct\n---\n\n# RolmOCR by [Reducto AI](https://reducto.ai/)\n\nEarlier this year, the [Allen Institute for AI](https://allenai.org/) released olmOCR, an open-source tool that performs document OCR using the Qwen2-VL-7B vision language model (VLM). We were excited to see a high-quality, openly available approach to parsing PDFs and other complex documents ‚Äî and curious to explore what else might be possible using newer foundation models and some lightweight optimizations.\n\nThe result is **RolmOCR**, a drop-in alternative to olmOCR that‚Äôs faster, uses less memory, and still performs well on a variety of document types. We''re releasing it under **Apache 2.0** for anyone to try out, explore, or build on.\n\nThis model is a fine-tuned version of [Qwen/Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) on the full [allenai/olmOCR-mix-0225](https://huggingface.co/datasets/allenai/olmOCR-mix-0225) dataset.\n\n## Key changes\nWe made three notable changes:¬†\n\n1. **New Base Model**: We swapped in a more recent version of the existing model (Qwen2.5-VL-7B) as the foundation.\n\n2. **No Metadata inputs**: Unlike the original, we don‚Äôt use metadata extracted from PDFs. This significantly reduces prompt length, which in turn lowers both processing time and VRAM usage ‚Äî without hurting accuracy in most cases.¬†\n\n3. **Rotation of training data:** About 15% of the training data was rotated to enhance robustness to off-angle documents. We otherwise use the same training set.¬†\n\n## Usage\n\nHost your model with vLLM:\n```bash\nexport VLLM_USE_V1=1\nvllm serve reducto/RolmOCR \n```\n\nCall the model via openai compatible server:\n```python\n# HOST YOUR OPENAI COMPATIBLE API WITH THE FOLLOWING COMMAND in VLLM:\n# export VLLM_USE_V1=1\n# vllm serve reducto/RolmOCR \n\nfrom openai import OpenAI\nimport base64\n\nclient = OpenAI(api_key="123", base_url="http://localhost:8000/v1")\n\nmodel = "reducto/RolmOCR-7b"\n\ndef encode_image(image_path):\n    with open(image_path, "rb") as image_file:\n        return base64.b64encode(image_file.read()).decode("utf-8")\n\ndef ocr_page_with_rolm(img_base64):\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\n                "role": "user",\n                "content": [\n                    {\n                        "type": "image_url",\n                        "image_url": {"url": f"data:image/png;base64,{img_base64}"},\n                    },\n                    {\n                        "type": "text",\n                        "text": "Return the plain text representation of this document as if you were reading it naturally.\n",\n                    },\n                ],\n            }\n        ],\n        temperature=0.2,\n        max_tokens=4096\n    )\n    return response.choices[0].message.content\n\ntest_img_path = "path/to/image.png"\nimg_base64 = encode_image(test_img_path)\nprint(ocr_page_with_rolm(img_base64))\n```\n\n## Limitations\n\n- RolmOCR, like other VLM-based OCR solutions, still suffer from hallucination or dropping contents.\n- Unlike the [Reducto Parsing API](https://app.reducto.ai/), RolmOCR cannot output layout bounding boxes.\n- We have not evaluated the performance of any quantized versions.\n\n## BibTex and citation info\n```\n@misc{RolmOCR,\n  author = {Reducto AI},\n  title = {RolmOCR: A Faster, Lighter Open Source OCR Model},\n  year = {2025},\n}\n```', '{"pipeline_tag":"image-to-text","library_name":"transformers","framework":"transformers","params":8292166656,"storage_bytes":16595836440,"files_count":17,"spaces_count":16,"gated":false,"private":false,"config":{"architectures":["Qwen2_5_VLForConditionalGeneration"],"model_type":"qwen2_5_vl","processor_config":{"chat_template":"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message[''role''] != ''system'' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message[''role''] }}\n{% if message[''content''] is string %}{{ message[''content''] }}<|im_end|>\n{% else %}{% for content in message[''content''] %}{% if content[''type''] == ''image'' or ''image'' in content or ''image_url'' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content[''type''] == ''video'' or ''video'' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif ''text'' in content %}{{ content[''text''] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"},"tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- messages[0][''content''] }}\n    {%- else %}\n        {{- ''You are a helpful assistant.'' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0][''content''] + ''<|im_end|>\\n'' }}\n    {%- else %}\n        {{- ''<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content %}\n            {{- ''\\n'' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[]', NULL, 'Apache-2.0', 'approved', 62.5, 'f25dcb024a7f19e7e497494eb33f655c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-microsoft-Phi-3.5-MoE-instruct', 'huggingface--microsoft--phi-3.5-moe-instruct', 'Phi-3.5-MoE-instruct', 'microsoft', '--- license: mit license_link: https://huggingface.co/microsoft/Phi-3.5-MoE-instruct/resolve/main/LICENSE language: - multilingual pipeline_tag: text-generation tags: - nlp - code widget: - messages: - role: user content: Can you provide ways to eat combinations of bananas and dragonfruits? library_name: transformers --- Phi-3.5-MoE is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available documents - with a focus on very...', '["transformers","safetensors","phimoe","text-generation","nlp","code","conversational","custom_code","multilingual","arxiv:2404.14219","arxiv:2407.13833","arxiv:2403.06412","license:mit","region:us"]', 'text-generation', 566, 108995, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/microsoft/Phi-3.5-MoE-instruct","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Phi-3.5-MoE-instruct/resolve/main/LICENSE\nlanguage:\n- multilingual\npipeline_tag: text-generation\ntags:\n- nlp\n- code\nwidget:\n- messages:\n  - role: user\n    content: Can you provide ways to eat combinations of bananas and dragonfruits?\nlibrary_name: transformers\n---\n\n## Model Summary\n\nPhi-3.5-MoE is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available documents - with a focus on very high-quality, reasoning dense data. The model supports multilingual and comes with 128K context length (in tokens). The model underwent a rigorous enhancement process, incorporating supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures. \n\nüè° [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\nüì∞ [Phi-3 Microsoft Blog](https://aka.ms/phi3.5-techblog) <br>\nüìñ [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219) <br>\nüë©‚Äçüç≥ [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\nüñ•Ô∏è [Try It](https://aka.ms/try-phi3.5moe) <br>\n\nMoE references:\nüìú[Phi-3.5-MoE Blog](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/announcing-the-availability-of-phi-3-5-moe-in-azure-ai-studio/ba-p/4256278) | üòÅ[GRIN MoE](https://huggingface.co/microsoft/GRIN-MoE)\n\n**Phi-3.5**: [[mini-instruct]](https://huggingface.co/microsoft/Phi-3.5-mini-instruct); [[MoE-instruct]](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct) ; [[vision-instruct]](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)\n\n## Intended Uses\n\n### Primary Use Cases\n\nThe model is intended for commercial and research use in multiple languages. The model provides uses for general purpose AI systems and applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. \n\n### Use Case Considerations\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\n\n***Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.*** \n\n## Usage\n\n### Requirements\nPhi-3.5-MoE-instruct is integrated in the official version of `transformers` starting from **4.46.0**. \nThe current `transformers` version can be verified with: `pip list | grep transformers`.\n\nExamples of required packages:\n```\nflash_attn==2.5.8\ntorch==2.3.1\naccelerate==0.31.0\ntransformers==4.46.0\n```\n\nPhi-3.5-MoE-instruct is also available in [Azure AI Studio](https://aka.ms/try-phi3.5moe)\n\n### Tokenizer\n\nPhi-3.5-MoE-Instruct supports a vocabulary size of up to `32064` tokens. The [tokenizer files](https://huggingface.co/microsoft/Phi-3.5-moe-instruct/blob/main/added_tokens.json) already provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model''s vocabulary size.\n\n### Input Formats\nGiven the nature of the training data, the Phi-3.5-MoE-instruct model is best suited for prompts using the chat format as follows:\n\n```\n<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nHow to explain Internet for a medieval knight?<|end|>\n<|assistant|>\n```\n\n### Loading the model locally\nAfter obtaining the Phi-3.5-MoE-instruct model checkpoints, users can use this sample code for inference.\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n\ntorch.random.manual_seed(0) \n\nmodel = AutoModelForCausalLM.from_pretrained( \n    "microsoft/Phi-3.5-MoE-instruct",  \n    device_map="cuda",  \n    torch_dtype="auto",  \n    trust_remote_code=False,  \n) \n\ntokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3.5-MoE-instruct") \n\nmessages = [ \n    {"role": "system", "content": "You are a helpful AI assistant."}, \n    {"role": "user", "content": "Can you provide ways to eat combinations of bananas and dragonfruits?"}, \n    {"role": "assistant", "content": "Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey."}, \n    {"role": "user", "content": "What about solving an 2x + 3 = 7 equation?"}, \n] \n\npipe = pipeline( \n    "text-generation", \n    model=model, \n    tokenizer=tokenizer, \n) \n\ngeneration_args = { \n    "max_new_tokens": 500, \n    "return_full_text": False, \n    "temperature": 0.0, \n    "do_sample": False, \n} \n\noutput = pipe(messages, **generation_args) \nprint(output[0][''generated_text''])\n```\n\n## Benchmarks\n\nTo understand the capabilities, we compare Phi-3.5-MoE with a set of models over a variety of benchmarks using our internal benchmark platform. At the high-level overview of the model quality on representative benchmarks:\n\n| Category | Benchmark | Phi-3.5-MoE-instruct | Mistral-Nemo-12B-instruct-2407 | Llama-3.1-8B-instruct | Gemma-2-9b-It | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|--|--|\n| Popular aggregated benchmark | Arena Hard | 37.9 | 39.4 | 25.7 | 42.0 | 55.2 | 75.0 |\n| | BigBench Hard CoT (0-shot) | 79.1 | 60.2 | 63.4 | 63.5 | 66.7 | 80.4 |\n| | MMLU (5-shot) | 78.9 | 67.2 | 68.1 | 71.3 | 78.7 | 77.2 |\n| | MMLU-Pro (0-shot, CoT) | 54.3 | 40.7 | 44.0 | 50.1 | 57.2 | 62.8 |\n| Reasoning | ARC Challenge (10-shot) | 91.0 | 84.8 | 83.1 | 89.8 | 92.8 | 93.5 |\n| | BoolQ (2-shot) | 84.6 | 82.5 | 82.8 | 85.7 | 85.8 | 88.7 |\n| | GPQA (0-shot, CoT) | 36.8 | 28.6 | 26.3 | 29.2 | 37.5 | 41.1 |\n| | HellaSwag (5-shot) | 83.8 | 76.7 | 73.5 | 80.9 | 67.5 | 87.1 |\n| | OpenBookQA (10-shot) | 89.6 | 84.4 | 84.8 | 89.6 | 89.0 | 90.0 |\n| | PIQA (5-shot) | 88.6 | 83.5 | 81.2 | 83.7 | 87.5 | 88.7 |\n| | Social IQA (5-shot) | 78.0 | 75.3 | 71.8 | 74.7 | 77.8 | 82.9 |\n| | TruthfulQA (MC2) (10-shot) | 77.5 | 68.1 | 69.2 | 76.6 | 76.6 | 78.2 |\n| | WinoGrande (5-shot) | 81.3 | 70.4 | 64.7 | 74.0 | 74.7 | 76.9 |\n| Multilingual | Multilingual MMLU (5-shot) | 69.9 | 58.9 | 56.2 | 63.8 | 77.2 | 72.9 |\n| | MGSM (0-shot CoT) | 58.7 | 63.3 | 56.7 | 75.1 | 75.8 | 81.7 |\n| Math | GSM8K (8-shot, CoT) | 88.7 | 84.2 | 82.4 | 84.9 | 82.4 | 91.3 |\n| | MATH (0-shot, CoT) | 59.5 | 31.2 | 47.6 | 50.9 | 38.0 | 70.2 |\n| Long context | Qasper | 40.0 | 30.7 | 37.2 | 13.9 | 43.5 | 39.8 |\n| | SQuALITY | 24.1 | 25.8 | 26.2 | 0.0 | 23.5 | 23.8 |\n| Code Generation | HumanEval (0-shot) | 70.7 | 63.4 | 66.5 | 61.0 | 74.4 | 86.6 |\n| | MBPP (3-shot) | 80.8 | 68.1 | 69.4 | 69.3 | 77.5 | 84.1 |\n| **Average** | | **69.2** | **61.3** | **61.0** | **63.3** | **68.5** | **74.9** |\n\nWe take a closer look at different categories across 80 public benchmark datasets at the table below:\n| Category | Phi-3.5-MoE-instruct | Mistral-Nemo-12B-instruct-2407 | Llama-3.1-8B-instruct | Gemma-2-9b-It | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|--|\n| Popular aggregated benchmark | 62.6 | 51.9 | 50.3 | 56.7 | 64.5 | 73.9 |\n| Reasoning | 78.7 | 72.2 | 70.5 | 75.4 | 77.7 | 80.0 |\n| Language understanding | 71.8 | 67.0 | 62.9 | 72.8 | 66.6 | 76.8 |\n| Robustness | 75.6 | 65.2 | 59.8 | 64.7 | 68.9 | 77.5 |\n| Long context | 25.5 | 24.5 | 25.5 | 0.0 | 27.0 | 25.4 |\n| Math | 74.1 | 57.7 | 65.0 | 67.9 | 60.2 | 80.8 |\n| Code generation | 68.3 | 56.9 | 65.8 | 58.3 | 66.8 | 69.9 |\n| Multilingual | 65.8 | 55.3 | 47.5 | 59.6 | 64.3 | 76.6 |\n\nOverall, Phi-3.5-MoE with only **6.6B active parameters** achieves a similar level of language understanding and math as much larger models. Moreover, the model outperforms bigger models in reasoning capability and only behind GPT-4o-mini. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much factual knowledge, therefore, users may experience factual incorrectness. However, we believe such weakness can be resolved by augmenting Phi-3.5 with a search engine, particularly when using the model under RAG settings.\n\n### Multilingual\n\nThe table below highlights multilingual capability of Phi-3.5-MoE on multilingual MMLU, MEGA, and multilingual MMLU-pro datasets. Overall, we observed that even with just 6.6B active parameters, the model is very competitive on multilingual tasks in comparison to other models with a much bigger active parameters.\n\n| Category | Phi-3.5-MoE-instruct | Mistral-Nemo-12B-instruct-2407 | Llama-3.1-8B-instruct | Gemma-2-9b-It | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|--|\n| Multilingual MMLU | 69.9 | 58.9 | 56.2 | 63.8 | 77.2 | 72.9 |\n| Multilingual MMLU-Pro | 45.3 | 34.0 | 21.4 | 43.0 | 57.9 | 53.2 |\n| MGSM | 58.7 | 63.3 | 56.7 | 75.1 | 75.8 | 81.7 |\n| MEGA MLQA | 65.3 | 61.2 | 45.2 | 54.4 | 61.6 | 70.0 |\n| MEGA TyDi QA | 67.1 | 63.7 | 54.5 | 65.6 | 63.6 | 81.8 |\n| MEGA UDPOS | 60.4 | 58.2 | 54.1 | 56.6 | 62.4 | 66.0 |\n| MEGA XCOPA | 76.6 | 10.8 | 21.1 | 31.2 | 95.0 | 90.3 |\n| MEGA XStoryCloze | 82.8 | 92.3 | 71.0 | 87.0 | 20.7 | 96.6 |\n| **Average** | **65.8** | **55.3** | **47.5** | **59.6** | **64.3** | **76.6** |\n\n### Long Context\n\nPhi-3.5-MoE supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA, multilingual context retrieval. We see that Phi-3.5 is clearly better than Gemma-2 family which only supports 8K context length. Phi-3.5-MoE-instruct is very competitive with other much larger open-weight models such as Llama-3.1-8B-instruct, and Mistral-Nemo-12B-instruct-2407.\n\n| Benchmark | Phi-3.5-MoE-instruct | Mistral-Nemo-12B-instruct-2407 | Llama-3.1-8B-instruct | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|\n| GovReport | 26.4 | 25.6 | 25.1 | 27.8 | 24.8 |\n| QMSum | 19.9 | 22.1 | 21.6 | 24.0 | 21.7 |\n| Qasper | 40.0 | 30.7 | 37.2 | 43.5 | 39.8 |\n| SQuALITY | 24.1 | 25.8 | 26.2 | 23.5 | 23.8 |\n| SummScreenFD | 16.9 | 18.2 | 17.6 | 16.3 | 17.0 |\n| **Average** | **25.5** | **24.5** | **25.5** | **27.0** | **25.4** |\n\nRULER: a retrieval-based benchmark for long context understanding\n| Model | 4K | 8K | 16K | 32K | 64K | 128K | Average |\n|--|--|--|--|--|--|--|--|\n| Phi-3.5-MoE-instruct | 94.8 | 93 | 93.2 | 91.6 | 85.7 | 64.2 | **87.1** |\n| Llama-3.1-8B-instruct | 95.5 | 93.8 | 91.6 | 87.4 | 84.7 | 77.0 | **88.3** |\n| Mistral-Nemo-12B-instruct-2407 | 87.8 | 87.2 | 87.7 | 69.0 | 46.8 | 19.0 | **66.2** |\n\nRepoQA: a benchmark for long context code understanding\n| Model | Python | C++ | Rust | Java | TypeScript | Average |\n|--|--|--|--|--|--|--|\n| Phi-3.5-MoE-instruct | 89 | 74 | 81 | 88 | 95 | **85** |\n| Llama-3.1-8B-instruct | 80 | 65 | 73 | 76 | 63 | **71** |\n| Mistral-7B-instruct-v0.3 | 61 | 57 | 51 | 61 | 80 | **62** |\n\n## Training\n\n### Model\n\n**Architecture:** Phi-3.5-MoE has 16x3.8B parameters with **6.6B active parameters** when using 2 experts. The model is a mixture-of-expert decoder-only Transformer model using the tokenizer with vocabulary size of 32,064.<br>\n**Inputs:** Text. It is best suited for prompts using chat format.<br>\n**Context length:** 128K tokens<br>\n**GPUs:** 512 H100-80G<br>\n**Training time:** 23 days<br>\n**Training data:** 4.9T tokens<br>\n**Outputs:** Generated text in response to the input<br>\n**Dates:** Trained between April and August 2024<br>\n**Status:** This is a static model trained on an offline dataset with cutoff date October 2023 for publicly available data. Future versions of the tuned models may be released as we improve models.<br>\n**Supported languages:** Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian<br>\n**Release date:** August 2024<br>\n\n### Training Datasets\nOur training data includes a wide variety of sources, totaling 4.9 trillion tokens (including 10% multilingual), and is a combination of \n1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code;\n2) newly created synthetic, ‚Äútextbook-like‚Äù data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);\n3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. \n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://arxiv.org/pdf/2404.14219).\n\n## Responsible AI Considerations\n\nLike other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:  \n* Quality of Service: The Phi models are trained primarily on English text and some additional multilingual text. Languages other than English will experience worse performance as well as performance disparities across non-English. English language varieties with less representation in the training data might experience worse performance than standard American English.\n* Multilingual performance and safety gaps: We believe it is important to make language models more widely available across different languages, but the Phi 3 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards.\n* Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.\n* Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.\n* Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.\n* Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as "typing, math, random, collections, datetime, itertools". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n* Long Conversation: Phi-3 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift\n\nDevelopers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. Phi-3 family of models are general purpose models. As developers plan to deploy these models for specific use cases, they are encouraged to fine-tune the models for their use case and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include: \n* Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n* High-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\n* Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\n* Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\n* Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n## Safety Evaluation and Red-Teaming\n\nWe leveraged various evaluation techniques including red teaming, adversarial conversation simulations, and multilingual safety evaluation benchmark datasets to \nevaluate Phi-3.5 models'' propensity to produce undesirable outputs across multiple languages and risk categories. \nSeveral approaches were used to compensate for the limitations of one approach alone. Findings across the various evaluation methods indicate that safety \npost-training that was done as detailed in the [Phi-3 Safety Post-Training paper](https://arxiv.org/pdf/2407.13833) had a positive impact across multiple languages and risk categories as observed by \nrefusal rates (refusal to output undesirable outputs) and robustness to jailbreak techniques. Note, however, while comprehensive red team evaluations were conducted \nacross all models in the prior release of Phi models, red teaming was largely focused on Phi-3.5 MOE across multiple languages and risk categories for this release as \nit is the largest and more capable model of the three models. Details on prior red team evaluations across Phi models can be found in the [Phi-3 Safety Post-Training paper](https://arxiv.org/pdf/2407.13833). \nFor this release, insights from red teaming indicate that the models may refuse to generate undesirable outputs in English, even when the request for undesirable output \nis in another language. Models may also be more susceptible to longer multi-turn jailbreak techniques across both English and non-English languages. These findings \nhighlight the need for industry-wide investment in the development of high-quality safety evaluation datasets across multiple languages, including low resource languages, \nand risk areas that account for cultural nuances where those languages are spoken.\n\n## Software\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [Transformers](https://github.com/huggingface/transformers)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n## Hardware\nNote that by default, the Phi-3.5-MoE-instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\n* NVIDIA A100\n* NVIDIA A6000\n* NVIDIA H100\n  \n## License\nThe model is licensed under the [MIT license](./LICENSE).\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow‚ÄØ[Microsoft‚Äôs Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party‚Äôs policies.\n\n\n## Appendix A: Korean benchmarks\n\nThe prompt is the same as the [CLIcK paper](https://arxiv.org/abs/2403.06412) prompt. The experimental results below were given with max_tokens=512 (zero-shot), max_tokens=1024 (5-shot), temperature=0.01. No system prompt used.\n\n- GPT-4o: 2024-05-13 version\n- GPT-4o-mini: 2024-07-18 version\n- GPT-4-turbo: 2024-04-09 version\n- GPT-3.5-turbo: 2023-06-13 version\n\nOverall, the Phi-3.5 MoE model with just 6.6B active params outperforms GPT-3.5-Turbo.\n\n| Benchmarks               |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:-------------------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| CLIcK                    |                  56.44 |                           29.12 |                   47.82 |    80.46 |         68.5  |         72.82 |           50.98 |\n| HAERAE 1.0               |                  61.83 |                           36.41 |                   53.9  |    85.7  |         76.4  |         77.76 |           52.67 |\n| KMMLU (0-shot, CoT)      |                  47.43 |                           30.82 |                   38.54 |    64.26 |         52.63 |         58.75 |           40.3  |\n| KMMLU (5-shot)           |                  47.92 |                           29.98 |                   20.21 |    64.28 |         51.62 |         59.29 |           42.28 |\n| KMMLU-HARD (0-shot, CoT) |                  25.34 |                           25.68 |                   24.03 |    39.62 |         24.56 |         30.56 |           20.97 |\n| KMMLU-HARD (5-shot)      |                  25.66 |                           25.73 |                   15.81 |    40.94 |         24.63 |         31.12 |           21.19 |\n| **Average**              |          **45.82** |                **29.99** |            **29.29** |    **62.54** |         **50.08** |         **56.74** |           **39.61** |\n\n#### CLIcK (Cultural and Linguistic Intelligence in Korean)\n\n##### Accuracy by supercategory\n| supercategory   |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Culture         |                  58.44 |                           29.74 |                   51.15 |    81.89 |         70.95 |         73.61 |           53.38 |\n| Language        |                  52.31 |                           27.85 |                   40.92 |    77.54 |         63.54 |         71.23 |           46    |\n| **Overall**     |                  56.44 |                           29.12 |                   47.82 |    80.46 |         68.5  |         72.82 |           50.98 |\n\n##### Accuracy by category\n| supercategory   | category    |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|:------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Culture         | Economy     |                  77.97 |                           28.81 |                   66.1  |    94.92 |         83.05 |         89.83 |           64.41 |\n| Culture         | Geography   |                  60.31 |                           29.01 |                   54.2  |    80.15 |         77.86 |         82.44 |           53.44 |\n| Culture         | History     |                  33.93 |                           30    |                   29.64 |    66.92 |         48.4  |         46.4  |           31.79 |\n| Culture         | Law         |                  52.51 |                           22.83 |                   44.29 |    70.78 |         57.53 |         61.19 |           41.55 |\n| Culture         | Politics    |                  70.24 |                           33.33 |                   59.52 |    88.1  |         83.33 |         89.29 |           65.48 |\n| Culture         | Pop Culture |                  80.49 |                           34.15 |                   60.98 |    97.56 |         85.37 |         92.68 |           75.61 |\n| Culture         | Society     |                  74.43 |                           31.72 |                   65.05 |    92.88 |         85.44 |         86.73 |           71.2  |\n| Culture         | Tradition   |                  58.11 |                           31.98 |                   54.95 |    87.39 |         74.77 |         79.28 |           55.86 |\n| Language        | Functional  |                  48    |                           24    |                   32.8  |    84.8  |         64.8  |         80    |           40    |\n| Language        | Grammar     |                  29.58 |                           23.33 |                   22.92 |    57.08 |         42.5  |         47.5  |           30    |\n| Language        | Textual     |                  73.33 |                           33.33 |                   59.65 |    91.58 |         80.7  |         87.37 |           62.11 |\n\n#### HAERAE 1.0\n\n| category              |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| General Knowledge     |                  39.77 |                           28.41 |                   34.66 |    77.27 |         53.41 |         66.48 |           40.91 |\n| History               |                  60.64 |                           22.34 |                   44.15 |    92.02 |         84.57 |         78.72 |           30.32 |\n| Loan Words            |                  70.41 |                           35.5  |                   63.31 |    79.88 |         76.33 |         78.11 |           59.17 |\n| Rare Words            |                  63.95 |                           42.96 |                   63.21 |    87.9  |         81.98 |         79.01 |           61.23 |\n| Reading Comprehension |                  64.43 |                           41.16 |                   51.9  |    85.46 |         77.18 |         80.09 |           56.15 |\n| Standard Nomenclature |                  66.01 |                           32.68 |                   58.82 |    88.89 |         75.82 |         79.08 |           53.59 |\n| **Overall**           |                  61.83 |                           36.41 |                   53.9  |    85.7  |         76.4  |         77.76 |           52.67 |\n\n#### KMMLU (0-shot, CoT)\n\n| supercategory   |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                  45.15 |                           31.68 |                   37.03 |    61.52 |         49.29 |         55.98 |           38.47 |\n| HUMSS           |                  49.75 |                           26.47 |                   37.29 |    69.45 |         56.59 |         63    |           40.9  |\n| Other           |                  47.24 |                           31.01 |                   39.15 |    63.79 |         52.35 |         57.53 |           40.19 |\n| STEM            |                  49.08 |                           31.9  |                   40.42 |    65.16 |         54.74 |         60.84 |           42.24 |\n| **Overall**     |                  47.43 |                           30.82 |                   38.54 |    64.26 |         52.63 |         58.75 |           40.3  |\n\n#### KMMLU (5-shot)\n\n| supercategory   |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                  45.9  |                           29.98 |                   19.24 |    61.47 |         48.66 |         56.85 |           40.22 |\n| HUMSS           |                  49.18 |                           27.27 |                   22.5  |    68.79 |         55.95 |         63.68 |           43.35 |\n| Other           |                  48.43 |                           30.76 |                   20.95 |    64.21 |         51.1  |         57.85 |           41.92 |\n| STEM            |                  49.21 |                           30.73 |                   19.55 |    65.28 |         53.29 |         61.08 |           44.43 |\n| **Overall**     |                  47.92 |                           29.98 |                   20.21 |    64.28 |         51.62 |         59.29 |           42.28 |\n\n#### KMMLU-HARD (0-shot, CoT)\n\n| supercategory   |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024)|   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                  25.83 |                           26.17 |                   26.25 |    37.12 |         22.25 |         29.17 |           21.07 |\n| HUMSS           |                  21.52 |                           24.38 |                   20.21 |    41.97 |         23.31 |         31.51 |           19.44 |\n| Other           |                  24.82 |                           24.82 |                   23.88 |    40.39 |         26.48 |         29.59 |           22.22 |\n| STEM            |                  28.18 |                           26.91 |                   24.64 |    39.82 |         26.36 |         32.18 |           20.91 |\n| **Overall**     |                  25.34 |                           25.68 |                   24.03 |    39.62 |         24.56 |         30.56 |           20.97 |\n\n#### KMMLU-HARD (5-shot) \n\n| supercategory   |   Phi-3.5-MoE-Instruct |  Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                  21    |                           29    |                   12    |    31    |         21    |         25    |           20    |\n| HUMSS           |                  22.88 |                           19.92 |                   14    |    43.98 |         23.47 |         33.53 |           19.53 |\n| Other           |                  25.13 |                           27.27 |                   12.83 |    39.84 |         28.34 |         29.68 |           23.22 |\n| STEM            |                  21.75 |                           25.25 |                   12.75 |    40.25 |         23.25 |         27.25 |           19.75 |\n| **Overall**     |                  25.66 |                           25.73 |                   15.81 |    40.94 |         24.63 |         31.12 |           21.19 |', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":41873153344,"storage_bytes":83747055891,"files_count":34,"spaces_count":12,"gated":false,"private":false,"config":{"architectures":["PhiMoEForCausalLM"],"auto_map":{"AutoConfig":"configuration_phimoe.PhiMoEConfig","AutoModelForCausalLM":"modeling_phimoe.PhiMoEForCausalLM"},"model_type":"phimoe","tokenizer_config":{"bos_token":"<s>","chat_template":"{% for message in messages %}{% if message[''role''] == ''system'' and message[''content''] %}{{''<|system|>\n'' + message[''content''] + ''<|end|>\n''}}{% elif message[''role''] == ''user'' %}{{''<|user|>\n'' + message[''content''] + ''<|end|>\n''}}{% elif message[''role''] == ''assistant'' %}{{''<|assistant|>\n'' + message[''content''] + ''<|end|>\n''}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ ''<|assistant|>\n'' }}{% else %}{{ eos_token }}{% endif %}","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:microsoft:Phi-3CookBook","source_url":"https://github.com/microsoft/Phi-3CookBook"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:HazyResearch:flash-attention","source_url":"https://github.com/HazyResearch/flash-attention"},{"type":"based_on_paper","target_id":"arxiv:2404.14219","source_url":"https://arxiv.org/abs/2404.14219"},{"type":"based_on_paper","target_id":"arxiv:2407.13833","source_url":"https://arxiv.org/abs/2407.13833"},{"type":"based_on_paper","target_id":"arxiv:2403.06412","source_url":"https://arxiv.org/abs/2403.06412"}]', NULL, 'MIT', 'approved', 77.5, '1733594dcbb8a5175564c1a097c6813f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mistralai-Ministral-8B-Instruct-2410', 'huggingface--mistralai--ministral-8b-instruct-2410', 'Ministral-8B-Instruct-2410', 'mistralai', '--- library_name: vllm language: - en - fr - de - es - it - pt - zh - ja - ru - ko license: other license_name: mrl inference: false license_link: https://mistral.ai/licenses/MRL-0.1.md extra_gated_prompt: >- # Mistral AI Research License If You want to use a Mistral Model, a Derivative or an Output for any purpose that is not expressly authorized under this Agreement, You must request a license from Mistral AI, which Mistral AI may grant to You in Mistral AI''s sole discretion. To discuss suc...', '["vllm","safetensors","mistral","mistral-common","en","fr","de","es","it","pt","zh","ja","ru","ko","license:other","region:us"]', 'other', 564, 261441, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mistralai/Ministral-8B-Instruct-2410","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: vllm\nlanguage:\n- en\n- fr\n- de\n- es\n- it\n- pt\n- zh\n- ja\n- ru\n- ko\nlicense: other\nlicense_name: mrl\ninference: false\nlicense_link: https://mistral.ai/licenses/MRL-0.1.md\nextra_gated_prompt: >-\n  # Mistral AI Research License\n\n  If You want to use a Mistral Model, a Derivative or an Output for any purpose\n  that is not expressly authorized under this Agreement, You must request a\n  license from Mistral AI, which Mistral AI may grant to You in Mistral AI''s\n  sole discretion. To discuss such a license, please contact Mistral AI via the\n  website contact form: https://mistral.ai/contact/\n\n  ## 1. Scope and acceptance\n\n  **1.1. Scope of the Agreement.** This Agreement applies to any use,\n  modification, or Distribution of any Mistral Model by You, regardless of the\n  source You obtained a copy of such Mistral Model.\n\n  **1.2. Acceptance.** By accessing, using, modifying, Distributing a Mistral\n  Model, or by creating, using or distributing a Derivative of the Mistral\n  Model, You agree to be bound by this Agreement.\n\n  **1.3. Acceptance on behalf of a third-party.** If You accept this Agreement\n  on behalf of Your employer or another person or entity, You warrant and\n  represent that You have the authority to act and accept this Agreement on\n  their behalf. In such a case, the word "You" in this Agreement will refer to\n  Your employer or such other person or entity.\n\n  ## 2. License\n\n  **2.1. Grant of rights**.  Subject to Section 3 below, Mistral AI hereby\n  grants You a non-exclusive, royalty-free, worldwide, non-sublicensable,\n  non-transferable, limited license to use, copy, modify, and Distribute under\n  the conditions provided in Section 2.2 below, the Mistral Model and any\n  Derivatives made by or for Mistral AI and to create Derivatives of the Mistral\n  Model.\n\n  **2.2. Distribution of Mistral Model and Derivatives made by or for Mistral\n  AI.** Subject to Section 3 below, You may Distribute copies of the Mistral\n  Model and/or Derivatives made by or for Mistral AI, under the following\n  conditions: You must make available a copy of this Agreement to third-party\n  recipients of the Mistral Models and/or Derivatives made by or for Mistral AI\n  you Distribute, it being specified that any rights to use the Mistral Models\n  and/or Derivatives made by or for Mistral AI shall be directly granted by\n  Mistral AI to said third-party recipients pursuant to the Mistral AI Research\n  License agreement executed between these parties; You must retain in all\n  copies of the Mistral Models the following attribution notice within a\n  "Notice" text file distributed as part of such copies: "Licensed by Mistral AI\n  under the Mistral AI Research License".\n\n  **2.3. Distribution of Derivatives made by or for You.** Subject to Section 3\n  below, You may Distribute any Derivatives made by or for You under additional\n  or different terms and conditions, provided that: In any event, the use and\n  modification of Mistral Model and/or Derivatives made by or for Mistral AI\n  shall remain governed by the terms and conditions of this Agreement; You\n  include in any such Derivatives made by or for You prominent notices stating\n  that You modified the concerned Mistral Model; and Any terms and conditions\n  You impose on any third-party recipients relating to Derivatives made by or\n  for You shall neither limit such third-party recipients'' use of the Mistral\n  Model or any Derivatives made by or for Mistral AI in accordance with the\n  Mistral AI Research License nor conflict with any of its terms and conditions.\n\n  ## 3. Limitations\n\n  **3.1. Misrepresentation.** You must not misrepresent or imply, through any\n  means, that the Derivatives made by or for You and/or any modified version of\n  the Mistral Model You Distribute under your name and responsibility is an\n  official product of Mistral AI or has been endorsed, approved or validated by\n  Mistral AI, unless You are authorized by Us to do so in writing.\n\n  **3.2. Usage Limitation.** You shall only use the Mistral Models, Derivatives\n  (whether or not created by Mistral AI) and Outputs for Research Purposes.\n\n  ## 4. Intellectual Property\n\n  **4.1. Trademarks.** No trademark licenses are granted under this Agreement,\n  and in connection with the Mistral Models, You may not use any name or mark\n  owned by or associated with Mistral AI or any of its affiliates, except (i) as\n  required for reasonable and customary use in describing and Distributing the\n  Mistral Models and Derivatives made by or for Mistral AI and (ii) for\n  attribution purposes as required by this Agreement.\n\n  **4.2. Outputs.** We claim no ownership rights in and to the Outputs. You are\n  solely responsible for the Outputs You generate and their subsequent uses in\n  accordance with this Agreement. Any Outputs shall be subject to the\n  restrictions set out in Section 3 of this Agreement.\n\n  **4.3. Derivatives.** By entering into this Agreement, You accept that any\n  Derivatives that You may create or that may be created for You shall be\n  subject to the restrictions set out in Section 3 of this Agreement.\n\n  ## 5. Liability\n\n  **5.1. Limitation of liability.** In no event, unless required by applicable\n  law (such as deliberate and grossly negligent acts) or agreed to in writing,\n  shall Mistral AI be liable to You for damages, including any direct, indirect,\n  special, incidental, or consequential damages of any character arising as a\n  result of this Agreement or out of the use or inability to use the Mistral\n  Models and Derivatives (including but not limited to damages for loss of data,\n  loss of goodwill, loss of expected profit or savings, work stoppage, computer\n  failure or malfunction, or any damage caused by malware or security breaches),\n  even if  Mistral AI has been advised of the possibility of such damages.\n\n  **5.2. Indemnification.** You agree to indemnify and hold harmless Mistral AI\n  from and against any claims, damages, or losses arising out of or related to\n  Your use or Distribution of the Mistral Models and Derivatives.\n\n  ## 6. Warranty\n\n  **6.1. Disclaimer.** Unless required by applicable law or prior agreed to by\n  Mistral AI in writing, Mistral AI provides the Mistral Models and Derivatives\n  on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n  express or implied, including, without limitation, any warranties or\n  conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n  PARTICULAR PURPOSE. Mistral AI does not represent nor warrant that the Mistral\n  Models and Derivatives will be error-free, meet Your or any third party''s\n  requirements, be secure or will allow You or any third party to achieve any\n  kind of result or generate any kind of content. You are solely responsible for\n  determining the appropriateness of using or Distributing the Mistral Models\n  and Derivatives and assume any risks associated with Your exercise of rights\n  under this Agreement.\n\n  ## 7. Termination\n\n  **7.1. Term.** This Agreement is effective as of the date of your acceptance\n  of this Agreement or access to the concerned Mistral Models or Derivatives and\n  will continue until terminated in accordance with the following terms.\n\n  **7.2. Termination.** Mistral AI may terminate this Agreement at any time if\n  You are in breach of this Agreement. Upon termination of this Agreement, You\n  must cease to use all Mistral Models and Derivatives and shall permanently\n  delete any copy thereof. The following provisions, in their relevant parts,\n  will survive any termination or expiration of this Agreement, each for the\n  duration necessary to achieve its own intended purpose (e.g. the liability\n  provision will survive until the end of the applicable limitation\n  period):Sections 5 (Liability), 6(Warranty), 7 (Termination) and 8 (General\n  Provisions).\n\n  **7.3. Litigation.** If You initiate any legal action or proceedings against\n  Us or any other entity (including a cross-claim or counterclaim in a lawsuit),\n  alleging that the Model or a Derivative, or any part thereof, infringe upon\n  intellectual property or other rights owned or licensable by You, then any\n  licenses granted to You under this Agreement will immediately terminate as of\n  the date such legal action or claim is filed or initiated.\n\n  ## 8. General provisions\n\n  **8.1. Governing laws.** This Agreement will be governed by the laws of\n  France, without regard to choice of law principles, and the UN Convention on\n  Contracts for the International Sale of Goods does not apply to this\n  Agreement.\n\n  **8.2. Competent jurisdiction.** The courts of Paris shall have exclusive\n  jurisdiction of any dispute arising out of this Agreement.\n\n  **8.3. Severability.** If any provision of this Agreement is held to be\n  invalid, illegal or unenforceable, the remaining provisions shall be\n  unaffected thereby and remain valid as if such provision had not been set\n  forth herein.\n\n  ## 9. Definitions\n\n  "Agreement": means this Mistral AI Research License agreement governing the\n  access, use, and Distribution of the Mistral Models, Derivatives and Outputs.\n\n  "Derivative": means any (i) modified version of the Mistral Model (including\n  but not limited to any customized or fine-tuned version thereof), (ii) work\n  based on the Mistral Model, or (iii) any other derivative work thereof.\n\n  "Distribution", "Distributing", "Distribute" or "Distributed": means\n  supplying, providing or making available, by any means, a copy of the Mistral\n  Models and/or the Derivatives as the case may be, subject to Section 3 of this\n  Agreement.\n\n  "Mistral AI", "We" or "Us": means Mistral AI, a French soci√©t√© par actions\n  simplifi√©e registered in the Paris commercial registry under the number 952\n  418 325, and having its registered seat at 15, rue des Halles, 75001 Paris.\n\n  "Mistral Model": means the foundational large language model(s), and its\n  elements which include algorithms, software, instructed checkpoints,\n  parameters, source code (inference code, evaluation code and, if applicable,\n  fine-tuning code) and any other elements associated thereto made available by\n  Mistral AI under this Agreement, including, if any, the technical\n  documentation, manuals and instructions for the use and operation thereof.\n\n  "Research Purposes": means any use of a Mistral Model,  Derivative, or Output\n  that is solely for (a) personal, scientific or academic research, and (b) for\n  non-profit and non-commercial purposes, and not directly or indirectly\n  connected to any commercial activities or business operations. For\n  illustration purposes, Research Purposes does not include (1) any usage of the\n  Mistral Model, Derivative or Output by individuals or contractors employed in\n  or engaged by companies in the context of (a) their daily tasks, or (b) any\n  activity (including but not limited to any testing or proof-of-concept) that\n  is intended to generate revenue, nor (2) any Distribution by a commercial\n  entity of the Mistral Model, Derivative or Output whether in return for\n  payment or free of charge, in any medium or form, including but not limited to\n  through a hosted or managed service (e.g. SaaS, cloud instances, etc.), or\n  behind a software layer.\n\n  "Outputs": means any content generated by the operation of the Mistral Models\n  or the Derivatives from  a prompt (i.e., text instructions) provided by users.\n  For the avoidance of doubt, Outputs do not include any components of a Mistral\n  Models, such as any fine-tuned versions of the Mistral Models, the weights, or\n  parameters.\n\n  "You": means the individual or entity entering into this Agreement with\n  Mistral AI.\n\n\n  *Mistral AI processes your personal data below to provide the model and\n  enforce its license. If you are affiliated with a commercial entity, we may\n  also send you communications about our models. For more information on your\n  rights and data handling, please see our <a\n  href="https://mistral.ai/terms/">privacy policy</a>.*\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Country: country\n  Affiliation: text\n  Job title: text\n  I understand that I can only use the model, any derivative versions and their outputs for non-commercial research purposes: checkbox\n  I understand that if I am a commercial entity, I am not permitted to use or distribute the model internally or externally, or expose it in my own offerings without a commercial license: checkbox\n  I understand that if I upload the model, or any derivative version, on any platform, I must include the Mistral Research License: checkbox\n  I understand that for commercial use of the model, I can contact Mistral or use the Mistral AI API on la Plateforme or any of our cloud provider partners: checkbox\n  By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Mistral Privacy Policy: checkbox\n  geo: ip_location\nextra_gated_description: >-\n  Mistral AI processes your personal data below to provide the model and enforce\n  its license. If you are affiliated with a commercial entity, we may also send\n  you communications about our models. For more information on your rights and\n  data handling, please see our <a href="https://mistral.ai/terms/">privacy\n  policy</a>.\nextra_gated_button_content: Submit\ntags:\n- mistral-common\n---\n\n# Model Card for Ministral-8B-Instruct-2410\n\nWe introduce two new state-of-the-art models for local intelligence, on-device computing, and at-the-edge use cases. We call them les Ministraux: Ministral 3B and Ministral 8B. \n\nThe Ministral-8B-Instruct-2410 Language Model is an instruct fine-tuned model significantly outperforming existing models of similar size, released under the Mistral Research License.\n\nIf you are interested in using Ministral-3B or Ministral-8B commercially, outperforming Mistral-7B, [reach out to us](https://mistral.ai/contact/).\n\nFor more details about les Ministraux please refer to our release [blog post](https://mistral.ai/news/ministraux).\n\n## Ministral 8B Key features\n- Released under the **Mistral Research License**, reach out to us for a commercial license\n- Trained with a **128k context window** with **interleaved sliding-window attention**\n- Trained on a large proportion of **multilingual and code data**\n- Supports **function calling**\n- Vocabulary size of **131k**, using the **V3-Tekken** tokenizer\n\n### Basic Instruct Template (V3-Tekken)\n\n```\n<s>[INST]user message[/INST]assistant response</s>[INST]new user message[/INST]\n```\n\n*For more information about the tokenizer please refer to [mistral-common](https://github.com/mistralai/mistral-common)*\n\n## Ministral 8B Architecture\n\n| Feature               | Value                |\n|:---------------------:|:--------------------:|\n| **Architecture**      | Dense Transformer    |\n| **Parameters**        | 8,019,808,256        |\n| **Layers**            | 36                   |\n| **Heads**             | 32                   |\n| **Dim**               | 4096                 |\n| **KV Heads (GQA)**    | 8                    |\n| **Hidden Dim**        | 12288                |\n| **Head Dim**          | 128                  |\n| **Vocab Size**        | 131,072              |\n| **Context Length**    | 128k                 |\n| **Attention Pattern** | Ragged (128k,32k,32k,32k) |\n\n## Benchmarks\n\n#### Base Models\n\n<u>Knowledge & Commonsense</u>\n\n| Model       | MMLU | AGIEval | Winogrande | Arc-c | TriviaQA |\n|:-------------:|:------:|:---------:|:------------:|:-------:|:----------:|\n| Mistral 7B Base  | 62.5 | 42.5    | 74.2   | 67.9  | 62.5 |\n| Llama 3.1 8B Base | 64.7 | 44.4    | 74.6       | 46.0  | 60.2     |\n| ***Ministral 8B Base*** | ***<u>65.0</u>*** | ***<u>48.3</u>*** | ***<u>75.3</u>***   | ***<u>71.9</u>*** | ***<u>65.5</u>*** |\n|  |  |     |        |   |      |\n| Gemma 2 2B Base | 52.4 | 33.8    | 68.7   | 42.6  | 47.8     |\n| Llama 3.2 3B Base | 56.2 | 37.4    | 59.6       | 43.1  | 50.7     |\n| ***Ministral 3B Base*** | ***<u>60.9</u>*** | ***<u>42.1</u>***    | ***<u>72.7</u>***       | ***<u>64.2</u>*** | ***<u>56.7</u>***     |\n\n<u>Code & Math</u>\n\n| Model       | HumanEval pass@1 |GSM8K maj@8 |\n|:-------------:|:-------------------:|:---------------:|\n| Mistral 7B Base  | 26.8              | 32.0           |\n| Llama 3.1 8B Base | ***<u>37.8</u>***          | 42.2           |\n| ***Ministral 8B Base***  | 34.8              | ***<u>64.5</u>***       |\n|   |               |            |\n| Gemma 2 2B  | 20.1              | 35.5           |\n| Llama 3.2 3B | 14.6              | 33.5           |\n| ***Ministral 3B*** | ***<u>34.2</u>***          | ***<u>50.9</u>***       |\n\n<u>Multilingual</u>\n\n| Model       | French MMLU | German MMLU | Spanish MMLU |\n|:-------------:|:-------------:|:-------------:|:-------------:|\n| Mistral 7B Base  | 50.6         | 49.6         | 51.4         |\n| Llama 3.1 8B Base | 50.8         | 52.8         | 54.6         |\n| ***Ministral 8B Base*** | ***<u>57.5</u>***     | ***<u>57.4</u>***     | ***<u>59.6</u>***     |\n|   |          |          |          |\n| Gemma 2 2B Base  | 41.0         | 40.1         | 41.7         |\n| Llama 3.2 3B Base | 42.3         | 42.2         | 43.1         |\n| ***Ministral 3B Base*** | ***<u>49.1</u>***     | ***<u>48.3</u>***     | ***<u>49.5</u>***     |\n\n### Instruct Models\n\n<u>Chat/Arena (gpt-4o judge)</u>\n\n| Model       | MTBench | Arena Hard | Wild bench |\n|:-------------:|:---------:|:------------:|:------------:|\n| Mistral 7B Instruct v0.3  | 6.7     | 44.3       | 33.1       |\n| Llama 3.1 8B Instruct | 7.5     | 62.4       | 37.0       |\n| Gemma 2 9B Instruct | 7.6     | 68.7       | ***<u>43.8</u>***       |\n| ***Ministral 8B Instruct*** | ***<u>8.3</u>*** | ***<u>70.9</u>***   | 41.3   |\n|   |      |        |        |\n| Gemma 2 2B Instruct  | 7.5     | 51.7       | 32.5       |\n| Llama 3.2 3B Instruct | 7.2     | 46.0       | 27.2       |\n| ***Ministral 3B Instruct*** | ***<u>8.1</u>*** | ***<u>64.3</u>***   | ***<u>36.3</u>***   |\n\n<u>Code & Math</u>\n\n| Model       | MBPP pass@1 | HumanEval pass@1 | Math maj@1 |\n|:-------------:|:-------------:|:------------------:|:-------------:|\n| Mistral 7B Instruct v0.3  | 50.2        | 38.4             | 13.2        |\n| Gemma 2 9B Instruct | 68.5   | 67.7             | 47.4        |\n Llama 3.1 8B Instruct | 69.7   | 67.1             | 49.3        |\n| ***Ministral 8B Instruct*** | ***<u>70.0</u>***        | ***<u>76.8</u>***         | ***<u>54.5</u>***   |\n|   |         |              |         |\n| Gemma 2 2B Instruct  | 54.5        | 42.7             | 22.8        |\n| Llama 3.2 3B Instruct | 64.6        | 61.0             | 38.4        |\n| ***Ministral 3B* Instruct** | ***<u>67.7</u>***   | ***<u>77.4</u>***         | ***<u>51.7</u>***   |\n\n<u>Function calling</u>\n\n| Model       | Internal bench |\n|:-------------:|:-----------------:|\n| Mistral 7B Instruct v0.3  | 6.9             |\n| Llama 3.1 8B Instruct | N/A             |\n| Gemma 2 9B Instruct | N/A             |\n| ***Ministral 8B Instruct*** | ***<u>31.6</u>***       |\n|   |              |\n| Gemma 2 2B Instruct  | N/A             |\n| Llama 3.2 3B Instruct | N/A             |\n| ***Ministral 3B Instruct*** | ***<u>28.4</u>***       |\n\n## Usage Examples\n\n### vLLM (recommended)\n\nWe recommend using this model with the [vLLM library](https://github.com/vllm-project/vllm)\nto implement production-ready inference pipelines.\n\n> [!IMPORTANT]\n> Currently vLLM is capped at 32k context size because interleaved attention kernels for paged attention are not yet implemented in vLLM.\n> Attention kernels for paged attention are being worked on and as soon as it is fully supported in vLLM, this model card will be updated.\n> To take advantage of the full 128k context size we recommend [Mistral Inference](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410#mistral-inference)\n\n**_Installation_**\n\n\nMake sure you install `vLLM >= v0.6.4`:\n\n```\npip install --upgrade vllm\n```\n\nAlso make sure you have `mistral_common >= 1.4.4` installed:\n\n```\npip install --upgrade mistral_common\n```\n\nYou can also make use of a ready-to-go [docker image](https://github.com/vllm-project/vllm/blob/main/Dockerfile).\n\n**_Offline_**\n\n```py\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\n\nmodel_name = "mistralai/Ministral-8B-Instruct-2410"\n\nsampling_params = SamplingParams(max_tokens=8192)\n\n# note that running Ministral 8B on a single GPU requires 24 GB of GPU RAM\n# If you want to divide the GPU requirement over multiple devices, please add *e.g.* `tensor_parallel=2`\nllm = LLM(model=model_name, tokenizer_mode="mistral", config_format="mistral", load_format="mistral")\n\nprompt = "Do we need to think for 10 seconds to find the answer of 1 + 1?"\n\nmessages = [\n    {\n        "role": "user",\n        "content": prompt\n    },\n]\n\noutputs = llm.chat(messages, sampling_params=sampling_params)\n\nprint(outputs[0].outputs[0].text)\n# You don''t need to think for 10 seconds to find the answer to 1 + 1. The answer is 2,\n# and you can easily add these two numbers in your mind very quickly without any delay.\n```\n\n**_Server_**\n\nYou can also use Ministral-8B in a server/client setting. \n\n1. Spin up a server:\n\n\n```\nvllm serve mistralai/Ministral-8B-Instruct-2410 --tokenizer_mode mistral --config_format mistral --load_format mistral\n```\n\n**Note:** Running Ministral-8B on a single GPU requires 24 GB of GPU RAM. \n\nIf you want to divide the GPU requirement over multiple devices, please add *e.g.* `--tensor_parallel=2`\n\n2. And ping the client:\n\n```\ncurl --location ''http://<your-node-url>:8000/v1/chat/completions'' \\n--header ''Content-Type: application/json'' \\n--header ''Authorization: Bearer token'' \\n--data ''{\n    "model": "mistralai/Ministral-8B-Instruct-2410",\n    "messages": [\n      {\n        "role": "user",\n        "content": "Do we need to think for 10 seconds to find the answer of 1 + 1?"\n      }\n    ]\n}''\n\n```\n\n### Mistral-inference\n\nWe recommend using [mistral-inference](https://github.com/mistralai/mistral-inference) to quickly try out / "vibe-check" the model.\n\n\n**_Install_**\n\nMake sure to have `mistral_inference >= 1.5.0` installed.\n\n```\npip install mistral_inference --upgrade\n```\n\n**_Download_**\n\n```py\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmistral_models_path = Path.home().joinpath(''mistral_models'', ''8B-Instruct'')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id="mistralai/Ministral-8B-Instruct-2410", allow_patterns=["params.json", "consolidated.safetensors", "tekken.json"], local_dir=mistral_models_path)\n```\n\n### Chat\n\nAfter installing `mistral_inference`, a `mistral-chat` CLI command should be available in your environment. You can chat with the model using\n\n```\nmistral-chat $HOME/mistral_models/8B-Instruct --instruct --max_tokens 256\n```\n\n### Passkey detection\n\n> [!IMPORTANT]\n> In this example the passkey message has over >100k tokens and mistral-inference\n> does not have a chunked pre-fill mechanism. Therefore you will need a lot of\n> GPU memory in order to run the below example (80 GB). For a more memory-efficient\n> solution we recommend using vLLM.\n\n```py\nfrom mistral_inference.transformer import Transformer\nfrom pathlib import Path\nimport json\nfrom mistral_inference.generate import generate\nfrom huggingface_hub import hf_hub_download\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\ndef load_passkey_request() -> ChatCompletionRequest:\n    passkey_file = hf_hub_download(repo_id="mistralai/Ministral-8B-Instruct-2410", filename="passkey_example.json")\n\n    with open(passkey_file, "r") as f:\n        data = json.load(f)\n\n    message_content = data["messages"][0]["content"]\n    return ChatCompletionRequest(messages=[UserMessage(content=message_content)])\n\ntokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tekken.json")\nmodel = Transformer.from_folder(mistral_models_path, softmax_fp32=False)\n\ncompletion_request = load_passkey_request()\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)  # The pass key is 13005.\n```\n\n\n### Instruct following\n\n```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\n\ntokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tekken.json")\nmodel = Transformer.from_folder(mistral_models_path)\n\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content="How often does the letter r occur in Mistral?")])\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n### Function calling\n\n```py\nfrom mistral_common.protocol.instruct.tool_calls import Function, Tool\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.tekken import SpecialTokenPolicy\n\n\ntokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tekken.json")\ntekken = tokenizer.instruct_tokenizer.tokenizer\ntekken.special_token_policy = SpecialTokenPolicy.IGNORE\n\nmodel = Transformer.from_folder(mistral_models_path)\n\ncompletion_request = ChatCompletionRequest(\n    tools=[\n        Tool(\n            function=Function(\n                name="get_current_weather",\n                description="Get the current weather",\n                parameters={\n                    "type": "object",\n                    "properties": {\n                        "location": {\n                            "type": "string",\n                            "description": "The city and state, e.g. San Francisco, CA",\n                        },\n                        "format": {\n                            "type": "string",\n                            "enum": ["celsius", "fahrenheit"],\n                            "description": "The temperature unit to use. Infer this from the users location.",\n                        },\n                    },\n                    "required": ["location", "format"],\n                },\n            )\n        )\n    ],\n    messages=[\n        UserMessage(content="What''s the weather like today in Paris?"),\n        ],\n)\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Abou Chahine, Alexandre Sablayrolles, Alexis Tacnet, Alodie Boissonnet, Alok Kothari, Am√©lie H√©liou, Andy Lo, Anna Peronnin, Antoine Meunier, Antoine Roux, Antonin Faure, Aritra Paul, Arthur Darcet, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Avinash Sooriyarachchi, Baptiste Rozi√®re, Barry Conklin, Bastien Bouillon, Blanche Savary de Beauregard, Carole Rambaud, Caroline Feldman, Charles de Freminville, Charline Mauro, Chih-Kuan Yeh, Chris Bamford, Clement Auguy, Corentin Heintz, Cyriaque Dubois, Devendra Singh Chaplot, Diego Las Casas, Diogo Costa, El√©onore Arcelin, Emma Bou Hanna, Etienne Metzger, Fanny Olivier Autran, Francois Lesage, Garance Gourdel, Gaspard Blanchet, Gaspard Donada Vidal, Gianna Maria Lengyel, Guillaume Bour, Guillaume Lample, Gustave Denis, Harizo Rajaona, Himanshu Jaju, Ian Mack, Ian Mathew, Jean-Malo Delignon, Jeremy Facchetti, Jessica Chudnovsky, Joachim Studnia, Justus Murke, Kartik Khandelwal, Kenneth Chiu, Kevin Riera, Leonard Blier, Leonard Suslian, Leonardo Deschaseaux, Louis Martin, Louis Ternon, Lucile Saulnier, L√©lio Renard Lavaud, Sophia Yang, Margaret Jennings, Marie Pellat, Marie Torelli, Marjorie Janiewicz, Mathis Felardos, Maxime Darrin, Michael Hoff, Micka√´l Seznec, Misha Jessel Kenyon, Nayef Derwiche, Nicolas Carmont Zaragoza, Nicolas Faurie, Nicolas Moreau, Nicolas Schuhl, Nikhil Raghuraman, Niklas Muhs, Olivier de Garrigues, Patricia Roz√©, Patricia Wang, Patrick von Platen, Paul Jacob, Pauline Buche, Pavankumar Reddy Muddireddy, Perry Savas, Pierre Stock, Pravesh Agrawal, Renaud de Peretti, Romain Sauvestre, Romain Sinthe, Roman Soletskyi, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Soham Ghosh, Sylvain Regnier, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibault Schueller, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, Valeriia Nemychnikova, Wendy Shang, William El Sayed, William Marshall', '{"pipeline_tag":null,"library_name":"vllm","framework":"vllm","params":8019808256,"storage_bytes":32111185775,"files_count":16,"spaces_count":17,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","chat_template":"{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n{%- set ns = namespace() %}\n{%- set ns.index = 0 %}\n{%- for message in loop_messages %}\n    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n        {%- endif %}\n        {%- set ns.index = ns.index + 1 %}\n    {%- endif %}\n{%- endfor %}\n\n{{- bos_token }}\n{%- for message in loop_messages %}\n    {%- if message[\"role\"] == \"user\" %}\n        {%- if tools is not none and (message == user_messages[-1]) %}\n            {{- \"[AVAILABLE_TOOLS][\" }}\n            {%- for tool in tools %}\n                {%- set tool = tool.function %}\n                {{- ''{\"type\": \"function\", \"function\": {'' }}\n                {%- for key, val in tool.items() if key != \"return\" %}\n                    {%- if val is string %}\n                        {{- ''\"'' + key + ''\": \"'' + val + ''\"'' }}\n                    {%- else %}\n                        {{- ''\"'' + key + ''\": '' + val|tojson }}\n                    {%- endif %}\n                    {%- if not loop.last %}\n                        {{- \", \" }}\n                    {%- endif %}\n                {%- endfor %}\n                {{- \"}}\" }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- else %}\n                    {{- \"]\" }}\n                {%- endif %}\n            {%- endfor %}\n            {{- \"[/AVAILABLE_TOOLS]\" }}\n            {%- endif %}\n        {%- if loop.last and system_message is defined %}\n            {{- \"[INST]\" + system_message + \"\\n\\n\" + message[\"content\"] + \"[/INST]\" }}\n        {%- else %}\n            {{- \"[INST]\" + message[\"content\"] + \"[/INST]\" }}\n        {%- endif %}\n    {%- elif (message.tool_calls is defined and message.tool_calls is not none) %}\n        {{- \"[TOOL_CALLS][\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {{- out[:-1] }}\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n            {%- endif %}\n            {{- '', \"id\": \"'' + tool_call.id + ''\"}'' }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- else %}\n                {{- \"]\" + eos_token }}\n            {%- endif %}\n        {%- endfor %}\n    {%- elif message[\"role\"] == \"assistant\" %}\n        {{- message[\"content\"] + eos_token}}\n    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n        {%- if message.content is defined and message.content.content is defined %}\n            {%- set content = message.content.content %}\n        {%- else %}\n            {%- set content = message.content %}\n        {%- endif %}\n        {{- ''[TOOL_RESULTS]{\"content\": '' + content|string + \", \" }}\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n        {%- endif %}\n        {{- ''\"call_id\": \"'' + message.tool_call_id + ''\"}[/TOOL_RESULTS]'' }}\n    {%- else %}\n        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n    {%- endif %}\n{%- endfor %}\n","eos_token":"</s>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:mistralai:mistral-common","source_url":"https://github.com/mistralai/mistral-common"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:mistralai:mistral-inference","source_url":"https://github.com/mistralai/mistral-inference"}]', NULL, 'Other', 'approved', 77.5, '0517519afb09e825b8ade511f0012a6f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-playgroundai-playground-v2-1024px-aesthetic', 'huggingface--playgroundai--playground-v2-1024px-aesthetic', 'playground-v2-1024px-aesthetic', 'playgroundai', '--- license: other license_name: playground-v2-community license_link: https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic/blob/main/LICENSE.md tags: - text-to-image - playground inference: parameters: guidance_scale: 3.0 --- This repository contains a model that generates highly aesthetic images of resolution 1024x1024. You can use the model with Hugging Face üß® Diffusers. !image/png **Playground v2** is a diffusion-based text-to-image generative model. The model was trained f...', '["diffusers","safetensors","text-to-image","playground","license:other","endpoints_compatible","diffusers:stablediffusionxlpipeline","region:us"]', 'text-to-image', 563, 774, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: playground-v2-community\nlicense_link: https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic/blob/main/LICENSE.md\ntags:\n- text-to-image\n- playground\ninference:\n  parameters:\n    guidance_scale: 3.0\n---\n# Playground v2 ‚Äì 1024px Aesthetic Model\n\nThis repository contains a model that generates highly aesthetic images of resolution 1024x1024. You can use the model with Hugging Face üß® Diffusers.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/63855d851769b7c4b10e1f76/p0up5GNQgO0vVIiJ672K7.png)\n\n\n**Playground v2** is a diffusion-based text-to-image generative model. The model was trained from scratch by the research team at [Playground](https://playground.com). \n\nImages generated by Playground v2 are favored **2.5** times more than those produced by Stable Diffusion XL, according to Playground‚Äôs [user study](#user-study).\n\nWe are thrilled to release [intermediate checkpoints](#intermediate-base-models) at different training stages, including evaluation metrics, to the community. We hope this will encourage further research into foundational models for image generation.\n\nLastly, we introduce a new benchmark, [MJHQ-30K](#mjhq-30k-benchmark), for automatic evaluation of a model‚Äôs aesthetic quality.\n\nPlease see our [blog](https://blog.playgroundai.com/playground-v2/) for more details.\n\n### Model Description\n\n- **Developed by:** [Playground](https://playground.com)\n- **Model type:** Diffusion-based text-to-image generative model\n- **License:** [Playground v2 Community License](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic/blob/main/LICENSE.md)\n- **Summary:** This model generates images based on text prompts. It is a Latent Diffusion Model that uses two fixed, pre-trained text encoders ([OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip) and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main)). It follows the same architecture as [Stable Diffusion XL](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0).\n\n### Using the model with üß® Diffusers\n\nInstall diffusers >= 0.24.0 and some dependencies:\n```\npip install transformers accelerate safetensors\n```\n\nTo use the model, run the following snippet.\n\n**Note**: It is recommend to use **`guidance_scale=3.0`**.\n\n```python\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\n    "playgroundai/playground-v2-1024px-aesthetic",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    add_watermarker=False,\n    variant="fp16"\n)\npipe.to("cuda")\n\nprompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"\nimage  = pipe(prompt=prompt, guidance_scale=3.0).images[0]\n```\n\n### Using the model with Automatic1111/ComfyUI\n\nIn order to use the model with software such as Automatic1111 or ComfyUI you can use [`playground-v2.fp16.safetensors`](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic/blob/main/playground-v2.fp16.safetensors) file.\n\n### User Study\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/63855d851769b7c4b10e1f76/8VzBkSYaUU3dt509Co9sk.png)\n\nAccording to user studies conducted by Playground, involving over 2,600 prompts and thousands of users, the images generated by Playground v2 are favored **2.5** times more than those produced by [Stable Diffusion XL](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0).\n\nWe report user preference metrics on [PartiPrompts](https://github.com/google-research/parti), following standard practice, and on an internal prompt dataset curated by the Playground team. The ‚ÄúInternal 1K‚Äù prompt dataset is diverse and covers various categories and tasks.\n\nDuring the user study, we give users instructions to evaluate image pairs based on both (1) their aesthetic preference and (2) the image-text alignment.\n\n### MJHQ-30K Benchmark\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/63855d851769b7c4b10e1f76/o3Bt62qFsTO9DkeX2yLua.png)\n\n| Model                                 | Overall FID   |\n| ------------------------------------- | ----- |\n| SDXL-1-0-refiner                      | 9.55  |\n| [playground-v2-1024px-aesthetic](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic)        | **7.07**  |\n\nWe introduce a new benchmark, [MJHQ-30K](https://huggingface.co/datasets/playgroundai/MJHQ-30K), for automatic evaluation of a model‚Äôs aesthetic quality. The benchmark computes FID on a high-quality dataset to gauge aesthetic quality.\n\nWe have curated a high-quality dataset from Midjourney, featuring 10 common categories, with each category containing 3,000 samples. Following common practice, we use aesthetic score and CLIP score to ensure high image quality and high image-text alignment. Furthermore, we take extra care to make the data diverse within each category.\n\nFor Playground v2, we report both the overall FID and per-category FID. All FID metrics are computed at resolution 1024x1024. Our benchmark results show that our model outperforms SDXL-1-0-refiner in overall FID and all category FIDs, especially in people and fashion categories. This is in line with the results of the user study, which indicates a correlation between human preference and FID score on the MJHQ-30K benchmark.\n\nWe release this benchmark to the public and encourage the community to adopt it for benchmarking their models‚Äô aesthetic quality.\n\n### Intermediate Base Models\n\n| Model                        | FID    | Clip Score |\n| ---------------------------- | ------ | ---------- |\n| SDXL-1-0-refiner             | 13.04  | 32.62      |\n| [playground-v2-256px-base](https://huggingface.co/playgroundai/playground-v2-256px-base)     | 9.83   | 31.90      |\n| [playground-v2-512px-base](https://huggingface.co/playgroundai/playground-v2-512px-base)     | 9.55   | 32.08      |\n\n\nApart from [playground-v2-1024px-aesthetic](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic), we release intermediate checkpoints at different training stages to the community in order to foster foundation model research in pixels. Here, we report the FID score and CLIP score on the MSCOCO14 evaluation set for the reference purposes. (Note that our reported numbers may differ from the numbers reported in SDXL‚Äôs published results, as our prompt list may be different.)\n\n### How to cite us\n\n```\n@misc{playground-v2,\n      url={[https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic)},\n      title={Playground v2},\n      author={Li, Daiqing and Kamko, Aleks and Sabet, Ali and Akhgari, Ehsan and Xu, Linmiao and Doshi, Suhail}\n}\n```', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":76318365586,"files_count":35,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionXLPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:mlfoundations:open_clip","source_url":"https://github.com/mlfoundations/open_clip"},{"type":"has_code","target_id":"github:openai:CLIP","source_url":"https://github.com/openai/CLIP"},{"type":"has_code","target_id":"github:google-research:parti","source_url":"https://github.com/google-research/parti"}]', NULL, 'Other', 'approved', 62.5, '776d436834d410b2dc60e7b99621a237', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen2.5-1.5B-Instruct', 'huggingface--qwen--qwen2.5-1.5b-instruct', 'Qwen2.5-1.5B-Instruct', 'Qwen', '--- license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct/blob/main/LICENSE language: - en pipeline_tag: text-generation base_model: Qwen/Qwen2.5-1.5B tags: - chat library_name: transformers --- Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2: - Significantly **mo...', '["transformers","safetensors","qwen2","text-generation","chat","conversational","en","arxiv:2407.10671","base_model:qwen/qwen2.5-1.5b","base_model:finetune:qwen/qwen2.5-1.5b","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 563, 5515322, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\nbase_model: Qwen/Qwen2.5-1.5B\ntags:\n- chat\nlibrary_name: transformers\n---\n\n# Qwen2.5-1.5B-Instruct\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 1.5B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\n- Number of Parameters: 1.54B\n- Number of Paramaters (Non-Embedding): 1.31B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 12 for Q and 2 for KV\n- Context Length: Full 32,768 tokens and generation 8192 tokens\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: ''qwen2''\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen2.5-1.5B-Instruct"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [üìë blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":1543714304,"storage_bytes":3087467144,"files_count":10,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- messages[0][''content''] }}\n    {%- else %}\n        {{- ''You are Qwen, created by Alibaba Cloud. You are a helpful assistant.'' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0][''content''] + ''<|im_end|>\\n'' }}\n    {%- else %}\n        {{- ''<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content %}\n            {{- ''\\n'' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2407.10671","source_url":"https://arxiv.org/abs/2407.10671"}]', NULL, 'Apache-2.0', 'approved', 62.5, 'adfee1e46e8057f1c6bc5f622f7cd383', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-HuggingFaceTB-SmolVLM-Instruct', 'huggingface--huggingfacetb--smolvlm-instruct', 'SmolVLM-Instruct', 'HuggingFaceTB', '--- library_name: transformers license: apache-2.0 datasets: - HuggingFaceM4/the_cauldron - HuggingFaceM4/Docmatix pipeline_tag: image-text-to-text language: - en base_model: - HuggingFaceTB/SmolLM2-1.7B-Instruct - google/siglip-so400m-patch14-384 --- <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/SmolVLM.png" width="800" height="auto" alt="Image description"> SmolVLM is a compact open multimodal model that accepts arbitrary sequences of image and text...', '["transformers","onnx","safetensors","idefics3","image-to-text","image-text-to-text","conversational","en","dataset:huggingfacem4/the_cauldron","dataset:huggingfacem4/docmatix","arxiv:2504.05299","base_model:huggingfacetb/smollm2-1.7b-instruct","license:apache-2.0","endpoints_compatible","region:us"]', 'image-text-to-text', 562, 51141, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlibrary_name: transformers\nlicense: apache-2.0\ndatasets:\n- HuggingFaceM4/the_cauldron\n- HuggingFaceM4/Docmatix\npipeline_tag: image-text-to-text\nlanguage:\n- en\nbase_model:\n- HuggingFaceTB/SmolLM2-1.7B-Instruct\n- google/siglip-so400m-patch14-384\n---\n\n<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/SmolVLM.png" width="800" height="auto" alt="Image description">\n\n# SmolVLM\n\nSmolVLM is a compact open multimodal model that accepts arbitrary sequences of image and text inputs to produce text outputs. Designed for efficiency, SmolVLM can answer questions about images, describe visual content, create stories grounded on multiple images, or function as a pure language model without visual inputs. Its lightweight architecture makes it suitable for on-device applications while maintaining strong performance on multimodal tasks.\n\n## Model Summary\n\n- **Developed by:** Hugging Face ü§ó\n- **Model type:** Multi-modal model (image+text)\n- **Language(s) (NLP):** English\n- **License:** Apache 2.0\n- **Architecture:** Based on [Idefics3](https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3) (see technical summary)\n\n## Resources\n\n- **Demo:** [SmolVLM Demo](https://huggingface.co/spaces/HuggingFaceTB/SmolVLM)\n- **Blog:** [Blog post](https://huggingface.co/blog/smolvlm)\n\n## Uses\n\nSmolVLM can be used for inference on multimodal (image + text) tasks where the input comprises text queries along with one or more images. Text and images can be interleaved arbitrarily, enabling tasks like image captioning, visual question answering, and storytelling based on visual content. The model does not support image generation.\n\nTo fine-tune SmolVLM on a specific task, you can follow the fine-tuning tutorial.\n<!-- todo: add link to fine-tuning tutorial -->\n\n### Technical Summary\n\nSmolVLM leverages the lightweight SmolLM2 language model to provide a compact yet powerful multimodal experience. It introduces several changes compared to previous Idefics models:\n\n- **Image compression:** We introduce a more radical image compression compared to Idefics3 to enable the model to infer faster and use less RAM.\n- **Visual Token Encoding:** SmolVLM uses 81 visual tokens to encode image patches of size 384√ó384. Larger images are divided into patches, each encoded separately, enhancing efficiency without compromising performance.\n\nMore details about the training and architecture are available in our technical report.\n\n\n### How to get started\n\nYou can use transformers to load, infer and fine-tune SmolVLM.\n\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom transformers.image_utils import load_image\n\nDEVICE = "cuda" if torch.cuda.is_available() else "cpu"\n\n# Load images\nimage1 = load_image("https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg")\nimage2 = load_image("https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg")\n\n# Initialize processor and model\nprocessor = AutoProcessor.from_pretrained("HuggingFaceTB/SmolVLM-Instruct")\nmodel = AutoModelForVision2Seq.from_pretrained(\n    "HuggingFaceTB/SmolVLM-Instruct",\n    torch_dtype=torch.bfloat16,\n    _attn_implementation="flash_attention_2" if DEVICE == "cuda" else "eager",\n).to(DEVICE)\n\n# Create input messages\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image"},\n            {"type": "image"},\n            {"type": "text", "text": "Can you describe the two images?"}\n        ]\n    },\n]\n\n# Prepare inputs\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=prompt, images=[image1, image2], return_tensors="pt")\ninputs = inputs.to(DEVICE)\n\n# Generate outputs\ngenerated_ids = model.generate(**inputs, max_new_tokens=500)\ngenerated_texts = processor.batch_decode(\n    generated_ids,\n    skip_special_tokens=True,\n)\n\nprint(generated_texts[0])\n"""\nAssistant: The first image shows a green statue of the Statue of Liberty standing on a stone pedestal in front of a body of water. \nThe statue is holding a torch in its right hand and a tablet in its left hand. The water is calm and there are no boats or other objects visible. \nThe sky is clear and there are no clouds. The second image shows a bee on a pink flower. \nThe bee is black and yellow and is collecting pollen from the flower. The flower is surrounded by green leaves.\n"""\n```\n\n\n### Model optimizations\n\n**Precision**: For better performance, load and run the model in half-precision (`torch.float16` or `torch.bfloat16`) if your hardware supports it.\n\n```python\nfrom transformers import AutoModelForVision2Seq\nimport torch\n\nmodel = AutoModelForVision2Seq.from_pretrained(\n    "HuggingFaceTB/SmolVLM-Instruct",\n    torch_dtype=torch.bfloat16\n).to("cuda")\n```\n\nYou can also load SmolVLM with 4/8-bit quantization using bitsandbytes, torchao or Quanto. Refer to [this page](https://huggingface.co/docs/transformers/en/main_classes/quantization) for other options.\n\n```python\nfrom transformers import AutoModelForVision2Seq, BitsAndBytesConfig\nimport torch\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nmodel = AutoModelForVision2Seq.from_pretrained(\n    "HuggingFaceTB/SmolVLM-Instruct",\n    quantization_config=quantization_config,\n)\n```\n\n**Vision Encoder Efficiency**: Adjust the image resolution by setting `size={"longest_edge": N*384}` when initializing the processor, where N is your desired value. The default `N=4` works well, which results in input images of\nsize 1536√ó1536. For documents, `N=5` might be beneficial. Decreasing N can save GPU memory and is appropriate for lower-resolution images. This is also useful if you want to fine-tune on videos.\n\n\n## Misuse and Out-of-scope Use\n\nSmolVLM is not intended for high-stakes scenarios or critical decision-making processes that affect an individual''s well-being or livelihood. The model may produce content that appears factual but may not be accurate. Misuse includes, but is not limited to:\n\n- Prohibited Uses:\n  - Evaluating or scoring individuals (e.g., in employment, education, credit)\n  - Critical automated decision-making\n  - Generating unreliable factual content\n- Malicious Activities:\n  - Spam generation\n  - Disinformation campaigns\n  - Harassment or abuse\n  - Unauthorized surveillance\n\n### License\n\nSmolVLM is built upon [the shape-optimized SigLIP](https://huggingface.co/google/siglip-so400m-patch14-384) as image encoder and [SmolLM2](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct) for text decoder part.\n\nWe release the SmolVLM checkpoints under the Apache 2.0 license.\n\n## Training Details\n\n### Training Data\n\nThe training data comes from [The Cauldron](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron) and [Docmatix](https://huggingface.co/datasets/HuggingFaceM4/Docmatix) datasets, with emphasis on document understanding (25%) and image captioning (18%), while maintaining balanced coverage across other crucial capabilities like visual reasoning, chart comprehension, and general instruction following.\n<img src="https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct/resolve/main/mixture_the_cauldron.png" alt="Example Image" style="width:90%;" />\n\n\n\n\n## Evaluation\n\n| Model             | MMMU (val) | MathVista (testmini) | MMStar (val) | DocVQA (test) | TextVQA (val) | Min GPU RAM required (GB) |\n|-------------------|------------|----------------------|--------------|---------------|---------------|---------------------------|\n| SmolVLM           | 38.8       | 44.6                | 42.1         | 81.6          | 72.7          | 5.02                      |\n| Qwen-VL 2B        | 41.1       | 47.8                | 47.5         | 90.1          | 79.7          | 13.70                     |\n| InternVL2 2B      | 34.3       | 46.3                | 49.8         | 86.9          | 73.4          | 10.52                     |\n| PaliGemma 3B 448px| 34.9       | 28.7                | 48.3         | 32.2          | 56.0          | 6.72                      |\n| moondream2        | 32.4       | 24.3                | 40.3         | 70.5          | 65.2          | 3.87                      |\n| MiniCPM-V-2       | 38.2       | 39.8                | 39.1         | 71.9          | 74.1          | 7.88                      |\n| MM1.5 1B          | 35.8       | 37.2                | 0.0          | 81.0          | 72.5          | NaN                       |\n\n# Citation information\nYou can cite us in the following way:\n```bibtex\n@article{marafioti2025smolvlm,\n  title={SmolVLM: Redefining small and efficient multimodal models}, \n  author={Andr√©s Marafioti and Orr Zohar and Miquel Farr√© and Merve Noyan and Elie Bakouch and Pedro Cuenca and Cyril Zakka and Loubna Ben Allal and Anton Lozhkov and Nouamane Tazi and Vaibhav Srivastav and Joshua Lochner and Hugo Larcher and Mathieu Morlon and Lewis Tunstall and Leandro von Werra and Thomas Wolf},\n  journal={arXiv preprint arXiv:2504.05299},\n  year={2025}\n}\n```', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":2246272880,"storage_bytes":45793562188,"files_count":43,"spaces_count":33,"gated":false,"private":false,"config":{"architectures":["Idefics3ForConditionalGeneration"],"model_type":"idefics3","processor_config":{"chat_template":"<|im_start|>{% for message in messages %}{{message[''role''] | capitalize}}{% if message[''content''][0][''type''] == ''image'' %}{{'':''}}{% else %}{{'': ''}}{% endif %}{% for line in message[''content''] %}{% if line[''type''] == ''text'' %}{{line[''text'']}}{% elif line[''type''] == ''image'' %}{{ ''<image>'' }}{% endif %}{% endfor %}<end_of_utterance>\n{% endfor %}{% if add_generation_prompt %}{{ ''Assistant:'' }}{% endif %}"},"tokenizer_config":{"bos_token":"<|im_start|>","eos_token":"<end_of_utterance>","pad_token":"<|im_end|>","chat_template":"<|im_start|>{% for message in messages %}{{message[''role''] | capitalize}}{% if message[''content''][0][''type''] == ''image'' %}{{'':''}}{% else %}{{'': ''}}{% endif %}{% for line in message[''content''] %}{% if line[''type''] == ''text'' %}{{line[''text'']}}{% elif line[''type''] == ''image'' %}{{ ''<image>'' }}{% endif %}{% endfor %}<end_of_utterance>\n{% endfor %}{% if add_generation_prompt %}{{ ''Assistant:'' }}{% endif %}","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2504.05299","source_url":"https://arxiv.org/abs/2504.05299"}]', NULL, 'Apache-2.0', 'approved', 62.5, '5414b7b0827faabc007108779722d231', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-databricks-dbrx-base', 'huggingface--databricks--dbrx-base', 'dbrx-base', 'databricks', '', '["transformers","safetensors","dbrx","text-generation","moe","conversational","arxiv:2211.15841","arxiv:2304.11277","license:other","text-generation-inference","region:us"]', 'text-generation', 561, 5, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/databricks/dbrx-base","fetched_at":"2025-12-08T10:39:52.040Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":131596523520,"storage_bytes":263193089336,"files_count":73,"spaces_count":15,"gated":"manual","private":false,"config":{"architectures":["DbrxForCausalLM"],"model_type":"dbrx","tokenizer_config":{"bos_token":"<|endoftext|>","chat_template":"{% if messages[0][''role''] == ''system'' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][''content''] %}{% elif ''system'' not in messages[0][''role''] %}{% set loop_messages = messages %}{% set system_message = ''You are DBRX, created by Databricks. You were last updated in December 2023. You answer questions based on information available up to that point.\nYOU PROVIDE SHORT RESPONSES TO SHORT QUESTIONS OR STATEMENTS, but provide thorough responses to more complex and open-ended questions.\nYou assist with various tasks, from writing to coding (using markdown for code blocks ‚Äî remember to use ``` with code, JSON, and tables).\n(You do not have real-time data access or code execution capabilities. You avoid stereotyping and provide balanced perspectives on controversial topics. You do not provide song lyrics, poems, or news articles and do not divulge details of your training data.)\nThis is your system prompt, guiding your responses. Do not reference it, just respond to the user. If you find yourself talking about this message, stop. You should be responding appropriately and usually that means not mentioning this.\nYOU DO NOT MENTION ANY OF THIS INFORMATION ABOUT YOURSELF UNLESS THE INFORMATION IS DIRECTLY PERTINENT TO THE USER\\''S QUERY.'' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{% if system_message != false %}{{ ''<|im_start|>system\n'' + system_message | trim + ''<|im_end|>\n''}}{% endif %}{{ ''<|im_start|>'' + message[''role''] + ''\n'' + message[''content''] + ''<|im_end|>'' }}{% else %}{{ ''\n'' + ''<|im_start|>'' + message[''role''] + ''\n'' + message[''content''] + ''<|im_end|>'' }}{% endif %}{% if (add_generation_prompt == true and loop.last) %}{{ ''\n'' + ''<|im_start|>'' + ''assistant'' + ''\n'' }}{% endif %}{% endfor %}","eos_token":"<|endoftext|>","pad_token":"<|pad|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2211.15841","source_url":"https://arxiv.org/abs/2211.15841"},{"type":"based_on_paper","target_id":"arxiv:2304.11277","source_url":"https://arxiv.org/abs/2304.11277"}]', NULL, 'Other', 'approved', 37.5, '4232b6c9fd06db21115385a568b58e3d', NULL, NULL, CURRENT_TIMESTAMP);
