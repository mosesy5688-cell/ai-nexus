/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-intfloat-multilingual-e5-large', 'huggingface--intfloat--multilingual-e5-large', 'multilingual-e5-large', 'intfloat', '--- tags: - mteb - Sentence Transformers - sentence-similarity - feature-extraction - sentence-transformers model-index: - name: multilingual-e5-large results: - task: type: Classification dataset: type: mteb/amazon_counterfactual name: MTEB AmazonCounterfactualClassification (en) config: en split: test revision: e8379541af4e31359cca9fbcf4b00f2671dba205 metrics: - type: accuracy value: 79.05970149253731 - type: ap value: 43.486574390835635 - type: f1 value: 73.32700092140148 - task: type: Cla...', '["sentence-transformers","pytorch","onnx","safetensors","openvino","xlm-roberta","mteb","sentence transformers","sentence-similarity","feature-extraction","multilingual","af","am","ar","as","az","be","bg","bn","br","bs","ca","cs","cy","da","de","el","en","eo","es","et","eu","fa","fi","fr","fy","ga","gd","gl","gu","ha","he","hi","hr","hu","hy","id","is","it","ja","jv","ka","kk","km","kn","ko","ku","ky","la","lo","lt","lv","mg","mk","ml","mn","mr","ms","my","ne","nl","no","om","or","pa","pl","ps","pt","ro","ru","sa","sd","si","sk","sl","so","sq","sr","su","sv","sw","ta","te","th","tl","tr","ug","uk","ur","uz","vi","xh","yi","zh","arxiv:2402.05672","arxiv:2108.08787","arxiv:2104.08663","arxiv:2210.07316","license:mit","model-index","text-embeddings-inference","endpoints_compatible","deploy:azure","region:us"]', 'feature-extraction', 1094, 3307315, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/intfloat/multilingual-e5-large","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- mteb\n- Sentence Transformers\n- sentence-similarity\n- feature-extraction\n- sentence-transformers\nmodel-index:\n- name: multilingual-e5-large\n  results:\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en)\n      config: en\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 79.05970149253731\n    - type: ap\n      value: 43.486574390835635\n    - type: f1\n      value: 73.32700092140148\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (de)\n      config: de\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 71.22055674518201\n    - type: ap\n      value: 81.55756710830498\n    - type: f1\n      value: 69.28271787752661\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en-ext)\n      config: en-ext\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 80.41979010494754\n    - type: ap\n      value: 29.34879922376344\n    - type: f1\n      value: 67.62475449011278\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (ja)\n      config: ja\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 77.8372591006424\n    - type: ap\n      value: 26.557560591210738\n    - type: f1\n      value: 64.96619417368707\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_polarity\n      name: MTEB AmazonPolarityClassification\n      config: default\n      split: test\n      revision: e2d317d38cd51312af73b3d32a06d1a08b442046\n    metrics:\n    - type: accuracy\n      value: 93.489875\n    - type: ap\n      value: 90.98758636917603\n    - type: f1\n      value: 93.48554819717332\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (en)\n      config: en\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 47.564\n    - type: f1\n      value: 46.75122173518047\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (de)\n      config: de\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 45.400000000000006\n    - type: f1\n      value: 44.17195682400632\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (es)\n      config: es\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 43.068\n    - type: f1\n      value: 42.38155696855596\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (fr)\n      config: fr\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 41.89\n    - type: f1\n      value: 40.84407321682663\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (ja)\n      config: ja\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 40.120000000000005\n    - type: f1\n      value: 39.522976223819114\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (zh)\n      config: zh\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 38.832\n    - type: f1\n      value: 38.0392533394713\n  - task:\n      type: Retrieval\n    dataset:\n      type: arguana\n      name: MTEB ArguAna\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 30.725\n    - type: map_at_10\n      value: 46.055\n    - type: map_at_100\n      value: 46.900999999999996\n    - type: map_at_1000\n      value: 46.911\n    - type: map_at_3\n      value: 41.548\n    - type: map_at_5\n      value: 44.297\n    - type: mrr_at_1\n      value: 31.152\n    - type: mrr_at_10\n      value: 46.231\n    - type: mrr_at_100\n      value: 47.07\n    - type: mrr_at_1000\n      value: 47.08\n    - type: mrr_at_3\n      value: 41.738\n    - type: mrr_at_5\n      value: 44.468999999999994\n    - type: ndcg_at_1\n      value: 30.725\n    - type: ndcg_at_10\n      value: 54.379999999999995\n    - type: ndcg_at_100\n      value: 58.138\n    - type: ndcg_at_1000\n      value: 58.389\n    - type: ndcg_at_3\n      value: 45.156\n    - type: ndcg_at_5\n      value: 50.123\n    - type: precision_at_1\n      value: 30.725\n    - type: precision_at_10\n      value: 8.087\n    - type: precision_at_100\n      value: 0.9769999999999999\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_3\n      value: 18.54\n    - type: precision_at_5\n      value: 13.542000000000002\n    - type: recall_at_1\n      value: 30.725\n    - type: recall_at_10\n      value: 80.868\n    - type: recall_at_100\n      value: 97.653\n    - type: recall_at_1000\n      value: 99.57300000000001\n    - type: recall_at_3\n      value: 55.619\n    - type: recall_at_5\n      value: 67.71000000000001\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-p2p\n      name: MTEB ArxivClusteringP2P\n      config: default\n      split: test\n      revision: a122ad7f3f0291bf49cc6f4d32aa80929df69d5d\n    metrics:\n    - type: v_measure\n      value: 44.30960650674069\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-s2s\n      name: MTEB ArxivClusteringS2S\n      config: default\n      split: test\n      revision: f910caf1a6075f7329cdf8c1a6135696f37dbd53\n    metrics:\n    - type: v_measure\n      value: 38.427074197498996\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/askubuntudupquestions-reranking\n      name: MTEB AskUbuntuDupQuestions\n      config: default\n      split: test\n      revision: 2000358ca161889fa9c082cb41daa8dcfb161a54\n    metrics:\n    - type: map\n      value: 60.28270056031872\n    - type: mrr\n      value: 74.38332673789738\n  - task:\n      type: STS\n    dataset:\n      type: mteb/biosses-sts\n      name: MTEB BIOSSES\n      config: default\n      split: test\n      revision: d3fb88f8f02e40887cd149695127462bbcf29b4a\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.05942144105269\n    - type: cos_sim_spearman\n      value: 82.51212105850809\n    - type: euclidean_pearson\n      value: 81.95639829909122\n    - type: euclidean_spearman\n      value: 82.3717564144213\n    - type: manhattan_pearson\n      value: 81.79273425468256\n    - type: manhattan_spearman\n      value: 82.20066817871039\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (de-en)\n      config: de-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 99.46764091858039\n    - type: f1\n      value: 99.37717466945023\n    - type: precision\n      value: 99.33194154488518\n    - type: recall\n      value: 99.46764091858039\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (fr-en)\n      config: fr-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 98.29407880255337\n    - type: f1\n      value: 98.11248073959938\n    - type: precision\n      value: 98.02443319392472\n    - type: recall\n      value: 98.29407880255337\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (ru-en)\n      config: ru-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 97.79009352268791\n    - type: f1\n      value: 97.5176076665512\n    - type: precision\n      value: 97.38136473848286\n    - type: recall\n      value: 97.79009352268791\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (zh-en)\n      config: zh-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 99.26276987888363\n    - type: f1\n      value: 99.20133403545726\n    - type: precision\n      value: 99.17500438827453\n    - type: recall\n      value: 99.26276987888363\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/banking77\n      name: MTEB Banking77Classification\n      config: default\n      split: test\n      revision: 0fd18e25b25c072e09e0d92ab615fda904d66300\n    metrics:\n    - type: accuracy\n      value: 84.72727272727273\n    - type: f1\n      value: 84.67672206031433\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-p2p\n      name: MTEB BiorxivClusteringP2P\n      config: default\n      split: test\n      revision: 65b79d1d13f80053f67aca9498d9402c2d9f1f40\n    metrics:\n    - type: v_measure\n      value: 35.34220182511161\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-s2s\n      name: MTEB BiorxivClusteringS2S\n      config: default\n      split: test\n      revision: 258694dd0231531bc1fd9de6ceb52a0853c6d908\n    metrics:\n    - type: v_measure\n      value: 33.4987096128766\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 25.558249999999997\n    - type: map_at_10\n      value: 34.44425000000001\n    - type: map_at_100\n      value: 35.59833333333333\n    - type: map_at_1000\n      value: 35.706916666666665\n    - type: map_at_3\n      value: 31.691749999999995\n    - type: map_at_5\n      value: 33.252916666666664\n    - type: mrr_at_1\n      value: 30.252666666666666\n    - type: mrr_at_10\n      value: 38.60675\n    - type: mrr_at_100\n      value: 39.42666666666666\n    - type: mrr_at_1000\n      value: 39.48408333333334\n    - type: mrr_at_3\n      value: 36.17441666666665\n    - type: mrr_at_5\n      value: 37.56275\n    - type: ndcg_at_1\n      value: 30.252666666666666\n    - type: ndcg_at_10\n      value: 39.683\n    - type: ndcg_at_100\n      value: 44.68541666666667\n    - type: ndcg_at_1000\n      value: 46.94316666666668\n    - type: ndcg_at_3\n      value: 34.961749999999995\n    - type: ndcg_at_5\n      value: 37.215666666666664\n    - type: precision_at_1\n      value: 30.252666666666666\n    - type: precision_at_10\n      value: 6.904166666666667\n    - type: precision_at_100\n      value: 1.0989999999999995\n    - type: precision_at_1000\n      value: 0.14733333333333334\n    - type: precision_at_3\n      value: 16.037666666666667\n    - type: precision_at_5\n      value: 11.413583333333333\n    - type: recall_at_1\n      value: 25.558249999999997\n    - type: recall_at_10\n      value: 51.13341666666666\n    - type: recall_at_100\n      value: 73.08366666666667\n    - type: recall_at_1000\n      value: 88.79483333333334\n    - type: recall_at_3\n      value: 37.989083333333326\n    - type: recall_at_5\n      value: 43.787833333333325\n  - task:\n      type: Retrieval\n    dataset:\n      type: climate-fever\n      name: MTEB ClimateFEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 10.338\n    - type: map_at_10\n      value: 18.360000000000003\n    - type: map_at_100\n      value: 19.942\n    - type: map_at_1000\n      value: 20.134\n    - type: map_at_3\n      value: 15.174000000000001\n    - type: map_at_5\n      value: 16.830000000000002\n    - type: mrr_at_1\n      value: 23.257\n    - type: mrr_at_10\n      value: 33.768\n    - type: mrr_at_100\n      value: 34.707\n    - type: mrr_at_1000\n      value: 34.766000000000005\n    - type: mrr_at_3\n      value: 30.977\n    - type: mrr_at_5\n      value: 32.528\n    - type: ndcg_at_1\n      value: 23.257\n    - type: ndcg_at_10\n      value: 25.733\n    - type: ndcg_at_100\n      value: 32.288\n    - type: ndcg_at_1000\n      value: 35.992000000000004\n    - type: ndcg_at_3\n      value: 20.866\n    - type: ndcg_at_5\n      value: 22.612\n    - type: precision_at_1\n      value: 23.257\n    - type: precision_at_10\n      value: 8.124\n    - type: precision_at_100\n      value: 1.518\n    - type: precision_at_1000\n      value: 0.219\n    - type: precision_at_3\n      value: 15.679000000000002\n    - type: precision_at_5\n      value: 12.117\n    - type: recall_at_1\n      value: 10.338\n    - type: recall_at_10\n      value: 31.154\n    - type: recall_at_100\n      value: 54.161\n    - type: recall_at_1000\n      value: 75.21900000000001\n    - type: recall_at_3\n      value: 19.427\n    - type: recall_at_5\n      value: 24.214\n  - task:\n      type: Retrieval\n    dataset:\n      type: dbpedia-entity\n      name: MTEB DBPedia\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 8.498\n    - type: map_at_10\n      value: 19.103\n    - type: map_at_100\n      value: 27.375\n    - type: map_at_1000\n      value: 28.981\n    - type: map_at_3\n      value: 13.764999999999999\n    - type: map_at_5\n      value: 15.950000000000001\n    - type: mrr_at_1\n      value: 65.5\n    - type: mrr_at_10\n      value: 74.53800000000001\n    - type: mrr_at_100\n      value: 74.71799999999999\n    - type: mrr_at_1000\n      value: 74.725\n    - type: mrr_at_3\n      value: 72.792\n    - type: mrr_at_5\n      value: 73.554\n    - type: ndcg_at_1\n      value: 53.37499999999999\n    - type: ndcg_at_10\n      value: 41.286\n    - type: ndcg_at_100\n      value: 45.972\n    - type: ndcg_at_1000\n      value: 53.123\n    - type: ndcg_at_3\n      value: 46.172999999999995\n    - type: ndcg_at_5\n      value: 43.033\n    - type: precision_at_1\n      value: 65.5\n    - type: precision_at_10\n      value: 32.725\n    - type: precision_at_100\n      value: 10.683\n    - type: precision_at_1000\n      value: 1.978\n    - type: precision_at_3\n      value: 50\n    - type: precision_at_5\n      value: 41.349999999999994\n    - type: recall_at_1\n      value: 8.498\n    - type: recall_at_10\n      value: 25.070999999999998\n    - type: recall_at_100\n      value: 52.383\n    - type: recall_at_1000\n      value: 74.91499999999999\n    - type: recall_at_3\n      value: 15.207999999999998\n    - type: recall_at_5\n      value: 18.563\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/emotion\n      name: MTEB EmotionClassification\n      config: default\n      split: test\n      revision: 4f58c6b202a23cf9a4da393831edf4f9183cad37\n    metrics:\n    - type: accuracy\n      value: 46.5\n    - type: f1\n      value: 41.93833713984145\n  - task:\n      type: Retrieval\n    dataset:\n      type: fever\n      name: MTEB FEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 67.914\n    - type: map_at_10\n      value: 78.10000000000001\n    - type: map_at_100\n      value: 78.333\n    - type: map_at_1000\n      value: 78.346\n    - type: map_at_3\n      value: 76.626\n    - type: map_at_5\n      value: 77.627\n    - type: mrr_at_1\n      value: 72.74199999999999\n    - type: mrr_at_10\n      value: 82.414\n    - type: mrr_at_100\n      value: 82.511\n    - type: mrr_at_1000\n      value: 82.513\n    - type: mrr_at_3\n      value: 81.231\n    - type: mrr_at_5\n      value: 82.065\n    - type: ndcg_at_1\n      value: 72.74199999999999\n    - type: ndcg_at_10\n      value: 82.806\n    - type: ndcg_at_100\n      value: 83.677\n    - type: ndcg_at_1000\n      value: 83.917\n    - type: ndcg_at_3\n      value: 80.305\n    - type: ndcg_at_5\n      value: 81.843\n    - type: precision_at_1\n      value: 72.74199999999999\n    - type: precision_at_10\n      value: 10.24\n    - type: precision_at_100\n      value: 1.089\n    - type: precision_at_1000\n      value: 0.11299999999999999\n    - type: precision_at_3\n      value: 31.268\n    - type: precision_at_5\n      value: 19.706000000000003\n    - type: recall_at_1\n      value: 67.914\n    - type: recall_at_10\n      value: 92.889\n    - type: recall_at_100\n      value: 96.42699999999999\n    - type: recall_at_1000\n      value: 97.92\n    - type: recall_at_3\n      value: 86.21\n    - type: recall_at_5\n      value: 90.036\n  - task:\n      type: Retrieval\n    dataset:\n      type: fiqa\n      name: MTEB FiQA2018\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 22.166\n    - type: map_at_10\n      value: 35.57\n    - type: map_at_100\n      value: 37.405\n    - type: map_at_1000\n      value: 37.564\n    - type: map_at_3\n      value: 30.379\n    - type: map_at_5\n      value: 33.324\n    - type: mrr_at_1\n      value: 43.519000000000005\n    - type: mrr_at_10\n      value: 51.556000000000004\n    - type: mrr_at_100\n      value: 52.344\n    - type: mrr_at_1000\n      value: 52.373999999999995\n    - type: mrr_at_3\n      value: 48.868\n    - type: mrr_at_5\n      value: 50.319\n    - type: ndcg_at_1\n      value: 43.519000000000005\n    - type: ndcg_at_10\n      value: 43.803\n    - type: ndcg_at_100\n      value: 50.468999999999994\n    - type: ndcg_at_1000\n      value: 53.111\n    - type: ndcg_at_3\n      value: 38.893\n    - type: ndcg_at_5\n      value: 40.653\n    - type: precision_at_1\n      value: 43.519000000000005\n    - type: precision_at_10\n      value: 12.253\n    - type: precision_at_100\n      value: 1.931\n    - type: precision_at_1000\n      value: 0.242\n    - type: precision_at_3\n      value: 25.617\n    - type: precision_at_5\n      value: 19.383\n    - type: recall_at_1\n      value: 22.166\n    - type: recall_at_10\n      value: 51.6\n    - type: recall_at_100\n      value: 76.574\n    - type: recall_at_1000\n      value: 92.192\n    - type: recall_at_3\n      value: 34.477999999999994\n    - type: recall_at_5\n      value: 41.835\n  - task:\n      type: Retrieval\n    dataset:\n      type: hotpotqa\n      name: MTEB HotpotQA\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 39.041\n    - type: map_at_10\n      value: 62.961999999999996\n    - type: map_at_100\n      value: 63.79899999999999\n    - type: map_at_1000\n      value: 63.854\n    - type: map_at_3\n      value: 59.399\n    - type: map_at_5\n      value: 61.669\n    - type: mrr_at_1\n      value: 78.082\n    - type: mrr_at_10\n      value: 84.321\n    - type: mrr_at_100\n      value: 84.49600000000001\n    - type: mrr_at_1000\n      value: 84.502\n    - type: mrr_at_3\n      value: 83.421\n    - type: mrr_at_5\n      value: 83.977\n    - type: ndcg_at_1\n      value: 78.082\n    - type: ndcg_at_10\n      value: 71.229\n    - type: ndcg_at_100\n      value: 74.10900000000001\n    - type: ndcg_at_1000\n      value: 75.169\n    - type: ndcg_at_3\n      value: 66.28699999999999\n    - type: ndcg_at_5\n      value: 69.084\n    - type: precision_at_1\n      value: 78.082\n    - type: precision_at_10\n      value: 14.993\n    - type: precision_at_100\n      value: 1.7239999999999998\n    - type: precision_at_1000\n      value: 0.186\n    - type: precision_at_3\n      value: 42.737\n    - type: precision_at_5\n      value: 27.843\n    - type: recall_at_1\n      value: 39.041\n    - type: recall_at_10\n      value: 74.96300000000001\n    - type: recall_at_100\n      value: 86.199\n    - type: recall_at_1000\n      value: 93.228\n    - type: recall_at_3\n      value: 64.105\n    - type: recall_at_5\n      value: 69.608\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/imdb\n      name: MTEB ImdbClassification\n      config: default\n      split: test\n      revision: 3d86128a09e091d6018b6d26cad27f2739fc2db7\n    metrics:\n    - type: accuracy\n      value: 90.23160000000001\n    - type: ap\n      value: 85.5674856808308\n    - type: f1\n      value: 90.18033354786317\n  - task:\n      type: Retrieval\n    dataset:\n      type: msmarco\n      name: MTEB MSMARCO\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 24.091\n    - type: map_at_10\n      value: 36.753\n    - type: map_at_100\n      value: 37.913000000000004\n    - type: map_at_1000\n      value: 37.958999999999996\n    - type: map_at_3\n      value: 32.818999999999996\n    - type: map_at_5\n      value: 35.171\n    - type: mrr_at_1\n      value: 24.742\n    - type: mrr_at_10\n      value: 37.285000000000004\n    - type: mrr_at_100\n      value: 38.391999999999996\n    - type: mrr_at_1000\n      value: 38.431\n    - type: mrr_at_3\n      value: 33.440999999999995\n    - type: mrr_at_5\n      value: 35.75\n    - type: ndcg_at_1\n      value: 24.742\n    - type: ndcg_at_10\n      value: 43.698\n    - type: ndcg_at_100\n      value: 49.145\n    - type: ndcg_at_1000\n      value: 50.23800000000001\n    - type: ndcg_at_3\n      value: 35.769\n    - type: ndcg_at_5\n      value: 39.961999999999996\n    - type: precision_at_1\n      value: 24.742\n    - type: precision_at_10\n      value: 6.7989999999999995\n    - type: precision_at_100\n      value: 0.95\n    - type: precision_at_1000\n      value: 0.104\n    - type: precision_at_3\n      value: 15.096000000000002\n    - type: precision_at_5\n      value: 11.183\n    - type: recall_at_1\n      value: 24.091\n    - type: recall_at_10\n      value: 65.068\n    - type: recall_at_100\n      value: 89.899\n    - type: recall_at_1000\n      value: 98.16\n    - type: recall_at_3\n      value: 43.68\n    - type: recall_at_5\n      value: 53.754999999999995\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (en)\n      config: en\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 93.66621067031465\n    - type: f1\n      value: 93.49622853272142\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (de)\n      config: de\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 91.94702733164272\n    - type: f1\n      value: 91.17043441745282\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (es)\n      config: es\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 92.20146764509674\n    - type: f1\n      value: 91.98359080555608\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (fr)\n      config: fr\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 88.99780770435328\n    - type: f1\n      value: 89.19746342724068\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (hi)\n      config: hi\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 89.78486912871998\n    - type: f1\n      value: 89.24578823628642\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (th)\n      config: th\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 88.74502712477394\n    - type: f1\n      value: 89.00297573881542\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (en)\n      config: en\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 77.9046967624259\n    - type: f1\n      value: 59.36787125785957\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (de)\n      config: de\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 74.5280360664976\n    - type: f1\n      value: 57.17723440888718\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (es)\n      config: es\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 75.44029352901934\n    - type: f1\n      value: 54.052855531072964\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (fr)\n      config: fr\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 70.5606013153774\n    - type: f1\n      value: 52.62215934386531\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (hi)\n      config: hi\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 73.11581211903908\n    - type: f1\n      value: 52.341291845645465\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (th)\n      config: th\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 74.28933092224233\n    - type: f1\n      value: 57.07918745504911\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (af)\n      config: af\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 62.38063214525892\n    - type: f1\n      value: 59.46463723443009\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (am)\n      config: am\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 56.06926698049766\n    - type: f1\n      value: 52.49084283283562\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ar)\n      config: ar\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 60.74983187626093\n    - type: f1\n      value: 56.960640620165904\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (az)\n      config: az\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 64.86550100874243\n    - type: f1\n      value: 62.47370548140688\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (bn)\n      config: bn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 63.971082716879636\n    - type: f1\n      value: 61.03812421957381\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (cy)\n      config: cy\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 54.98318762609282\n    - type: f1\n      value: 51.51207916008392\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (da)\n      config: da\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.45527908540686\n    - type: f1\n      value: 66.16631905400318\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (de)\n      config: de\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.32750504371216\n    - type: f1\n      value: 66.16755288646591\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (el)\n      config: el\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.09213180901143\n    - type: f1\n      value: 66.95654394661507\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (en)\n      config: en\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 73.75588433086752\n    - type: f1\n      value: 71.79973779656923\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (es)\n      config: es\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.49428379287154\n    - type: f1\n      value: 68.37494379215734\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fa)\n      config: fa\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.90921318090115\n    - type: f1\n      value: 66.79517376481645\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fi)\n      config: fi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.12104909213181\n    - type: f1\n      value: 67.29448842879584\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fr)\n      config: fr\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.34095494283793\n    - type: f1\n      value: 67.01134288992947\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (he)\n      config: he\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.61264290517822\n    - type: f1\n      value: 64.68730512660757\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hi)\n      config: hi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.79757901815738\n    - type: f1\n      value: 65.24938539425598\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hu)\n      config: hu\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.68728984532616\n    - type: f1\n      value: 67.0487169762553\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hy)\n      config: hy\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 62.07464694014795\n    - type: f1\n      value: 59.183532276789286\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (id)\n      config: id\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.04707464694015\n    - type: f1\n      value: 67.66829629003848\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (is)\n      config: is\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 62.42434431741762\n    - type: f1\n      value: 59.01617226544757\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (it)\n      config: it\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.53127101546738\n    - type: f1\n      value: 68.10033760906255\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ja)\n      config: ja\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.50504371217215\n    - type: f1\n      value: 69.74931103158923\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (jv)\n      config: jv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 57.91190316072628\n    - type: f1\n      value: 54.05551136648796\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ka)\n      config: ka\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 51.78211163416275\n    - type: f1\n      value: 49.874888544058535\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (km)\n      config: km\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 47.017484868863484\n    - type: f1\n      value: 44.53364263352014\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (kn)\n      config: kn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 62.16207128446537\n    - type: f1\n      value: 59.01185692320829\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ko)\n      config: ko\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.42501681237391\n    - type: f1\n      value: 67.13169450166086\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (lv)\n      config: lv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.0780094149294\n    - type: f1\n      value: 64.41720167850707\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ml)\n      config: ml\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 65.57162071284466\n    - type: f1\n      value: 62.414138683804424\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (mn)\n      config: mn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 61.71149966375252\n    - type: f1\n      value: 58.594805125087234\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ms)\n      config: ms\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 66.03900470746471\n    - type: f1\n      value: 63.87937257883887\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (my)\n      config: my\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 60.8776059179556\n    - type: f1\n      value: 57.48587618059131\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (nb)\n      config: nb\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.87895090786819\n    - type: f1\n      value: 66.8141299430347\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (nl)\n      config: nl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.45057162071285\n    - type: f1\n      value: 67.46444039673516\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (pl)\n      config: pl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.546738399462\n    - type: f1\n      value: 68.63640876702655\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (pt)\n      config: pt\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.72965702757229\n    - type: f1\n      value: 68.54119560379115\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ro)\n      config: ro\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 68.35574983187625\n    - type: f1\n      value: 65.88844917691927\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ru)\n      config: ru\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.70477471418964\n    - type: f1\n      value: 69.19665697061978\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sl)\n      config: sl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.0880968392737\n    - type: f1\n      value: 64.76962317666086\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sq)\n      config: sq\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 65.18493611297916\n    - type: f1\n      value: 62.49984559035371\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sv)\n      config: sv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.75857431069265\n    - type: f1\n      value: 69.20053687623418\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sw)\n      config: sw\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 58.500336247478145\n    - type: f1\n      value: 55.2972398687929\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ta)\n      config: ta\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 62.68997982515132\n    - type: f1\n      value: 59.36848202755348\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (te)\n      config: te\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 63.01950235373235\n    - type: f1\n      value: 60.09351954625423\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (th)\n      config: th\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 68.29186281102892\n    - type: f1\n      value: 67.57860496703447\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (tl)\n      config: tl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 64.77471418964357\n    - type: f1\n      value: 61.913983147713836\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (tr)\n      config: tr\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.87222595830532\n    - type: f1\n      value: 66.03679033708141\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ur)\n      config: ur\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 64.04505716207127\n    - type: f1\n      value: 61.28569169817908\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (vi)\n      config: vi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.38466711499663\n    - type: f1\n      value: 67.20532357036844\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (zh-CN)\n      config: zh-CN\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.12306657700067\n    - type: f1\n      value: 68.91251226588182\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (zh-TW)\n      config: zh-TW\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 66.20040349697378\n    - type: f1\n      value: 66.02657347714175\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (af)\n      config: af\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 68.73907195696032\n    - type: f1\n      value: 66.98484521791418\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (am)\n      config: am\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 60.58843308675185\n    - type: f1\n      value: 58.95591723092005\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ar)\n      config: ar\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.22730329522528\n    - type: f1\n      value: 66.0894499712115\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (az)\n      config: az\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.48285137861465\n    - type: f1\n      value: 65.21963176785157\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (bn)\n      config: bn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 67.74714189643578\n    - type: f1\n      value: 66.8212192745412\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (cy)\n      config: cy\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 59.09213180901143\n    - type: f1\n      value: 56.70735546356339\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (da)\n      config: da\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.05716207128448\n    - type: f1\n      value: 74.8413712365364\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (de)\n      config: de\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.69737726967047\n    - type: f1\n      value: 74.7664341963\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (el)\n      config: el\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.90383322125084\n    - type: f1\n      value: 73.59201554448323\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (en)\n      config: en\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.51176866173503\n    - type: f1\n      value: 77.46104434577758\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (es)\n      config: es\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.31069266980496\n    - type: f1\n      value: 74.61048660675635\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fa)\n      config: fa\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 72.95225285810356\n    - type: f1\n      value: 72.33160006574627\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fi)\n      config: fi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.12373907195696\n    - type: f1\n      value: 73.20921012557481\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fr)\n      config: fr\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.86684599865501\n    - type: f1\n      value: 73.82348774610831\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (he)\n      config: he\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.40215198386012\n    - type: f1\n      value: 71.11945183971858\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hi)\n      config: hi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 72.12844653665098\n    - type: f1\n      value: 71.34450495911766\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hu)\n      config: hu\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.52252858103566\n    - type: f1\n      value: 73.98878711342999\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hy)\n      config: hy\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 64.93611297915265\n    - type: f1\n      value: 63.723200467653385\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (id)\n      config: id\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.11903160726295\n    - type: f1\n      value: 73.82138439467096\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (is)\n      config: is\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 67.15198386012105\n    - type: f1\n      value: 66.02172193802167\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (it)\n      config: it\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.32414256893072\n    - type: f1\n      value: 74.30943421170574\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ja)\n      config: ja\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.46805648957633\n    - type: f1\n      value: 77.62808409298209\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (jv)\n      config: jv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 63.318762609280434\n    - type: f1\n      value: 62.094284066075076\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ka)\n      config: ka\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 58.34902488231338\n    - type: f1\n      value: 57.12893860987984\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (km)\n      config: km\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 50.88433086751849\n    - type: f1\n      value: 48.2272350802058\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (kn)\n      config: kn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.4425016812374\n    - type: f1\n      value: 64.61463095996173\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ko)\n      config: ko\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.04707464694015\n    - type: f1\n      value: 75.05099199098998\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (lv)\n      config: lv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 70.50437121721586\n    - type: f1\n      value: 69.83397721096314\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ml)\n      config: ml\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 69.94283792871553\n    - type: f1\n      value: 68.8704663703913\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (mn)\n      config: mn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 64.79488903833222\n    - type: f1\n      value: 63.615424063345436\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ms)\n      config: ms\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 69.88231338264963\n    - type: f1\n      value: 68.57892302593237\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (my)\n      config: my\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 63.248150638870214\n    - type: f1\n      value: 61.06680605338809\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (nb)\n      config: nb\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.84196368527236\n    - type: f1\n      value: 74.52566464968763\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (nl)\n      config: nl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.8285137861466\n    - type: f1\n      value: 74.8853197608802\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (pl)\n      config: pl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.13248150638869\n    - type: f1\n      value: 74.3982040999179\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (pt)\n      config: pt\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.49024882313383\n    - type: f1\n      value: 73.82153848368573\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ro)\n      config: ro\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.72158708809684\n    - type: f1\n      value: 71.85049433180541\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ru)\n      config: ru\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.137861466039\n    - type: f1\n      value: 75.37628348188467\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sl)\n      config: sl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.86953597848016\n    - type: f1\n      value: 71.87537624521661\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sq)\n      config: sq\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 70.27572293207801\n    - type: f1\n      value: 68.80017302344231\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sv)\n      config: sv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.09952925353059\n    - type: f1\n      value: 76.07992707688408\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sw)\n      config: sw\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 63.140551445864155\n    - type: f1\n      value: 61.73855010331415\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ta)\n      config: ta\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.27774041694687\n    - type: f1\n      value: 64.83664868894539\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (te)\n      config: te\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.69468728984533\n    - type: f1\n      value: 64.76239666920868\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (th)\n      config: th\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.44653665097512\n    - type: f1\n      value: 73.14646052013873\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (tl)\n      config: tl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 67.71351714862139\n    - type: f1\n      value: 66.67212180163382\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (tr)\n      config: tr\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.9946200403497\n    - type: f1\n      value: 73.87348793725525\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ur)\n      config: ur\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 68.15400134498992\n    - type: f1\n      value: 67.09433241421094\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (vi)\n      config: vi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.11365164761264\n    - type: f1\n      value: 73.59502539433753\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (zh-CN)\n      config: zh-CN\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.82582380632145\n    - type: f1\n      value: 76.89992945316313\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (zh-TW)\n      config: zh-TW\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.81237390719569\n    - type: f1\n      value: 72.36499770986265\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-p2p\n      name: MTEB MedrxivClusteringP2P\n      config: default\n      split: test\n      revision: e7a26af6f3ae46b30dde8737f02c07b1505bcc73\n    metrics:\n    - type: v_measure\n      value: 31.480506569594695\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-s2s\n      name: MTEB MedrxivClusteringS2S\n      config: default\n      split: test\n      revision: 35191c8c0dca72d8ff3efcd72aa802307d469663\n    metrics:\n    - type: v_measure\n      value: 29.71252128004552\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/mind_small\n      name: MTEB MindSmallReranking\n      config: default\n      split: test\n      revision: 3bdac13927fdc888b903db93b2ffdbd90b295a69\n    metrics:\n    - type: map\n      value: 31.421396787056548\n    - type: mrr\n      value: 32.48155274872267\n  - task:\n      type: Retrieval\n    dataset:\n      type: nfcorpus\n      name: MTEB NFCorpus\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 5.595\n    - type: map_at_10\n      value: 12.642000000000001\n    - type: map_at_100\n      value: 15.726\n    - type: map_at_1000\n      value: 17.061999999999998\n    - type: map_at_3\n      value: 9.125\n    - type: map_at_5\n      value: 10.866000000000001\n    - type: mrr_at_1\n      value: 43.344\n    - type: mrr_at_10\n      value: 52.227999999999994\n    - type: mrr_at_100\n      value: 52.898999999999994\n    - type: mrr_at_1000\n      value: 52.944\n    - type: mrr_at_3\n      value: 49.845\n    - type: mrr_at_5\n      value: 51.115\n    - type: ndcg_at_1\n      value: 41.949999999999996\n    - type: ndcg_at_10\n      value: 33.995\n    - type: ndcg_at_100\n      value: 30.869999999999997\n    - type: ndcg_at_1000\n      value: 39.487\n    - type: ndcg_at_3\n      value: 38.903999999999996\n    - type: ndcg_at_5\n      value: 37.236999999999995\n    - type: precision_at_1\n      value: 43.344\n    - type: precision_at_10\n      value: 25.480000000000004\n    - type: precision_at_100\n      value: 7.672\n    - type: precision_at_1000\n      value: 2.028\n    - type: precision_at_3\n      value: 36.636\n    - type: precision_at_5\n      value: 32.632\n    - type: recall_at_1\n      value: 5.595\n    - type: recall_at_10\n      value: 16.466\n    - type: recall_at_100\n      value: 31.226\n    - type: recall_at_1000\n      value: 62.778999999999996\n    - type: recall_at_3\n      value: 9.931\n    - type: recall_at_5\n      value: 12.884\n  - task:\n      type: Retrieval\n    dataset:\n      type: nq\n      name: MTEB NQ\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 40.414\n    - type: map_at_10\n      value: 56.754000000000005\n    - type: map_at_100\n      value: 57.457\n    - type: map_at_1000\n      value: 57.477999999999994\n    - type: map_at_3\n      value: 52.873999999999995\n    - type: map_at_5\n      value: 55.175\n    - type: mrr_at_1\n      value: 45.278\n    - type: mrr_at_10\n      value: 59.192\n    - type: mrr_at_100\n      value: 59.650000000000006\n    - type: mrr_at_1000\n      value: 59.665\n    - type: mrr_at_3\n      value: 56.141\n    - type: mrr_at_5\n      value: 57.998000000000005\n    - type: ndcg_at_1\n      value: 45.278\n    - type: ndcg_at_10\n      value: 64.056\n    - type: ndcg_at_100\n      value: 66.89\n    - type: ndcg_at_1000\n      value: 67.364\n    - type: ndcg_at_3\n      value: 56.97\n    - type: ndcg_at_5\n      value: 60.719\n    - type: precision_at_1\n      value: 45.278\n    - type: precision_at_10\n      value: 9.994\n    - type: precision_at_100\n      value: 1.165\n    - type: precision_at_1000\n      value: 0.121\n    - type: precision_at_3\n      value: 25.512\n    - type: precision_at_5\n      value: 17.509\n    - type: recall_at_1\n      value: 40.414\n    - type: recall_at_10\n      value: 83.596\n    - type: recall_at_100\n      value: 95.72\n    - type: recall_at_1000\n      value: 99.24\n    - type: recall_at_3\n      value: 65.472\n    - type: recall_at_5\n      value: 74.039\n  - task:\n      type: Retrieval\n    dataset:\n      type: quora\n      name: MTEB QuoraRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 70.352\n    - type: map_at_10\n      value: 84.369\n    - type: map_at_100\n      value: 85.02499999999999\n    - type: map_at_1000\n      value: 85.04\n    - type: map_at_3\n      value: 81.42399999999999\n    - type: map_at_5\n      value: 83.279\n    - type: mrr_at_1\n      value: 81.05\n    - type: mrr_at_10\n      value: 87.401\n    - type: mrr_at_100\n      value: 87.504\n    - type: mrr_at_1000\n      value: 87.505\n    - type: mrr_at_3\n      value: 86.443\n    - type: mrr_at_5\n      value: 87.10799999999999\n    - type: ndcg_at_1\n      value: 81.04\n    - type: ndcg_at_10\n      value: 88.181\n    - type: ndcg_at_100\n      value: 89.411\n    - type: ndcg_at_1000\n      value: 89.507\n    - type: ndcg_at_3\n      value: 85.28099999999999\n    - type: ndcg_at_5\n      value: 86.888\n    - type: precision_at_1\n      value: 81.04\n    - type: precision_at_10\n      value: 13.406\n    - type: precision_at_100\n      value: 1.5350000000000001\n    - type: precision_at_1000\n      value: 0.157\n    - type: precision_at_3\n      value: 37.31\n    - type: precision_at_5\n      value: 24.54\n    - type: recall_at_1\n      value: 70.352\n    - type: recall_at_10\n      value: 95.358\n    - type: recall_at_100\n      value: 99.541\n    - type: recall_at_1000\n      value: 99.984\n    - type: recall_at_3\n      value: 87.111\n    - type: recall_at_5\n      value: 91.643\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering\n      name: MTEB RedditClustering\n      config: default\n      split: test\n      revision: 24640382cdbf8abc73003fb0fa6d111a705499eb\n    metrics:\n    - type: v_measure\n      value: 46.54068723291946\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering-p2p\n      name: MTEB RedditClusteringP2P\n      config: default\n      split: test\n      revision: 282350215ef01743dc01b456c7f5241fa8937f16\n    metrics:\n    - type: v_measure\n      value: 63.216287629895994\n  - task:\n      type: Retrieval\n    dataset:\n      type: scidocs\n      name: MTEB SCIDOCS\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 4.023000000000001\n    - type: map_at_10\n      value: 10.071\n    - type: map_at_100\n      value: 11.892\n    - type: map_at_1000\n      value: 12.196\n    - type: map_at_3\n      value: 7.234\n    - type: map_at_5\n      value: 8.613999999999999\n    - type: mrr_at_1\n      value: 19.900000000000002\n    - type: mrr_at_10\n      value: 30.516\n    - type: mrr_at_100\n      value: 31.656000000000002\n    - type: mrr_at_1000\n      value: 31.723000000000003\n    - type: mrr_at_3\n      value: 27.400000000000002\n    - type: mrr_at_5\n      value: 29.270000000000003\n    - type: ndcg_at_1\n      value: 19.900000000000002\n    - type: ndcg_at_10\n      value: 17.474\n    - type: ndcg_at_100\n      value: 25.020999999999997\n    - type: ndcg_at_1000\n      value: 30.728\n    - type: ndcg_at_3\n      value: 16.588\n    - type: ndcg_at_5\n      value: 14.498\n    - type: precision_at_1\n      value: 19.900000000000002\n    - type: precision_at_10\n      value: 9.139999999999999\n    - type: precision_at_100\n      value: 2.011\n    - type: precision_at_1000\n      value: 0.33899999999999997\n    - type: precision_at_3\n      value: 15.667\n    - type: precision_at_5\n      value: 12.839999999999998\n    - type: recall_at_1\n      value: 4.023000000000001\n    - type: recall_at_10\n      value: 18.497\n    - type: recall_at_100\n      value: 40.8\n    - type: recall_at_1000\n      value: 68.812\n    - type: recall_at_3\n      value: 9.508\n    - type: recall_at_5\n      value: 12.983\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sickr-sts\n      name: MTEB SICK-R\n      config: default\n      split: test\n      revision: a6ea5a8cab320b040a23452cc28066d9beae2cee\n    metrics:\n    - type: cos_sim_pearson\n      value: 83.967008785134\n    - type: cos_sim_spearman\n      value: 80.23142141101837\n    - type: euclidean_pearson\n      value: 81.20166064704539\n    - type: euclidean_spearman\n      value: 80.18961335654585\n    - type: manhattan_pearson\n      value: 81.13925443187625\n    - type: manhattan_spearman\n      value: 80.07948723044424\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts12-sts\n      name: MTEB STS12\n      config: default\n      split: test\n      revision: a0d554a64d88156834ff5ae9920b964011b16384\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.94262461316023\n    - type: cos_sim_spearman\n      value: 80.01596278563865\n    - type: euclidean_pearson\n      value: 83.80799622922581\n    - type: euclidean_spearman\n      value: 79.94984954947103\n    - type: manhattan_pearson\n      value: 83.68473841756281\n    - type: manhattan_spearman\n      value: 79.84990707951822\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts13-sts\n      name: MTEB STS13\n      config: default\n      split: test\n      revision: 7e90230a92c190f1bf69ae9002b8cea547a64cca\n    metrics:\n    - type: cos_sim_pearson\n      value: 80.57346443146068\n    - type: cos_sim_spearman\n      value: 81.54689837570866\n    - type: euclidean_pearson\n      value: 81.10909881516007\n    - type: euclidean_spearman\n      value: 81.56746243261762\n    - type: manhattan_pearson\n      value: 80.87076036186582\n    - type: manhattan_spearman\n      value: 81.33074987964402\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts14-sts\n      name: MTEB STS14\n      config: default\n      split: test\n      revision: 6031580fec1f6af667f0bd2da0a551cf4f0b2375\n    metrics:\n    - type: cos_sim_pearson\n      value: 79.54733787179849\n    - type: cos_sim_spearman\n      value: 77.72202105610411\n    - type: euclidean_pearson\n      value: 78.9043595478849\n    - type: euclidean_spearman\n      value: 77.93422804309435\n    - type: manhattan_pearson\n      value: 78.58115121621368\n    - type: manhattan_spearman\n      value: 77.62508135122033\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts15-sts\n      name: MTEB STS15\n      config: default\n      split: test\n      revision: ae752c7c21bf194d8b67fd573edf7ae58183cbe3\n    metrics:\n    - type: cos_sim_pearson\n      value: 88.59880017237558\n    - type: cos_sim_spearman\n      value: 89.31088630824758\n    - type: euclidean_pearson\n      value: 88.47069261564656\n    - type: euclidean_spearman\n      value: 89.33581971465233\n    - type: manhattan_pearson\n      value: 88.40774264100956\n    - type: manhattan_spearman\n      value: 89.28657485627835\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts16-sts\n      name: MTEB STS16\n      config: default\n      split: test\n      revision: 4d8694f8f0e0100860b497b999b3dbed754a0513\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.08055117917084\n    - type: cos_sim_spearman\n      value: 85.78491813080304\n    - type: euclidean_pearson\n      value: 84.99329155500392\n    - type: euclidean_spearman\n      value: 85.76728064677287\n    - type: manhattan_pearson\n      value: 84.87947428989587\n    - type: manhattan_spearman\n      value: 85.62429454917464\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (ko-ko)\n      config: ko-ko\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 82.14190939287384\n    - type: cos_sim_spearman\n      value: 82.27331573306041\n    - type: euclidean_pearson\n      value: 81.891896953716\n    - type: euclidean_spearman\n      value: 82.37695542955998\n    - type: manhattan_pearson\n      value: 81.73123869460504\n    - type: manhattan_spearman\n      value: 82.19989168441421\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (ar-ar)\n      config: ar-ar\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 76.84695301843362\n    - type: cos_sim_spearman\n      value: 77.87790986014461\n    - type: euclidean_pearson\n      value: 76.91981583106315\n    - type: euclidean_spearman\n      value: 77.88154772749589\n    - type: manhattan_pearson\n      value: 76.94953277451093\n    - type: manhattan_spearman\n      value: 77.80499230728604\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-ar)\n      config: en-ar\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 75.44657840482016\n    - type: cos_sim_spearman\n      value: 75.05531095119674\n    - type: euclidean_pearson\n      value: 75.88161755829299\n    - type: euclidean_spearman\n      value: 74.73176238219332\n    - type: manhattan_pearson\n      value: 75.63984765635362\n    - type: manhattan_spearman\n      value: 74.86476440770737\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-de)\n      config: en-de\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.64700140524133\n    - type: cos_sim_spearman\n      value: 86.16014210425672\n    - type: euclidean_pearson\n      value: 86.49086860843221\n    - type: euclidean_spearman\n      value: 86.09729326815614\n    - type: manhattan_pearson\n      value: 86.43406265125513\n    - type: manhattan_spearman\n      value: 86.17740150939994\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-en)\n      config: en-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.91170098764921\n    - type: cos_sim_spearman\n      value: 88.12437004058931\n    - type: euclidean_pearson\n      value: 88.81828254494437\n    - type: euclidean_spearman\n      value: 88.14831794572122\n    - type: manhattan_pearson\n      value: 88.93442183448961\n    - type: manhattan_spearman\n      value: 88.15254630778304\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-tr)\n      config: en-tr\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 72.91390577997292\n    - type: cos_sim_spearman\n      value: 71.22979457536074\n    - type: euclidean_pearson\n      value: 74.40314008106749\n    - type: euclidean_spearman\n      value: 72.54972136083246\n    - type: manhattan_pearson\n      value: 73.85687539530218\n    - type: manhattan_spearman\n      value: 72.09500771742637\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (es-en)\n      config: es-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 80.9301067983089\n    - type: cos_sim_spearman\n      value: 80.74989828346473\n    - type: euclidean_pearson\n      value: 81.36781301814257\n    - type: euclidean_spearman\n      value: 80.9448819964426\n    - type: manhattan_pearson\n      value: 81.0351322685609\n    - type: manhattan_spearman\n      value: 80.70192121844177\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (es-es)\n      config: es-es\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.13820465980005\n    - type: cos_sim_spearman\n      value: 86.73532498758757\n    - type: euclidean_pearson\n      value: 87.21329451846637\n    - type: euclidean_spearman\n      value: 86.57863198601002\n    - type: manhattan_pearson\n      value: 87.06973713818554\n    - type: manhattan_spearman\n      value: 86.47534918791499\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (fr-en)\n      config: fr-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.48720108904415\n    - type: cos_sim_spearman\n      value: 85.62221757068387\n    - type: euclidean_pearson\n      value: 86.1010129512749\n    - type: euclidean_spearman\n      value: 85.86580966509942\n    - type: manhattan_pearson\n      value: 86.26800938808971\n    - type: manhattan_spearman\n      value: 85.88902721678429\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (it-en)\n      config: it-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 83.98021347333516\n    - type: cos_sim_spearman\n      value: 84.53806553803501\n    - type: euclidean_pearson\n      value: 84.61483347248364\n    - type: euclidean_spearman\n      value: 85.14191408011702\n    - type: manhattan_pearson\n      value: 84.75297588825967\n    - type: manhattan_spearman\n      value: 85.33176753669242\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (nl-en)\n      config: nl-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.51856644893233\n    - type: cos_sim_spearman\n      value: 85.27510748506413\n    - type: euclidean_pearson\n      value: 85.09886861540977\n    - type: euclidean_spearman\n      value: 85.62579245860887\n    - type: manhattan_pearson\n      value: 84.93017860464607\n    - type: manhattan_spearman\n      value: 85.5063988898453\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (en)\n      config: en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 62.581573200584195\n    - type: cos_sim_spearman\n      value: 63.05503590247928\n    - type: euclidean_pearson\n      value: 63.652564812602094\n    - type: euclidean_spearman\n      value: 62.64811520876156\n    - type: manhattan_pearson\n      value: 63.506842893061076\n    - type: manhattan_spearman\n      value: 62.51289573046917\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (de)\n      config: de\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 48.2248801729127\n    - type: cos_sim_spearman\n      value: 56.5936604678561\n    - type: euclidean_pearson\n      value: 43.98149464089\n    - type: euclidean_spearman\n      value: 56.108561882423615\n    - type: manhattan_pearson\n      value: 43.86880305903564\n    - type: manhattan_spearman\n      value: 56.04671150510166\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (es)\n      config: es\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 55.17564527009831\n    - type: cos_sim_spearman\n      value: 64.57978560979488\n    - type: euclidean_pearson\n      value: 58.8818330154583\n    - type: euclidean_spearman\n      value: 64.99214839071281\n    - type: manhattan_pearson\n      value: 58.72671436121381\n    - type: manhattan_spearman\n      value: 65.10713416616109\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (pl)\n      config: pl\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 26.772131864023297\n    - type: cos_sim_spearman\n      value: 34.68200792408681\n    - type: euclidean_pearson\n      value: 16.68082419005441\n    - type: euclidean_spearman\n      value: 34.83099932652166\n    - type: manhattan_pearson\n      value: 16.52605949659529\n    - type: manhattan_spearman\n      value: 34.82075801399475\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (tr)\n      config: tr\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 54.42415189043831\n    - type: cos_sim_spearman\n      value: 63.54594264576758\n    - type: euclidean_pearson\n      value: 57.36577498297745\n    - type: euclidean_spearman\n      value: 63.111466379158074\n    - type: manhattan_pearson\n      value: 57.584543715873885\n    - type: manhattan_spearman\n      value: 63.22361054139183\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (ar)\n      config: ar\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 47.55216762405518\n    - type: cos_sim_spearman\n      value: 56.98670142896412\n    - type: euclidean_pearson\n      value: 50.15318757562699\n    - type: euclidean_spearman\n      value: 56.524941926541906\n    - type: manhattan_pearson\n      value: 49.955618528674904\n    - type: manhattan_spearman\n      value: 56.37102209240117\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (ru)\n      config: ru\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 49.20540980338571\n    - type: cos_sim_spearman\n      value: 59.9009453504406\n    - type: euclidean_pearson\n      value: 49.557749853620535\n    - type: euclidean_spearman\n      value: 59.76631621172456\n    - type: manhattan_pearson\n      value: 49.62340591181147\n    - type: manhattan_spearman\n      value: 59.94224880322436\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (zh)\n      config: zh\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 51.508169956576985\n    - type: cos_sim_spearman\n      value: 66.82461565306046\n    - type: euclidean_pearson\n      value: 56.2274426480083\n    - type: euclidean_spearman\n      value: 66.6775323848333\n    - type: manhattan_pearson\n      value: 55.98277796300661\n    - type: manhattan_spearman\n      value: 66.63669848497175\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (fr)\n      config: fr\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 72.86478788045507\n    - type: cos_sim_spearman\n      value: 76.7946552053193\n    - type: euclidean_pearson\n      value: 75.01598530490269\n    - type: euclidean_spearman\n      value: 76.83618917858281\n    - type: manhattan_pearson\n      value: 74.68337628304332\n    - type: manhattan_spearman\n      value: 76.57480204017773\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (de-en)\n      config: de-en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 55.922619099401984\n    - type: cos_sim_spearman\n      value: 56.599362477240774\n    - type: euclidean_pearson\n      value: 56.68307052369783\n    - type: euclidean_spearman\n      value: 54.28760436777401\n    - type: manhattan_pearson\n      value: 56.67763566500681\n    - type: manhattan_spearman\n      value: 53.94619541711359\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (es-en)\n      config: es-en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 66.74357206710913\n    - type: cos_sim_spearman\n      value: 72.5208244925311\n    - type: euclidean_pearson\n      value: 67.49254562186032\n    - type: euclidean_spearman\n      value: 72.02469076238683\n    - type: manhattan_pearson\n      value: 67.45251772238085\n    - type: manhattan_spearman\n      value: 72.05538819984538\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (it)\n      config: it\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 71.25734330033191\n    - type: cos_sim_spearman\n      value: 76.98349083946823\n    - type: euclidean_pearson\n      value: 73.71642838667736\n    - type: euclidean_spearman\n      value: 77.01715504651384\n    - type: manhattan_pearson\n      value: 73.61712711868105\n    - type: manhattan_spearman\n      value: 77.01392571153896\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (pl-en)\n      config: pl-en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 63.18215462781212\n    - type: cos_sim_spearman\n      value: 65.54373266117607\n    - type: euclidean_pearson\n      value: 64.54126095439005\n    - type: euclidean_spearman\n      value: 65.30410369102711\n    - type: manhattan_pearson\n      value: 63.50332221148234\n    - type: manhattan_spearman\n      value: 64.3455878104313\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (zh-en)\n      config: zh-en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 62.30509221440029\n    - type: cos_sim_spearman\n      value: 65.99582704642478\n    - type: euclidean_pearson\n      value: 63.43818859884195\n    - type: euclidean_spearman\n      value: 66.83172582815764\n    - type: manhattan_pearson\n      value: 63.055779168508764\n    - type: manhattan_spearman\n      value: 65.49585020501449\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (es-it)\n      config: es-it\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 59.587830825340404\n    - type: cos_sim_spearman\n      value: 68.93467614588089\n    - type: euclidean_pearson\n      value: 62.3073527367404\n    - type: euclidean_spearman\n      value: 69.69758171553175\n    - type: manhattan_pearson\n      value: 61.9074580815789\n    - type: manhattan_spearman\n      value: 69.57696375597865\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (de-fr)\n      config: de-fr\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 57.143220125577066\n    - type: cos_sim_spearman\n      value: 67.78857859159226\n    - type: euclidean_pearson\n      value: 55.58225107923733\n    - type: euclidean_spearman\n      value: 67.80662907184563\n    - type: manhattan_pearson\n      value: 56.24953502726514\n    - type: manhattan_spearman\n      value: 67.98262125431616\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (de-pl)\n      config: de-pl\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 21.826928900322066\n    - type: cos_sim_spearman\n      value: 49.578506634400405\n    - type: euclidean_pearson\n      value: 27.939890138843214\n    - type: euclidean_spearman\n      value: 52.71950519136242\n    - type: manhattan_pearson\n      value: 26.39878683847546\n    - type: manhattan_spearman\n      value: 47.54609580342499\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (fr-pl)\n      config: fr-pl\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 57.27603854632001\n    - type: cos_sim_spearman\n      value: 50.709255283710995\n    - type: euclidean_pearson\n      value: 59.5419024445929\n    - type: euclidean_spearman\n      value: 50.709255283710995\n    - type: manhattan_pearson\n      value: 59.03256832438492\n    - type: manhattan_spearman\n      value: 61.97797868009122\n  - task:\n      type: STS\n    dataset:\n      type: mteb/stsbenchmark-sts\n      name: MTEB STSBenchmark\n      config: default\n      split: test\n      revision: b0fddb56ed78048fa8b90373c8a3cfc37b684831\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.00757054859712\n    - type: cos_sim_spearman\n      value: 87.29283629622222\n    - type: euclidean_pearson\n      value: 86.54824171775536\n    - type: euclidean_spearman\n      value: 87.24364730491402\n    - type: manhattan_pearson\n      value: 86.5062156915074\n    - type: manhattan_spearman\n      value: 87.15052170378574\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/scidocs-reranking\n      name: MTEB SciDocsRR\n      config: default\n      split: test\n      revision: d3c5e1fc0b855ab6097bf1cda04dd73947d7caab\n    metrics:\n    - type: map\n      value: 82.03549357197389\n    - type: mrr\n      value: 95.05437645143527\n  - task:\n      type: Retrieval\n    dataset:\n      type: scifact\n      name: MTEB SciFact\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 57.260999999999996\n    - type: map_at_10\n      value: 66.259\n    - type: map_at_100\n      value: 66.884\n    - type: map_at_1000\n      value: 66.912\n    - type: map_at_3\n      value: 63.685\n    - type: map_at_5\n      value: 65.35499999999999\n    - type: mrr_at_1\n      value: 60.333000000000006\n    - type: mrr_at_10\n      value: 67.5\n    - type: mrr_at_100\n      value: 68.013\n    - type: mrr_at_1000\n      value: 68.038\n    - type: mrr_at_3\n      value: 65.61099999999999\n    - type: mrr_at_5\n      value: 66.861\n    - type: ndcg_at_1\n      value: 60.333000000000006\n    - type: ndcg_at_10\n      value: 70.41\n    - type: ndcg_at_100\n      value: 73.10600000000001\n    - type: ndcg_at_1000\n      value: 73.846\n    - type: ndcg_at_3\n      value: 66.133\n    - type: ndcg_at_5\n      value: 68.499\n    - type: precision_at_1\n      value: 60.333000000000006\n    - type: precision_at_10\n      value: 9.232999999999999\n    - type: precision_at_100\n      value: 1.0630000000000002\n    - type: precision_at_1000\n      value: 0.11299999999999999\n    - type: precision_at_3\n      value: 25.667\n    - type: precision_at_5\n      value: 17.067\n    - type: recall_at_1\n      value: 57.260999999999996\n    - type: recall_at_10\n      value: 81.94399999999999\n    - type: recall_at_100\n      value: 93.867\n    - type: recall_at_1000\n      value: 99.667\n    - type: recall_at_3\n      value: 70.339\n    - type: recall_at_5\n      value: 76.25\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/sprintduplicatequestions-pairclassification\n      name: MTEB SprintDuplicateQuestions\n      config: default\n      split: test\n      revision: d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46\n    metrics:\n    - type: cos_sim_accuracy\n      value: 99.74356435643564\n    - type: cos_sim_ap\n      value: 93.13411948212683\n    - type: cos_sim_f1\n      value: 86.80521991300147\n    - type: cos_sim_precision\n      value: 84.00374181478017\n    - type: cos_sim_recall\n      value: 89.8\n    - type: dot_accuracy\n      value: 99.67920792079208\n    - type: dot_ap\n      value: 89.27277565444479\n    - type: dot_f1\n      value: 83.9276990718124\n    - type: dot_precision\n      value: 82.04393505253104\n    - type: dot_recall\n      value: 85.9\n    - type: euclidean_accuracy\n      value: 99.74257425742574\n    - type: euclidean_ap\n      value: 93.17993008259062\n    - type: euclidean_f1\n      value: 86.69396110542476\n    - type: euclidean_precision\n      value: 88.78406708595388\n    - type: euclidean_recall\n      value: 84.7\n    - type: manhattan_accuracy\n      value: 99.74257425742574\n    - type: manhattan_ap\n      value: 93.14413755550099\n    - type: manhattan_f1\n      value: 86.82483594144371\n    - type: manhattan_precision\n      value: 87.66564729867483\n    - type: manhattan_recall\n      value: 86\n    - type: max_accuracy\n      value: 99.74356435643564\n    - type: max_ap\n      value: 93.17993008259062\n    - type: max_f1\n      value: 86.82483594144371\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering\n      name: MTEB StackExchangeClustering\n      config: default\n      split: test\n      revision: 6cbc1f7b2bc0622f2e39d2c77fa502909748c259\n    metrics:\n    - type: v_measure\n      value: 57.525863806168566\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering-p2p\n      name: MTEB StackExchangeClusteringP2P\n      config: default\n      split: test\n      revision: 815ca46b2622cec33ccafc3735d572c266efdb44\n    metrics:\n    - type: v_measure\n      value: 32.68850574423839\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/stackoverflowdupquestions-reranking\n      name: MTEB StackOverflowDupQuestions\n      config: default\n      split: test\n      revision: e185fbe320c72810689fc5848eb6114e1ef5ec69\n    metrics:\n    - type: map\n      value: 49.71580650644033\n    - type: mrr\n      value: 50.50971903913081\n  - task:\n      type: Summarization\n    dataset:\n      type: mteb/summeval\n      name: MTEB SummEval\n      config: default\n      split: test\n      revision: cda12ad7615edc362dbf25a00fdd61d3b1eaf93c\n    metrics:\n    - type: cos_sim_pearson\n      value: 29.152190498799484\n    - type: cos_sim_spearman\n      value: 29.686180371952727\n    - type: dot_pearson\n      value: 27.248664793816342\n    - type: dot_spearman\n      value: 28.37748983721745\n  - task:\n      type: Retrieval\n    dataset:\n      type: trec-covid\n      name: MTEB TRECCOVID\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 0.20400000000000001\n    - type: map_at_10\n      value: 1.6209999999999998\n    - type: map_at_100\n      value: 9.690999999999999\n    - type: map_at_1000\n      value: 23.733\n    - type: map_at_3\n      value: 0.575\n    - type: map_at_5\n      value: 0.885\n    - type: mrr_at_1\n      value: 78\n    - type: mrr_at_10\n      value: 86.56700000000001\n    - type: mrr_at_100\n      value: 86.56700000000001\n    - type: mrr_at_1000\n      value: 86.56700000000001\n    - type: mrr_at_3\n      value: 85.667\n    - type: mrr_at_5\n      value: 86.56700000000001\n    - type: ndcg_at_1\n      value: 76\n    - type: ndcg_at_10\n      value: 71.326\n    - type: ndcg_at_100\n      value: 54.208999999999996\n    - type: ndcg_at_1000\n      value: 49.252\n    - type: ndcg_at_3\n      value: 74.235\n    - type: ndcg_at_5\n      value: 73.833\n    - type: precision_at_1\n      value: 78\n    - type: precision_at_10\n      value: 74.8\n    - type: precision_at_100\n      value: 55.50000000000001\n    - type: precision_at_1000\n      value: 21.836\n    - type: precision_at_3\n      value: 78\n    - type: precision_at_5\n      value: 78\n    - type: recall_at_1\n      value: 0.20400000000000001\n    - type: recall_at_10\n      value: 1.894\n    - type: recall_at_100\n      value: 13.245999999999999\n    - type: recall_at_1000\n      value: 46.373\n    - type: recall_at_3\n      value: 0.613\n    - type: recall_at_5\n      value: 0.991\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (sqi-eng)\n      config: sqi-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 95.89999999999999\n    - type: f1\n      value: 94.69999999999999\n    - type: precision\n      value: 94.11666666666667\n    - type: recall\n      value: 95.89999999999999\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (fry-eng)\n      config: fry-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 68.20809248554913\n    - type: f1\n      value: 63.431048720066066\n    - type: precision\n      value: 61.69143958161298\n    - type: recall\n      value: 68.20809248554913\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (kur-eng)\n      config: kur-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 71.21951219512195\n    - type: f1\n      value: 66.82926829268293\n    - type: precision\n      value: 65.1260162601626\n    - type: recall\n      value: 71.21951219512195\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (tur-eng)\n      config: tur-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.2\n    - type: f1\n      value: 96.26666666666667\n    - type: precision\n      value: 95.8\n    - type: recall\n      value: 97.2\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (deu-eng)\n      config: deu-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 99.3\n    - type: f1\n      value: 99.06666666666666\n    - type: precision\n      value: 98.95\n    - type: recall\n      value: 99.3\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (nld-eng)\n      config: nld-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.39999999999999\n    - type: f1\n      value: 96.63333333333333\n    - type: precision\n      value: 96.26666666666668\n    - type: recall\n      value: 97.39999999999999\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ron-eng)\n      config: ron-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 96\n    - type: f1\n      value: 94.86666666666666\n    - type: precision\n      value: 94.31666666666668\n    - type: recall\n      value: 96\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ang-eng)\n      config: ang-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 47.01492537313433\n    - type: f1\n      value: 40.178867566927266\n    - type: precision\n      value: 38.179295828549556\n    - type: recall\n      value: 47.01492537313433\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ido-eng)\n      config: ido-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 86.5\n    - type: f1\n      value: 83.62537480063796\n    - type: precision\n      value: 82.44555555555554\n    - type: recall\n      value: 86.5\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (jav-eng)\n      config: jav-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 80.48780487804879\n    - type: f1\n      value: 75.45644599303138\n    - type: precision\n      value: 73.37398373983739\n    - type: recall\n      value: 80.48780487804879\n  - task:\n      type: BitextMining\n    dataset:\n   ', '{"pipeline_tag":"feature-extraction","library_name":"sentence-transformers","framework":"sentence-transformers","params":559890946,"storage_bytes":10114130126,"files_count":23,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["XLMRobertaModel"],"model_type":"xlm-roberta","tokenizer_config":{"bos_token":"<s>","cls_token":"<s>","eos_token":"</s>","mask_token":{"__type":"AddedToken","content":"<mask>","lstrip":true,"normalized":true,"rstrip":false,"single_word":false},"pad_token":"<pad>","sep_token":"</s>","unk_token":"<unk>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2402.05672","source_url":"https://arxiv.org/abs/2402.05672"},{"type":"based_on_paper","target_id":"arxiv:2108.08787","source_url":"https://arxiv.org/abs/2108.08787"},{"type":"based_on_paper","target_id":"arxiv:2104.08663","source_url":"https://arxiv.org/abs/2104.08663"},{"type":"based_on_paper","target_id":"arxiv:2210.07316","source_url":"https://arxiv.org/abs/2210.07316"}]', NULL, 'MIT', 'approved', 80, 'bd0e66d16cf3efedabeb0bf058482cfa', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-sentence-transformers-paraphrase-multilingual-MiniLM-L12-v2', 'huggingface--sentence-transformers--paraphrase-multilingual-minilm-l12-v2', 'paraphrase-multilingual-MiniLM-L12-v2', 'sentence-transformers', '--- language: - multilingual - ar - bg - ca - cs - da - de - el - en - es - et - fa - fi - fr - gl - gu - he - hi - hr - hu - hy - id - it - ja - ka - ko - ku - lt - lv - mk - mn - mr - ms - my - nb - nl - pl - pt - ro - ru - sk - sl - sq - sr - sv - th - tr - uk - ur - vi license: apache-2.0 library_name: sentence-transformers tags: - sentence-transformers - feature-extraction - sentence-similarity - transformers language_bcp47: - fr-ca - pt-br - zh-cn - zh-tw pipeline_tag: sentence-similari...', '["sentence-transformers","pytorch","tf","onnx","safetensors","openvino","bert","feature-extraction","sentence-similarity","transformers","multilingual","ar","bg","ca","cs","da","de","el","en","es","et","fa","fi","fr","gl","gu","he","hi","hr","hu","hy","id","it","ja","ka","ko","ku","lt","lv","mk","mn","mr","ms","my","nb","nl","pl","pt","ro","ru","sk","sl","sq","sr","sv","th","tr","uk","ur","vi","arxiv:1908.10084","license:apache-2.0","text-embeddings-inference","endpoints_compatible","region:us"]', 'sentence-similarity', 1072, 16352771, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- multilingual\n- ar\n- bg\n- ca\n- cs\n- da\n- de\n- el\n- en\n- es\n- et\n- fa\n- fi\n- fr\n- gl\n- gu\n- he\n- hi\n- hr\n- hu\n- hy\n- id\n- it\n- ja\n- ka\n- ko\n- ku\n- lt\n- lv\n- mk\n- mn\n- mr\n- ms\n- my\n- nb\n- nl\n- pl\n- pt\n- ro\n- ru\n- sk\n- sl\n- sq\n- sr\n- sv\n- th\n- tr\n- uk\n- ur\n- vi\nlicense: apache-2.0\nlibrary_name: sentence-transformers\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\nlanguage_bcp47:\n- fr-ca\n- pt-br\n- zh-cn\n- zh-tw\npipeline_tag: sentence-similarity\n---\n\n# sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = ["This is an example sentence", "Each sentence is converted"]\n\nmodel = SentenceTransformer(''sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = [''This is an example sentence'', ''Each sentence is converted'']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(''sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'')\nmodel = AutoModel.from_pretrained(''sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=''pt'')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input[''attention_mask''])\n\nprint("Sentence embeddings:")\nprint(sentence_embeddings)\n```\n\n\n\n## Full Model Architecture\n```\nSentenceTransformer(\n  (0): Transformer({''max_seq_length'': 128, ''do_lower_case'': False}) with Transformer model: BertModel \n  (1): Pooling({''word_embedding_dimension'': 384, ''pooling_mode_cls_token'': False, ''pooling_mode_mean_tokens'': True, ''pooling_mode_max_tokens'': False, ''pooling_mode_mean_sqrt_len_tokens'': False})\n)\n```\n\n## Citing & Authors\n\nThis model was trained by [sentence-transformers](https://www.sbert.net/). \n        \nIf you find this model helpful, feel free to cite our publication [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084):\n```bibtex \n@inproceedings{reimers-2019-sentence-bert,\n    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",\n    author = "Reimers, Nils and Gurevych, Iryna",\n    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",\n    month = "11",\n    year = "2019",\n    publisher = "Association for Computational Linguistics",\n    url = "http://arxiv.org/abs/1908.10084",\n}\n```', '{"pipeline_tag":"sentence-similarity","library_name":"sentence-transformers","framework":"sentence-transformers","params":117654272,"storage_bytes":6650862358,"files_count":28,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BertModel"],"model_type":"bert","tokenizer_config":{"unk_token":"<unk>","sep_token":"</s>","pad_token":"<pad>","cls_token":"<s>","mask_token":{"content":"<mask>","single_word":false,"lstrip":true,"rstrip":false,"normalized":true,"__type":"AddedToken"},"bos_token":"<s>","eos_token":"</s>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1908.10084","source_url":"https://arxiv.org/abs/1908.10084"}]', NULL, 'Apache-2.0', 'approved', 65, '187022630cdc9060fbcdd645d95f89af', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-235B-A22B', 'huggingface--qwen--qwen3-235b-a22b', 'Qwen3-235B-A22B', 'Qwen', '--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-235B-A22B/blob/main/LICENSE pipeline_tag: text-generation --- <a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of d...', '["transformers","safetensors","qwen3_moe","text-generation","conversational","arxiv:2309.00071","arxiv:2505.09388","license:apache-2.0","endpoints_compatible","region:us"]', 'text-generation', 1056, 365681, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-235B-A22B","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-235B-A22B/blob/main/LICENSE\npipeline_tag: text-generation\n---\n\n# Qwen3-235B-A22B\n<a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Qwen3 Highlights\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n- **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n## Model Overview\n\n**Qwen3-235B-A22B** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 235B in total and 22B activated\n- Number of Paramaters (Non-Embedding): 234B\n- Number of Layers: 94\n- Number of Attention Heads (GQA): 64 for Q and 4 for KV\n- Number of Experts: 128\n- Number of Activated Experts: 8\n- Context Length: 32,768 natively and [131,072 tokens with YaRN](#processing-long-texts). \n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Quickstart\n\nThe code of Qwen3-MoE has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: ''qwen3_moe''\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen3-235B-A22B"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\n\n# prepare the model input\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")\n\nprint("thinking content:", thinking_content)\nprint("content:", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B --reasoning-parser qwen3 --tp 8\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-235B-A22B --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Switching Between Thinking and Non-Thinking Mode\n\n> [!TIP]\n> The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. \n> Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### `enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `<think>...</think>` block, followed by the final response.\n\n> [!NOTE]\n> For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n\n### `enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model''s thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `<think>...</think>` block.\n\n> [!NOTE]\n> For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n### Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model''s behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model''s thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name="Qwen/Qwen3-235B-A22B"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{"role": "user", "content": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors="pt")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({"role": "user", "content": user_input})\n        self.history.append({"role": "assistant", "content": response})\n\n        return response\n\n# Example Usage\nif __name__ == "__main__":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = "How many r''s in strawberries?"\n    print(f"User: {user_input_1}")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f"Bot: {response_1}")\n    print("----------------------")\n\n    # Second input with /no_think\n    user_input_2 = "Then, how many r''s in blueberries? /no_think"\n    print(f"User: {user_input_2}")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f"Bot: {response_2}") \n    print("----------------------")\n\n    # Third input with /think\n    user_input_3 = "Really? /think"\n    print(f"User: {user_input_3}")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f"Bot: {response_3}")\n```\n\n> [!NOTE]\n> For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `<think>...</think>`. However, the content inside this block may be empty if thinking is disabled.\n> When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `<think>...</think>` block.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    ''model'': ''Qwen3-235B-A22B'',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # ''model_type'': ''qwen_dashscope'',\n    # ''api_key'': os.getenv(''DASHSCOPE_API_KEY''),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    ''model_server'': ''http://localhost:8000/v1'',  # api_base\n    ''api_key'': ''EMPTY'',\n\n    # Other parameters:\n    # ''generate_cfg'': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         ''thought_in_content'': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {''mcpServers'': {  # You can specify the MCP configuration file\n            ''time'': {\n                ''command'': ''uvx'',\n                ''args'': [''mcp-server-time'', ''--local-timezone=Asia/Shanghai'']\n            },\n            "fetch": {\n                "command": "uvx",\n                "args": ["mcp-server-fetch"]\n            }\n        }\n    },\n  ''code_interpreter'',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{''role'': ''user'', ''content'': ''https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen''}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Processing Long Texts\n\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model''s performance on context lengths of up to 131,072 tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\n\nYaRN is currently supported by several inference frameworks, e.g., `transformers` and `llama.cpp` for local use, `vllm` and `sglang` for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\n\n- Modifying the model files:\n  In the `config.json` file, add the `rope_scaling` fields:\n    ```json\n    {\n        ...,\n        "rope_scaling": {\n            "rope_type": "yarn",\n            "factor": 4.0,\n            "original_max_position_embeddings": 32768\n        }\n    }\n    ```\n  For `llama.cpp`, you need to regenerate the GGUF file after the modification.\n\n- Passing command line arguments:\n\n  For `vllm`, you can use\n    ```shell\n    vllm serve ... --rope-scaling ''{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}'' --max-model-len 131072  \n    ```\n\n  For `sglang`, you can use\n    ```shell\n    python -m sglang.launch_server ... --json-model-override-args ''{"rope_scaling":{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}}''\n    ```\n\n  For `llama-server` from `llama.cpp`, you can use\n    ```shell\n    llama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\n    ```\n\n> [!IMPORTANT]\n> If you encounter the following warning\n> ```\n> Unrecognized keys in `rope_scaling` for ''rope_type''=''yarn'': {''original_max_position_embeddings''}\n> ```\n> please upgrade `transformers>=4.51.0`.\n\n> [!NOTE]\n> All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\n> We advise adding the `rope_scaling` configuration only when processing long contexts is required. \n> It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set `factor` as 2.0. \n\n> [!NOTE]\n> The default `max_position_embeddings` in `config.json` is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\n\n> [!TIP]\n> The endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n   - For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include "Please reason step by step, and put your final answer within \boxed{}." in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: "Please show your choice in the `answer` field with only the choice letter, e.g., `"answer": "C"`."\n\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":235093634560,"storage_bytes":470203304443,"files_count":128,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen3MoeForCausalLM"],"model_type":"qwen3_moe","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {{- messages[0].content + ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(''<tool_response>'') and message.content.endswith(''</tool_response>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '''' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '''' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if ''</think>'' in content %}\n                {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n                {%- set content = content.split(''</think>'')[-1].lstrip(''\\n'') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- ''<|im_start|>'' + message.role + ''\\n<think>\\n'' + reasoning_content.strip(''\\n'') + ''\\n</think>\\n\\n'' + content.lstrip(''\\n'') }}\n            {%- else %}\n                {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- ''<think>\\n\\n</think>\\n\\n'' }}\n    {%- endif %}\n{%- endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen3","source_url":"https://github.com/QwenLM/Qwen3"},{"type":"has_code","target_id":"github:QwenLM:Qwen-Agent","source_url":"https://github.com/QwenLM/Qwen-Agent"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2505.09388","source_url":"https://arxiv.org/abs/2505.09388"}]', NULL, 'Apache-2.0', 'approved', 80, 'df01f706f5623d81eef465ce16ff9248', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-ibm-granite-granite-docling-258M', 'huggingface--ibm-granite--granite-docling-258m', 'granite-docling-258M', 'ibm-granite', '--- license: apache-2.0 datasets: - ds4sd/SynthCodeNet - ds4sd/SynthFormulaNet - ds4sd/SynthChartNet - HuggingFaceM4/DoclingMatix tags: - text-generation - documents - code - formula - chart - ocr - layout - table - document-parse - docling - granite - extraction - math language: - en pipeline_tag: image-text-to-text library_name: transformers --- <div style="display: flex; align-items: center;"> <img src="https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/granite_docling.pn...', '["transformers","safetensors","idefics3","image-to-text","text-generation","documents","code","formula","chart","ocr","layout","table","document-parse","docling","granite","extraction","math","image-text-to-text","conversational","en","dataset:ds4sd/synthcodenet","dataset:ds4sd/synthformulanet","dataset:ds4sd/synthchartnet","dataset:huggingfacem4/doclingmatix","arxiv:2501.17887","arxiv:2503.11576","arxiv:2305.03393","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'image-text-to-text', 1041, 100708, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/ibm-granite/granite-docling-258M","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ndatasets:\n- ds4sd/SynthCodeNet\n- ds4sd/SynthFormulaNet\n- ds4sd/SynthChartNet\n- HuggingFaceM4/DoclingMatix\ntags:\n- text-generation\n- documents\n- code\n- formula\n- chart\n- ocr\n- layout\n- table\n- document-parse\n- docling\n- granite\n- extraction\n- math\nlanguage:\n- en\npipeline_tag: image-text-to-text\nlibrary_name: transformers\n---\n   \n# granite-docling-258m\n<div style="display: flex; align-items: center;">\n    <img src="https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/granite_docling.png" alt="Granite Docling Logo" style="width: 200px; height: auto; margin-right: 20px;">\n    <div>\n        <p>Granite Docling is a multimodal Image-Text-to-Text model engineered for efficient document conversion. It preserves the core features of Docling while maintaining seamless integration with <a href="https://docling-project.github.io/docling ">DoclingDocuments</a> to ensure full compatibility. </p>\n    </div>\n</div>\n\n**Model Summary**: \n\nGranite Docling 258M builds upon the Idefics3 architecture, but introduces two key modifications: it replaces the vision encoder with siglip2-base-patch16-512 and substitutes the language model with a Granite 165M LLM. Try out our [Granite-Docling-258](https://huggingface.co/spaces/ibm-granite/granite-docling-258m-demo) demo today.\n\n- **Developed by**: IBM Research\n- **Model type**: Multi-modal model (image+text-to-text)\n- **Language(s)**: English (NLP)\n- **License**: [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n- **Release Date**: September 17, 2025\n\nGranite-docling-258M is fully integrated into the Docling pipelines, carrying over existing [features](https://huggingface.co/ds4sd/SmolDocling-256M-preview) while introducing a number of powerful new features, including:\n\n-  Enhanced Equation Recognition: More accurate detection and formatting of mathematical formulas\n-  Flexible Inference Modes: Choose between full-page inference, bbox-guided region inference\n-  Improved Stability: Tends to avoid infinite loops more effectively\n-  Enhanced Inline Equations: Better inline math recognition\n-  Document Element QA: Answer questions about a documents structure such as the presence and order of document elements\n-  Japanese, Arabic and Chinese support (_experimental_)\n\n\n\n## Getting started\n\nThe easiest way to use this model is through the [Docling](https://github.com/docling-project/docling) library. It will automatically download this model and convert documents to various formats for you. \n\nInstall the latest version of `docling` through pip, then use the following CLI command:\n\n```sh\n# Convert to HTML and Markdown:\ndocling --to html --to md --pipeline vlm --vlm-model granite_docling "https://arxiv.org/pdf/2501.17887" # accepts files, urls or directories\n\n# Convert to HTML including layout visualization:\ndocling --to html_split_page --show-layout --pipeline vlm --vlm-model granite_docling "https://arxiv.org/pdf/2501.17887"\n\n```\n\n<p align="center">\n<img src="https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/assets/granite_docling_split_page.png" alt="GraniteDocling result in split page view" width="900"/>\n</p>\n\n<details>\n<summary>You can also set this model up within the Docling SDK:</summary>\n  \n```python\nfrom docling.datamodel import vlm_model_specs\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.datamodel.pipeline_options import (\n    VlmPipelineOptions,\n)\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\nfrom docling.pipeline.vlm_pipeline import VlmPipeline\n\nsource = "https://arxiv.org/pdf/2501.17887"\n\n###### USING SIMPLE DEFAULT VALUES\n# - GraniteDocling model\n# - Using the transformers framework\n\nconverter = DocumentConverter(\n    format_options={\n        InputFormat.PDF: PdfFormatOption(\n            pipeline_cls=VlmPipeline,\n        ),\n    }\n)\n\ndoc = converter.convert(source=source).document\n\nprint(doc.export_to_markdown())\n\n\n###### USING MACOS MPS ACCELERATOR\n# For more options see the compare_vlm_models.py example.\n\npipeline_options = VlmPipelineOptions(\n    vlm_options=vlm_model_specs.GRANITEDOCLING_MLX,\n)\n\nconverter = DocumentConverter(\n    format_options={\n        InputFormat.PDF: PdfFormatOption(\n            pipeline_cls=VlmPipeline,\n            pipeline_options=pipeline_options,\n        ),\n    }\n)\n\ndoc = converter.convert(source=source).document\n\nprint(doc.export_to_markdown())\n```\n</details>\n\n\nAlternatively, you can use bare **transformers**, **vllm**, **onnx** or **mlx-vlm** to perform inference, and [docling-core](https://github.com/docling-project/docling-core) APIs to convert results to variety of output formats (md, html, etc.):\n\n<details>\n<summary> Single page image inference using plain  tranformers </summary>\n\n```python\n# Prerequisites:\n# pip install torch\n# pip install docling_core\n# pip install transformers\n\nimport torch\nfrom docling_core.types.doc import DoclingDocument\nfrom docling_core.types.doc.document import DocTagsDocument\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom transformers.image_utils import load_image\nfrom pathlib import Path\n\nDEVICE = "cuda" if torch.cuda.is_available() else "cpu"\n\n# Load images\nimage = load_image("https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/assets/new_arxiv.png")\n\n# Initialize processor and model\nprocessor = AutoProcessor.from_pretrained("ibm-granite/granite-docling-258M")\nmodel = AutoModelForVision2Seq.from_pretrained(\n    "ibm-granite/granite-docling-258M",\n    torch_dtype=torch.bfloat16,\n    _attn_implementation="flash_attention_2" if DEVICE == "cuda" else "sdpa",\n).to(DEVICE)\n\n# Create input messages\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image"},\n            {"type": "text", "text": "Convert this page to docling."}\n        ]\n    },\n]\n\n# Prepare inputs\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=prompt, images=[image], return_tensors="pt")\ninputs = inputs.to(DEVICE)\n\n# Generate outputs\ngenerated_ids = model.generate(**inputs, max_new_tokens=8192)\nprompt_length = inputs.input_ids.shape[1]\ntrimmed_generated_ids = generated_ids[:, prompt_length:]\ndoctags = processor.batch_decode(\n    trimmed_generated_ids,\n    skip_special_tokens=False,\n)[0].lstrip()\n\nprint(f"DocTags: \n{doctags}\n")\n\n\n# Populate document\ndoctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\n# create a docling document\ndoc = DoclingDocument.load_from_doctags(doctags_doc, document_name="Document")\nprint(f"Markdown:\n{doc.export_to_markdown()}\n")\n\n## export as any format.\n# Path("out/").mkdir(parents=True, exist_ok=True)\n# HTML:\n# output_path_html = Path("out/") / "example.html"\n# doc.save_as_html(output_path_html)\n# Markdown:\n# output_path_md = Path("out/") / "example.md"\n# doc.save_as_markdown(output_path_md)\n\n```\n</details>\n\n\n<details>\n<summary>  Fast Batch Inference with VLLM</summary>\n\n```python\n# Prerequisites:\n# pip install vllm\n# pip install docling_core\n# place page images you want to convert into "img/" dir\n\nimport time\nimport os\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoProcessor\nfrom PIL import Image\nfrom docling_core.types.doc import DoclingDocument\nfrom docling_core.types.doc.document import DocTagsDocument\nfrom pathlib import Path\n\n# Configuration\nMODEL_PATH = "ibm-granite/granite-docling-258M"\nIMAGE_DIR = "img/"  # Place your page images here\nOUTPUT_DIR = "out/"\nPROMPT_TEXT = "Convert this page to docling."\n\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image"},\n            {"type": "text", "text": PROMPT_TEXT},\n        ],\n    },\n]\n\n\n# Ensure output directory exists\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Initialize LLM\nllm = LLM(model=MODEL_PATH, revision="untied", limit_mm_per_prompt={"image": 1})\nprocessor = AutoProcessor.from_pretrained(MODEL_PATH)\n\nsampling_params = SamplingParams(\n    temperature=0.0,\n    max_tokens=8192,\n    skip_special_tokens=False,\n)\n\n# Load and prepare all images and prompts up front\nbatched_inputs = []\nimage_names = []\n\nfor img_file in sorted(os.listdir(IMAGE_DIR)):\n    if img_file.lower().endswith((".png", ".jpg", ".jpeg")):\n        img_path = os.path.join(IMAGE_DIR, img_file)\n        with Image.open(img_path) as im:\n            image = im.convert("RGB")\n\n        prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n        batched_inputs.append({"prompt": prompt, "multi_modal_data": {"image": image}})\n        image_names.append(os.path.splitext(img_file)[0])\n\n# Run batch inference\nstart_time = time.time()\noutputs = llm.generate(batched_inputs, sampling_params=sampling_params)\n\n# Postprocess all results\nfor img_fn, output, input_data in zip(image_names, outputs, batched_inputs):\n    doctags = output.outputs[0].text\n    output_path_dt = Path(OUTPUT_DIR) / f"{img_fn}.dt"\n    output_path_md = Path(OUTPUT_DIR) / f"{img_fn}.md"\n\n    with open(output_path_dt, "w", encoding="utf-8") as f:\n        f.write(doctags)\n\n    # Convert to DoclingDocument and save markdown\n    doctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [input_data["multi_modal_data"]["image"]])\n    doc = DoclingDocument.load_from_doctags(doctags_doc, document_name="Document")\n    doc.save_as_markdown(output_path_md)\n\nprint(f"Total time: {time.time() - start_time:.2f} sec")\n\n```\n</details>\n\n Local inference on Apple Silicon with MLX: [see here](https://huggingface.co/ibm-granite/granite-docling-258M-mlx)\n\n If you see trouble running granite-docling with the codes above, check the troubleshooting section at the bottom . \n\n## Intended Use \nGranite-Docling is designed to complement the Docling library, not replace it. It integrates as a component within larger Docling library, consolidating the functions of multiple single-purpose models into a single, compact VLM. \nHowever, Granite-Docling is **not** intended for general image understanding. For tasks focused solely on image-text input, we recommend using [Granite Vision models](https://huggingface.co/collections/ibm-granite/granite-vision-models-67b3bd4ff90c915ba4cd2800), which are purpose-built and optimized for image-text processing.\n\n## Evaluations\nA comprehensive discussion of evaluation methods and findings has already been presented in our previous publication [[citation](https://arxiv.org/pdf/2503.11576)]. As this model is an update, we refer readers to that work for additional details.\nThe evaluation can be performed using the [docling-eval](https://github.com/docling-project/docling-eval) framework for the document related tasks, and [lmms-eval](https://github.com/EvolvingLMMs-Lab/lmms-eval) for MMStar and OCRBench.\n\n<table>\n  <thead>\n    <tr><th colspan="5"><b>Layout</b></th></tr>\n    <tr>\n      <th></th>\n      <th>MAP </th>\n      <th>F1 </th>\n      <th>Precision </th>\n      <th>Recall </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>smoldocling-256m-preview</b></td>\n      <td>0.23</td><td>0.85</td><td>0.9</td><td>0.84</td>\n    </tr>\n    <tr>\n      <td><b>granite-docling-258m</b></td>\n      <td><b>0.27</b></td><td><b>0.86</b></td><td><b>0.92</b></td><td><b>0.88</b></td>\n    </tr>\n  </tbody>\n</table>\n\n<table>\n  <thead>\n    <tr><th colspan="7"><b>Full Page OCR</b></th></tr>\n    <tr>\n      <th></th>\n      <th>Edit-distance </th>\n      <th>F1 </th>\n      <th>Precision </th>\n      <th>Recall </th>\n      <th>BLEU </th>\n      <th>Meteor </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>smoldocling-256m-preview</b></td>\n      <td>0.48</td><td>0.80</td><td>0.89</td>\n      <td>0.79</td><td>0.58</td><td>0.67</td>\n    </tr>\n    <tr>\n      <td><b>granite-docling-258m</b></td>\n      <td><b>0.45</b></td><td><b>0.84</b></td><td><b>0.91</b></td>\n      <td><b>0.83</b></td><td><b>0.65</b></td><td><b>0.72</b></td>\n    </tr>\n  </tbody>\n  <thead>\n    <tr><th colspan="7"><b>Code Recognition</b></th></tr>\n    <tr>\n      <th></th>\n      <th>Edit-distance </th>\n      <th>F1 </th>\n      <th>Precision </th>\n      <th>Recall </th>\n      <th>BLEU </th>\n      <th>Meteor </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>smoldocling-256m-preview</b></td>\n      <td>0.114</td><td>0.915</td><td>0.94</td><td>0.909</td><td>0.875</td><td>0.889</td>\n    </tr>\n    <tr>\n      <td><b>granite-docling-258m</b></td>\n      <td><b>0.013</b></td><td><b>0.988</b></td><td><b>0.99</b></td><td><b>0.988</b></td>\n      <td><b>0.983</b></td><td><b>0.986</b></td>\n    </tr>\n  </tbody>\n  <thead>\n    <tr><th colspan="7"><b>Equation Recognition</b></th></tr>\n    <tr>\n      <th></th>\n      <th>Edit-distance </th>\n      <th>F1 </th>\n      <th>Precision </th>\n      <th>Recall </th>\n      <th>BLEU </th>\n      <th>Meteor </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>smoldocling-256m-preview</b></td>\n      <td>0.119</td><td>0.947</td><td>0.959</td><td>0.941</td><td>0.824</td><td>0.878</td>\n    </tr>\n    <tr>\n      <td><b>granite-docling-258m</b></td>\n      <td><b>0.073</b></td><td><b>0.968</b></td><td><b>0.968</b></td><td><b>0.969</b></td>\n      <td><b>0.893</b></td><td><b>0.927</b></td>\n    </tr>\n  </tbody>\n</table>\n<table>\n  <thead>\n    <tr><th colspan="3"><b>Table Recognition (FinTabNet 150dpi)</b></th></tr>\n    <tr>\n      <th></th>\n      <th>TEDS (structure) </th>\n      <th>TEDS (w/content) </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>smoldocling-256m-preview</b></td>\n      <td>0.82</td><td>0.76</td>\n    </tr>\n    <tr>\n      <td><b>granite-docling-258m</b></td>\n      <td><b>0.97</b></td><td><b>0.96</b></td>\n    </tr>\n  </tbody>\n</table>\n<table>\n  <thead>\n    <tr><th colspan="3"><b>Other Benchmarks</b></th></tr>\n    <tr>\n      <th></th>\n      <th>MMStar </th>\n      <th>OCRBench </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>smoldocling-256m-preview</b></td>\n      <td>0.17</td><td>338</td>\n    </tr>\n    <tr>\n      <td><b>granite-docling-258m</b></td>\n      <td><b>0.30</b></td><td><b>500</b></td>\n    </tr>\n  </tbody>\n</table>\n\n\n\n Local inference on Apple Silicon with MLX: [see here](https://huggingface.co/ibm-granite/granite-docling-258M-mlx)\n\n\n## Supported Instructions\n\n<table>\n  <tr>\n    <th>Description</th>\n    <th>Instruction</th>\n    <th>Short Instruction</th>\n  </tr>\n  <tr>\n    <td><b>Full conversion</b></td>\n    <td>Convert this page to docling.</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td><b>Chart</b></td>\n    <td>Convert chart to table.</td>\n    <td><code>&lt;chart&gt;</code></td>\n  </tr>\n  <tr>\n    <td><b>Formula</b></td>\n    <td>Convert formula to LaTeX.</td>\n    <td><code>&lt;formula&gt;</code></td>\n  </tr>\n  <tr>\n    <td><b>Code</b></td>\n    <td>Convert code to text.</td>\n    <td><code>&lt;code&gt;</code></td>\n  </tr>\n  <tr>\n    <td><b>Table</b></td>\n    <td>Convert table to OTSL. (<a href="https://arxiv.org/pdf/2305.03393">Lysak et al., 2023</a>)</td>\n    <td><code>&lt;otsl&gt;</code></td>\n  </tr>\n  <tr>\n    <td rowspan="4"><b>Actions and Pipelines</b></td>\n    <td>OCR the text in a specific location: &lt;loc_155&gt;&lt;loc_233&gt;&lt;loc_206&gt;&lt;loc_237&gt;</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Identify element at: &lt;loc_247&gt;&lt;loc_482&gt;&lt;loc_252&gt;&lt;loc_486&gt;</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Find all ''text'' elements on the page, retrieve all section headers.</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Detect footer elements on the page.</td>\n    <td>-</td>\n  </tr>\n</table>\n\n\n\n# Model Architecture:\n\nThe architecture of granite-docling-258m consists of the following components:\n\n(1) Vision encoder: [siglip2-base-patch16-512](https://huggingface.co/google/siglip2-base-patch16-512).\n\n(2) Vision-language connector: pixel shuffle projector (as in idefics3) \n\n(3) Large language model: Granite 165M.\n\nWe built upon [Idefics3](https://huggingface.co/docs/transformers/en/model_doc/idefics3) to train our model. We incorporated DocTags into our LLMs supervised fine-tuning (SFT) data to help the model become familiar with the format, enabling faster convergence and mitigating issues previously observed with SmolDocling.\nThe model was trained using the [nanoVLM](https://github.com/huggingface/nanoVLM) framework, which provides a lightweight and efficient training setup for vision-language models\n\n\n**Training Data**: Our training corpus consists of two principal sources: (1) publicly available datasets and (2) internally constructed synthetic datasets designed to elicit specific document understanding capabilities.\n\nIn particular, we incorporate:\n\n* [**SynthCodeNet**](https://huggingface.co/datasets/ds4sd/SynthCodeNet)  a large-scale collection of synthetically rendered code snippets spanning over 50 programming languages\n* [**SynthFormulaNet**](https://huggingface.co/datasets/ds4sd/SynthFormulaNet)  a dataset of synthetic mathematical expressions paired with ground-truth LaTeX representations\n* [**SynthChartNet**](https://huggingface.co/datasets/ds4sd/SynthChartNet)  synthetic chart images annotated with structured table outputs\n* [**DoclingMatix**](https://huggingface.co/datasets/HuggingFaceM4/DoclingMatix)  a curated corpus of real-world document pages sampled from diverse domains\n\n\n**Infrastructure**: We train granite-docling-258m using IBM''s super computing cluster, Blue Vela, which is outfitted with NVIDIA H100 GPUs. This cluster provides a scalable and efficient infrastructure for training our models over thousands of GPUs.\n\n**Responsible Use and Limitations** Some use cases for Vision Language Models can trigger certain risks and ethical considerations, including but not limited to: bias and fairness, misinformation, and autonomous decision-making. \nAlthough our alignment processes include safety considerations, the model may in some cases produce inaccurate, biased, offensive or unwanted responses to user prompts. Additionally, whether smaller models may exhibit increased susceptibility \nto hallucination in generation scenarios due to their reduced sizes, which could limit their ability to generate coherent and contextually accurate responses, remains uncertain. This aspect is currently an active area of research, \nand we anticipate more rigorous exploration, comprehension, and mitigations in this domain. We urge the community to use granite-docling-258m in a responsible way and avoid any malicious utilization. We recommend using this model only as part of the Docling library.\nMore general vision tasks may pose higher inherent risks of triggering unwanted output. To enhance safety, we recommend using granite-docling-258m alongside Granite Guardian. Granite Guardian is a fine-tuned instruct model designed to detect and flag risks in prompts and responses across key dimensions outlined in the IBM AI Risk Atlas.\nIts training, which includes both human-annotated and synthetic data informed by internal red-teaming, enables it to outperform similar open-source models on standard benchmarks, providing an additional layer of safety.\n\n**Resources**\n\n-  Learn about the latest updates with Docling: https://docling-project.github.io/docling/#features\n-  Get started with Docling concepts, integrations and tutorials: https://docling-project.github.io/docling/getting_started/\n-  Learn about the latest Granite learning resources: https://ibm.biz/granite-learning-resources\n-  Learn more about how to use Granite-Docling, explore the Docling library, and see whats coming next for Docling in the release blog: https://ibm.com/new/announcements/granite-docling-end-to-end-document-conversion\n\n## Troubleshooting\n\n**Running with VLLM**\n\n1. You receive `AttributeError: ''LlamaModel'' object has no attribute ''wte''` when launching the model through VLLM.\n   \n    With current versions of VLLM (including 0.10.2), support for tied weights as used in granite-docling is limited and breaks. We provide a version with untied weights on the `untied` branch of this model repo.\n    To use the untied version, please pass the `revision` argument to VLLM:\n    \n    ```sh\n    # Serve the model through VLLM\n    $> vllm serve ibm-granite/granite-docling-258M --revision untied\n    ``` \n    \n    ```python\n    # If using the VLLM python SDK:\n    from vllm import LLM\n    ... \n\n    llm = LLM(model=MODEL_PATH, revision="untied", limit_mm_per_prompt={"image": 1})\n    ```\n\n2. The model outputs only exclamation marks (i.e. "!!!!!!!!!!!!!!!").\n\n   This is seen on older NVIDIA GPUs, such as the T4 GPU available in Google Colab, because it lacks support for `bfloat16` format.\n   You can work around it by setting the `dtype` to `float32`.\n\n   ```sh\n    # Serve the model through VLLM\n    $> vllm serve ibm-granite/granite-docling-258M --revision untied --dtype float32\n    ``` \n    \n    ```python\n    # If using the VLLM python SDK:\n    from vllm import LLM\n    ... \n\n    llm = LLM(model=MODEL_PATH, revision="untied", limit_mm_per_prompt={"image": 1}, dtype="float32")\n    ```\n\n    \n   \n\n\n', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":257517120,"storage_bytes":3045691426,"files_count":17,"spaces_count":11,"gated":false,"private":false,"config":{"architectures":["Idefics3ForConditionalGeneration"],"model_type":"idefics3","tokenizer_config":{"bos_token":"<|start_of_role|>","eos_token":"<|end_of_text|>","pad_token":"<|end_of_text|>","unk_token":"<|unk|>"},"chat_template_jinja":"{%- for message in messages -%}\n{{- ''<|start_of_role|>'' + message[''role''] + ''<|end_of_role|>'' -}}\n{%- if message[''content''] is string -%}\n{{- message[''content''] -}}\n{%- else -%}\n{%- for part in message[''content''] -%}\n{%- if part[''type''] == ''text'' -%}\n{{- part[''text''] -}}\n{%- elif part[''type''] == ''image'' -%}\n{{- ''<image>'' -}}\n{%- endif -%}\n{%- endfor -%}\n{%- endif -%}\n{{- ''<|end_of_text|>\n'' -}}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n{{- ''<|start_of_role|>assistant'' -}}\n{%- if controls -%}{{- '' '' + controls | tojson() -}}{%- endif -%}\n{{- ''<|end_of_role|>'' -}}\n{%- endif -%}\n"}}', '[]', '[{"type":"has_code","target_id":"github:docling-project:docling","source_url":"https://github.com/docling-project/docling"},{"type":"has_code","target_id":"github:docling-project:docling-core","source_url":"https://github.com/docling-project/docling-core"},{"type":"has_code","target_id":"github:docling-project:docling-eval","source_url":"https://github.com/docling-project/docling-eval"},{"type":"has_code","target_id":"github:EvolvingLMMs-Lab:lmms-eval","source_url":"https://github.com/EvolvingLMMs-Lab/lmms-eval"},{"type":"has_code","target_id":"github:huggingface:nanoVLM","source_url":"https://github.com/huggingface/nanoVLM"},{"type":"based_on_paper","target_id":"arxiv:2501.17887","source_url":"https://arxiv.org/abs/2501.17887"},{"type":"based_on_paper","target_id":"arxiv:2503.11576","source_url":"https://arxiv.org/abs/2503.11576"},{"type":"based_on_paper","target_id":"arxiv:2305.03393","source_url":"https://arxiv.org/abs/2305.03393"}]', NULL, 'Apache-2.0', 'approved', 100, '104efa0e537fc1c5f45e26b2264496fd', NULL, 'https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/assets/granite_docling_split_page.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-ibm-granite-granite-docling-258M from https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/assets/granite_docling_split_page.png
Image converted to WebP: data/images/huggingface-ibm-granite-granite-docling-258M.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-ProsusAI-finbert', 'huggingface--prosusai--finbert', 'finbert', 'ProsusAI', '--- language: "en" tags: - financial-sentiment-analysis - sentiment-analysis widget: - text: "Stocks rallied and the British pound gained." --- FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning. For more details, please see the ...', '["transformers","pytorch","tf","jax","bert","text-classification","financial-sentiment-analysis","sentiment-analysis","en","arxiv:1908.10063","endpoints_compatible","deploy:azure","region:us"]', 'text-classification', 1037, 2811323, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/ProsusAI/finbert","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage: "en"\ntags:\n- financial-sentiment-analysis\n- sentiment-analysis\nwidget:\n- text: "Stocks rallied and the British pound gained."\n---\n\nFinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. [Financial PhraseBank](https://www.researchgate.net/publication/251231107_Good_Debt_or_Bad_Debt_Detecting_Semantic_Orientations_in_Economic_Texts) by Malo et al. (2014) is used for fine-tuning. For more details, please see the paper [FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063) and our related [blog post](https://medium.com/prosus-ai-tech-blog/finbert-financial-sentiment-analysis-with-bert-b277a3607101) on Medium.\n\nThe model will give softmax outputs for three labels: positive, negative or neutral.\n\n---\n\nAbout Prosus\n\nProsus is a global consumer internet group and one of the largest technology investors in the world. Operating and investing globally in markets with long-term growth potential, Prosus builds leading consumer internet companies that empower people and enrich communities. For more information, please visit www.prosus.com.\n\nContact information\n\nPlease contact Dogu Araci dogu.araci[at]prosus[dot]com and Zulkuf Genc zulkuf.genc[at]prosus[dot]com about any FinBERT related issues and questions.\n', '{"pipeline_tag":"text-classification","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":3504463245,"files_count":9,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BertForSequenceClassification"],"model_type":"bert","tokenizer_config":{"unk_token":"[UNK]","sep_token":"[SEP]","pad_token":"[PAD]","cls_token":"[CLS]","mask_token":"[MASK]"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1908.10063","source_url":"https://arxiv.org/abs/1908.10063"}]', NULL, NULL, 'pending', 40, '3a6324fdcec290f9f4032d51fd5adee9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-flan-t5-base', 'huggingface--google--flan-t5-base', 'flan-t5-base', 'google', '--- language: - en - fr - ro - de - multilingual tags: - text2text-generation widget: - text: "Translate to German: My name is Arthur" example_title: "Translation" - text: "Please answer to the following question. Who is going to be the next Ballon d''or?" example_title: "Question Answering" - text: "Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering." example_title: "Logical reasoning" - text: "Please answer the following question. What is t...', '["transformers","pytorch","tf","jax","safetensors","t5","text2text-generation","en","fr","ro","de","multilingual","dataset:svakulenk0/qrecc","dataset:taskmaster2","dataset:djaym7/wiki_dialog","dataset:deepmind/code_contests","dataset:lambada","dataset:gsm8k","dataset:aqua_rat","dataset:esnli","dataset:quasc","dataset:qed","arxiv:2210.11416","arxiv:1910.09700","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'other', 1025, 1055369, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/flan-t5-base","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: \n- en\n- fr\n- ro\n- de\n- multilingual\n\ntags:\n- text2text-generation\n\nwidget:\n- text: "Translate to German:  My name is Arthur"\n  example_title: "Translation"\n- text: "Please answer to the following question. Who is going to be the next Ballon d''or?"\n  example_title: "Question Answering"\n- text: "Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering."\n  example_title: "Logical reasoning"\n- text: "Please answer the following question. What is the boiling point of Nitrogen?"\n  example_title: "Scientific knowledge"\n- text: "Answer the following yes/no question. Can you write a whole Haiku in a single tweet?"\n  example_title: "Yes/no question"\n- text: "Answer the following yes/no question by reasoning step-by-step. Can you write a whole Haiku in a single tweet?"\n  example_title: "Reasoning task"\n- text: "Q: ( False or not False or False ) is? A: Let''s think step by step"\n  example_title: "Boolean Expressions"\n- text: "The square root of x is the cube root of y. What is y to the power of 2, if x = 4?"\n  example_title: "Math reasoning"\n- text: "Premise:  At my age you will probably have learnt one lesson. Hypothesis:  It''s not certain how many lessons you''ll learn by your thirties. Does the premise entail the hypothesis?"\n  example_title: "Premise and hypothesis"\n\ndatasets:\n- svakulenk0/qrecc\n- taskmaster2\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- lambada\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed\n\n\nlicense: apache-2.0\n---\n\n# Model Card for FLAN-T5 base\n\n<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/flan2_architecture.jpg"\nalt="drawing" width="600"/>\n\n#  Table of Contents\n\n0. [TL;DR](#TL;DR)\n1. [Model Details](#model-details)\n2. [Usage](#usage)\n3. [Uses](#uses)\n4. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n5. [Training Details](#training-details)\n6. [Evaluation](#evaluation)\n7. [Environmental Impact](#environmental-impact)\n8. [Citation](#citation)\n9. [Model Card Authors](#model-card-authors)\n\n# TL;DR\n\nIf you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. \nAs mentioned in the first few lines of the abstract : \n>  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.\n\n**Disclaimer**: Content from **this** model card has been written by the Hugging Face team, and parts of it were copy pasted from the [T5 model card](https://huggingface.co/t5-large).\n\n# Model Details\n\n## Model Description\n\n\n- **Model type:** Language model\n- **Language(s) (NLP):** English, Spanish, Japanese, Persian, Hindi, French, Chinese, Bengali, Gujarati, German, Telugu, Italian, Arabic, Polish, Tamil, Marathi, Malayalam, Oriya, Panjabi, Portuguese, Urdu, Galician, Hebrew, Korean, Catalan, Thai, Dutch, Indonesian, Vietnamese, Bulgarian, Filipino, Central Khmer, Lao, Turkish, Russian, Croatian, Swedish, Yoruba, Kurdish, Burmese, Malay, Czech, Finnish, Somali, Tagalog, Swahili, Sinhala, Kannada, Zhuang, Igbo, Xhosa, Romanian, Haitian, Estonian, Slovak, Lithuanian, Greek, Nepali, Assamese, Norwegian\n- **License:** Apache 2.0\n- **Related Models:** [All FLAN-T5 Checkpoints](https://huggingface.co/models?search=flan-t5)\n- **Original Checkpoints:** [All Original FLAN-T5 Checkpoints](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)\n- **Resources for more information:**\n  - [Research paper](https://arxiv.org/pdf/2210.11416.pdf)\n  - [GitHub Repo](https://github.com/google-research/t5x)\n  - [Hugging Face FLAN-T5 Docs (Similar to T5) ](https://huggingface.co/docs/transformers/model_doc/t5)\n\n# Usage\n\nFind below some example scripts on how to use the model in `transformers`:\n\n## Using the Pytorch model\n\n### Running the model on a CPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-base")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-base")\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-base")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-base", device_map="auto")\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU using different precisions\n\n#### FP16\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-base")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-base", device_map="auto", torch_dtype=torch.float16)\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n#### INT8\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-base")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-base", device_map="auto", load_in_8bit=True)\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n# Uses\n\n## Direct Use and Downstream Use\n\nThe authors write in [the original paper''s model card](https://arxiv.org/pdf/2210.11416.pdf) that: \n\n> The primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models\n\nSee the [research paper](https://arxiv.org/pdf/2210.11416.pdf) for further details.\n\n## Out-of-Scope Use\n\nMore information needed.\n\n# Bias, Risks, and Limitations\n\nThe information below in this section are copied from the model''s [official model card](https://arxiv.org/pdf/2210.11416.pdf):\n\n> Language models, including Flan-T5, can potentially be used for language generation in a harmful way, according to Rae et al. (2021). Flan-T5 should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.\n\n## Ethical considerations and risks\n\n> Flan-T5 is fine-tuned on a large corpus of text data that was not filtered for explicit content or assessed for existing biases. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\n\n## Known Limitations\n\n> Flan-T5 has not been tested in real world applications.\n\n## Sensitive Use:\n\n> Flan-T5 should not be applied for any unacceptable use cases, e.g., generation of abusive speech.\n\n# Training Details\n\n## Training Data\n\nThe model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2):\n\n![table.png](https://s3.amazonaws.com/moonup/production/uploads/1666363265279-62441d1d9fdefb55a0b7d12c.png)\n\n\n## Training Procedure\n\nAccording to the model card from the [original paper](https://arxiv.org/pdf/2210.11416.pdf):\n\n> These models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size.\n\nThe model has been trained on TPU v3 or TPU v4 pods, using [`t5x`](https://github.com/google-research/t5x) codebase together with [`jax`](https://github.com/google/jax).\n\n\n# Evaluation\n\n## Testing Data, Factors & Metrics\n\nThe authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation:\n![image.png](https://s3.amazonaws.com/moonup/production/uploads/1668072995230-62441d1d9fdefb55a0b7d12c.png)\nFor full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf).\n\n## Results \n\nFor full results for FLAN-T5-Base, see the [research paper](https://arxiv.org/pdf/2210.11416.pdf), Table 3.\n\n# Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** Google Cloud TPU Pods - TPU v3 or TPU v4  | Number of chips  4.\n- **Hours used:** More information needed\n- **Cloud Provider:** GCP\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@misc{https://doi.org/10.48550/arxiv.2210.11416,\n  doi = {10.48550/ARXIV.2210.11416},\n  \n  url = {https://arxiv.org/abs/2210.11416},\n  \n  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n  \n  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {Scaling Instruction-Finetuned Language Models},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```\n## Model Recycling\n\n[Evaluation on 36 datasets](https://ibm.github.io/model-recycling/model_gain_chart?avg=9.16&mnli_lp=nan&20_newsgroup=3.34&ag_news=1.49&amazon_reviews_multi=0.21&anli=13.91&boolq=16.75&cb=23.12&cola=9.97&copa=34.50&dbpedia=6.90&esnli=5.37&financial_phrasebank=18.66&imdb=0.33&isear=1.37&mnli=11.74&mrpc=16.63&multirc=6.24&poem_sentiment=14.62&qnli=3.41&qqp=6.18&rotten_tomatoes=2.98&rte=24.26&sst2=0.67&sst_5bins=5.44&stsb=20.68&trec_coarse=3.95&trec_fine=10.73&tweet_ev_emoji=13.39&tweet_ev_emotion=4.62&tweet_ev_hate=3.46&tweet_ev_irony=9.04&tweet_ev_offensive=1.69&tweet_ev_sentiment=0.75&wic=14.22&wnli=9.44&wsc=5.53&yahoo_answers=4.14&model_name=google%2Fflan-t5-base&base_name=google%2Ft5-v1_1-base) using google/flan-t5-base as a base model yields average score of 77.98 in comparison to 68.82 by google/t5-v1_1-base.\n\nThe model is ranked 1st among all tested models for the google/t5-v1_1-base architecture as of 06/02/2023\nResults:\n\n|   20_newsgroup |   ag_news |   amazon_reviews_multi |    anli |   boolq |      cb |    cola |   copa |   dbpedia |   esnli |   financial_phrasebank |   imdb |   isear |    mnli |    mrpc |   multirc |   poem_sentiment |    qnli |     qqp |   rotten_tomatoes |     rte |    sst2 |   sst_5bins |    stsb |   trec_coarse |   trec_fine |   tweet_ev_emoji |   tweet_ev_emotion |   tweet_ev_hate |   tweet_ev_irony |   tweet_ev_offensive |   tweet_ev_sentiment |     wic |   wnli |     wsc |   yahoo_answers |\n|---------------:|----------:|-----------------------:|--------:|--------:|--------:|--------:|-------:|----------:|--------:|-----------------------:|-------:|--------:|--------:|--------:|----------:|-----------------:|--------:|--------:|------------------:|--------:|--------:|------------:|--------:|--------------:|------------:|-----------------:|-------------------:|----------------:|-----------------:|---------------------:|---------------------:|--------:|-------:|--------:|----------------:|\n|        86.2188 |   89.6667 |                  67.12 | 51.9688 | 82.3242 | 78.5714 | 80.1534 |     75 |   77.6667 | 90.9507 |                   85.4 | 93.324 |  72.425 | 87.2457 | 89.4608 |   62.3762 |          82.6923 | 92.7878 | 89.7724 |           89.0244 | 84.8375 | 94.3807 |     57.2851 | 89.4759 |          97.2 |        92.8 |           46.848 |            80.2252 |         54.9832 |          76.6582 |              84.3023 |              70.6366 | 70.0627 | 56.338 | 53.8462 |            73.4 |\n\n\nFor more information, see: [Model Recycling](https://ibm.github.io/model-recycling/)\n', '{"pipeline_tag":null,"library_name":"transformers","framework":"transformers","params":247577856,"storage_bytes":7894822589,"files_count":12,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["T5ForConditionalGeneration"],"model_type":"t5","tokenizer_config":{"eos_token":"</s>","pad_token":"<pad>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google:jax","source_url":"https://github.com/google/jax"},{"type":"based_on_paper","target_id":"arxiv:2210.11416","source_url":"https://arxiv.org/abs/2210.11416"},{"type":"based_on_paper","target_id":"arxiv:1910.09700","source_url":"https://arxiv.org/abs/1910.09700"}]', NULL, 'Apache-2.0', 'approved', 80, 'f645454075588cc7a5356d39eb07e1d4', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-dreamlike-art-dreamlike-diffusion-1.0', 'huggingface--dreamlike-art--dreamlike-diffusion-1.0', 'dreamlike-diffusion-1.0', 'dreamlike-art', '--- language: - en license: other tags: - stable-diffusion - stable-diffusion-diffusers - text-to-image - art - artistic - diffusers inference: false --- Use the same prompts as you would for SD 1.5. Add **dreamlikeart** if the artstyle is too weak. Non-square aspect ratios work better for some prompts. If you want a portrait photo, try using a 2:3 or a 9:16 aspect ratio. If you want a landscape photo, try using a 3:2 or a 16:9 aspect ratio. Use slightly higher resolution for better results: ...', '["diffusers","safetensors","stable-diffusion","stable-diffusion-diffusers","text-to-image","art","artistic","en","license:other","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 1025, 6071, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\nlicense: other\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\n- art\n- artistic\n- diffusers\ninference: false\n---\n\n# Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by [dreamlike.art](https://dreamlike.art/).\n\n# If you want to use dreamlike models on your website/app/etc., check the license at the bottom first!  \n\nUse the same prompts as you would for SD 1.5. Add **dreamlikeart** if the artstyle is too weak.    \nNon-square aspect ratios work better for some prompts. If you want a portrait photo, try using a 2:3 or a 9:16 aspect ratio. If you want a landscape photo, try using a 3:2 or a 16:9 aspect ratio.  \nUse slightly higher resolution for better results: 640x640px, 512x768px, 768x512px, etc.  \n\n# We''ve just released Dreamlike Photoreal 2.0, check it out!\n\n[https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0](https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0)\n\n<img src="https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0/resolve/main/preview1.jpg" style="max-width: 400px;" width="100%"/>\n\n### Examples\n\n<img src="https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/preview.jpg" style="max-width: 800px;" width="100%"/>\n<img src="https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/1.jpg" style="max-width: 800px;" width="100%"/>\n<img src="https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/2.jpg" style="max-width: 800px;" width="100%"/>\n\n### dreamlike.art\n\nYou can use this model for free on [dreamlike.art](https://dreamlike.art/)!\n\n<img src="https://huggingface.co/dreamlike-art/dreamlike-photoreal-1.0/resolve/main/dreamlike.jpg" style="max-width: 1000px;" width="100%"/>\n\n### Gradio\n\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI to run dreamlike-diffusion-1.0:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/dreamlike-diffusion-1.0)\n\n### CompVis\n\n[Download dreamlike-diffusion-1.0.ckpt (2.13GB)](https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/dreamlike-diffusion-1.0.ckpt)\n\n###  Diffusers\n\nThis model can be used just like any other Stable Diffusion model. For more information,\nplease have a look at the [Stable Diffusion Pipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\n\n```python\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = "dreamlike-art/dreamlike-diffusion-1.0"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to("cuda")\n\nprompt = "dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors"\nimage = pipe(prompt).images[0]\n\nimage.save("./result.jpg")\n```\n\n# License\n\nThis model is licesed under a **modified** CreativeML OpenRAIL-M license.\n\n- **You can''t host or use the model or its derivatives on websites/apps/etc., from which you earn, will earn, or plan to earn revenue or donations. If you want to, please email us at contact@dreamlike.art**\n- **You are free to host the model card and files (Without any actual inference or finetuning) on both commercial and non-commercial websites/apps/etc.  Please state the full model name (Dreamlike Diffusion 1.0) and include a link to the model card (https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0)**  \n- **You are free to host the model or its derivatives on completely non-commercial websites/apps/etc (Meaning you are not getting ANY revenue or donations). Please state the full model name (Dreamlike Diffusion 1.0) and include a link to the model card (https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0)**\n- **You are free to use the outputs of the model or the outputs of the model''s derivatives for commercial purposes in teams of 10 or less**\n- You can''t use the model to deliberately produce nor share illegal or harmful outputs or content\n- The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n- You may re-distribute the weights. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the **modified** CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully) Please read the full license here: https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/blob/main/LICENSE.md\n', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":23223080959,"files_count":24,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"}]', NULL, 'Other', 'approved', 65, 'ebf55764034102e987fe0ee7b0a74ebb', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tiiuae-falcon-7b-instruct', 'huggingface--tiiuae--falcon-7b-instruct', 'falcon-7b-instruct', 'tiiuae', '--- datasets: - tiiuae/falcon-refinedweb language: - en inference: true new_version: tiiuae/falcon-11B widget: - text: "Hey Falcon! Any recommendations for my holidays in Abu Dhabi?" example_title: "Abu Dhabi Trip" - text: "What''s the Everett interpretation of quantum mechanics?" example_title: "Q/A: Quantum & Answers" - text: "Give me a list of the top 10 dive sites you would recommend around the world." example_title: "Diving Top 10" - text: "Can you tell me more about deep-water soloing?" ...', '["transformers","pytorch","coreml","safetensors","falcon","text-generation","conversational","custom_code","en","dataset:tiiuae/falcon-refinedweb","arxiv:2205.14135","arxiv:1911.02150","arxiv:2005.14165","arxiv:2104.09864","arxiv:2306.01116","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 1024, 47803, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tiiuae/falcon-7b-instruct","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\ndatasets:\n  - tiiuae/falcon-refinedweb\nlanguage:\n  - en\ninference: true\nnew_version: tiiuae/falcon-11B\nwidget:\n  - text: "Hey Falcon! Any recommendations for my holidays in Abu Dhabi?"\n    example_title: "Abu Dhabi Trip"\n  - text: "What''s the Everett interpretation of quantum mechanics?"\n    example_title: "Q/A: Quantum & Answers"\n  - text: "Give me a list of the top 10 dive sites you would recommend around the world."\n    example_title: "Diving Top 10"\n  - text: "Can you tell me more about deep-water soloing?"\n    example_title: "Extreme sports"\n  - text: "Can you write a short tweet about the Apache 2.0 release of our latest AI model, Falcon LLM?"\n    example_title: "Twitter Helper"\n  - text: "What are the responsabilities of a Chief Llama Officer?"\n    example_title: "Trendy Jobs"\nlicense: apache-2.0\n---\n\n#  Falcon-7B-Instruct\n\n**Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by [TII](https://www.tii.ae) based on [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) and finetuned on a mixture of chat/instruct datasets. It is made available under the Apache 2.0 license.**\n\n*Paper coming soon .*\n\n To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading [this great blogpost fron HF](https://huggingface.co/blog/falcon)!\n\n## Why use Falcon-7B-Instruct?\n\n* **You are looking for a ready-to-use chat/instruct model based on [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b).**\n* **Falcon-7B is a strong base model, outperforming comparable open-source models** (e.g., [MPT-7B](https://huggingface.co/mosaicml/mpt-7b), [StableLM](https://github.com/Stability-AI/StableLM), [RedPajama](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1) etc.), thanks to being trained on 1,500B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora. See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n* **It features an architecture optimized for inference**, with FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135)) and multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)). \n\n **This is an instruct model, which may not be ideal for further finetuning.** If you are interested in building your own instruct/chat model, we recommend starting from [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b). \n\n **Looking for an even more powerful model?** [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) is Falcon-7B-Instruct''s big brother!\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = "tiiuae/falcon-7b-instruct"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    "text-generation",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map="auto",\n)\nsequences = pipeline(\n   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f"Result: {seq[''generated_text'']}")\n\n```\n\n **Falcon LLMs require PyTorch 2.0 for use with `transformers`!**\n\nFor fast inference with Falcon, check-out [Text Generation Inference](https://github.com/huggingface/text-generation-inference)! Read more in this [blogpost]((https://huggingface.co/blog/falcon). \n\nYou will need **at least 16GB of memory** to swiftly run inference with Falcon-7B-Instruct.\n\n\n# Model Card for Falcon-7B-Instruct\n\n## Model Details\n\n### Model Description\n\n- **Developed by:** [https://www.tii.ae](https://www.tii.ae);\n- **Model type:** Causal decoder-only;\n- **Language(s) (NLP):** English and French;\n- **License:** Apache 2.0;\n- **Finetuned from model:** [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b).\n\n### Model Source\n\n- **Paper:** *coming soon*.\n\n## Uses\n\n### Direct Use\n\nFalcon-7B-Instruct has been finetuned on a mixture of instruct and chat datasets.\n\n### Out-of-Scope Use\n\nProduction use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful. \n\n## Bias, Risks, and Limitations\n\nFalcon-7B-Instruct is mostly trained on English data, and will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online.\n\n### Recommendations\n\nWe recommend users of Falcon-7B-Instruct to develop guardrails and to take appropriate precautions for any production use.\n\n## How to Get Started with the Model\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = "tiiuae/falcon-7b-instruct"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    "text-generation",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map="auto",\n)\nsequences = pipeline(\n   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f"Result: {seq[''generated_text'']}")\n\n```\n\n## Training Details\n\n### Training Data\n\nFalcon-7B-Instruct was finetuned on a 250M tokens mixture of instruct/chat datasets.\n\n| **Data source**    | **Fraction** | **Tokens** | **Description**                       |\n|--------------------|--------------|------------|-----------------------------------|\n| [Bai ze](https://github.com/project-baize/baize-chatbot) | 65%          | 164M     | chat                 |\n| [GPT4All](https://github.com/nomic-ai/gpt4all)              | 25%           | 62M       | instruct                                  |\n| [GPTeacher](https://github.com/teknium1/GPTeacher)      | 5%           | 11M        | instruct |\n| [RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) | 5%          | 13M     | massive web crawl                 |\n\n\nThe data was tokenized with the Falcon-[7B](https://huggingface.co/tiiuae/falcon-7b)/[40B](https://huggingface.co/tiiuae/falcon-40b) tokenizer.\n\n\n## Evaluation\n\n*Paper coming soon.*\n\nSee the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) for early results.\n\nNote that this model variant is not optimized for NLP benchmarks. \n\n\n## Technical Specifications \n\nFor more information about pretraining, see [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b).\n\n### Model Architecture and Objective\n\nFalcon-7B is a causal decoder-only model trained on a causal language modeling task (i.e., predict the next token).\n\nThe architecture is broadly adapted from the GPT-3 paper ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)), with the following differences:\n\n* **Positionnal embeddings:** rotary ([Su et al., 2021](https://arxiv.org/abs/2104.09864));\n* **Attention:** multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) and FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135));\n* **Decoder-block:** parallel attention/MLP with a single layer norm.\n\n| **Hyperparameter** | **Value** | **Comment**                            |\n|--------------------|-----------|----------------------------------------|\n| Layers             | 32        |                                        |\n| `d_model`          | 4544      | Increased to compensate for multiquery                                       |\n| `head_dim`         | 64        | Reduced to optimise for FlashAttention |\n| Vocabulary         | 65024     |                                        |\n| Sequence length    | 2048      |                                        |\n\n### Compute Infrastructure\n\n#### Hardware\n\nFalcon-7B-Instruct was trained on AWS SageMaker, on 32 A100 40GB GPUs in P4d instances. \n\n#### Software\n\nFalcon-7B-Instruct was trained a custom distributed training codebase, Gigatron. It uses a 3D parallelism approach combined with ZeRO and high-performance Triton kernels (FlashAttention, etc.)\n\n\n## Citation\n\n*Paper coming soon* . In the meanwhile, you can use the following information to cite: \n```\n@article{falcon40b,\n  title={{Falcon-40B}: an open large language model with state-of-the-art performance},\n  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},\n  year={2023}\n}\n```\n\nTo learn more about the pretraining dataset, see the  [RefinedWeb paper](https://arxiv.org/abs/2306.01116).\n\n```\n@article{refinedweb,\n  title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},\n  author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},\n  journal={arXiv preprint arXiv:2306.01116},\n  eprint={2306.01116},\n  eprinttype = {arXiv},\n  url={https://arxiv.org/abs/2306.01116},\n  year={2023}\n}\n```\n\n\n## License\n\nFalcon-7B-Instruct is made available under the Apache 2.0 license.\n\n## Contact\nfalconllm@tii.ae', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7217189760,"storage_bytes":56610389723,"files_count":19,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["FalconForCausalLM"],"auto_map":{"AutoConfig":"configuration_falcon.FalconConfig","AutoModel":"modeling_falcon.FalconModel","AutoModelForSequenceClassification":"modeling_falcon.FalconForSequenceClassification","AutoModelForTokenClassification":"modeling_falcon.FalconForTokenClassification","AutoModelForQuestionAnswering":"modeling_falcon.FalconForQuestionAnswering","AutoModelForCausalLM":"modeling_falcon.FalconForCausalLM"},"model_type":"falcon","tokenizer_config":{"eos_token":"<|endoftext|>","chat_template":"{% if messages[0][''role''] == ''system'' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][''content''] %}{% else %}{% set loop_messages = messages %}{% set system_message = '''' %}{% endif %}{% for message in loop_messages %}{% if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}{{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}{% endif %}{% if loop.index0 == 0 %}{{ system_message.strip() }}{% endif %}{% if message[''role''] == ''user'' %}{{ ''\n\nUser: '' + message[''content''].strip().replace(''\r\n'', ''\n'').replace(''\n\n'', ''\n'') }}{% elif message[''role''] == ''assistant'' %}{{ ''\n\nAssistant: '' + message[''content''].strip().replace(''\r\n'', ''\n'').replace(''\n\n'', ''\n'') }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ ''\n\nAssistant:'' }}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:Stability-AI:StableLM","source_url":"https://github.com/Stability-AI/StableLM"},{"type":"has_code","target_id":"github:huggingface:text-generation-inference","source_url":"https://github.com/huggingface/text-generation-inference"},{"type":"has_code","target_id":"github:project-baize:baize-chatbot","source_url":"https://github.com/project-baize/baize-chatbot"},{"type":"has_code","target_id":"github:nomic-ai:gpt4all","source_url":"https://github.com/nomic-ai/gpt4all"},{"type":"has_code","target_id":"github:teknium1:GPTeacher","source_url":"https://github.com/teknium1/GPTeacher"},{"type":"based_on_paper","target_id":"arxiv:2205.14135","source_url":"https://arxiv.org/abs/2205.14135"},{"type":"based_on_paper","target_id":"arxiv:1911.02150","source_url":"https://arxiv.org/abs/1911.02150"},{"type":"based_on_paper","target_id":"arxiv:2005.14165","source_url":"https://arxiv.org/abs/2005.14165"},{"type":"based_on_paper","target_id":"arxiv:2104.09864","source_url":"https://arxiv.org/abs/2104.09864"},{"type":"based_on_paper","target_id":"arxiv:2306.01116","source_url":"https://arxiv.org/abs/2306.01116"}]', NULL, 'Apache-2.0', 'approved', 65, '265785c21586b0e60598f4865e1d797b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-3-4b-it', 'huggingface--google--gemma-3-4b-it', 'gemma-3-4b-it', 'google', '', '["transformers","safetensors","gemma3","any-to-any","image-text-to-text","conversational","arxiv:1905.07830","arxiv:1905.10044","arxiv:1911.11641","arxiv:1904.09728","arxiv:1705.03551","arxiv:1911.01547","arxiv:1907.10641","arxiv:1903.00161","arxiv:2009.03300","arxiv:2304.06364","arxiv:2103.03874","arxiv:2110.14168","arxiv:2311.12022","arxiv:2108.07732","arxiv:2107.03374","arxiv:2210.03057","arxiv:2106.03193","arxiv:1910.11856","arxiv:2502.12404","arxiv:2502.21228","arxiv:2404.16816","arxiv:2104.12756","arxiv:2311.16502","arxiv:2203.10244","arxiv:2404.12390","arxiv:1810.12440","arxiv:1908.02660","arxiv:2312.11805","base_model:google/gemma-3-4b-pt","base_model:finetune:google/gemma-3-4b-pt","license:gemma","text-generation-inference","endpoints_compatible","region:us"]', 'image-text-to-text', 1019, 1030699, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-3-4b-it","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":4300079472,"storage_bytes":50506149149,"files_count":15,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["Gemma3ForConditionalGeneration"],"model_type":"gemma3","processor_config":{"chat_template":"{{ bos_token }}\n{%- if messages[0][''role''] == ''system'' -%}\n    {%- if messages[0][''content''] is string -%}\n        {%- set first_user_prefix = messages[0][''content''] + ''\n\n'' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0][''content''][0][''text''] + ''\n\n'' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message[''role''] == ''assistant'') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message[''role''] -%}\n    {%- endif -%}\n    {{ ''<start_of_turn>'' + role + ''\n'' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message[''content''] is string -%}\n        {{ message[''content''] | trim }}\n    {%- elif message[''content''] is iterable -%}\n        {%- for item in message[''content''] -%}\n            {%- if item[''type''] == ''image'' -%}\n                {{ ''<start_of_image>'' }}\n            {%- elif item[''type''] == ''text'' -%}\n                {{ item[''text''] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ ''<end_of_turn>\n'' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{''<start_of_turn>model\n''}}\n{%- endif -%}\n"},"tokenizer_config":{"bos_token":"<bos>","chat_template":"{{ bos_token }}\n{%- if messages[0][''role''] == ''system'' -%}\n    {%- if messages[0][''content''] is string -%}\n        {%- set first_user_prefix = messages[0][''content''] + ''\n\n'' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0][''content''][0][''text''] + ''\n\n'' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message[''role''] == ''assistant'') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message[''role''] -%}\n    {%- endif -%}\n    {{ ''<start_of_turn>'' + role + ''\n'' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message[''content''] is string -%}\n        {{ message[''content''] | trim }}\n    {%- elif message[''content''] is iterable -%}\n        {%- for item in message[''content''] -%}\n            {%- if item[''type''] == ''image'' -%}\n                {{ ''<start_of_image>'' }}\n            {%- elif item[''type''] == ''text'' -%}\n                {{ item[''text''] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ ''<end_of_turn>\n'' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{''<start_of_turn>model\n''}}\n{%- endif -%}\n","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1903.00161","source_url":"https://arxiv.org/abs/1903.00161"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:2304.06364","source_url":"https://arxiv.org/abs/2304.06364"},{"type":"based_on_paper","target_id":"arxiv:2103.03874","source_url":"https://arxiv.org/abs/2103.03874"},{"type":"based_on_paper","target_id":"arxiv:2110.14168","source_url":"https://arxiv.org/abs/2110.14168"},{"type":"based_on_paper","target_id":"arxiv:2311.12022","source_url":"https://arxiv.org/abs/2311.12022"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"},{"type":"based_on_paper","target_id":"arxiv:2210.03057","source_url":"https://arxiv.org/abs/2210.03057"},{"type":"based_on_paper","target_id":"arxiv:2106.03193","source_url":"https://arxiv.org/abs/2106.03193"},{"type":"based_on_paper","target_id":"arxiv:1910.11856","source_url":"https://arxiv.org/abs/1910.11856"},{"type":"based_on_paper","target_id":"arxiv:2502.12404","source_url":"https://arxiv.org/abs/2502.12404"},{"type":"based_on_paper","target_id":"arxiv:2502.21228","source_url":"https://arxiv.org/abs/2502.21228"},{"type":"based_on_paper","target_id":"arxiv:2404.16816","source_url":"https://arxiv.org/abs/2404.16816"},{"type":"based_on_paper","target_id":"arxiv:2104.12756","source_url":"https://arxiv.org/abs/2104.12756"},{"type":"based_on_paper","target_id":"arxiv:2311.16502","source_url":"https://arxiv.org/abs/2311.16502"},{"type":"based_on_paper","target_id":"arxiv:2203.10244","source_url":"https://arxiv.org/abs/2203.10244"},{"type":"based_on_paper","target_id":"arxiv:2404.12390","source_url":"https://arxiv.org/abs/2404.12390"},{"type":"based_on_paper","target_id":"arxiv:1810.12440","source_url":"https://arxiv.org/abs/1810.12440"},{"type":"based_on_paper","target_id":"arxiv:1908.02660","source_url":"https://arxiv.org/abs/1908.02660"},{"type":"based_on_paper","target_id":"arxiv:2312.11805","source_url":"https://arxiv.org/abs/2312.11805"}]', NULL, 'Gemma', 'approved', 40, '9b7b15d46be05318e2a91092e3586bc0', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-openbmb-MiniCPM-V-4-5', 'huggingface--openbmb--minicpm-v-4-5', 'MiniCPM-V-4_5', 'openbmb', '--- pipeline_tag: image-text-to-text datasets: - openbmb/RLAIF-V-Dataset library_name: transformers language: - multilingual tags: - minicpm-v - vision - ocr - multi-image - video - custom_code license: apache-2.0 --- <h1>A GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone</h1> GitHub | CookBook | Technical Report | Demo </a> **MiniCPM-V 4.5** is the latest and most capable model in the MiniCPM-V series. The model is built on Qwen3-8B and SigLIP2-4...', '["transformers","safetensors","minicpmv","feature-extraction","minicpm-v","vision","ocr","multi-image","video","custom_code","image-text-to-text","conversational","multilingual","dataset:openbmb/rlaif-v-dataset","arxiv:2509.18154","arxiv:2403.11703","license:apache-2.0","region:us"]', 'image-text-to-text', 1019, 49678, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/openbmb/MiniCPM-V-4_5","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\npipeline_tag: image-text-to-text\ndatasets:\n- openbmb/RLAIF-V-Dataset\nlibrary_name: transformers\nlanguage:\n- multilingual\ntags:\n- minicpm-v\n- vision\n- ocr\n- multi-image\n- video\n- custom_code\nlicense: apache-2.0\n---\n\n<h1>A GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone</h1>\n\n[GitHub](https://github.com/OpenBMB/MiniCPM-o) | [CookBook](https://github.com/OpenSQZ/MiniCPM-V-CookBook) | [Technical Report](https://huggingface.co/papers/2509.18154) | [Demo](http://101.126.42.235:30910/) </a> \n\n\n\n## MiniCPM-V 4.5\n\n**MiniCPM-V 4.5** is the latest and most capable model in the MiniCPM-V series. The model is built on Qwen3-8B and SigLIP2-400M with a total of 8B parameters. It exhibits a significant performance improvement over previous MiniCPM-V and MiniCPM-o models, and introduces new useful features. Notable features of MiniCPM-V 4.5 include:\n\n-  **State-of-the-art Vision-Language Capability.**\n  MiniCPM-V 4.5 achieves an average score of 77.0 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. **With only 8B parameters, it surpasses widely used proprietary models like GPT-4o-latest, Gemini-2.0 Pro, and strong open-source models like Qwen2.5-VL 72B** for vision-language capabilities, making it the most performant MLLM under 30B parameters.\n\n-  **Efficient High-FPS and Long Video Understanding.** Powered by a new unified 3D-Resampler over images and videos, MiniCPM-V 4.5 can now achieve 96x compression rate for video tokens, where 6 448x448 video frames can be jointly compressed into 64 video tokens (normally 1,536 tokens for most MLLMs). This means that the model can perceive significantly more video frames without increasing the LLM inference cost. This brings state-of-the-art high-FPS (up to 10FPS) video understanding and long video understanding capabilities on Video-MME, LVBench, MLVU, MotionBench, FavorBench, etc., efficiently.\n\n-  **Controllable Hybrid Fast/Deep Thinking.** MiniCPM-V 4.5 supports both fast thinking for efficient frequent usage with competitive performance, and deep thinking for more complex problem solving. To cover efficiency and performance trade-offs in different user scenarios, this fast/deep thinking mode can be switched in a highly controlled fashion.\n\n-  **Strong OCR, Document Parsing and Others.**\nBased on [LLaVA-UHD](https://arxiv.org/pdf/2403.11703) architecture, MiniCPM-V 4.5 can process high-resolution images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344), using 4x less visual tokens than most MLLMs. The model achieves **leading performance on OCRBench, surpassing proprietary models such as GPT-4o-latest and Gemini 2.5**. It also achieves state-of-the-art performance for PDF document parsing capability on OmniDocBench among general MLLMs. Based on the latest [RLAIF-V](https://github.com/RLHF-V/RLAIF-V/) and [VisCPM](https://github.com/OpenBMB/VisCPM) techniques, it features **trustworthy behaviors**, outperforming GPT-4o-latest on MMHal-Bench, and supports **multilingual capabilities** in more than 30 languages.\n\n-  **Easy Usage.**\nMiniCPM-V 4.5 can be easily used in various ways: (1) [llama.cpp](https://github.com/tc-mb/llama.cpp/blob/Support-MiniCPM-V-4.5/docs/multimodal/minicpmv4.5.md) and [ollama](https://github.com/tc-mb/ollama/tree/MIniCPM-V) support for efficient CPU inference on local devices, (2) [int4](https://huggingface.co/openbmb/MiniCPM-V-4_5-int4), [GGUF](https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf) and [AWQ](https://github.com/tc-mb/AutoAWQ) format quantized models in 16 sizes, (3) [SGLang](https://github.com/tc-mb/sglang/tree/main) and [vLLM](#efficient-inference-with-llamacpp-ollama-vllm) support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks with [Transformers](https://github.com/tc-mb/transformers/tree/main) and [LLaMA-Factory](./docs/llamafactory_train_and_infer.md), (5) quick [local WebUI demo](#chat-with-our-demo-on-gradio), (6) optimized [local iOS app](https://github.com/tc-mb/MiniCPM-o-demo-iOS) on iPhone and iPad, and (7) online web demo on [server](http://101.126.42.235:30910/). See our [Cookbook](https://github.com/OpenSQZ/MiniCPM-V-CookBook) for full usages!\n\n\n### Key Techniques\n\n\n<div align="center">\n<img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpm-v-4dot5-framework.png" , width=100%>\n</div>\n\n- **Architechture: Unified 3D-Resampler for High-density Video Compression.** MiniCPM-V 4.5 introduces a 3D-Resampler that overcomes the performance-efficiency trade-off in video understanding. By grouping and jointly compressing up to 6 consecutive video frames into just 64 tokens (the same token count used for a single image in MiniCPM-V series), MiniCPM-V 4.5 achieves a 96 compression rate for video tokens. This allows the model to process more video frames without additional LLM computational cost, enabling high-FPS video and long video understanding. The architecture supports unified encoding for images, multi-image inputs, and videos, ensuring seamless capability and knowledge transfer.\n\n- **Pre-training: Unified Learning for OCR and Knowledge from Documents.** Existing MLLMs learn OCR capability and knowledge from documents in isolated training approaches. We observe that the essential difference between these two training approaches is the visibility of the text in images. By dynamically corrupting text regions in documents with varying noise levels and asking the model to reconstruct the text, the model learns to adaptively and properly switch between accurate text recognition (when text is visible) and multimodal context-based knowledge reasoning (when text is heavily obscured). This eliminates reliance on error-prone document parsers in knowledge learning from documents, and prevents hallucinations from over-augmented OCR data, resulting in top-tier OCR and multimodal knowledge performance with minimal engineering overhead.\n\n- **Post-training: Hybrid Fast/Deep Thinking with Multimodal RL.** MiniCPM-V 4.5 offers a balanced reasoning experience through two switchable modes: fast thinking for efficient daily use and deep thinking for complex tasks. Using a new hybrid reinforcement learning method, the model jointly optimizes both modes, significantly enhancing fast-mode performance without compromising deep-mode capability. Incorporated with [RLPR](https://github.com/OpenBMB/RLPR) and [RLAIF-V](https://github.com/RLHF-V/RLAIF-V), it generalizes robust reasoning skills from broad multimodal data while effectively reducing hallucinations.\n\n### Evaluation\n\n<div align="center">\n  <img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/radar_minicpm_v45.png", width=60%>\n</div>\n<div align="center">\n<img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv_4_5_evaluation_result.png" , width=100%>\n</div>\n\n### Inference Efficiency \n\n**OpenCompass**\n<div align="left">\n<table style="margin: 0px auto;">\n    <thead>\n            <tr>\n              <th align="left">Model</th>\n              <th>Size</th>\n              <th>Avg Score </th>\n              <th>Total Inference Time </th>\n            </tr>\n    </thead>\n    <tbody align="center">\n        <tr>\n            <td nowrap="nowrap" align="left">GLM-4.1V-9B-Thinking</td>\n            <td>10.3B</td>\n            <td>76.6</td>\n            <td>17.5h</td>\n        </tr>\n        <tr>\n            <td nowrap="nowrap" align="left">MiMo-VL-7B-RL</td>\n            <td>8.3B</td>\n            <td>76.4</td>\n            <td>11h</td>\n        </tr>\n        <tr>\n            <td nowrap="nowrap" align="left">MiniCPM-V 4.5</td>\n            <td>8.7B</td>\n            <td><b>77.0</td>\n            <td><b>7.5h</td>\n        </tr>\n    </tbody>\n</table>\n</div>\n\n**Video-MME**\n\n<div align="left">\n<table style="margin: 0px auto;">\n    <thead>\n          <tr>\n              <th align="left">Model</th>\n              <th>Size</th>\n              <th>Avg Score </th>\n              <th>Total Inference Time </th>\n              <th>GPU Mem </th>\n          </tr>\n    </thead>\n    <tbody align="center">\n          <tr>\n              <td nowrap="nowrap" align="left">Qwen2.5-VL-7B-Instruct</td>\n              <td>8.3B</td>\n              <td>71.6</td>\n              <td>3h</td>\n              <td>60G</td>\n          </tr>\n          <tr>\n              <td nowrap="nowrap" align="left">GLM-4.1V-9B-Thinking</td>\n              <td>10.3B</td>\n              <td><b>73.6</td>\n              <td>2.63h</td>\n              <td>32G</td>\n          </tr>\n          <tr>\n              <td nowrap="nowrap" align="left">MiniCPM-V 4.5</td>\n              <td>8.7B</td>\n              <td>73.5</td>\n              <td><b>0.26h</td>\n              <td><b>28G</td>\n        </tr>\n    </tbody>\n</table>\n</div>\n\nBoth Video-MME and OpenCompass were evaluated using 8A100 GPUs for inference. The reported inference time of Video-MME includes full model-side computation, and excludes the external cost of video frame extraction (dependent on specific frame extraction tools) for fair comparison.\n\n### Examples\n\n<div align="center">\n  <a href="https://www.youtube.com/watch?v=Cn23FujYMMU"><img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/MiniCPM-V%204.5-8.26_img.jpeg", width=70%></a>\n</div>\n\n<div style="display: flex; flex-direction: column; align-items: center;">\n  <img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/en_case1.png" alt="en_case1" style="margin-bottom: 5px;">\n  <img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/en_case2.png" alt="en_case2" style="margin-bottom: 5px;">\n  <img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/en_case3.jpeg" alt="en_case3" style="margin-bottom: 5px;">\n</div>\n\nWe deploy MiniCPM-V 4.5 on iPad M4 with [iOS demo](https://github.com/tc-mb/MiniCPM-o-demo-iOS). The demo video is the raw screen recording without editing.\n\n<div align="center">\n  <img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/v45_en_handwriting.gif" width="45%" style="display: inline-block; margin: 0 10px;"/>\n  <img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/v45_en_cot.gif" width="45%" style="display: inline-block; margin: 0 10px;"/>\n</div>\n\n<div align="center">\n  <img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/v45_cn_handwriting.gif" width="45%" style="display: inline-block; margin: 0 10px;"/>\n  <img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/v45_cn_travel.gif" width="45%" style="display: inline-block; margin: 0 10px;"/>\n</div> \n\n## Framework Support Matrix\n<table>\n  <thead>\n    <tr>\n      <th>Category</th>\n      <th>Framework</th>\n      <th>Cookbook Link</th>\n      <th>Upstream PR</th>\n      <th>Supported since(branch)</th>\n      <th>Supported since(release)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td rowspan="2">Edge(On-device)</td>\n      <td>Llama.cpp</td>\n      <td><a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/llama.cpp/minicpm-v4_5_llamacpp.md">Llama.cpp Doc</a></td>\n      <td><a href="https://github.com/ggml-org/llama.cpp/pull/15575">#15575</a>(2025-08-26)</td>\n      <td>master(2025-08-26)</td>\n      <td><a href="https://github.com/ggml-org/llama.cpp/releases/tag/b6282">b6282</a></td>\n    </tr>\n    <tr>\n      <td>Ollama</td>\n      <td><a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/ollama/minicpm-v4_5_ollama.md">Ollama Doc</a></td>\n      <td><a href="https://github.com/ollama/ollama/pull/12078">#12078</a>(2025-08-26)</td>\n      <td>Merging</td>\n      <td>Waiting for official release</td>\n    </tr>\n    <tr>\n      <td rowspan="2">Serving(Cloud)</td>\n      <td>vLLM</td>\n      <td><a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/vllm/minicpm-v4_5_vllm.md">vLLM Doc</a></td>\n      <td><a href="https://github.com/vllm-project/vllm/pull/23586">#23586</a>(2025-08-26)</td>\n      <td>main(2025-08-27)</td>\n      <td><a href="https://github.com/vllm-project/vllm/releases/tag/v0.10.2">v0.10.2</td>\n    </tr>\n    <tr>\n      <td>SGLang</td>\n      <td><a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/sglang/MiniCPM-v4_5_sglang.md">SGLang Doc</a></td>\n      <td><a href="https://github.com/sgl-project/sglang/pull/9610">#9610</a>(2025-08-26)</td>\n      <td>Merging</td>\n      <td>Waiting for official release</td>\n    </tr>\n    <tr>\n      <td>Finetuning</td>\n      <td>LLaMA-Factory</td>\n      <td><a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/finetune/finetune_llamafactory.md">LLaMA-Factory Doc</a></td>\n      <td><a href="https://github.com/hiyouga/LLaMA-Factory/pull/9022">#9022</a>(2025-08-26)</td>\n      <td>main(2025-08-26)</td>\n      <td>Waiting for official release</td>\n    </tr>\n    <tr>\n      <td rowspan="3">Quantization</td>\n      <td>GGUF</td>\n      <td><a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/quantization/gguf/minicpm-v4_5_gguf_quantize.md">GGUF Doc</a></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <td>BNB</td>\n      <td><a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/quantization/bnb/minicpm-v4_5_bnb_quantize.md">BNB Doc</a></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <td>AWQ</td>\n      <td><a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/quantization/awq/minicpm-v4_5_awq_quantize.md">AWQ Doc</a></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <td>Demos</td>\n      <td>Gradio Demo</td>\n      <td><a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/demo/web_demo/gradio/README.md">Gradio Demo Doc</a></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n  </tbody>\n </table>\n \n> Note: If you''d like us to prioritize support for another open-source framework, please let us know via this [short form](https://docs.google.com/forms/d/e/1FAIpQLSdyTUrOPBgWqPexs3ORrg47ZcZ1r4vFQaA4ve2iA7L9sMfMWw/viewform).\n\n## Usage\n\nIf you wish to enable thinking mode, provide the argument `enable_thinking=True` to the chat function.\n\n#### Chat with Image\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\ntorch.manual_seed(100)\n\nmodel = AutoModel.from_pretrained(''openbmb/MiniCPM-V-4_5'', trust_remote_code=True, # or openbmb/MiniCPM-o-2_6\n    attn_implementation=''sdpa'', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained(''openbmb/MiniCPM-V-4_5'', trust_remote_code=True) # or openbmb/MiniCPM-o-2_6\n\nimage = Image.open(''./assets/minicpmo2_6/show_demo.jpg'').convert(''RGB'')\n\nenable_thinking=False # If `enable_thinking=True`, the thinking mode is enabled.\nstream=True # If `stream=True`, the answer is string\n\n# First round chat \nquestion = "What is the landform in the picture?"\nmsgs = [{''role'': ''user'', ''content'': [image, question]}]\n\nanswer = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    enable_thinking=enable_thinking,\n    stream=True\n)\n\ngenerated_text = ""\nfor new_text in answer:\n    generated_text += new_text\n    print(new_text, flush=True, end='''')\n\n# Second round chat, pass history context of multi-turn conversation\nmsgs.append({"role": "assistant", "content": [generated_text]})\nmsgs.append({"role": "user", "content": ["What should I pay attention to when traveling here?"]})\n\nanswer = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    stream=True\n)\n\ngenerated_text = ""\nfor new_text in answer:\n    generated_text += new_text\n    print(new_text, flush=True, end='''')\n```\n\nYou will get the following output:\n\n```shell\n# round1\nThe landform in the picture is karst topography. Karst landscapes are characterized by distinctive, jagged limestone hills or mountains with steep, irregular peaks and deep valleysexactly what you see here These unique formations result from the dissolution of soluble rocks like limestone over millions of years through water erosion.\n\nThis scene closely resembles the famous karst landscape of Guilin and Yangshuo in Chinas Guangxi Province. The area features dramatic, pointed limestone peaks rising dramatically above serene rivers and lush green forests, creating a breathtaking and iconic natural beauty that attracts millions of visitors each year for its picturesque views.\n\n# round2\nWhen traveling to a karst landscape like this, here are some important tips:\n\n1. Wear comfortable shoes: The terrain can be uneven and hilly.\n2. Bring water and snacks for energy during hikes or boat rides.\n3. Protect yourself from the sun with sunscreen, hats, and sunglassesespecially since youll likely spend time outdoors exploring scenic spots.\n4. Respect local customs and nature regulations by not littering or disturbing wildlife.\n\nBy following these guidelines, you''ll have a safe and enjoyable trip while appreciating the stunning natural beauty of places such as Guilins karst mountains.\n```\n\n\n#### Chat with Video\n\n```python\n## The 3d-resampler compresses multiple frames into 64 tokens by introducing temporal_ids. \n# To achieve this, you need to organize your video data into two corresponding sequences: \n#   frames: List[Image]\n#   temporal_ids: List[List[Int]].\n\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nfrom decord import VideoReader, cpu    # pip install decord\nfrom scipy.spatial import cKDTree\nimport numpy as np\nimport math\n\nmodel = AutoModel.from_pretrained(''openbmb/MiniCPM-V-4_5'', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6\n    attn_implementation=''sdpa'', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained(''openbmb/MiniCPM-V-4_5'', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6\n\nMAX_NUM_FRAMES=180 # Indicates the maximum number of frames received after the videos are packed. The actual maximum number of valid frames is MAX_NUM_FRAMES * MAX_NUM_PACKING.\nMAX_NUM_PACKING=3  # indicates the maximum packing number of video frames. valid range: 1-6\nTIME_SCALE = 0.1 \n\ndef map_to_nearest_scale(values, scale):\n    tree = cKDTree(np.asarray(scale)[:, None])\n    _, indices = tree.query(np.asarray(values)[:, None])\n    return np.asarray(scale)[indices]\n\n\ndef group_array(arr, size):\n    return [arr[i:i+size] for i in range(0, len(arr), size)]\n\ndef encode_video(video_path, choose_fps=3, force_packing=None):\n    def uniform_sample(l, n):\n        gap = len(l) / n\n        idxs = [int(i * gap + gap / 2) for i in range(n)]\n        return [l[i] for i in idxs]\n    vr = VideoReader(video_path, ctx=cpu(0))\n    fps = vr.get_avg_fps()\n    video_duration = len(vr) / fps\n        \n    if choose_fps * int(video_duration) <= MAX_NUM_FRAMES:\n        packing_nums = 1\n        choose_frames = round(min(choose_fps, round(fps)) * min(MAX_NUM_FRAMES, video_duration))\n        \n    else:\n        packing_nums = math.ceil(video_duration * choose_fps / MAX_NUM_FRAMES)\n        if packing_nums <= MAX_NUM_PACKING:\n            choose_frames = round(video_duration * choose_fps)\n        else:\n            choose_frames = round(MAX_NUM_FRAMES * MAX_NUM_PACKING)\n            packing_nums = MAX_NUM_PACKING\n\n    frame_idx = [i for i in range(0, len(vr))]      \n    frame_idx =  np.array(uniform_sample(frame_idx, choose_frames))\n\n    if force_packing:\n        packing_nums = min(force_packing, MAX_NUM_PACKING)\n    \n    print(video_path, '' duration:'', video_duration)\n    print(f''get video frames={len(frame_idx)}, packing_nums={packing_nums}'')\n    \n    frames = vr.get_batch(frame_idx).asnumpy()\n\n    frame_idx_ts = frame_idx / fps\n    scale = np.arange(0, video_duration, TIME_SCALE)\n\n    frame_ts_id = map_to_nearest_scale(frame_idx_ts, scale) / TIME_SCALE\n    frame_ts_id = frame_ts_id.astype(np.int32)\n\n    assert len(frames) == len(frame_ts_id)\n\n    frames = [Image.fromarray(v.astype(''uint8'')).convert(''RGB'') for v in frames]\n    frame_ts_id_group = group_array(frame_ts_id, packing_nums)\n    \n    return frames, frame_ts_id_group\n\n\nvideo_path="video_test.mp4"\nfps = 5 # fps for video\nforce_packing = None # You can set force_packing to ensure that 3D packing is forcibly enabled; otherwise, encode_video will dynamically set the packing quantity based on the duration.\nframes, frame_ts_id_group = encode_video(video_path, fps, force_packing=force_packing)\n\nquestion = "Describe the video"\nmsgs = [\n    {''role'': ''user'', ''content'': frames + [question]}, \n]\n\n\nanswer = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    use_image_id=False,\n    max_slice_nums=1,\n    temporal_ids=frame_ts_id_group\n)\nprint(answer)\n```\n\n#### Chat with multiple images\n<details>\n<summary> Click to show Python code running MiniCPM-V 4.5 with multiple images input. </summary>\n  \n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained(''openbmb/MiniCPM-V-4_5'', trust_remote_code=True,\n    attn_implementation=''sdpa'', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained(''openbmb/MiniCPM-V-4_5'', trust_remote_code=True)\n\nimage1 = Image.open(''image1.jpg'').convert(''RGB'')\nimage2 = Image.open(''image2.jpg'').convert(''RGB'')\nquestion = ''Compare image 1 and image 2, tell me about the differences between image 1 and image 2.''\n\nmsgs = [{''role'': ''user'', ''content'': [image1, image2, question]}]\n\nanswer = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n```\n</details>\n\n\n#### In-context few-shot learning\n<details>\n<summary> Click to view Python code running MiniCPM-V 4.5 with few-shot input. </summary>\n\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained(''openbmb/MiniCPM-V-4_5'', trust_remote_code=True,\n    attn_implementation=''sdpa'', torch_dtype=torch.bfloat16)\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained(''openbmb/MiniCPM-V-4_5'', trust_remote_code=True)\n\nquestion = "production date" \nimage1 = Image.open(''example1.jpg'').convert(''RGB'')\nanswer1 = "2023.08.04"\nimage2 = Image.open(''example2.jpg'').convert(''RGB'')\nanswer2 = "2007.04.24"\nimage_test = Image.open(''test.jpg'').convert(''RGB'')\n\nmsgs = [\n    {''role'': ''user'', ''content'': [image1, question]}, {''role'': ''assistant'', ''content'': [answer1]},\n    {''role'': ''user'', ''content'': [image2, question]}, {''role'': ''assistant'', ''content'': [answer2]},\n    {''role'': ''user'', ''content'': [image_test, question]}\n]\n\nanswer = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n```\n</details>\n\n\n## License\n#### Model License\n* The MiniCPM-o/V model weights and code are open-sourced under the [Apache-2.0](https://github.com/OpenBMB/MiniCPM-V/blob/main/LICENSE) license.\n* To help us better understand and support our users, we would deeply appreciate it if you could consider optionally filling out a brief registration ["questionnaire"](https://modelbest.feishu.cn/share/base/form/shrcnpV5ZT9EJ6xYjh3Kx0J6v8g).\n\n#### Statement\n* As an LMM, MiniCPM-V 4.5 generates contents by learning a large amount of multimodal corpora, but it cannot comprehend, express personal opinions or make value judgement. Anything generated by MiniCPM-V 4.5 does not represent the views and positions of the model developers\n* We will not be liable for any problems arising from the use of the MinCPM-V models, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination or misuse of the model.\n\n## Key Techniques and Other Multimodal Projects\n\n Welcome to explore key techniques of MiniCPM-V 4.5 and other multimodal projects of our team:\n\n[VisCPM](https://github.com/OpenBMB/VisCPM/tree/main) | [RLPR](https://github.com/OpenBMB/RLPR) |  [RLHF-V](https://github.com/RLHF-V/RLHF-V) | [LLaVA-UHD](https://github.com/thunlp/LLaVA-UHD)  | [RLAIF-V](https://github.com/RLHF-V/RLAIF-V)\n\n## Citation\n\nIf you find our work helpful, please consider citing our papers  and liking this project \n\n```bib\n@misc{yu2025minicpmv45cookingefficient,\n      title={MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe}, \n      author={Tianyu Yu and Zefan Wang and Chongyi Wang and Fuwei Huang and Wenshuo Ma and Zhihui He and Tianchi Cai and Weize Chen and Yuxiang Huang and Yuanqian Zhao and Bokai Xu and Junbo Cui and Yingjing Xu and Liqing Ruan and Luoyuan Zhang and Hanyu Liu and Jingkun Tang and Hongyuan Liu and Qining Guo and Wenhao Hu and Bingxiang He and Jie Zhou and Jie Cai and Ji Qi and Zonghao Guo and Chi Chen and Guoyang Zeng and Yuxuan Li and Ganqu Cui and Ning Ding and Xu Han and Yuan Yao and Zhiyuan Liu and Maosong Sun},\n      year={2025},\n      eprint={2509.18154},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2509.18154}, \n}\n\n@article{yao2024minicpm,\n  title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},\n  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},\n  journal={Nat Commun 16, 5509 (2025)},\n  year={2025}\n}\n\n```', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":8695895280,"storage_bytes":17403328052,"files_count":23,"spaces_count":14,"gated":false,"private":false,"config":{"architectures":["MiniCPMV"],"auto_map":{"AutoConfig":"configuration_minicpm.MiniCPMVConfig","AutoModel":"modeling_minicpmv.MiniCPMV","AutoModelForCausalLM":"modeling_minicpmv.MiniCPMV"},"model_type":"minicpmv","tokenizer_config":{"bos_token":"<|im_start|>","chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {{- messages[0].content + ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(message.content.startswith(''<tool_response>'') and message.content.endswith(''</tool_response>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set content = message.content %}\n        {%- set reasoning_content = '''' %}\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if ''</think>'' in message.content %}\n                {%- set content = message.content.split(''</think>'')[-1].lstrip(''\\n'') %}\n                {%- set reasoning_content = message.content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- ''<|im_start|>'' + message.role + ''\\n<think>\\n'' + reasoning_content.strip(''\\n'') + ''\\n</think>\\n\\n'' + content.lstrip(''\\n'') }}\n            {%- else %}\n                {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- ''<think>\\n\\n</think>\\n\\n'' }}\n    {%- endif %}\n    {%- if enable_thinking is defined and enable_thinking is true %}\n        {{- ''<think>\\n'' }}\n    {%- endif %}\n{%- endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:OpenBMB:MiniCPM-o","source_url":"https://github.com/OpenBMB/MiniCPM-o"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:RLHF-V:RLAIF-V","source_url":"https://github.com/RLHF-V/RLAIF-V"},{"type":"has_code","target_id":"github:OpenBMB:VisCPM","source_url":"https://github.com/OpenBMB/VisCPM"},{"type":"has_code","target_id":"github:tc-mb:llama.cpp","source_url":"https://github.com/tc-mb/llama.cpp"},{"type":"has_code","target_id":"github:tc-mb:ollama","source_url":"https://github.com/tc-mb/ollama"},{"type":"has_code","target_id":"github:tc-mb:AutoAWQ","source_url":"https://github.com/tc-mb/AutoAWQ"},{"type":"has_code","target_id":"github:tc-mb:sglang","source_url":"https://github.com/tc-mb/sglang"},{"type":"has_code","target_id":"github:tc-mb:transformers","source_url":"https://github.com/tc-mb/transformers"},{"type":"has_code","target_id":"github:tc-mb:MiniCPM-o-demo-iOS","source_url":"https://github.com/tc-mb/MiniCPM-o-demo-iOS"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:OpenBMB:RLPR","source_url":"https://github.com/OpenBMB/RLPR"},{"type":"has_code","target_id":"github:RLHF-V:RLAIF-V","source_url":"https://github.com/RLHF-V/RLAIF-V"},{"type":"has_code","target_id":"github:tc-mb:MiniCPM-o-demo-iOS","source_url":"https://github.com/tc-mb/MiniCPM-o-demo-iOS"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:ggml-org:llama.cpp","source_url":"https://github.com/ggml-org/llama.cpp"},{"type":"has_code","target_id":"github:ggml-org:llama.cpp","source_url":"https://github.com/ggml-org/llama.cpp"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:ollama:ollama","source_url":"https://github.com/ollama/ollama"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory","source_url":"https://github.com/hiyouga/LLaMA-Factory"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:OpenBMB:MiniCPM-V","source_url":"https://github.com/OpenBMB/MiniCPM-V"},{"type":"has_code","target_id":"github:OpenBMB:VisCPM","source_url":"https://github.com/OpenBMB/VisCPM"},{"type":"has_code","target_id":"github:OpenBMB:RLPR","source_url":"https://github.com/OpenBMB/RLPR"},{"type":"has_code","target_id":"github:RLHF-V:RLHF-V","source_url":"https://github.com/RLHF-V/RLHF-V"},{"type":"has_code","target_id":"github:thunlp:LLaVA-UHD","source_url":"https://github.com/thunlp/LLaVA-UHD"},{"type":"has_code","target_id":"github:RLHF-V:RLAIF-V","source_url":"https://github.com/RLHF-V/RLAIF-V"},{"type":"based_on_paper","target_id":"arxiv:2509.18154","source_url":"https://arxiv.org/abs/2509.18154"},{"type":"based_on_paper","target_id":"arxiv:2403.11703","source_url":"https://arxiv.org/abs/2403.11703"}]', NULL, 'Apache-2.0', 'approved', 80, '6b80e37d4354664d062928ca5f73c007', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-adept-fuyu-8b', 'huggingface--adept--fuyu-8b', 'fuyu-8b', 'adept', '--- license: cc-by-nc-4.0 --- Were releasing Fuyu-8B, a small version of the multimodal model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because: 1. It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy. 2. Its designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answe...', '["transformers","safetensors","fuyu","any-to-any","license:cc-by-nc-4.0","endpoints_compatible","region:us"]', 'any-to-any', 1014, 47615, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/adept/fuyu-8b","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: cc-by-nc-4.0\n---\n# Fuyu-8B Model Card\n\nWere releasing Fuyu-8B, a small version of the multimodal model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:\n1. It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.\n2. Its designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.\n3. Its fast - we can get responses for large images in less than 100 milliseconds.\n4. Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.\n\nPlease note that **the model we have released is a base model. We expect you to need to finetune the model for specific use cases like verbose captioning or multimodal chat.** In our experience, the model responds well to few-shotting and fine-tuning for a variety of use-cases. \n\n## Model\n\n[Fuyu-8B](https://www.adept.ai/blog/fuyu-8b) is a multi-modal text and image transformer trained by [Adept AI](https://www.adept.ai/).\n\nArchitecturally, Fuyu is a vanilla decoder-only transformer - there is no image encoder. \nImage patches are instead linearly projected into the first layer of the transformer, bypassing the embedding lookup. \nWe simply treat the transformer decoder like an image transformer (albeit with no pooling and causal attention).\nSee the below diagram for more details.\n\n![architecture](architecture.png)\n\nThis simplification allows us to support arbitrary image resolutions. \nTo accomplish this, we treat the sequence of image tokens like the sequence of text tokens. \nWe remove image-specific position embeddings and feed in as many image tokens as necessary in raster-scan order. \nTo tell the model when a line has broken, we simply use a special image-newline character. \nThe model can use its existing position embeddings to reason about different image sizes, and we can use images of arbitrary size at training time, removing the need for separate high and low-resolution training stages.\n\n### Model Description\n\n- **Developed by:** Adept-AI\n- **Model type:** Decoder-only multi-modal transformer model \n- **License:** [CC-BY-NC](https://creativecommons.org/licenses/by-nc/4.0/deed.en)\n- **Model Description:** This is a multi-modal model that can consume images and text and produce text. \n- **Resources for more information:** Check out our [blog post](https://www.adept.ai/blog/fuyu-8b).\n\n## Evaluation\nThough not the focus of this model, we did evaluate it on standard image understanding benchmarks:\n\n| Eval Task           | Fuyu-8B | Fuyu-Medium       | LLaVA 1.5 (13.5B) | QWEN-VL (10B) | PALI-X (55B) | PALM-e-12B | PALM-e-562B |\n| ------------------- | ------- | ----------------- | ----------------- | ------------- | ------------ | ---------- | ----------- |\n| VQAv2               | 74.2    |     77.4          | 80                | 79.5          | 86.1         | 76.2       | 80.0        |\n| OKVQA               | 60.6    |     63.1          | n/a               | 58.6          | 66.1         | 55.5       | 66.1        |\n| COCO Captions       | 141     |     138           | n/a               | n/a           | 149          | 135        | 138         |\n| AI2D                | 64.5    |     73.7          | n/a               | 62.3          | 81.2         | n/a        | n/a         |\n\n## How to Use\n\nYou can load the model and perform inference as follows:\n```python\nfrom transformers import FuyuProcessor, FuyuForCausalLM\nfrom PIL import Image\nimport requests\n\n# load model and processor\nmodel_id = "adept/fuyu-8b"\nprocessor = FuyuProcessor.from_pretrained(model_id)\nmodel = FuyuForCausalLM.from_pretrained(model_id, device_map="cuda:0")\n\n# prepare inputs for the model\ntext_prompt = "Generate a coco-style caption.\n"\nurl = "https://huggingface.co/adept/fuyu-8b/resolve/main/bus.png"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=text_prompt, images=image, return_tensors="pt").to("cuda:0")\n\n# autoregressively generate text\ngeneration_output = model.generate(**inputs, max_new_tokens=7)\ngeneration_text = processor.batch_decode(generation_output[:, -7:], skip_special_tokens=True)\nassert generation_text == [''A blue bus parked on the side of a road.'']\n```\n\nN.B.: The token `|SPEAKER|` is a placeholder token for image patch embeddings, so it will show up in the model context (e.g., in the portion of `generation_output` representing the model context).\n`|NEWLINE|` is the "image newline" token, denoting new rows in the raster scan order input of the image patches.\n`\x04` is the "beginning of answer" token.\n\nFuyu can also perform some question answering on natural images and charts/diagrams (thought fine-tuning may be required for good performance):\n```python\ntext_prompt = "What color is the bus?\n"\nurl = "https://huggingface.co/adept/fuyu-8b/resolve/main/bus.png"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=text_prompt, images=image, return_tensors="pt").to("cuda:0")\n\ngeneration_output = model.generate(**inputs, max_new_tokens=6)\ngeneration_text = processor.batch_decode(generation_output[:, -6:], skip_special_tokens=True)\nassert generation_text == ["The bus is blue.\n"]\n\n\ntext_prompt = "What is the highest life expectancy at birth of male?\n"\nurl = "https://huggingface.co/adept/fuyu-8b/resolve/main/chart.png"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel_inputs = processor(text=text_prompt, images=image, return_tensors="pt").to("cuda:0")\n\ngeneration_output = model.generate(**model_inputs, max_new_tokens=16)\ngeneration_text = processor.batch_decode(generation_output[:, -16:], skip_special_tokens=True)\nassert generation_text == ["The life expectancy at birth of males in 2018 is 80.7.\n"]\n```\nFor best performance, it''s recommended to end questions with `\n`, as shown above!\n\n## Uses\n\n### Direct Use\n\nThe model is intended for research purposes only. \n**Because this is a raw model release, we have not added further finetuning, postprocessing or sampling strategies to control for undesirable outputs. You should expect to have to fine-tune the model for your use-case.**\n\nPossible research areas and tasks include\n\n- Applications in computer control or digital agents.\n- Research on multi-modal models generally.\n\nExcluded uses are described below.\n\n### Out-of-Scope Use\n\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n## Limitations and Bias\n\n### Limitations\n\n- Faces and people in general may not be generated properly.\n\n### Bias\nWhile the capabilities of these models are impressive, they can also reinforce or exacerbate social biases.', '{"pipeline_tag":"any-to-any","library_name":"transformers","framework":"transformers","params":9408238592,"storage_bytes":37678352448,"files_count":17,"spaces_count":55,"gated":false,"private":false,"config":{"architectures":["FuyuForCausalLM"],"model_type":"fuyu","tokenizer_config":{"bos_token":"|ENDOFTEXT|","eos_token":"|ENDOFTEXT|","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":true}}}', '[]', '[]', NULL, 'CC-BY-NC-4.0', 'approved', 85, '1581e56ae03f606824f18ca395ff8b44', NULL, 'https://huggingface.co/adept/fuyu-8b/resolve/main/architecture.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-adept-fuyu-8b from https://huggingface.co/adept/fuyu-8b/resolve/main/architecture.png
Image converted to WebP: data/images/huggingface-adept-fuyu-8b.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-openbmb-MiniCPM-V-2-6', 'huggingface--openbmb--minicpm-v-2-6', 'MiniCPM-V-2_6', 'openbmb', '', '["transformers","safetensors","minicpmv","feature-extraction","minicpm-v","vision","ocr","multi-image","video","custom_code","image-text-to-text","conversational","multilingual","dataset:openbmb/rlaif-v-dataset","arxiv:2408.01800","region:us"]', 'image-text-to-text', 1013, 100147, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/openbmb/MiniCPM-V-2_6","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":8099175152,"storage_bytes":16221778104,"files_count":24,"spaces_count":28,"gated":"auto","private":false,"config":{"architectures":["MiniCPMV"],"auto_map":{"AutoConfig":"configuration_minicpm.MiniCPMVConfig","AutoModel":"modeling_minicpmv.MiniCPMV","AutoModelForCausalLM":"modeling_minicpmv.MiniCPMV"},"model_type":"minicpmv","tokenizer_config":{"bos_token":"<|im_start|>","chat_template":"{% for message in messages %}{% if loop.first and messages[0][''role''] != ''system'' %}{{ ''<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n'' }}{% endif %}{{''<|im_start|>'' + message[''role''] + ''\n'' + message[''content''] + ''<|im_end|>'' + ''\n''}}{% endfor %}{% if add_generation_prompt %}{{ ''<|im_start|>assistant\n'' }}{% endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":"<unk>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2408.01800","source_url":"https://arxiv.org/abs/2408.01800"}]', NULL, NULL, 'pending', 50, '28f3a737bbf64fc3387ce729dd038168', NULL, 'https://huggingface.co/openbmb/MiniCPM-V-2_6/resolve/main/assets/radar_final.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-openbmb-MiniCPM-V-2-6 from https://huggingface.co/openbmb/MiniCPM-V-2_6/resolve/main/assets/radar_final.png
HTTP error: 401 Unauthorized

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-V3.1-Base', 'huggingface--deepseek-ai--deepseek-v3.1-base', 'DeepSeek-V3.1-Base', 'deepseek-ai', '--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/d...', '["transformers","safetensors","deepseek_v3","text-generation","conversational","custom_code","arxiv:2412.19437","license:mit","text-generation-inference","endpoints_compatible","fp8","region:us"]', 'text-generation', 1003, 7784, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-V3.1\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n## Introduction\n\nDeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode. Compared to the previous version, this upgrade brings improvements in multiple aspects:\n\n- **Hybrid thinking mode**: One model supports both thinking mode and non-thinking mode by changing the chat template. \n\n- **Smarter tool calling**: Through post-training optimization, the model''s performance in tool usage and agent tasks has significantly improved.\n\n- **Higher thinking efficiency**: DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly.\n\nDeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. We have expanded our dataset by collecting additional long documents and substantially extending both training phases. The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens.\n\nAdditionally, DeepSeek-V3.1 is trained using the **UE8M0 FP8 scale data format on both model weights and activations** to ensure compatibility with microscaling data formats. Please refer to [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM) for more details.\n\n## Model Downloads\n\n<div align="center">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-V3.1-Base | 671B | 37B | 128K | [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base) \| [ModelScope](https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.1-Base) |\n| DeepSeek-V3.1 | 671B | 37B | 128K | [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3.1) \| [ModelScope](https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.1) |\n\n</div>\n\n## Chat Template\n\nThe details of our chat template is described in `tokenizer_config.json` and `assets/chat_template.jinja`. Here is a brief description.\n\n### Non-Thinking\n\n#### First-Turn\n\nPrefix:\n`<beginofsentence>{system prompt}<User>{query}<Assistant></think>`\n\nWith the given prefix, DeepSeek V3.1 generates responses to queries in non-thinking mode. Unlike DeepSeek V3,  it introduces an additional token `</think>`.\n\n#### Multi-Turn\nContext:\n`<beginofsentence>{system prompt}<User>{query}<Assistant></think>{response}<endofsentence>...<User>{query}<Assistant></think>{response}<endofsentence>`\n\nPrefix:\n`<User>{query}<Assistant></think>`\n\nBy concatenating the context and the prefix, we obtain the correct prompt for the query.\n\n### Thinking\n\n#### First-Turn\nPrefix:\n`<beginofsentence>{system prompt}<User>{query}<Assistant><think>`\n\nThe prefix of thinking mode is similar to DeepSeek-R1. \n\n\n#### Multi-Turn\nContext:\n`<beginofsentence>{system prompt}<User>{query}<Assistant></think>{response}<endofsentence>...<User>{query}<Assistant></think>{response}<endofsentence>`\n\nPrefix:\n`<User>{query}<Assistant><think>`\n\nThe multi-turn template is the same with non-thinking multi-turn chat template. It means the thinking token in the last turn will be dropped but the `</think>` is retained in every turn of context. \n\n### ToolCall\nToolcall is supported in non-thinking mode. The format is: \n\n`<beginofsentence>{system prompt}\n\n{tool_description}<User>{query}<Assistant></think>` where the tool_description is \n\n```\n## Tools\nYou have access to the following tools:\n\n### {tool_name1}\nDescription: {description}\n\nParameters: {json.dumps(parameters)}\n\nIMPORTANT: ALWAYS adhere to this exact format for tool use:\n<toolcallsbegin><toolcallbegin>tool_call_name<toolsep>tool_call_arguments<toolcallend>{additional_tool_calls}<toolcallsend>\n\nWhere:\n- `tool_call_name` must be an exact match to one of the available tools\n- `tool_call_arguments` must be valid JSON that strictly follows the tool''s Parameters Schema\n- For multiple tool calls, chain them directly without separators or spaces\n```\n\n### Code-Agent\nWe support various code agent frameworks. Please refer to the above toolcall format to create your own code agents. An example is shown in `assets/code_agent_trajectory.html`.\n\n### Search-Agent\nWe design a specific format for searching toolcall in thinking mode, to support search agent. \n\nFor complex questions that require accessing external or up-to-date information, DeepSeek-V3.1 can leverage a user-provided search tool through a multi-turn tool-calling process.\n\nPlease refer to the `assets/search_tool_trajectory.html` and `assets/search_python_tool_trajectory.html` for the detailed template.\n\n## Evaluation\n| Category | Benchmark (Metric)              | DeepSeek V3.1-NonThinking | DeepSeek V3 0324 | DeepSeek V3.1-Thinking     | DeepSeek R1 0528\n|----------|----------------------------------|-----------------|---|---|---|\n| General  |\n|          | MMLU-Redux (EM)              | 91.8     | 90.5    | 93.7          | 93.4\n|          | MMLU-Pro (EM)                  | 83.7  | 81.2    | 84.8          | 85.0\n|          | GPQA-Diamond (Pass@1)           | 74.9   | 68.4   | 80.1            | 81.0\n|          | Humanity''s Last Exam (Pass@1)   | -    |       -            | 15.9         | 17.7\n|Search Agent| \n|          | BrowseComp       | -      | -  | 30.0 | 8.9\n|          | BrowseComp_zh       | -     | -  | 49.2      | 35.7\n|          | Humanity''s Last Exam (Python + Search)      |-   | -    | 29.8         | 24.8\n|          | SimpleQA             | -      | -    | 93.4  | 92.3\n| Code |\n|          | LiveCodeBench (2408-2505) (Pass@1)     | 56.4    | 43.0    | 74.8          | 73.3\n|          | Codeforces-Div1 (Rating)        | -   | -    | 2091            | 1930\n|          | Aider-Polyglot (Acc.)           | 68.4    | 55.1   | 76.3           | 71.6\n| Code Agent|\n|          | SWE Verified (Agent mode)           | 66.0       | 45.4  | -    | 44.6\n|          | SWE-bench Multilingual (Agent mode)         | 54.5    | 29.3   | -            | 30.5\n|          | Terminal-bench (Terminus 1 framework)       | 31.3     | 13.3      | -         | 5.7\n| Math |\n|          | AIME 2024 (Pass@1)                | 66.3     | 59.4     | 93.1      | 91.4\n|          | AIME 2025 (Pass@1)                     | 49.8  | 51.3 | 88.4          | 87.5\n|          | HMMT 2025 (Pass@1)        | 33.5    | 29.2   | 84.2 | 79.4 |\n\nNote: \n- Search agents are evaluated with our internal search framework, which uses a commercial search API + webpage filter + 128K context window. Seach agent results of R1-0528 are evaluated with a pre-defined workflow. \n\n- SWE-bench is evaluated with our internal code agent framework.\n\n- HLE is evaluated with the text-only subset.\n\n### Usage Example\n\n```python\nimport transformers\n\ntokenizer = transformers.AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-V3.1")\n\nmessages = [\n    {"role": "system", "content": "You are a helpful assistant"},\n    {"role": "user", "content": "Who are you?"},\n    {"role": "assistant", "content": "<think>Hmm</think>I am DeepSeek"},\n    {"role": "user", "content": "1+1=?"}\n]\n\ntokenizer.apply_chat_template(messages, tokenize=False, thinking=True, add_generation_prompt=True)\n# ''<beginofsentence>You are a helpful assistant<User>Who are you?<Assistant></think>I am DeepSeek<endofsentence><User>1+1=?<Assistant><think>''\n\ntokenizer.apply_chat_template(messages, tokenize=False, thinking=False, add_generation_prompt=True)\n# ''<beginofsentence>You are a helpful assistant<User>Who are you?<Assistant></think>I am DeepSeek<endofsentence><User>1+1=?<Assistant></think>''\n```\n\n## How to Run Locally\n\nThe model structure of DeepSeek-V3.1 is the same as DeepSeek-V3. Please visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running this model locally.\n\n**Usage Recommendations:**\n\n1. **The `mlp.gate.e_score_correction_bias `parameters should be loaded and computed in FP32 precision.**\n2. **Ensure that FP8 model weights and activations are formatted using the UE8M0 scale format.**\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}\n```\n\n## Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":684531386000,"storage_bytes":688587108667,"files_count":177,"spaces_count":14,"gated":false,"private":false,"config":{"architectures":["DeepseekV3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_deepseek.DeepseekV3Config","AutoModel":"modeling_deepseek.DeepseekV3Model","AutoModelForCausalLM":"modeling_deepseek.DeepseekV3ForCausalLM"},"model_type":"deepseek_v3","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<beginofsentence>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<endofsentence>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<endofsentence>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% if not thinking is defined %}{% set thinking = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, system_prompt='''', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message[''content''] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + ''\n\n'' + message[''content''] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{''<User>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''tool_calls''] is defined and message[''tool_calls''] is not none %}{%- if ns.is_last_user %}{{''<Assistant></think>''}}{%- endif %}{%- set ns.is_last_user = false -%}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls''] %}{%- if not ns.is_first %}{%- if message[''content''] is none %}{{''<toolcallsbegin><toolcallbegin>''+ tool[''function''][''name''] + ''<toolsep>'' + tool[''function''][''arguments''] + ''<toolcallend>''}}{%- else %}{{message[''content''] + ''<toolcallsbegin><toolcallbegin>'' + tool[''function''][''name''] + ''<toolsep>'' + tool[''function''][''arguments''] + ''<toolcallend>''}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{''<toolcallbegin>''+ tool[''function''][''name''] + ''<toolsep>'' + tool[''function''][''arguments''] + ''<toolcallend>''}}{%- endif %}{%- endfor %}{{''<toolcallsend><endofsentence>''}}{%- endif %}{%- if message[''role''] == ''assistant'' and (message[''tool_calls''] is not defined or message[''tool_calls''] is none) %}{%- if ns.is_last_user %}{{''<Assistant>''}}{%- if message[''prefix''] is defined and message[''prefix''] and thinking %}{{''<think>''}}  {%- else %}{{''</think>''}}{%- endif %}{%- endif %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{message[''content''] + ''<endofsentence>''}}{%- set ns.is_tool = false -%}{%- else %}{%- set content = message[''content''] -%}{%- if ''</think>'' in content %}{%- set content = content.split(''</think>'', 1)[1] -%}{%- endif %}{{content + ''<endofsentence>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{{''<tooloutputbegin>'' + message[''content''] + ''<tooloutputend>''}}{%- endif %}{%- endfor -%}{%- if add_generation_prompt and ns.is_last_user and not ns.is_tool %}{{''<Assistant>''}}{%- if not thinking %}{{''</think>''}}{%- else %}{{''<think>''}}{%- endif %}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepGEMM","source_url":"https://github.com/deepseek-ai/DeepGEMM"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"based_on_paper","target_id":"arxiv:2412.19437","source_url":"https://arxiv.org/abs/2412.19437"}]', NULL, 'MIT', 'approved', 80, '6b87557a767151299f2ad5247bf1ec2d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-stable-video-diffusion-img2vid', 'huggingface--stabilityai--stable-video-diffusion-img2vid', 'stable-video-diffusion-img2vid', 'stabilityai', '--- pipeline_tag: image-to-video license: other license_name: stable-video-diffusion-community license_link: LICENSE.md --- <!-- Provide a quick summary of what the model is/does. --> !row01 Stable Video Diffusion (SVD) Image-to-Video is a diffusion model that takes in a still image as a conditioning frame, and generates a video from it. Please note: For commercial use of this model, please refer to https://stability.ai/license. (SVD) Image-to-Video is a latent diffusion model trained to gene...', '["diffusers","safetensors","image-to-video","license:other","diffusers:stablevideodiffusionpipeline","region:us"]', 'image-to-video', 1000, 532407, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/stable-video-diffusion-img2vid","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\npipeline_tag: image-to-video\nlicense: other\nlicense_name: stable-video-diffusion-community\nlicense_link: LICENSE.md\n---\n\n# Stable Video Diffusion Image-to-Video Model Card\n\n<!-- Provide a quick summary of what the model is/does. -->\n![row01](output_tile.gif)\nStable Video Diffusion (SVD) Image-to-Video is a diffusion model that takes in a still image as a conditioning frame, and generates a video from it. \n\nPlease note: For commercial use of this model, please refer to https://stability.ai/license.\n\n## Model Details\n\n### Model Description\n\n(SVD) Image-to-Video is a latent diffusion model trained to generate short video clips from an image conditioning. \nThis model was trained to generate 14 frames at resolution 576x1024 given a context frame of the same size.\nWe also finetune the widely used [f8-decoder](https://huggingface.co/docs/diffusers/api/models/autoencoderkl#loading-from-the-original-format) for temporal consistency. \nFor convenience, we additionally provide the model with the \nstandard frame-wise decoder [here](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid/blob/main/svd_image_decoder.safetensors).\n\n\n- **Developed by:** Stability AI\n- **Funded by:** Stability AI\n- **Model type:** Generative image-to-video model\n\n### Model Sources\n\nFor research purposes, we recommend our `generative-models` Github repository (https://github.com/Stability-AI/generative-models), \nwhich implements the most popular diffusion frameworks (both training and inference).\n\n- **Repository:** https://github.com/Stability-AI/generative-models\n- **Paper:** https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets\n\n## Evaluation\n![comparison](comparison.png)\nThe chart above evaluates user preference for SVD-Image-to-Video over [GEN-2](https://research.runwayml.com/gen2) and [PikaLabs](https://www.pika.art/).\nSVD-Image-to-Video is preferred by human voters in terms of video quality. For details on the user study, we refer to the [research paper](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets)\n\n## Uses\n\n### Direct Use\n\nThe model is intended for research purposes only. Possible research areas and tasks include\n\n- Research on generative models.\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n\nExcluded uses are described below.\n\n### Out-of-Scope Use\n\nThe model was not trained to be factual or true representations of people or events, \nand therefore using the model to generate such content is out-of-scope for the abilities of this model.\nThe model should not be used in any way that violates Stability AI''s [Acceptable Use Policy](https://stability.ai/use-policy).\n\n## Limitations and Bias\n\n### Limitations\n- The generated videos are rather short (<= 4sec), and the model does not achieve perfect photorealism.\n- The model may generate videos without motion, or very slow camera pans.\n- The model cannot be controlled through text.\n- The model cannot render legible text.\n- Faces and people in general may not be generated properly.\n- The autoencoding part of the model is lossy.\n\n\n### Recommendations\n\nThe model is intended for research purposes only.\n\n## How to Get Started with the Model\n\nCheck out https://github.com/Stability-AI/generative-models\n\n\n\n# Appendix:\nAll considered potential data sources were included for final training, with none held out as the proposed data filtering methods described in the SVD paper handle the quality control/filtering of the dataset. With regards to safety/NSFW filtering, sources considered were either deemed safe or filtered with the in-house NSFW filters. No explicit human labor is involved in training data preparation. However, human evaluation for model outputs and quality was extensively used to evaluate model quality and performance. The evaluations were performed with third-party contractor platforms (Amazon Sagemaker, Amazon Mechanical Turk, Prolific) with fluent English-speaking contractors from various countries, primarily from the USA, UK, and Canada. Each worker was paid $12/hr for the time invested in the evaluation. No other third party was involved in the development of this model; the model was fully developed in-house at Stability AI. Training the SVD checkpoints required a total of approximately 200,000 A100 80GB hours. The majority of the training occurred on 48 * 8 A100s, while some stages took more/less than that. The resulting CO2 emission is ~19,000kg CO2 eq., and energy consumed is ~64000 kWh. The released checkpoints (SVD/SVD-XT) are image-to-video models that generate short videos/animations closely following the given input image. Since the model relies on an existing supplied image, the potential risks of disclosing specific material or novel unsafe content are minimal. This was also evaluated by third-party independent red-teaming services, which agree with our conclusion to a high degree of confidence (>90% in various areas of safety red-teaming). The external evaluations were also performed for trustworthiness, leading to >95% confidence in real, trustworthy videos. With the default settings at the time of release, SVD takes ~100s for generation, and SVD-XT takes ~180s on an A100 80GB card. Several optimizations to trade off quality / memory / speed can be done to perform faster inference or inference on lower VRAM cards. The information related to the model and its development process and usage protocols can be found in the GitHub repo, associated research paper, and HuggingFace model page/cards. The released model inference & demo code has image-level watermarking enabled by default, which can be used to detect the outputs. This is done via the imWatermark Python library.\nThe model can be used to generate videos from static initial images. However, we prohibit unlawful, obscene, or misleading uses of the model consistent with the terms of our license and Acceptable Use Policy. For the open-weights release, our training data filtering mitigations alleviate this risk to some extent. These restrictions are explicitly enforced on user-facing interfaces at stablevideo.com, where a warning is issued. We do not take any responsibility for third-party interfaces. Submitting initial images that bypass input filters to tease out offensive or inappropriate content listed above is also prohibited. Safety filtering checks at stablevideo.com run on model inputs and outputs independently. More details on our user-facing interfaces can be found here: https://www.stablevideo.com/faq. Beyond the Acceptable Use Policy and other mitigations and conditions described here, the model is not subject to additional model behavior interventions of the type described in the Foundation Model Transparency Index.\nFor stablevideo.com, we store preference data in the form of upvotes/downvotes on user-generated videos, and we have a pairwise ranker that runs while a user generates videos. This usage data is solely used for improving Stability AIs future image/video models and services. No other third-party entities are given access to the usage data beyond Stability AI and maintainers of stablevideo.com. For usage statistics of SVD, we refer interested users to HuggingFace model download/usage statistics as a primary indicator. Third-party applications also have reported model usage statistics. We might also consider releasing aggregate usage statistics of stablevideo.com on reaching some milestones.', '{"pipeline_tag":"image-to-video","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":32608765959,"files_count":19,"spaces_count":97,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableVideoDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:Stability-AI:generative-models","source_url":"https://github.com/Stability-AI/generative-models"},{"type":"has_code","target_id":"github:Stability-AI:generative-models","source_url":"https://github.com/Stability-AI/generative-models"},{"type":"has_code","target_id":"github:Stability-AI:generative-models","source_url":"https://github.com/Stability-AI/generative-models"}]', NULL, 'Other', 'approved', 65, '4972f0d945118055291a5246d77335aa', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-xai-org-grok-2', 'huggingface--xai-org--grok-2', 'grok-2', 'xai-org', 'This repository contains the weights of Grok 2, a model trained and used at xAI in 2024. - Download the weights. You can replace with any other folder name you prefer. You might encounter some errors during the download. Please retry until the download is successful. If the download succeeds, the folder should contain **42 files** and be approximately 500 GB. - Launch a server. Install the latest SGLang inference engine (>= v0.5.1) from https://github.com/sgl-project/sglang/ Use the command b...', '["git","region:us"]', 'other', 996, 2489, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/xai-org/grok-2","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '# Grok 2\n\nThis repository contains the weights of Grok 2, a model trained and used at xAI in 2024.\n\n## Usage: Serving with SGLang\n\n- Download the weights. You can replace `/local/grok-2` with any other folder name you prefer.\n\n  ```\n  hf download xai-org/grok-2 --local-dir /local/grok-2\n  ```\n\n  You might encounter some errors during the download. Please retry until the download is successful.  \n  If the download succeeds, the folder should contain **42 files** and be approximately 500 GB.\n\n- Launch a server.\n\n  Install the latest SGLang inference engine (>= v0.5.1) from https://github.com/sgl-project/sglang/\n\n  Use the command below to launch an inference server. This checkpoint is TP=8, so you will need 8 GPUs (each with > 40GB of memory).\n  ```\n  python3 -m sglang.launch_server --model /local/grok-2 --tokenizer-path /local/grok-2/tokenizer.tok.json --tp 8 --quantization fp8 --attention-backend triton\n  ```\n\n- Send a request.\n\n  This is a post-trained model, so please use the correct [chat template](https://github.com/sgl-project/sglang/blob/97a38ee85ba62e268bde6388f1bf8edfe2ca9d76/python/sglang/srt/tokenizer/tiktoken_tokenizer.py#L106).\n\n  ```\n  python3 -m sglang.test.send_one --prompt "Human: What is your name?<|separator|>\n\nAssistant:"\n  ```\n\n  You should be able to see the model output its name, Grok.\n\n  Learn more about other ways to send requests [here](https://docs.sglang.ai/basic_usage/send_request.html).\n\n## License\n\nThe weights are licensed under the [Grok 2 Community License Agreement](https://huggingface.co/xai-org/grok-2/blob/main/LICENSE).', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":539032697512,"files_count":44,"spaces_count":5,"gated":false,"private":false,"config":{"architectures":["Grok1ForCausalLM"],"model_type":"git"}}', '[]', '[{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"}]', NULL, NULL, 'pending', 40, 'b7b02a48a0d0ac223856f6f182d3b891', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Lykon-DreamShaper', 'huggingface--lykon--dreamshaper', 'DreamShaper', 'Lykon', '--- language: - en license: other tags: - stable-diffusion - stable-diffusion-diffusers - text-to-image - art - artistic - diffusers - anime inference: false --- Read more about this model here: https://civitai.com/models/4384/dreamshaper Also please support by giving 5 stars and a heart, which will notify new updates. Please consider supporting me on Patreon or buy me a coffee - https://www.patreon.com/Lykon275 - https://snipfeed.co/lykon You can run this model on: - https://huggingface.co/s...', '["diffusers","stable-diffusion","stable-diffusion-diffusers","text-to-image","art","artistic","anime","en","doi:10.57967/hf/0453","license:other","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 994, 92211, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Lykon/DreamShaper","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\nlicense: other\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\n- art\n- artistic\n- diffusers\n- anime\ninference: false\n---\n\n# Dream Shaper\n## Official Repository\n\nRead more about this model here: https://civitai.com/models/4384/dreamshaper\n\nAlso please support by giving 5 stars and a heart, which will notify new updates.\n\nPlease consider supporting me on Patreon or buy me a coffee\n- https://www.patreon.com/Lykon275\n- https://snipfeed.co/lykon\n\nYou can run this model on:\n- https://huggingface.co/spaces/Lykon/DreamShaper-webui\n- Mage.space, sinkin.ai and more', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":359810959620,"files_count":74,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[]', NULL, 'Other', 'approved', 50, '53f2d23141175aada2f5fc7ab393604d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-R1-0528-Qwen3-8B', 'huggingface--deepseek-ai--deepseek-r1-0528-qwen3-8b', 'DeepSeek-R1-0528-Qwen3-8B', 'deepseek-ai', '--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/d...', '["transformers","safetensors","qwen3","text-generation","conversational","arxiv:2501.12948","license:mit","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 993, 400747, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1-0528\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n \n\n<p align="center">\n  <a href="https://arxiv.org/pdf/2501.12948"><b>Paper Link</b></a>\n</p>\n\n\n## 1. Introduction\n\nThe DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro.\n\n<p align="center">\n  <img width="80%" src="figures/benchmark.png">\n</p>\n\nCompared to the previous version, the upgraded model shows significant improvements in handling complex reasoning tasks. For instance, in the AIME 2025 test, the models accuracy has increased from 70% in the previous version to 87.5% in the current version. This advancement stems from enhanced thinking depth during the reasoning process: in the AIME test set, the previous model used an average of 12K tokens per question, whereas the new version averages 23K tokens per question.\n\nBeyond its improved reasoning capabilities, this version also offers a reduced hallucination rate, enhanced support for function calling, and better experience for vibe coding.\n\n## 2. Evaluation Results\n\n### DeepSeek-R1-0528\n For all our models, the maximum generation length is set to 64K tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 16 responses per query to estimate pass@1.\n<div align="center">\n\n| Category | Benchmark (Metric)               | DeepSeek R1     | DeepSeek R1 0528\n|----------|----------------------------------|-----------------|---|\n| General  |\n|          | MMLU-Redux (EM)                   | 92.9            | 93.4\n|          | MMLU-Pro (EM)                     | 84.0            | 85.0\n|          | GPQA-Diamond (Pass@1)             | 71.5            | 81.0\n|          | SimpleQA (Correct)                | 30.1            | 27.8\n|          | FRAMES (Acc.)                     | 82.5            | 83.0\n|          | Humanity''s Last Exam (Pass@1)                     | 8.5            | 17.7\n| Code |\n|          | LiveCodeBench (2408-2505) (Pass@1)        | 63.5          | 73.3\n|          | Codeforces-Div1 (Rating)          | 1530            | 1930\n|          | SWE Verified (Resolved)           | 49.2            | 57.6\n|          | Aider-Polyglot (Acc.)             | 53.3            | 71.6\n| Math |\n|          | AIME 2024 (Pass@1)                | 79.8            | 91.4\n|          | AIME 2025 (Pass@1)                     | 70.0           | 87.5\n|          | HMMT 2025 (Pass@1)            | 41.7 | 79.4 |\n|          | CNMO 2024 (Pass@1)                | 78.8            | 86.9\n| Tools |\n|          | BFCL_v3_MultiTurn (Acc)     | -            | 37.0 |\n|          | Tau-Bench   (Pass@1)       | -            | 53.5(Airline)/63.9(Retail)\n\n</div>\nNote: We use Agentless framework to evaluate model performance on SWE-Verified. We only evaluate text-only prompts in HLE testsets.  GPT-4.1 is employed to act user role in Tau-bench evaluation.\n\n### DeepSeek-R1-0528-Qwen3-8B\nMeanwhile, we distilled the chain-of-thought from DeepSeek-R1-0528 to post-train Qwen3 8B Base, obtaining DeepSeek-R1-0528-Qwen3-8B. This model achieves state-of-the-art (SOTA) performance among open-source models on the AIME 2024, surpassing Qwen3 8B by +10.0% and matching the performance of Qwen3-235B-thinking. We believe that the chain-of-thought from DeepSeek-R1-0528 will hold significant importance for both academic research on reasoning models and industrial development focused on small-scale models.\n\n|                                | AIME 24 | AIME 25 | HMMT Feb 25 | GPQA Diamond | LiveCodeBench (2408-2505) |\n|--------------------------------|---------|---------|-------------|--------------|---------------------------|\n| Qwen3-235B-A22B	                | 85.7    | 81.5    | 62.5        | 71.1         | 66.5                  |\n| Qwen3-32B                      | 81.4    | 72.9    | -           | 68.4         | -                         |\n| Qwen3-8B                      | 76.0   | 67.3    | -           | 62.0       | -                         |\n| Phi-4-Reasoning-Plus-14B       | 81.3    | 78.0    | 53.6        | 69.3         | -          |\n| Gemini-2.5-Flash-Thinking-0520 | 82.3    | 72.0    | 64.2        | 82.8         | 62.3                  |\n| o3-mini (medium)               | 79.6    | 76.7    | 53.3        | 76.8         | 65.9                     |\n| DeepSeek-R1-0528-Qwen3-8B      | 86.0   | 76.3    | 61.5        | 61.1         | 60.5                      |\n\n## 3. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek''s official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in), and switch on the button "DeepThink"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 4. How to Run Locally\n\nPlease visit [DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1) repository for more information about running DeepSeek-R1-0528 locally.\n\nCompared to previous versions of DeepSeek-R1, the usage recommendations for DeepSeek-R1-0528 have the following changes:\n\n1. System prompt is supported now.\n2. It is not required to add "\<think\>\n" at the beginning of the output to force the model into thinking pattern.\n\nThe model architecture of DeepSeek-R1-0528-Qwen3-8B is identical to that of Qwen3-8B, but it shares the same tokenizer configuration as DeepSeek-R1-0528. This model can be run in the same manner as Qwen3-8B, but it is essential to ensure that all configuration files are sourced from our repository rather than the original Qwen3 project.\n\n### System Prompt\nIn the official DeepSeek web/app, we use the same system prompt with a specific date.\n```\nDeepSeek-R1\n{current date}\n```\nFor example,\n```\nDeepSeek-R1\n2025528\n```\n### Temperature\nIn our web and application environments, the temperature parameter $T_{model}$ is set to 0.6. \n### Prompts for File Uploading and Web Search\nFor file uploading, please follow the template to create prompts, where {file_name}, {file_content} and {question} are arguments.\n```\nfile_template = \\n"""[file name]: {file_name}\n[file content begin]\n{file_content}\n[file content end]\n{question}"""\n```\nFor Web Search, {search_results}, {cur_date}, and {question} are arguments.\nFor Chinese query, we use the prompt:\n```\nsearch_answer_zh_template = \\n''''''# :\n{search_results}\n[webpage X begin]...[webpage X end]X[citation:X][citation:3][citation:5]\n\n- {cur_date}\n- \n- 10\n- [citation:3][citation:5]\n- 5\n- \n- \n- \n- \n# \n{question}''''''\n```\nFor English query, we use the prompt:\n```\nsearch_answer_en_template = \\n''''''# The following contents are the search results related to the user''s message:\n{search_results}\nIn the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.\nWhen responding, please keep the following points in mind:\n- Today is {cur_date}.\n- Not all content in the search results is closely related to the user''s question. You need to evaluate and filter the search results based on the question.\n- For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.\n- For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user''s requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.\n- If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.\n- For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.\n- Choose an appropriate and visually appealing format for your response based on the user''s requirements and the content of the answer, ensuring strong readability.\n- Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.\n- Unless the user requests otherwise, your response should be in the same language as the user''s question.\n# The user''s message is:\n{question}''''''\n```\n\n## 5. License\nThis code repository is licensed under [MIT License](LICENSE). The use of DeepSeek-R1 models is also subject to [MIT License](LICENSE). DeepSeek-R1 series (including Base and Chat) supports commercial use and distillation.\n\n## 6. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n```\n\n## 7. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":8190735360,"storage_bytes":16381839296,"files_count":10,"spaces_count":53,"gated":false,"private":false,"config":{"architectures":["Qwen3ForCausalLM"],"model_type":"qwen3","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<beginofsentence>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<endofsentence>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<endofsentence>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message[''content''] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + ''\n\n'' + message[''content''] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{% set content = message[''content''] %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{''<User>'' + content + ''<Assistant>''}}{%- endif %}{%- if message[''role''] == ''assistant'' %}{% if ''</think>'' in content %}{% set content = content.split(''</think>'')[-1] %}{% endif %}{% endif %}{%- if message[''role''] == ''assistant'' and message[''tool_calls''] is defined and message[''tool_calls''] is not none %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{''<tooloutputsend>''}}{%- endif %}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- set ns.is_output_first = true %}{%- for tool in message[''tool_calls''] %}{%- if not ns.is_first %}{%- if content is none %}{{''<toolcallsbegin><toolcallbegin>'' + tool[''type''] + ''<toolsep>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<toolcallend>''}}{%- else %}{{content + ''<toolcallsbegin><toolcallbegin>'' + tool[''type''] + ''<toolsep>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<toolcallend>''}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{''\n'' + ''<toolcallbegin>'' + tool[''type''] + ''<toolsep>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<toolcallend>''}}{%- endif %}{%- endfor %}{{''<toolcallsend><endofsentence>''}}{%- endif %}{%- if message[''role''] == ''assistant'' and (message[''tool_calls''] is not defined or message[''tool_calls''] is none)%}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{''<tooloutputsend>'' + content + ''<endofsentence>''}}{%- set ns.is_tool = false -%}{%- else %}{{content + ''<endofsentence>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<tooloutputsbegin><tooloutputbegin>'' + content + ''<tooloutputend>''}}{%- set ns.is_output_first = false %}{%- else %}{{''\n<tooloutputbegin>'' + content + ''<tooloutputend>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<tooloutputsend>''}}{% endif %}{% if add_generation_prompt and not ns.is_last_user and not ns.is_tool %}{{''<Assistant>''}}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"based_on_paper","target_id":"arxiv:2501.12948","source_url":"https://arxiv.org/abs/2501.12948"}]', NULL, 'MIT', 'approved', 100, '62a460061dad9fa94147871981de7400', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B/resolve/main/figures/benchmark.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-R1-0528-Qwen3-8B from https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B/resolve/main/figures/benchmark.png
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-R1-0528-Qwen3-8B.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tencent-HunyuanImage-3.0', 'huggingface--tencent--hunyuanimage-3.0', 'HunyuanImage-3.0', 'tencent', '--- license: other license_name: tencent-hunyuan-community license_link: LICENSE pipeline_tag: text-to-image library_name: transformers --- <div align="center"> <img src="./assets/logo.png" alt="HunyuanImage-3.0 Logo" width="600"> </div> <div align="center"> <img src="./assets/banner.png" alt="HunyuanImage-3.0 Banner" width="800"> </div> <div align="center"> <a href=https://hunyuan.tencent.com/image target="_blank"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage...', '["transformers","safetensors","hunyuan_image_3_moe","text-generation","text-to-image","custom_code","arxiv:2509.23951","license:other","region:us"]', 'text-to-image', 992, 56056, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tencent/HunyuanImage-3.0","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: tencent-hunyuan-community\nlicense_link: LICENSE\npipeline_tag: text-to-image\nlibrary_name: transformers\n---\n\n<div align="center">\n\n<img src="./assets/logo.png" alt="HunyuanImage-3.0 Logo" width="600">\n\n#  HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation\n\n</div>\n\n\n<div align="center">\n<img src="./assets/banner.png" alt="HunyuanImage-3.0 Banner" width="800">\n\n</div>\n\n<div align="center">\n  <a href=https://hunyuan.tencent.com/image target="_blank"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanImage-3.0 target="_blank"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanImage-3.0 target="_blank"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href=https://arxiv.org/pdf/2509.23951 target="_blank"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://x.com/TencentHunyuan target="_blank"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n  <a href=https://docs.qq.com/doc/DUVVadmhCdG9qRXBU target="_blank"><img src=https://img.shields.io/badge/-PromptHandBook-blue.svg?logo=book height=22px></a>\n</div>\n\n\n<p align="center">\n     Join our <a href="./assets/WECHAT.md" target="_blank">WeChat</a> and <a href="https://discord.gg/ehjWMqF5wY">Discord</a> | \n <a href="https://hunyuan.tencent.com/modelSquare/home/play?modelId=289&from=/visual">Official website() Try our model!</a>&nbsp&nbsp\n</p>\n\n##  News\n- **September 28, 2025**:  **HunyuanImage-3.0 Technical Report Released** - Comprehensive technical documentation now available\n- **September 28, 2025**:  **HunyuanImage-3.0 Open Source Release** - Inference code and model weights publicly available\n\n\n##  Community Contributions\n\nIf you develop/use HunyuanImage-3.0 in your projects, welcome to let us know.\n\n##  Open-source Plan\n\n- HunyuanImage-3.0 (Image Generation Model)\n  - [x] Inference \n  - [x] HunyuanImage-3.0 Checkpoints\n  - [ ] HunyuanImage-3.0-Instruct Checkpoints (with reasoning)\n  - [ ] VLLM Support\n  - [ ] Distilled Checkpoints\n  - [ ] Image-to-Image Generation\n  - [ ] Multi-turn Interaction\n\n\n##  Contents\n- [ News](#-news)\n- [ Community Contributions](#-community-contributions)\n- [ Open-source Plan](#-open-source-plan)\n- [ Introduction](#-introduction)\n- [ Key Features](#-key-features)\n- [ Dependencies and Installation](#-dependencies-and-installation)\n  - [ System Requirements](#-system-requirements)\n  - [ Environment Setup](#-environment-setup)\n  - [ Install Dependencies](#-install-dependencies)\n  - [Performance Optimizations](#performance-optimizations)\n- [ Usage](#-usage)\n  - [ Quick Start with Transformers](#-quick-start-with-transformers)\n  - [ Local Installation & Usage](#-local-installation--usage)\n  - [ Interactive Gradio Demo](#-interactive-gradio-demo)\n- [ Models Cards](#-models-cards)\n- [ Prompt Guide](#-prompt-guide)\n  - [Manually Writing Prompts](#manually-writing-prompts)\n  - [System Prompt For Automatic Rewriting the Prompt](#system-prompt-for-automatic-rewriting-the-prompt)\n  - [Advanced Tips](#advanced-tips)\n  - [More Cases](#more-cases)\n- [ Evaluation](#-evaluation)\n- [ Citation](#-citation)\n- [ Acknowledgements](#-acknowledgements)\n- [  Github Star History](#-github-star-history)\n\n---\n\n##  Introduction\n\n**HunyuanImage-3.0** is a groundbreaking native multimodal model that unifies multimodal understanding and generation within an autoregressive framework. Our text-to-image module achieves performance **comparable to or surpassing** leading closed-source models.\n\n\n<div align="center">\n  <img src="./assets/framework.png" alt="HunyuanImage-3.0 Framework" width="90%">\n</div>\n\n##  Key Features\n\n*  **Unified Multimodal Architecture:** Moving beyond the prevalent DiT-based architectures, HunyuanImage-3.0 employs a unified autoregressive framework. This design enables a more direct and integrated modeling of text and image modalities, leading to surprisingly effective and contextually rich image generation.\n\n*  **The Largest Image Generation MoE Model:** This is the largest open-source image generation Mixture of Experts (MoE) model to date. It features 64 experts and a total of 80 billion parameters, with 13 billion activated per token, significantly enhancing its capacity and performance.\n\n*  **Superior Image Generation Performance:** Through rigorous dataset curation and advanced reinforcement learning post-training, we''ve achieved an optimal balance between semantic accuracy and visual excellence. The model demonstrates exceptional prompt adherence while delivering photorealistic imagery with stunning aesthetic quality and fine-grained details.\n\n*  **Intelligent World-Knowledge Reasoning:** The unified multimodal architecture endows HunyuanImage-3.0 with powerful reasoning capabilities. It leverages its extensive world knowledge to intelligently interpret user intent, automatically elaborating on sparse prompts with contextually appropriate details to produce superior, more complete visual outputs.\n\n\n##  Dependencies and Installation\n\n###  System Requirements\n\n*  **Operating System:** Linux\n*  **GPU:** NVIDIA GPU with CUDA support\n*  **Disk Space:** 170GB for model weights\n*  **GPU Memory:** 380GB (480GB recommended for better performance)\n\n###  Environment Setup\n\n*  **Python:** 3.12+ (recommended and tested)\n*  **PyTorch:** 2.7.1\n*  **CUDA:** 12.8\n\n###  Install Dependencies\n\n```bash\n# 1. First install PyTorch (CUDA 12.8 Version)\npip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --index-url https://download.pytorch.org/whl/cu128\n\n# 2. Then install tencentcloud-sdk\npip install -i https://mirrors.tencent.com/pypi/simple/ --upgrade tencentcloud-sdk-python\n\n# 3. Then install other dependencies\npip install -r requirements.txt\n```\n\n#### Performance Optimizations\n\nFor **up to 3x faster inference**, install these optimizations:\n\n```bash\n# FlashAttention for faster attention computation\npip install flash-attn==2.8.3 --no-build-isolation\n\n# FlashInfer for optimized moe inference. v0.3.1 is tested.\npip install flashinfer-python\n```\n> **Installation Tips:** It is critical that the CUDA version used by PyTorch matches the system''s CUDA version. \n> FlashInfer relies on this compatibility when compiling kernels at runtime. Pytorch 2.7.1+cu128 is tested.\n> GCC version >=9 is recommended for compiling FlashAttention and FlashInfer.\n\n>  **Performance Tips:** These optimizations can significantly speed up your inference!\n\n> **Notation:** When FlashInfer is enabled, the first inference may be slower (about 10 minutes) due to kernel compilation. Subsequent inferences on the same machine will be much faster.\n\n##  Usage\n\n###  Quick Start with Transformers\n\n#### 1 Download model weights\n\n```bash\n# Download from HuggingFace and rename the directory.\n# Notice that the directory name should not contain dots, which may cause issues when loading using Transformers.\nhf download tencent/HunyuanImage-3.0 --local-dir ./HunyuanImage-3\n```\n\n#### 2 Run with Transformers\n\n```python\nfrom transformers import AutoModelForCausalLM\n\n# Load the model\nmodel_id = "./HunyuanImage-3"\n# Currently we can not load the model using HF model_id `tencent/HunyuanImage-3.0` directly \n# due to the dot in the name.\n\nkwargs = dict(\n    attn_implementation="sdpa",     # Use "flash_attention_2" if FlashAttention is installed\n    trust_remote_code=True,\n    torch_dtype="auto",\n    device_map="auto",\n    moe_impl="eager",   # Use "flashinfer" if FlashInfer is installed\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)\nmodel.load_tokenizer(model_id)\n\n# generate the image\nprompt = "A brown and white dog is running on the grass"\nimage = model.generate_image(prompt=prompt, stream=True)\nimage.save("image.png")\n```\n\n###  Local Installation & Usage\n\n#### 1 Clone the Repository\n\n```bash\ngit clone https://github.com/Tencent-Hunyuan/HunyuanImage-3.0.git\ncd HunyuanImage-3.0/\n```\n\n#### 2 Download Model Weights\n\n```bash\n# Download from HuggingFace\nhf download tencent/HunyuanImage-3.0 --local-dir ./HunyuanImage-3\n```\n\n#### 3 Run the Demo\nThe Pretrain Checkpoint does not automatically rewrite or enhance input prompts, for optimal results currently, we recommend community partners to use deepseek to rewrite the prompts. You can go to [Tencent Cloud](https://cloud.tencent.com/document/product/1772/115963#.E5.BF.AB.E9.80.9F.E6.8E.A5.E5.85.A5) to apply for an API Key.\n\n```bash\n# set env\nexport DEEPSEEK_KEY_ID="your_deepseek_key_id"\nexport DEEPSEEK_KEY_SECRET="your_deepseek_key_secret"\n\npython3 run_image_gen.py --model-id ./HunyuanImage-3 --verbose 1 --sys-deepseek-prompt "universal" --prompt "A brown and white dog is running on the grass"\n```\n\n#### 4 Command Line Arguments\n\n| Arguments               | Description                                                  | Default     |\n| ----------------------- | ------------------------------------------------------------ | ----------- |\n| `--prompt`              | Input prompt                                                 | (Required)  |\n| `--model-id`            | Model path                                                   | (Required)  |\n| `--attn-impl`           | Attention implementation. Either `sdpa` or `flash_attention_2`. | `sdpa`      |\n| `--moe-impl`            | MoE implementation. Either `eager` or `flashinfer`           | `eager`     |\n| `--seed`                | Random seed for image generation                             | `None`      |\n| `--diff-infer-steps`    | Diffusion infer steps                                        | `50`        |\n| `--image-size`          | Image resolution. Can be `auto`, like `1280x768` or `16:9`   | `auto`      |\n| `--save`                | Image save path.                                             | `image.png` |\n| `--verbose`             | Verbose level. 0: No log; 1: log inference information.      | `0`         |\n| `--rewrite`             | Whether to enable rewriting                                  | `1`         |\n| `--sys-deepseek-prompt` | Select sys-prompt from `universal` or `text_rendering`       | `universal` |\n\n###  Interactive Gradio Demo\n\nLaunch an interactive web interface for easy text-to-image generation.\n\n#### 1 Install Gradio\n\n```bash\npip install gradio>=4.21.0\n```\n\n#### 2 Configure Environment\n\n```bash\n# Set your model path\nexport MODEL_ID="path/to/your/model"\n\n# Optional: Configure GPU usage (default: 0,1,2,3)\nexport GPUS="0,1,2,3"\n\n# Optional: Configure host and port (default: 0.0.0.0:443)\nexport HOST="0.0.0.0"\nexport PORT="443"\n```\n\n#### 3 Launch the Web Interface\n\n**Basic Launch:**\n```bash\nsh run_app.sh\n```\n\n**With Performance Optimizations:**\n```bash\n# Use both optimizations for maximum performance\nsh run_app.sh --moe-impl flashinfer --attn-impl flash_attention_2\n```\n\n#### 4 Access the Interface\n\n>  **Web Interface:** Open your browser and navigate to `http://localhost:443` (or your configured port)\n\n\n##  Models Cards\n\n| Model                     | Params | Download | Recommended VRAM | Supported |\n|---------------------------| --- | --- | --- | --- |\n| HunyuanImage-3.0          | 80B total (13B active) | [HuggingFace](https://huggingface.co/tencent/HunyuanImage-3.0) |  3  80 GB |  Text-to-Image\n| HunyuanImage-3.0-Instruct | 80B total (13B active) | [HuggingFace](https://huggingface.co/tencent/HunyuanImage-3.0-Instruct) |  3  80 GB |  Text-to-Image<br> Prompt Self-Rewrite <br> CoT Think\n\n\n\nNotes:\n- Install performance extras (FlashAttention, FlashInfer) for faster inference.\n- MultiGPU inference is recommended for the Base model.\n\n\n##  Prompt Guide\n\n### Manually Writing Prompts.\nThe Pretrain Checkpoint does not automatically rewrite or enhance input prompts, Instruct Checkpoint can rewrite or enhance input prompts with thinking . For optimal results currently, we recommend community partners consulting our official guide on how to write effective prompts.\n\nReference: [HunyuanImage 3.0 Prompt Handbook](\nhttps://docs.qq.com/doc/DUVVadmhCdG9qRXBU)\n\n\n### System Prompt For Automatic Rewriting the Prompt.\n\nWe''ve included two system prompts in the PE folder of this repository that leverage DeepSeek to automatically enhance user inputs:\n\n* **system_prompt_universal**: This system prompt converts photographic style, artistic prompts into a detailed one.\n* **system_prompt_text_rendering**: This system prompt converts UI/Poster/Text Rending prompts to a deailed on that suits the model.\n\nNote that these system prompts are in Chinese because Deepseek works better with Chinese system prompts. If you want to use it for English oriented model, you may translate it into English or refer to the comments in the PE file as a guide.\n\nWe also create a [Yuanqi workflow](https://yuanqi.tencent.com/agent/H69VgtJdj3Dz) to implement the universal one, you can directly try it.\n\n### Advanced Tips\n- **Content Priority**: Focus on describing the main subject and action first, followed by details about the environment and style. A more general description framework is: **Main subject and scene + Image quality and style + Composition and perspective + Lighting and atmosphere + Technical parameters**. Keywords can be added both before and after this structure.\n\n- **Image resolution**: Our model not only supports multiple resolutions but also offers both **automatic and specified resolution** options. In auto mode, the model automatically predicts the image resolution based on the input prompt. In specified mode (like traditional DiT), the model outputs an image resolution that strictly aligns with the user''s chosen resolution.\n\n### More Cases\nOur model can follow complex instructions to generate highquality, creative images.\n\n<div align="center">\n  <img src="./assets/banner_all.jpg" width=100% alt="HunyuanImage 3.0 Demo">\n</div>\n\nOur model can effectively process very long text inputs, enabling users to precisely control the finer details of generated images. Extended prompts allow for intricate elements to be accurately captured, making it ideal for complex projects requiring precision and creativity.\n\n<p align="center">\n<table>\n<thead>\n</thead>\n<tbody>\n<tr>\n<td>\n<img src="./assets/pg_imgs/image1.png" width=100%><details>\n<summary>Show prompt</summary>\nA cinematic medium shot captures a single Asian woman seated on a chair within a dimly lit room, creating an intimate and theatrical atmosphere. The composition is focused on the subject, rendered with rich colors and intricate textures that evoke a nostalgic and moody feeling.\n\nThe primary subject is a young Asian woman with a thoughtful and expressive countenance, her gaze directed slightly away from the camera. She is seated in a relaxed yet elegant posture on an ornate, vintage armchair. The chair is upholstered in a deep red velvet, its fabric showing detailed, intricate textures and slight signs of wear. She wears a simple, elegant dress in a dark teal hue, the material catching the light in a way that reveals its fine-woven texture. Her skin has a soft, matte quality, and the light delicately models the contours of her face and arms.\n\nThe surrounding room is characterized by its vintage decor, which contributes to the historic and evocative mood. In the immediate background, partially blurred due to a shallow depth of field consistent with a f/2.8 aperture, the wall is covered with wallpaper featuring a subtle, damask pattern. The overall color palette is a carefully balanced interplay of deep teal and rich red hues, creating a visually compelling and cohesive environment. The entire scene is detailed, from the fibers of the upholstery to the subtle patterns on the wall.\n\nThe lighting is highly dramatic and artistic, defined by high contrast and pronounced shadow play. A single key light source, positioned off-camera, projects gobo lighting patterns onto the scene, casting intricate shapes of light and shadow across the woman and the back wall. These dramatic shadows create a strong sense of depth and a theatrical quality. While some shadows are deep and defined, others remain soft, gently wrapping around the subject and preventing the loss of detail in darker areas. The soft focus on the background enhances the intimate feeling, drawing all attention to the expressive subject. The overall image presents a cinematic, photorealistic photography style.\n</details>\n</td>\n<td><img src="./assets/pg_imgs/image2.png" width=100%><details>\n<summary>Show prompt</summary>\nA cinematic, photorealistic medium shot captures a high-contrast urban street corner, defined by the sharp intersection of light and shadow. The primary subject is the exterior corner of a building, rendered in a low-saturation, realistic style.\n\nThe building wall, which occupies the majority of the frame, is painted a warm orange with a finely detailed, rough stucco texture. Horizontal white stripes run across its surface. The base of the building is constructed from large, rough-hewn stone blocks, showing visible particles and texture. On the left, illuminated side of the building, there is a single window with closed, dark-colored shutters. Adjacent to the window, a simple black pendant lamp hangs from a thin, taut rope, casting a distinct, sharp-edged shadow onto the sunlit orange wall. The composition is split diagonally, with the right side of the building enveloped in a deep brown shadow. At the bottom of the frame, a smooth concrete sidewalk is visible, upon which the dynamic silhouette of a person is captured mid-stride, walking from right to left.\n\nIn the shallow background, the faint, out-of-focus outlines of another building and the bare, skeletal branches of trees are softly visible, contributing to the quiet urban atmosphere and adding a sense of depth to the scene. These elements are rendered with minimal detail to keep the focus on the foreground architecture.\n\nThe scene is illuminated by strong, natural sunlight originating from the upper left, creating a dramatic chiaroscuro effect. This hard light source casts deep, well-defined shadows, producing a sharp contrast between the brightly lit warm orange surfaces and the deep brown shadow areas. The lighting highlights the fine details in the wall texture and stone particles, emphasizing the photorealistic quality. The overall presentation reflects a high-quality photorealistic photography style, infused with a cinematic film noir aesthetic.\n</details>\n</td>\n</tr>\n<tr>\n<td>\n<img src="./assets/pg_imgs/image3.png" width=100%><details>\n<summary>Show prompt</summary>\n\n\n\n\n\n\n\n</details>\n</td>\n<td>\n<img src="./assets/pg_imgs/image4.png" width=100%><details>\n<summary>Show prompt</summary>\n\n\n\n\n\n\n\n\n\n</details>\n</td>\n</tr>\n<tr>\n<td>\n<img src="./assets/pg_imgs/image5.png" width=100%><details>\n<summary>Show prompt</summary>\n\n\n\n\n\n\n\n\n\n\n3D\n</details>\n</td>\n<td>\n<img src="./assets/pg_imgs/image6.png" width=100%><details>\n<summary>Show prompt</summary>\n\n\n8\n\nT\n\n\n\nImpasto\n</details>\n</td>\n</tr>\n<tr>\n<td>\n<img src="./assets/pg_imgs/image7.png" width=100%><details>\n<summary>Show prompt</summary>\n\n\n123\n\n456\n\n789\n\n\n</details>\n</td>\n<td>\n<img src="./assets/pg_imgs/image8.png" width=100%><details>\n<summary>Show prompt</summary>\n\n\nQQ3D\n\n\n\nHunyuan Image 3.0\n\n\n</details>\n</td>\n</tr>\n</tbody>\n</table>\n</p>\n\n##  Evaluation\n\n*  **SSAE (Machine Evaluation)**   \nSSAE (Structured Semantic Alignment Evaluation) is an intelligent evaluation metric for image-text alignment based on advanced multimodal large language models (MLLMs). We extracted 3500 key points across 12 categories, then used multimodal large language models to automatically evaluate and score by comparing the generated images with these key points based on the visual content of the images. Mean Image Accuracy represents the image-wise average score across all key points, while Global Accuracy directly calculates the average score across all key points.\n\n<p align="center">\n  <img src="./assets/ssae_side_by_side_comparison.png" width=98% alt="Human Evaluation with Other Models">\n</p>\n\n<p align="center">\n  <img src="./assets/ssae_side_by_side_heatmap.png" width=98% alt="Human Evaluation with Other Models">\n</p>\n\n\n*  **GSB (Human Evaluation)** \n\nWe adopted the GSB (Good/Same/Bad) evaluation method commonly used to assess the relative performance between two models from an overall image perception perspective. In total, we utilized 1,000 text prompts, generating an equal number of image samples for all compared models in a single run. For a fair comparison, we conducted inference only once for each prompt, avoiding any cherry-picking of results. When comparing with the baseline methods, we maintained the default settings for all selected models. The evaluation was performed by more than 100 professional evaluators. \n\n<p align="center">\n  <img src="./assets/gsb.png" width=98% alt="Human Evaluation with Other Models">\n</p>\n\n\n##  Citation\n\nIf you find HunyuanImage-3.0 useful in your research, please cite our work:\n\n```bibtex\n@article{cao2025hunyuanimage,\n  title={HunyuanImage 3.0 Technical Report},\n  author={Cao, Siyu and Chen, Hangting and Chen, Peng and Cheng, Yiji and Cui, Yutao and Deng, Xinchi and Dong, Ying and Gong, Kipper and Gu, Tianpeng and Gu, Xiusen and others},\n  journal={arXiv preprint arXiv:2509.23951},\n  year={2025}\n}\n```\n\n##  Acknowledgements\n\nWe extend our heartfelt gratitude to the following open-source projects and communities for their invaluable contributions:\n\n*  [Transformers](https://github.com/huggingface/transformers) - State-of-the-art NLP library\n*  [Diffusers](https://github.com/huggingface/diffusers) - Diffusion models library  \n*  [HuggingFace](https://huggingface.co/) - AI model hub and community\n*  [FlashAttention](https://github.com/Dao-AILab/flash-attention) - Memory-efficient attention\n*  [FlashInfer](https://github.com/flashinfer-ai/flashinfer) - Optimized inference engine\n\n##  Github Star History\n\n[![GitHub stars](https://img.shields.io/github/stars/Tencent-Hunyuan/HunyuanImage-3.0?style=social)](https://github.com/Tencent-Hunyuan/HunyuanImage-3.0)\n[![GitHub forks](https://img.shields.io/github/forks/Tencent-Hunyuan/HunyuanImage-3.0?style=social)](https://github.com/Tencent-Hunyuan/HunyuanImage-3.0)\n\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Tencent-Hunyuan/HunyuanImage-3.0&type=Date)](https://www.star-history.com/#Tencent-Hunyuan/HunyuanImage-3.0&Date)', '{"pipeline_tag":"text-to-image","library_name":"transformers","framework":"transformers","params":83009199459,"storage_bytes":168599289097,"files_count":68,"spaces_count":53,"gated":false,"private":false,"config":{"architectures":["HunyuanImage3ForCausalMM"],"auto_map":{"AutoConfig":"configuration_hunyuan.HunyuanImage3Config","AutoModel":"hunyuan.HunyuanImage3Model","AutoModelForCausalLM":"hunyuan.HunyuanImage3ForCausalMM"},"model_type":"hunyuan_image_3_moe","tokenizer_config":{"bos_token":"<|startoftext|>","eos_token":"<|endoftext|>","pad_token":"<pad>"}}}', '[]', '[{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanImage-3.0","source_url":"https://github.com/Tencent-Hunyuan/HunyuanImage-3.0"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanImage-3.0.git","source_url":"https://github.com/Tencent-Hunyuan/HunyuanImage-3.0.git"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:Dao-AILab:flash-attention","source_url":"https://github.com/Dao-AILab/flash-attention"},{"type":"has_code","target_id":"github:flashinfer-ai:flashinfer","source_url":"https://github.com/flashinfer-ai/flashinfer"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanImage-3.0","source_url":"https://github.com/Tencent-Hunyuan/HunyuanImage-3.0"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanImage-3.0","source_url":"https://github.com/Tencent-Hunyuan/HunyuanImage-3.0"},{"type":"based_on_paper","target_id":"arxiv:2509.23951","source_url":"https://arxiv.org/abs/2509.23951"}]', NULL, 'Other', 'approved', 100, '3b5f845b8cf66b8d47c3c990c28d1c27', NULL, 'https://huggingface.co/tencent/HunyuanImage-3.0/resolve/main/assets/banner.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-tencent-HunyuanImage-3.0 from https://huggingface.co/tencent/HunyuanImage-3.0/resolve/main/assets/banner.png
Image converted to WebP: data/images/huggingface-tencent-HunyuanImage-3.0.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Envvi-Inkpunk-Diffusion', 'huggingface--envvi--inkpunk-diffusion', 'Inkpunk-Diffusion', 'Envvi', '--- license: creativeml-openrail-m language: - en tags: - stable-diffusion - text-to-image - diffusers --- Finetuned Stable Diffusion model trained on dreambooth. Vaguely inspired by Gorillaz, FLCL, and Yoji Shinkawa. Use **_nvinkpunk_** in your prompts. We support a Gradio Web UI to run Inkpunk-Diffusion: !output Samples v2 !output Samples v2', '["diffusers","stable-diffusion","text-to-image","en","license:creativeml-openrail-m","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 990, 927, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Envvi/Inkpunk-Diffusion","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\nlanguage:\n- en\ntags:\n- stable-diffusion\n- text-to-image\n- diffusers\n---\n\n# Inkpunk Diffusion\n\nFinetuned Stable Diffusion model trained on dreambooth. Vaguely inspired by Gorillaz, FLCL, and Yoji Shinkawa. Use **_nvinkpunk_** in your prompts.\n\n# Gradio\n\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI to run Inkpunk-Diffusion:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/Inkpunk-Diffusion)\n\n# Sample images\n![output Samples v2](https://huggingface.co/Envvi/Inkpunk-Diffusion/resolve/main/inkpunk-v2-samples-1.png)\n![output Samples v2](https://huggingface.co/Envvi/Inkpunk-Diffusion/resolve/main/inkpunk-v2-samples-2.png)', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":28191407827,"files_count":23,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"}]', NULL, 'creativeml-openrail-m', 'approved', 50, '5c4dc42d4b4a12415442356b880ef693', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-manycore-research-SpatialLM-Llama-1B', 'huggingface--manycore-research--spatiallm-llama-1b', 'SpatialLM-Llama-1B', 'manycore-research', '--- license: llama3.2 library_name: transformers base_model: - meta-llama/Llama-3.2-1B-Instruct --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <picture> <source srcset="https://cdn-uploads.huggingface.co/production/uploads/63efbb1efc92a63ac81126d0/_dK14CT3do8rBG3QrHUjN.png" media="(prefers-color-scheme: dark)"> <img src="https://cdn-uploads.huggingface.co/production/uploads/63efbb1efc92a...', '["transformers","safetensors","spatiallm_llama","text-generation","conversational","base_model:meta-llama/llama-3.2-1b-instruct","license:llama3.2","endpoints_compatible","region:us"]', 'text-generation', 990, 172, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/manycore-research/SpatialLM-Llama-1B","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: llama3.2\nlibrary_name: transformers\nbase_model:\n  - meta-llama/Llama-3.2-1B-Instruct\n---\n\n# SpatialLM-Llama-1B\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <picture>\n    <source srcset="https://cdn-uploads.huggingface.co/production/uploads/63efbb1efc92a63ac81126d0/_dK14CT3do8rBG3QrHUjN.png" media="(prefers-color-scheme: dark)">\n    <img src="https://cdn-uploads.huggingface.co/production/uploads/63efbb1efc92a63ac81126d0/bAZyeIXOMVASHR6-xVlQU.png" width="60%" alt="SpatialLM""/>\n  </picture>\n</div>\n<hr style="margin-top: 0; margin-bottom: 8px;">\n<div align="center" style="margin-top: 0; padding-top: 0; line-height: 1;">\n    <a href="https://manycore-research.github.io/SpatialLM" target="_blank" style="margin: 2px;"><img alt="Project"\n    src="https://img.shields.io/badge/%20Website-SpatialLM-ffc107?color=42a5f5&logoColor=white" style="display: inline-block; vertical-align: middle;"/></a>\n    <a href="https://github.com/manycore-research/SpatialLM" target="_blank" style="margin: 2px;"><img alt="GitHub"\n    src="https://img.shields.io/badge/GitHub-SpatialLM-24292e?logo=github&logoColor=white" style="display: inline-block; vertical-align: middle;"/></a>\n</div>\n<div align="center" style="line-height: 1;">\n    <a href="https://huggingface.co/manycore-research/SpatialLM-Llama-1B" target="_blank" style="margin: 2px;"><img alt="Hugging Face"\n    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-SpatialLM%201B-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/></a>\n    <a href="https://huggingface.co/datasets/manycore-research/SpatialLM-Testset" target="_blank" style="margin: 2px;"><img alt="Dataset"\n    src="https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-SpatialLM-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/></a>\n</div>\n\n## Introduction\n\nSpatialLM is a 3D large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object bounding boxes with their semantic categories. Unlike previous methods that require specialized equipment for data collection, SpatialLM can handle point clouds from diverse sources such as monocular video sequences, RGBD images, and LiDAR sensors. This multimodal architecture effectively bridges the gap between unstructured 3D geometric data and structured 3D representations, offering high-level semantic understanding. It enhances spatial reasoning capabilities for applications in embodied robotics, autonomous navigation, and other complex 3D scene analysis tasks.\n\n<div align="center">\n  <video controls autoplay src="https://cdn-uploads.huggingface.co/production/uploads/63efbb1efc92a63ac81126d0/3bz_jNRCLD2L9uj11HPnP.mp4" poster="https://cdn-uploads.huggingface.co/production/uploads/63efbb1efc92a63ac81126d0/euo94dNx28qBNe51_oiB1.png"></video>\n  <p><i>SpatialLM reconstructs 3D layout from a monocular RGB video with MASt3R-SLAM. Results aligned to video with GT cameras for visualization.</i></p>\n</div>\n\n## SpatialLM Models\n\n<div align="center">\n\n|      **Model**      | **Download**                                                                   |\n| :-----------------: | ------------------------------------------------------------------------------ |\n| SpatialLM-Llama-1B  | [ HuggingFace](https://huggingface.co/manycore-research/SpatialLM-Llama-1B)  |\n| SpatialLM-Qwen-0.5B | [ HuggingFace](https://huggingface.co/manycore-research/SpatialLM-Qwen-0.5B) |\n\n</div>\n\n## Usage\n\n### Installation\n\nTested with the following environment:\n\n- Python 3.11\n- Pytorch 2.4.1\n- CUDA Version 12.4\n\n```bash\n# clone the repository\ngit clone https://github.com/manycore-research/SpatialLM.git\ncd SpatialLM\n\n# create a conda environment with cuda 12.4\nconda create -n spatiallm python=3.11\nconda activate spatiallm\nconda install -y nvidia/label/cuda-12.4.0::cuda-toolkit conda-forge::sparsehash\n\n# Install dependencies with poetry\npip install poetry && poetry config virtualenvs.create false --local\npoetry install\npoe install-torchsparse # Building wheel for torchsparse will take a while\n```\n\n### Inference\n\nIn the current version of SpatialLM, input point clouds are considered axis-aligned where the z-axis is the up axis. This orientation is crucial for maintaining consistency in spatial understanding and scene interpretation across different datasets and applications.\nExample preprocessed point clouds, reconstructed from RGB videos using [MASt3R-SLAM](https://github.com/rmurai0610/MASt3R-SLAM), are available in [SpatialLM-Testset](#spatiallm-testset).\n\nDownload an example point cloud:\n\n```bash\nhuggingface-cli download manycore-research/SpatialLM-Testset pcd/scene0000_00.ply --repo-type dataset --local-dir .\n```\n\nRun inference:\n\n```bash\npython inference.py --point_cloud pcd/scene0000_00.ply --output scene0000_00.txt --model_path manycore-research/SpatialLM-Llama-1B\n```\n\n### Visualization\n\nUse `rerun` to visualize the point cloud and the predicted structured 3D layout output:\n\n```bash\n# Convert the predicted layout to Rerun format\npython visualize.py --point_cloud pcd/scene0000_00.ply --layout scene0000_00.txt --save scene0000_00.rrd\n\n# Visualize the point cloud and the predicted layout\nrerun scene0000_00.rrd\n```\n\n### Evaluation\n\nTo evaluate the performance of SpatialLM, we provide `eval.py` script that reports the benchmark results on the SpatialLM-Testset in the table below in section [Benchmark Results](#benchmark-results).\n\nDownload the testset:\n\n```bash\nhuggingface-cli download manycore-research/SpatialLM-Testset --repo-type dataset --local-dir SpatialLM-Testset\n```\n\nRun evaluation:\n\n```bash\n# Run inference on the PLY point clouds in folder SpatialLM-Testset/pcd with SpatialLM-Llama-1B model\npython inference.py --point_cloud SpatialLM-Testset/pcd --output SpatialLM-Testset/pred --model_path manycore-research/SpatialLM-Llama-1B\n\n# Evaluate the predicted layouts\npython eval.py --metadata SpatialLM-Testset/test.csv --gt_dir SpatialLM-Testset/layout --pred_dir SpatialLM-Testset/pred --label_mapping SpatialLM-Testset/benchmark_categories.tsv\n```\n\n## SpatialLM Testset\n\nWe provide a test set of 107 preprocessed point clouds, reconstructed from RGB videos using [MASt3R-SLAM](https://github.com/rmurai0610/MASt3R-SLAM). SpatialLM-Testset is quite challenging compared to prior clean RGBD scans datasets due to the noises and occlusions in the point clouds reconstructed from monocular RGB videos.\n\n<div align="center">\n\n|    **Dataset**    | **Download**                                                                       |\n| :---------------: | ---------------------------------------------------------------------------------- |\n| SpatialLM-Testset | [ Datasets](https://huggingface.co/datasets/manycore-research/SpatialLM-TestSet) |\n\n</div>\n\n## Benchmark Results\n\nBenchmark results on the challenging SpatialLM-Testset are reported in the following table:\n\n<div align="center">\n\n| **Method**       | **SpatialLM-Llama-1B** | **SpatialLM-Qwen-0.5B** |\n| ---------------- | ---------------------- | ----------------------- |\n| **Floorplan**    | **mean IoU**           |                         |\n| wall             | 78.62                  | 74.81                   |\n|                  |                        |                         |\n| **Objects**      | **F1 @.25 IoU (3D)**   |                         |\n| curtain          | 27.35                  | 28.59                   |\n| nightstand       | 57.47                  | 54.39                   |\n| chandelier       | 38.92                  | 40.12                   |\n| wardrobe         | 23.33                  | 30.60                   |\n| bed              | 95.24                  | 93.75                   |\n| sofa             | 65.50                  | 66.15                   |\n| chair            | 21.26                  | 14.94                   |\n| cabinet          | 8.47                   | 8.44                    |\n| dining table     | 54.26                  | 56.10                   |\n| plants           | 20.68                  | 26.46                   |\n| tv cabinet       | 33.33                  | 10.26                   |\n| coffee table     | 50.00                  | 55.56                   |\n| side table       | 7.60                   | 2.17                    |\n| air conditioner  | 20.00                  | 13.04                   |\n| dresser          | 46.67                  | 23.53                   |\n|                  |                        |                         |\n| **Thin Objects** | **F1 @.25 IoU (2D)**   |                         |\n| painting         | 50.04                  | 53.81                   |\n| carpet           | 31.76                  | 45.31                   |\n| tv               | 67.31                  | 52.29                   |\n| door             | 50.35                  | 42.15                   |\n| window           | 45.4                   | 45.9                    |\n\n</div>\n\n## License\n\nSpatialLM-Llama-1B is derived from Llama3.2-1B-Instruct, which is licensed under the Llama3.2 license.\nSpatialLM-Qwen-0.5B is derived from the Qwen-2.5 series, originally licensed under the Apache 2.0 License.\n\nAll models are built upon the SceneScript point cloud encoder, licensed under the CC-BY-NC-4.0 License. TorchSparse, utilized in this project, is licensed under the MIT License.\n\n## Citation\n\nIf you find this work useful, please consider citing:\n\n```bibtex\n@misc{spatiallm,\n  title        = {SpatialLM: Large Language Model for Spatial Understanding},\n  author       = {ManyCore Research Team},\n  howpublished = {\url{https://github.com/manycore-research/SpatialLM}},\n  year         = {2025}\n}\n```\n\n## Acknowledgements\n\nWe would like to thank the following projects that made this work possible:\n\n[Llama3.2](https://github.com/meta-llama) | [Qwen2.5](https://github.com/QwenLM/Qwen2.5) | [Transformers](https://github.com/huggingface/transformers) | [SceneScript](https://github.com/facebookresearch/scenescript) | [TorchSparse](https://github.com/mit-han-lab/torchsparse)\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":1247355840,"storage_bytes":2532929485,"files_count":8,"spaces_count":0,"gated":false,"private":false,"config":{"architectures":["SpatialLMLlamaForCausalLM"],"model_type":"spatiallm_llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","chat_template":"{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- if strftime_now is defined %}\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n    {%- else %}\n        {%- set date_string = \"26 Jul 2024\" %}\n    {%- endif %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][''content'']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there''s no first user message!\") }}\n{%- endif %}\n    {{- ''<|start_header_id|>user<|end_header_id|>\\n\\n'' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == ''ipython'' or message.role == ''tool'' or ''tool_calls'' in message) %}\n        {{- ''<|start_header_id|>'' + message[''role''] + ''<|end_header_id|>\\n\\n''+ message[''content''] | trim + ''<|eot_id|>'' }}\n    {%- elif ''tool_calls'' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' -}}\n        {{- ''{\"name\": \"'' + tool_call.name + ''\", '' }}\n        {{- ''\"parameters\": '' }}\n        {{- tool_call.arguments | tojson }}\n        {{- \"}\" }}\n        {{- \"<|eot_id|>\" }}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' }}\n{%- endif %}\n","eos_token":"<|eot_id|>","pad_token":"<|eot_id|>"}}}', '[]', '[{"type":"has_code","target_id":"github:manycore-research:SpatialLM\"","source_url":"https://github.com/manycore-research/SpatialLM\""},{"type":"has_code","target_id":"github:manycore-research:SpatialLM.git","source_url":"https://github.com/manycore-research/SpatialLM.git"},{"type":"has_code","target_id":"github:rmurai0610:MASt3R-SLAM","source_url":"https://github.com/rmurai0610/MASt3R-SLAM"},{"type":"has_code","target_id":"github:rmurai0610:MASt3R-SLAM","source_url":"https://github.com/rmurai0610/MASt3R-SLAM"},{"type":"has_code","target_id":"github:manycore-research:SpatialLM}},","source_url":"https://github.com/manycore-research/SpatialLM}},"},{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:facebookresearch:scenescript","source_url":"https://github.com/facebookresearch/scenescript"},{"type":"has_code","target_id":"github:mit-han-lab:torchsparse","source_url":"https://github.com/mit-han-lab/torchsparse"}]', NULL, 'llama3.2', 'approved', 80, 'b398c3deda3dd1ca45c2ddc3839f790c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-miqudev-miqu-1-70b', 'huggingface--miqudev--miqu-1-70b', 'miqu-1-70b', 'miqudev', '--- {} --- Leaked from            ...', '["gguf","endpoints_compatible","region:us","conversational"]', 'other', 986, 596, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/miqudev/miqu-1-70b","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\n{}\n---\n# miqu 70b\n\nLeaked from\n\n\n                                                                     \n                                                            \n                                                \n                                             \n                               \n               \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n        \n                \n                    \n                   \n                        \n                           \n\n## Model card\n\nFirst model in the potential series.\n\n## Prompt format: Mistral\n\n```\n<s> [INST] QUERY_1 [/INST] ANSWER_1</s> [INST] QUERY_2 [/INST] ANSWER_2</s>...\n```\n\nBeware that some backends (like llama.cpp) add bos already (by default), so you don''t need to prepend it yourself.\n\n## Settings\n\nDO NOT CHANGE ROPE SETTINGS. This model uses high freq base with 32k seen tokens, it should be fine for most tasks.\n\nOnly tested with temp 1 and top_p 0.95 with everything else disabled.\n\n<video src="https://cdn-uploads.huggingface.co/production/uploads/65ab93082bf3e0cbbf717850/cIEP5e43VP0k0caRzl16e.mp4" controls="controls" style="max-width: 720px;">\n</video>', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":115639116384,"files_count":5,"spaces_count":0,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, NULL, 'pending', 54.9, 'a84d44e597feeb2292d93a25501cde5c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nuigurumi-basil-mix', 'huggingface--nuigurumi--basil-mix', 'basil_mix', 'nuigurumi', '--- license: other --- - merged model. - realistic texture and Asian face. - designed to maintain a responsive reaction to danbooru based prompts. - This model and its derivatives(image, merged model) can be freely used for non-profit purposes only. - You may not use this model and its derivatives on websites, apps, or other platforms where you can or plan to earn income or donations. If you wish to use it for such purposes, please contact nuigurumi. - Introducing the model itself is allowed ...', '["diffusers","license:other","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 984, 22808, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nuigurumi/basil_mix","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\n---\n## Model Description\n\n- merged model.\n- realistic texture and Asian face.\n- designed to maintain a responsive reaction to danbooru based prompts.\n\n## License\n  \n- This model and its derivatives(image, merged model) can be freely used for non-profit purposes only.\n- You may not use this model and its derivatives on websites, apps, or other platforms where you can or plan to earn income or donations. If you wish to use it for such purposes, please contact nuigurumi.\n- Introducing the model itself is allowed for both commercial and non-commercial purposes, but please include the model name and a link to this repository when doing so.\n\n- ()\n- Web[nuigurumi](https://twitter.com/nuigurumi1_KR)\n- \n\n- check [License](https://huggingface.co/nuigurumi/basil_mix/blob/main/License.md)\n  \n  \n  _  \n  fanboxpatreon  \n  (cilled_re...)   \n  _ \n\n# Gradio\n\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI to run basil_mix:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/basil_mix)\n\n\n## Recommendations\n\n- VAE: [vae-ft-mse-840000](https://huggingface.co/stabilityai/sd-vae-ft-mse-original) from StabilityAI\n- Prompting: Simple prompts are better. Large amounts of quality tags and negative prompts can have negative effects.', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":17450358760,"files_count":21,"spaces_count":5,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"}]', NULL, 'Other', 'approved', 49.9, '0403d07e994ea57e0c2ff79932b8edfd', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-HiDream-ai-HiDream-I1-Full', 'huggingface--hidream-ai--hidream-i1-full', 'HiDream-I1-Full', 'HiDream-ai', '--- license: mit tags: - image-generation - HiDream.ai language: - en pipeline_tag: text-to-image library_name: diffusers --- !HiDream-I1 Demo is a new open-source image generative foundation model with 17B parameters that achieves state-of-the-art image generation quality within seconds. <span style="color: #FF5733; font-weight: bold">For more features and to experience the full capabilities of our product, please visit https://vivago.ai/.</span> -  **July 16, 2025**: We''ve open-sourced th...', '["diffusers","safetensors","image-generation","hidream.ai","text-to-image","en","arxiv:2505.22705","license:mit","diffusers:hidreamimagepipeline","region:us"]', 'text-to-image', 980, 22144, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/HiDream-ai/HiDream-I1-Full","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\ntags:\n- image-generation\n- HiDream.ai\nlanguage:\n- en\npipeline_tag: text-to-image\nlibrary_name: diffusers\n---\n\n![HiDream-I1 Demo](demo.jpg)\n\n`HiDream-I1` is a new open-source image generative foundation model with 17B parameters that achieves state-of-the-art image generation quality within seconds.\n\n<span style="color: #FF5733; font-weight: bold">For more features and to experience the full capabilities of our product, please visit [https://vivago.ai/](https://vivago.ai/).</span>\n\n## Project Updates\n-  **July 16, 2025**: We''ve open-sourced the updated image editing model [**HiDream-E1.1**](https://huggingface.co/HiDream-ai/HiDream-E1-1). \n-  **May 28, 2025**: We''ve released our technical report [HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer](https://arxiv.org/abs/2505.22705).\n-  **April 28, 2025**: We''ve open-sourced the instruction-based-image-editing model [**HiDream-E1-Full**](https://github.com/HiDream-ai/HiDream-E1). Experience at [https://huggingface.co/spaces/HiDream-ai/HiDream-E1-Full](https://huggingface.co/spaces/HiDream-ai/HiDream-E1-Full)!.\n\n## Key Features\n-  **Superior Image Quality** - Produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. Achieves state-of-the-art HPS v2.1 score, which aligns with human preferences.\n-  **Best-in-Class Prompt Following** - Achieves industry-leading scores on GenEval and DPG benchmarks, outperforming all other open-source models.\n-  **Open Source** - Released under the MIT license to foster scientific advancement and enable creative innovation.\n-  **Commercial-Friendly** - Generated images can be freely used for personal projects, scientific research, and commercial applications.\n\n## Quick Start\nPlease make sure you have installed [Flash Attention](https://github.com/Dao-AILab/flash-attention). We recommend CUDA version 12.4 for the manual installation.\n```\npip install -r requirements.txt\n```\nClone the GitHub repo:\n```\ngit clone https://github.com/HiDream-ai/HiDream-I1\n```\n\nThen you can run the inference scripts to generate images:\n\n```python\n# For full model inference\npython ./inference.py --model_type full\n\n# For distilled dev model inference\npython ./inference.py --model_type dev\n\n# For distilled fast model inference\npython ./inference.py --model_type fast\n```\n> **Note:** The inference script will automatically download `meta-llama/Meta-Llama-3.1-8B-Instruct` model files. If you encounter network issues, you can download these files ahead of time and place them in the appropriate cache directory to avoid download failures during inference.\n\n## Gradio Demo\n\nWe also provide a Gradio demo for interactive image generation. You can run the demo with:\n\n```python\npython gradio_demo.py \n```\n\n## Evaluation Metrics\n\n### DPG-Bench\n| Model           | Overall   | Global    | Entity    | Attribute | Relation  | Other     |\n|-----------------|-----------|-----------|-----------|-----------|-----------|-----------|\n| PixArt-alpha    |    71.11  | 74.97     | 79.32     | 78.60     | 82.57     | 76.96     |\n| SDXL            |    74.65  | 83.27     | 82.43     | 80.91     | 86.76     | 80.41     |\n| DALL-E 3        |    83.50  | 90.97     | 89.61     | 88.39     | 90.58     | 89.83     |\n| Flux.1-dev      |    83.79  | 85.80     | 86.79     | 89.98     | 90.04     | 89.90     |\n| SD3-Medium      |    84.08  | 87.90     | 91.01     | 88.83     | 80.70     | 88.68     |\n| Janus-Pro-7B    |    84.19  | 86.90     | 88.90     | 89.40     | 89.32     | 89.48     |\n| CogView4-6B     |    85.13  | 83.85     | 90.35     | 91.17     | 91.14     | 87.29     |\n| **HiDream-I1**  |  **85.89**| 76.44 	  | 90.22     | 89.48     | 93.74     | 91.83     | \n\n### GenEval\n\n| Model           | Overall  | Single Obj. | Two Obj. | Counting | Colors   | Position | Color attribution |\n|-----------------|----------|-------------|----------|----------|----------|----------|-------------------|\n| SDXL            |    0.55  | 0.98        | 0.74     | 0.39     | 0.85     | 0.15     | 0.23              |\n| PixArt-alpha    |    0.48  | 0.98        | 0.50     | 0.44     | 0.80     | 0.08     | 0.07              |\n| Flux.1-dev      |    0.66  | 0.98        | 0.79     | 0.73     | 0.77     | 0.22     | 0.45              |\n| DALL-E 3        |    0.67  | 0.96        | 0.87     | 0.47     | 0.83     | 0.43     | 0.45              |\n| CogView4-6B     |    0.73  | 0.99        | 0.86     | 0.66     | 0.79     | 0.48     | 0.58              |\n| SD3-Medium      |    0.74  | 0.99        | 0.94     | 0.72     | 0.89     | 0.33     | 0.60              |\n| Janus-Pro-7B    |    0.80  | 0.99        | 0.89     | 0.59     | 0.90     | 0.79     | 0.66              |\n| **HiDream-I1**  |  **0.83**| 1.00        | 0.98 	  | 0.79 	 | 0.91 	| 0.60 	   | 0.72              |\n\n### HPSv2.1 benchmark\n\n|  Model                  |     Averaged   | Animation  |  Concept-art  |   Painting   |   Photo    |\n|-------------------------|----------------|------------|---------------|--------------|------------|\n|  Stable Diffusion v2.0  |       26.38    |	27.09   |      26.02    |    25.68     |    26.73   |\n|  Midjourney V6          |       30.29    |    32.02   |      30.29    |    29.74     |    29.10   |\n|  SDXL	                  |       30.64    |    32.84   |      31.36    |    30.86     |    27.48   |\n|  Dall-E3	              |       31.44    |    32.39   |      31.09    |    31.18     |    31.09   |\n|  SD3                    |       31.53    |    32.60   |      31.82    |    32.06     |    29.62   |\n|  Midjourney V5          |       32.33    |    34.05   |      32.47    |    32.24     |    30.56   |\n|  CogView4-6B            |       32.31    |    33.23   |      32.60    |    32.89     |    30.52   |\n|  Flux.1-dev             |       32.47    |    33.87   |      32.27    |    32.62     |    31.11   |\n|  stable cascade         |       32.95    |    34.58   |      33.13    |    33.29     |    30.78   |\n|  **HiDream-I1**         |     **33.82**  |    35.05   |      33.74    |    33.88     |    32.61   |\n\n\n## License Agreement\nThe Transformer models in this repository are licensed under the MIT License. The VAE is from `FLUX.1 [schnell]`, and the text encoders from `google/t5-v1_1-xxl` and `meta-llama/Meta-Llama-3.1-8B-Instruct`. Please follow the license terms specified for these components. You own all content you create with this model. You can use your generated content freely, but you must comply with this license agreement. You are responsible for how you use the models. Do not create illegal content, harmful material, personal information that could harm others, false information, or content targeting vulnerable groups.\n\n\n## Acknowledgements\n- The VAE component is from `FLUX.1 [schnell]`, licensed under Apache 2.0. \n- The text encoders are from `google/t5-v1_1-xxl` (licensed under Apache 2.0) and `meta-llama/Meta-Llama-3.1-8B-Instruct` (licensed under the Llama 3.1 Community License Agreement).\n\n\n## Citation\n\n```bibtex\n@article{hidreami1technicalreport,\n  title={HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer},\n  author={Cai, Qi and Chen, Jingwen and Chen, Yang and Li, Yehao and Long, Fuchen and Pan, Yingwei and Qiu, Zhaofan and Zhang, Yiheng and Gao, Fengbin and Xu, Peihan and others},\n  journal={arXiv preprint arXiv:2505.22705},\n  year={2025}\n}\n```', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":47186202978,"files_count":36,"spaces_count":99,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"HiDreamImagePipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:HiDream-ai:HiDream-E1","source_url":"https://github.com/HiDream-ai/HiDream-E1"},{"type":"has_code","target_id":"github:Dao-AILab:flash-attention","source_url":"https://github.com/Dao-AILab/flash-attention"},{"type":"has_code","target_id":"github:HiDream-ai:HiDream-I1","source_url":"https://github.com/HiDream-ai/HiDream-I1"},{"type":"based_on_paper","target_id":"arxiv:2505.22705","source_url":"https://arxiv.org/abs/2505.22705"}]', NULL, 'MIT', 'approved', 84.9, '1732e90072de6d5076b1a7d1d433efd2', NULL, 'https://huggingface.co/HiDream-ai/HiDream-I1-Full/resolve/main/demo.jpg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-HiDream-ai-HiDream-I1-Full from https://huggingface.co/HiDream-ai/HiDream-I1-Full/resolve/main/demo.jpg
Image converted to WebP: data/images/huggingface-HiDream-ai-HiDream-I1-Full.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-moka-ai-m3e-base', 'huggingface--moka-ai--m3e-base', 'm3e-base', 'moka-ai', '--- language: - zh - en tags: - embedding - text-embedding library_name: sentence-transformers --- m3e-small | m3e-base M3E  Moka Massive Mixed Embedding  - Moka MokaAI  uniem  BenchMark  MTEB-zh - Massive**** (2200w+)  - Mixed - Embedding - 2023.06.24 M3E  notebook<a target="_blank" href="https://colab.research.google.com/github/wangyuxinwhy/uniem/blob/main...', '["sentence-transformers","pytorch","safetensors","bert","embedding","text-embedding","zh","en","region:us"]', 'other', 974, 152775, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/moka-ai/m3e-base","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- zh\n- en\ntags:\n- embedding\n- text-embedding\nlibrary_name: sentence-transformers\n---\n\n#  M3E Models\n\n[m3e-small](https://huggingface.co/moka-ai/m3e-small) | [m3e-base](https://huggingface.co/moka-ai/m3e-base)\n\nM3E  Moka Massive Mixed Embedding \n\n- Moka MokaAI  [uniem](https://github.com/wangyuxinwhy/uniem/blob/main/scripts/train_m3e.py)  BenchMark  [MTEB-zh](https://github.com/wangyuxinwhy/uniem/tree/main/mteb-zh)\n- Massive**** (2200w+) \n- Mixed\n- Embedding\n\n##  \n\n- 2023.06.24 M3E  [notebook](https://github.com/wangyuxinwhy/uniem/blob/main/examples/finetune.ipynb)<a target="_blank" href="https://colab.research.google.com/github/wangyuxinwhy/uniem/blob/main/examples/finetune.ipynb">\n  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>\n</a>\n- 2023.06.14 UER, ErLangShen, DMetaSoul\n- 2023.06.08 T2Ranking 1W m3e-base  ndcg@10  0.8004 openai-ada-002  0.7786\n- 2023.06.07 6 m3e-base  accuracy  0.6157 openai-ada-002  0.5956\n\n##  \n\n|           |  |  |  |  | s2s | s2p | s2c |  |  | s2s Acc | s2p ndcg@10 |\n| --------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | ---- | ---------- | ------------ | -------- |\n| m3e-small | 24M      | 512      |        |        |        |        |        |    |          | 0.5834       | 0.7262   |\n| m3e-base  | 110M     | 768      |        |        |        |        |        |    |          | **0.6157**       | **0.8004**   |\n| text2vec  | 110M     | 768      |        |        |        |        |        |    |          | 0.5755       | 0.6346   |\n| openai-ada-002    |      | 1536     |        |        |        |        |        |    |          | 0.5956       | 0.7786   |\n\n\n- s2s,  sentence to sentence \n- s2p,  sentence to passage GPT \n- s2c,  sentence to code \n-  m3e  text2vec  sentence-transformers  openai \n- ACC & ndcg@10\n\nTips:\n-  m3e \n-  openai text-embedding-ada-002\n-  openai text-embedding-ada-002\n-  S2S \n\n##   M3E\n\n sentence-transformers\n\n```bash\npip install -U sentence-transformers\n```\n\n M3E Models\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(''moka-ai/m3e-base'')\n\n#Our sentences we like to encode\nsentences = [\n    ''* Moka  MokaAI  uniem'',\n    ''* Massive ****'',\n    ''* Mixed ALL in one''\n]\n\n#Sentences are encoded by calling model.encode()\nembeddings = model.encode(sentences)\n\n#Print the embeddings\nfor sentence, embedding in zip(sentences, embeddings):\n    print("Sentence:", sentence)\n    print("Embedding:", embedding)\n    print("")\n```\n\n\nM3E  [sentence-transformers](https://www.sbert.net/) **** sentence-transformers **** M3E Models [chroma](https://docs.trychroma.com/getting-started), [guidance](https://github.com/microsoft/guidance), [semantic-kernel](https://github.com/microsoft/semantic-kernel) \n\n##  \n\n`uniem`  finetune \n\n```python\nfrom datasets import load_dataset\n\nfrom uniem.finetuner import FineTuner\n\ndataset = load_dataset(''shibing624/nli_zh'', ''STS-B'')\n#  m3e-small\nfinetuner = FineTuner.from_pretrained(''moka-ai/m3e-small'', dataset=dataset)\nfinetuner.run(epochs=1)\n```\n\n [uniem ](https://github.com/wangyuxinwhy/uniem/blob/main/examples/finetune.ipynb)\n\n<a target="_blank" href="https://colab.research.google.com/github/wangyuxinwhy/uniem/blob/main/examples/finetune.ipynb">\n  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>\n</a>\n\n##  \n\nM3E  in-batch  in-batch  A100 80G  batch-size 2200W+  1 epoch [uniem](https://github.com/wangyuxinwhy/uniem/blob/main/scripts/train_m3e.py)\n\n##  \n\n- M3E  2200W  [M3E ](#M3E)\n- M3E  MEDI 145W  [MEDI ](https://drive.google.com/file/d/1vZ5c2oJNonGOvXzppNg5mHz24O6jcc52/view) [instructor team](https://github.com/HKUNLP/instructor-embedding) \n- M3E  300W +  M3E  [instructor-embedding](https://github.com/HKUNLP/instructor-embedding)\n- M3E  hfl  [Roberta](https://huggingface.co/hfl/chinese-roberta-wwm-ext)   small   base \n- ALL IN ONEM3E  ALL IN ONE \n\n##  MTEB-zh \n\n- [text2vec](https://github.com/shibing624/text2vec), m3e-base, m3e-small, openai text-embedding-ada-002, [DMetaSoul](https://huggingface.co/DMetaSoul/sbert-chinese-general-v2), [UER](https://huggingface.co/uer/sbert-base-chinese-nli), [ErLangShen](https://huggingface.co/IDEA-CCNL/Erlangshen-SimCSE-110M-Chinese)\n-  [MTEB-zh] (https://github.com/wangyuxinwhy/uniem/blob/main/mteb-zh)\n\n### \n\n-  HuggingFace  6 \n-  MTEB  Accuracy\n\n|                   | text2vec | m3e-small | m3e-base | openai | DMetaSoul   | uer     | erlangshen  |\n| ----------------- | -------- | --------- | -------- | ------ | ----------- | ------- | ----------- |\n| TNews             | 0.43     | 0.4443    | **0.4827**   | 0.4594 | 0.3084      | 0.3539  | 0.4361      |\n| JDIphone          | 0.8214   | 0.8293    | **0.8533**   | 0.746  | 0.7972      | 0.8283  | 0.8356      |\n| GubaEastmony      | 0.7472   | 0.712     | 0.7621   | 0.7574 | 0.735       | 0.7534  | **0.7787**      |\n| TYQSentiment      | 0.6099   | 0.6596    | **0.7188**   | 0.68   | 0.6437      | 0.6662  | 0.6444      |\n| StockComSentiment | 0.4307   | 0.4291    | 0.4363   | **0.4819** | 0.4309      | 0.4555  | 0.4482      |\n| IFlyTek           | 0.414    | 0.4263    | 0.4409   | **0.4486** | 0.3969      | 0.3762  | 0.4241      |\n| Average           | 0.5755   | 0.5834    | **0.6157**   | 0.5956 | 0.552016667 | 0.57225 | 0.594516667 |\n\n### \n\n#### T2Ranking 1W\n\n-  [T2Ranking](https://github.com/THUIR/T2Ranking/tree/main)  T2Ranking openai  api  T2Ranking  10000 \n-  MTEB  map@1, map@10, mrr@1, mrr@10, ndcg@1, ndcg@10\n-  M3E  openai \n\n|         | text2vec | openai-ada-002 | m3e-small | m3e-base | DMetaSoul | uer     | erlangshen |\n| ------- | -------- | -------------- | --------- | -------- | --------- | ------- | ---------- |\n| map@1   | 0.4684   | 0.6133         | 0.5574    | **0.626**    | 0.25203   | 0.08647 | 0.25394    |\n| map@10  | 0.5877   | 0.7423         | 0.6878    | **0.7656**   | 0.33312   | 0.13008 | 0.34714    |\n| mrr@1   | 0.5345   | 0.6931         | 0.6324    | **0.7047**   | 0.29258   | 0.10067 | 0.29447    |\n| mrr@10  | 0.6217   | 0.7668         | 0.712     | **0.7841**   | 0.36287   | 0.14516 | 0.3751     |\n| ndcg@1  | 0.5207   | 0.6764         | 0.6159    | **0.6881**   | 0.28358   | 0.09748 | 0.28578    |\n| ndcg@10 | 0.6346   | 0.7786         | 0.7262    | **0.8004**   | 0.37468   | 0.15783 | 0.39329    |\n\n#### T2Ranking\n\n-  T2Ranking openai-ada-002  T2Ranking 10W  T2Ranking 50W T2Ranking ... 128G \n-  MTEB  ndcg@10\n\n|         | text2vec | m3e-small | m3e-base |\n| ------- | -------- | --------- | -------- |\n| t2r-1w  | 0.6346   | 0.72621   | **0.8004**   |\n| t2r-10w | 0.44644  | 0.5251    | **0.6263**   |\n| t2r-50w | 0.33482  | 0.38626   | **0.47364**  |\n\n\n-  text2vec  text2vec \n\n##  M3E\n\n [uniem process_zh_datasets](https://github.com/wangyuxinwhy/uniem/blob/main/scripts/process_zh_datasets.py)  huggingface  huggingface \n\n|            |  |       |           | Prompt |  |                                                    |                                                          | / |  |  | Done | URL                                                          |  |\n| -------------------- | ---- | --------- | ----------------- | ------ | ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ----------------- | -------- | ---- | ---- | ------------------------------------------------------------ | -------- |\n| cmrc2018             |  | 14,363    |               |    |    | Yiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, Guoping Hu | https://github.com/ymcui/cmrc2018/blob/master/README_CN.md  |                 |        |    |    | https://huggingface.co/datasets/cmrc2018                     |        |\n| belle_2m             |  | 2,000,000 |           |      |    | LianjiaTech/BELLE                                            | belle  self instruct  gpt3.5  |                 |        |    |    | https://huggingface.co/datasets/BelleGroup/train_2M_CN       |        |\n| firefily             |  | 1,649,399 |           |      |    | YeungNLP                                                     | Firefly Instruction TuningZeRO  |             |    |    |    | https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M  |        |\n| alpaca_gpt4          |  | 48,818    |           |      |    | Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao | AlpacaGPT4self-instruct5 |                 |        |    |    | https://huggingface.co/datasets/shibing624/alpaca-zh         |        |\n| zhihu_kol            |  | 1,006,218 |               |    |    | wangrui6                                                     |                                                      |             |    |    |    | https://huggingface.co/datasets/wangrui6/Zhihu-KOL           |        |\n| hc3_chinese          |  | 39,781    |               |    |    | Hello-SimpleAI                                               |  GPT                             |                 |    |    |    | https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese   |        |\n| amazon_reviews_multi |  | 210,000   |       |    |    |                                                        |                                          |                 |        |    |    | https://huggingface.co/datasets/amazon_reviews_multi/viewer/zh/train?row=8 |        |\n| mlqa                 |  | 85,853    |               |    |    | patrickvonplaten                                             |                        |                 |    |    |    | https://huggingface.co/datasets/mlqa/viewer/mlqa-translate-train.zh/train?p=2 |        |\n| xlsum                |  | 93,404    |               |    |    | BUET CSE NLP Group                                           | BBC                                      |                 |        |    |    | https://huggingface.co/datasets/csebuetnlp/xlsum/viewer/chinese_simplified/train?row=259 |        |\n| ocnli                |  | 17,726    |       |    |    | Thomas Wolf                                                  |                                            |                 |        |    |    | https://huggingface.co/datasets/clue/viewer/ocnli            |        |\n| BQ                   |  | 60,000    |           |    |    | Intelligent Computing Research Center, Harbin Institute of Technology(Shenzhen) | http://icrc.hitsz.edu.cn/info/1037/1162.htm BQ  120000 100000 10000 10000    |                 |        |    |    | https://huggingface.co/datasets/shibing624/nli_zh/viewer/BQ  |        |\n| lcqmc                |  | 149,226   |           |    |    | Ming Xu                                                      | LCQMC  COLING2018  |                 |        |    |    | https://huggingface.co/datasets/shibing624/nli_zh/viewer/LCQMC/train |        |\n| paws-x               |  | 23,576    |           |    |    | Bhavitvya Malik                                              | PAWS Wiki                                            |                 |        |    |    | https://huggingface.co/datasets/paws-x/viewer/zh/train       |        |\n| wiki_atomic_edit     |  | 1,213,780 |           |    |    | abhishek thakur                                              |                        |             |    |    |    | https://huggingface.co/datasets/wiki_atomic_edits            |        |\n| chatmed_consult      |  | 549,326   |               |    |    | Wei Zhu                                                      |  gpt3.5                |                 |        |    |    | https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset |        |\n| webqa                |  | 42,216    |               |    |    | suolyer                                                      | 2016 |                 |    |    |    | https://huggingface.co/datasets/suolyer/webqa/viewer/suolyer--webqa/train?p=3 |        |\n| dureader_robust      |  | 65,937    |   |    |    |                                                          | DuReader robust |                 |        |    |    | https://huggingface.co/datasets/PaddlePaddle/dureader_robust/viewer/plain_text/train?row=96 |        |\n| csl                  |  | 395,927   |               |    |    | Yudong Li, Yuqing Zhang, Zhe Zhao, Linlin Shen, Weijie Liu, Weiquan Mao and Hui Zhang | CSL 396,209  CSL NLP  |                 |        |    |    | https://huggingface.co/datasets/neuclir/csl                  |        |\n| miracl-corpus        |  | 4,934,368 |               |    |    | MIRACL                                                       | The corpus for each language is prepared from a Wikipedia dump, where we keep only the plain text and discard images, tables, etc. Each article is segmented into multiple passages using WikiExtractor based on natural discourse units (e.g., \n\n in the wiki markup). Each of these passages comprises a "document" or unit of retrieval. We preserve the Wikipedia article title of each passage. |                 |        |    |    | https://huggingface.co/datasets/miracl/miracl-corpus         |        |\n| lawzhidao            |  | 36,368    |               |    |    | -Ustinian                                            |                                      |                 |        |    |    | https://www.heywhale.com/mw/dataset/5e953ca8e7ec38002d02fca7/content |        |\n| CINLID               |  | 34,746    |           |    |    |                                                        | Chinese Idioms Natural Language Inference Dataset106832entailmentcontradictionneutralNLI |                 |        |    |    | https://www.luge.ai/#/luge/dataDetail?id=39                  |        |\n| DuSQL                | SQL  | 25,003    | NL2SQL            | SQL    |    |                                                          | DuSQL200164 |                 |        |    |    | https://www.luge.ai/#/luge/dataDetail?id=13                  |        |\n| Zhuiyi-NL2SQL        | SQL  | 45,918    | NL2SQL            | SQL    |    |                                                | NL2SQL |                 |        |    |    | https://www.luge.ai/#/luge/dataDetail?id=12                  |        |\n| Cspider              | SQL  | 7,785     | NL2SQL            | SQL    |    |                                                  | CSpider |                 |        |    |    | https://www.luge.ai/#/luge/dataDetail?id=11                  |        |\n| news2016zh           |  | 2,507,549 |               |    |    | Bright Xu                                                    | 2506.3 |                 |        |    |    | https://github.com/brightmart/nlp_chinese_corpus             |        |\n| baike2018qa          |  | 1,470,142 |               |    |    | Bright Xu                                                    | 15049210434 |                 |        |    |    | https://github.com/brightmart/nlp_chinese_corpus             |        |\n| webtext2019zh        |  | 4,258,310 |               |    |    | Bright Xu                                                    | 4102.8 |                 |        |    |    | https://github.com/brightmart/nlp_chinese_corpus             |        |\n| SimCLUE              |  | 775,593   |           |    |    |  simCLUE                                 |  |                 |        |    |    | https://github.com/CLUEbenchmark/SimCLUE                     |        |\n| Chinese-SQuAD        |  | 76,449    |       |    |    | junzeng-pluto                                                | Squad |                 |        |    |    | https://github.com/pluto-junzeng/ChineseSquad                |        |\n\n##  \n\n- [x]  MTEB  BenchMark, [MTEB-zh](https://github.com/wangyuxinwhy/uniem/tree/main/mteb-zh)\n- [x]  Large \n- [x]  Finetuner \n- [ ] \n- [ ]  M3E  m3e-hq huggingface \n- [ ]  m3e-hq  hard negative  m3e-hq-with-score huggingface \n- [ ]  m3e-hq-with-score  [cosent loss](https://github.com/wangyuxinwhy/uniem/blob/main/uniem/criteria.py#LL24C39-L24C39) loss CoSent [](https://kexue.fm/archives/8847)\n- [ ]  M3E models\n\n##  \n\n\n\n##  License\n\nM3E models  M3E models  M3E \n\n## Citation\nPlease cite this model using the following format:\n```\n  @software {Moka Massive Mixed Embedding,  \n  author = {Wang Yuxin,Sun Qingxuan,He sicheng},  \n  title = {M3E: Moka Massive Mixed Embedding Model},  \n  year = {2023}\n  }\n```', '{"pipeline_tag":null,"library_name":"sentence-transformers","framework":"sentence-transformers","params":102268160,"storage_bytes":818238909,"files_count":12,"spaces_count":29,"gated":false,"private":false,"config":{"architectures":["BertModel"],"model_type":"bert","tokenizer_config":{"cls_token":"[CLS]","mask_token":"[MASK]","pad_token":"[PAD]","sep_token":"[SEP]","unk_token":"[UNK]"}}}', '[]', '[{"type":"has_code","target_id":"github:wangyuxinwhy:uniem","source_url":"https://github.com/wangyuxinwhy/uniem"},{"type":"has_code","target_id":"github:wangyuxinwhy:uniem","source_url":"https://github.com/wangyuxinwhy/uniem"},{"type":"has_code","target_id":"github:wangyuxinwhy:uniem","source_url":"https://github.com/wangyuxinwhy/uniem"},{"type":"has_code","target_id":"github:microsoft:guidance","source_url":"https://github.com/microsoft/guidance"},{"type":"has_code","target_id":"github:microsoft:semantic-kernel","source_url":"https://github.com/microsoft/semantic-kernel"},{"type":"has_code","target_id":"github:wangyuxinwhy:uniem","source_url":"https://github.com/wangyuxinwhy/uniem"},{"type":"has_code","target_id":"github:wangyuxinwhy:uniem","source_url":"https://github.com/wangyuxinwhy/uniem"},{"type":"has_code","target_id":"github:HKUNLP:instructor-embedding","source_url":"https://github.com/HKUNLP/instructor-embedding"},{"type":"has_code","target_id":"github:HKUNLP:instructor-embedding","source_url":"https://github.com/HKUNLP/instructor-embedding"},{"type":"has_code","target_id":"github:shibing624:text2vec","source_url":"https://github.com/shibing624/text2vec"},{"type":"has_code","target_id":"github:wangyuxinwhy:uniem","source_url":"https://github.com/wangyuxinwhy/uniem"},{"type":"has_code","target_id":"github:THUIR:T2Ranking","source_url":"https://github.com/THUIR/T2Ranking"},{"type":"has_code","target_id":"github:wangyuxinwhy:uniem","source_url":"https://github.com/wangyuxinwhy/uniem"},{"type":"has_code","target_id":"github:ymcui:cmrc2018","source_url":"https://github.com/ymcui/cmrc2018"},{"type":"has_code","target_id":"github:brightmart:nlp_chinese_corpus","source_url":"https://github.com/brightmart/nlp_chinese_corpus"},{"type":"has_code","target_id":"github:brightmart:nlp_chinese_corpus","source_url":"https://github.com/brightmart/nlp_chinese_corpus"},{"type":"has_code","target_id":"github:brightmart:nlp_chinese_corpus","source_url":"https://github.com/brightmart/nlp_chinese_corpus"},{"type":"has_code","target_id":"github:CLUEbenchmark:SimCLUE","source_url":"https://github.com/CLUEbenchmark/SimCLUE"},{"type":"has_code","target_id":"github:pluto-junzeng:ChineseSquad","source_url":"https://github.com/pluto-junzeng/ChineseSquad"},{"type":"has_code","target_id":"github:wangyuxinwhy:uniem","source_url":"https://github.com/wangyuxinwhy/uniem"},{"type":"has_code","target_id":"github:wangyuxinwhy:uniem","source_url":"https://github.com/wangyuxinwhy/uniem"}]', NULL, NULL, 'pending', 69.9, '358e1476a8c12009fb539a9716e5d7f6', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-ByteDance-AnimateDiff-Lightning', 'huggingface--bytedance--animatediff-lightning', 'AnimateDiff-Lightning', 'ByteDance', '--- license: creativeml-openrail-m tags: - text-to-video - stable-diffusion - animatediff library_name: diffusers inference: false --- <video src=''https://huggingface.co/ByteDance/AnimateDiff-Lightning/resolve/main/animatediff_lightning_samples_t2v.mp4'' width="100%" autoplay muted loop playsinline style=''margin:0''></video> <video src=''https://huggingface.co/ByteDance/AnimateDiff-Lightning/resolve/main/animatediff_lightning_samples_v2v.mp4'' width="100%" autoplay muted loop playsinline style=''m...', '["diffusers","text-to-video","stable-diffusion","animatediff","arxiv:2403.12706","license:creativeml-openrail-m","region:us"]', 'text-to-video', 974, 48133, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/ByteDance/AnimateDiff-Lightning","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n- text-to-video\n- stable-diffusion\n- animatediff\nlibrary_name: diffusers\ninference: false\n---\n# AnimateDiff-Lightning\n\n<video src=''https://huggingface.co/ByteDance/AnimateDiff-Lightning/resolve/main/animatediff_lightning_samples_t2v.mp4'' width="100%" autoplay muted loop playsinline style=''margin:0''></video>\n<video src=''https://huggingface.co/ByteDance/AnimateDiff-Lightning/resolve/main/animatediff_lightning_samples_v2v.mp4'' width="100%" autoplay muted loop playsinline style=''margin:0''></video>\n\nAnimateDiff-Lightning is a lightning-fast text-to-video generation model. It can generate videos more than ten times faster than the original AnimateDiff. For more information, please refer to our research paper: [AnimateDiff-Lightning: Cross-Model Diffusion Distillation](https://arxiv.org/abs/2403.12706). We release the model as part of the research.\n\nOur models are distilled from [AnimateDiff SD1.5 v2](https://huggingface.co/guoyww/animatediff). This repository contains checkpoints for 1-step, 2-step, 4-step, and 8-step distilled models. The generation quality of our 2-step, 4-step, and 8-step model is great. Our 1-step model is only provided for research purposes.\n\n\n## Demo\n\nTry AnimateDiff-Lightning using our text-to-video generation [demo](https://huggingface.co/spaces/ByteDance/AnimateDiff-Lightning).\n\n\n## Recommendation\n\nAnimateDiff-Lightning produces the best results when used with stylized base models. We recommend using the following base models:\n\nRealistic\n- [epiCRealism](https://civitai.com/models/25694)\n- [Realistic Vision](https://civitai.com/models/4201)\n- [DreamShaper](https://civitai.com/models/4384)\n- [AbsoluteReality](https://civitai.com/models/81458)\n- [MajicMix Realistic](https://civitai.com/models/43331)\n\nAnime & Cartoon\n- [ToonYou](https://civitai.com/models/30240)\n- [IMP](https://civitai.com/models/56680)\n- [Mistoon Anime](https://civitai.com/models/24149)\n- [DynaVision](https://civitai.com/models/75549)\n- [RCNZ Cartoon 3d](https://civitai.com/models/66347)\n- [MajicMix Reverie](https://civitai.com/models/65055)\n\nAdditionally, feel free to explore different settings. We find using 3 inference steps on the 2-step model produces great results. We find certain base models produces better results with CFG. We also recommend using [Motion LoRAs](https://huggingface.co/guoyww/animatediff/tree/main) as they produce stronger motion. We use Motion LoRAs with strength 0.7~0.8 to avoid watermark.\n\n## Diffusers Usage\n\n```python\nimport torch\nfrom diffusers import AnimateDiffPipeline, MotionAdapter, EulerDiscreteScheduler\nfrom diffusers.utils import export_to_gif\nfrom huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\n\ndevice = "cuda"\ndtype = torch.float16\n\nstep = 4  # Options: [1,2,4,8]\nrepo = "ByteDance/AnimateDiff-Lightning"\nckpt = f"animatediff_lightning_{step}step_diffusers.safetensors"\nbase = "emilianJR/epiCRealism"  # Choose to your favorite base model.\n\nadapter = MotionAdapter().to(device, dtype)\nadapter.load_state_dict(load_file(hf_hub_download(repo ,ckpt), device=device))\npipe = AnimateDiffPipeline.from_pretrained(base, motion_adapter=adapter, torch_dtype=dtype).to(device)\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing="trailing", beta_schedule="linear")\n\noutput = pipe(prompt="A girl smiling", guidance_scale=1.0, num_inference_steps=step)\nexport_to_gif(output.frames[0], "animation.gif")\n```\n\n## ComfyUI Usage\n\n1. Download [animatediff_lightning_workflow.json](https://huggingface.co/ByteDance/AnimateDiff-Lightning/raw/main/comfyui/animatediff_lightning_workflow.json) and import it in ComfyUI.\n1. Install nodes. You can install them manually or use [ComfyUI-Manager](https://github.com/ltdrdata/ComfyUI-Manager).\n    * [ComfyUI-AnimateDiff-Evolved](https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved)\n    * [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)\n1. Download your favorite base model checkpoint and put them under `/models/checkpoints/`\n1. Download AnimateDiff-Lightning checkpoint `animatediff_lightning_Nstep_comfyui.safetensors` and put them under `/custom_nodes/ComfyUI-AnimateDiff-Evolved/models/`\n\n\n![ComfyUI Workflow](https://huggingface.co/ByteDance/AnimateDiff-Lightning/resolve/main/comfyui/animatediff_lightning_workflow.jpg)\n\n\n## Video-to-Video Generation\n\nAnimateDiff-Lightning is great for video-to-video generation. We provide the simplist comfyui workflow using ControlNet.\n\n1. Download [animatediff_lightning_v2v_openpose_workflow.json](https://huggingface.co/ByteDance/AnimateDiff-Lightning/raw/main/comfyui/animatediff_lightning_v2v_openpose_workflow.json) and import it in ComfyUI.\n1. Install nodes. You can install them manually or use [ComfyUI-Manager](https://github.com/ltdrdata/ComfyUI-Manager).\n    * [ComfyUI-AnimateDiff-Evolved](https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved)\n    * [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)\n    * [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)\n    * [comfyui_controlnet_aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n1. Download your favorite base model checkpoint and put them under `/models/checkpoints/`\n1. Download AnimateDiff-Lightning checkpoint `animatediff_lightning_Nstep_comfyui.safetensors` and put them under `/custom_nodes/ComfyUI-AnimateDiff-Evolved/models/`\n1. Download [ControlNet OpenPose](https://huggingface.co/lllyasviel/ControlNet-v1-1/tree/main) `control_v11p_sd15_openpose.pth` checkpoint to `/models/controlnet/`\n1. Upload your video and run the pipeline.\n\nAdditional notes:\n\n1. Video shouldn''t be too long or too high resolution. We used 576x1024 8 second 30fps videos for testing.\n1. Set the frame rate to match your input video. This allows audio to match with the output video.\n1. DWPose will download checkpoint itself on its first run.\n1. DWPose may get stuck in UI, but the pipeline is actually still running in the background. Check ComfyUI log and your output folder.\n\n![ComfyUI OpenPose Workflow](https://huggingface.co/ByteDance/AnimateDiff-Lightning/resolve/main/comfyui/animatediff_lightning_v2v_openpose_workflow.jpg)\n\n# Cite Our Work\n```\n@misc{lin2024animatedifflightning,\n      title={AnimateDiff-Lightning: Cross-Model Diffusion Distillation}, \n      author={Shanchuan Lin and Xiao Yang},\n      year={2024},\n      eprint={2403.12706},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```', '{"pipeline_tag":"text-to-video","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":7286508236,"files_count":18,"spaces_count":76,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:ltdrdata:ComfyUI-Manager","source_url":"https://github.com/ltdrdata/ComfyUI-Manager"},{"type":"has_code","target_id":"github:Kosinkadink:ComfyUI-AnimateDiff-Evolved","source_url":"https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved"},{"type":"has_code","target_id":"github:Kosinkadink:ComfyUI-VideoHelperSuite","source_url":"https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite"},{"type":"has_code","target_id":"github:ltdrdata:ComfyUI-Manager","source_url":"https://github.com/ltdrdata/ComfyUI-Manager"},{"type":"has_code","target_id":"github:Kosinkadink:ComfyUI-AnimateDiff-Evolved","source_url":"https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved"},{"type":"has_code","target_id":"github:Kosinkadink:ComfyUI-VideoHelperSuite","source_url":"https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite"},{"type":"has_code","target_id":"github:Kosinkadink:ComfyUI-Advanced-ControlNet","source_url":"https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet"},{"type":"has_code","target_id":"github:Fannovel16:comfyui_controlnet_aux","source_url":"https://github.com/Fannovel16/comfyui_controlnet_aux"},{"type":"based_on_paper","target_id":"arxiv:2403.12706","source_url":"https://arxiv.org/abs/2403.12706"}]', NULL, 'creativeml-openrail-m', 'approved', 64.9, '331318cf442d6425c2dd5b2f9cbe87ef', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-microsoft-Phi-3-vision-128k-instruct', 'huggingface--microsoft--phi-3-vision-128k-instruct', 'Phi-3-vision-128k-instruct', 'microsoft', '--- license: mit license_link: https://huggingface.co/microsoft/Phi-3-vision-128k-instruct/resolve/main/LICENSE language: - multilingual pipeline_tag: text-generation tags: - nlp - code - vision inference: parameters: temperature: 0.7 widget: - messages: - role: user content: <|image_1|>Can you describe what you see in the image? ---  **Phi-3.5**: [[mini-instruct]](https://huggingface.co/microsoft/Phi-3.5-mini-instruct); [[MoE-instruct]](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct...', '["transformers","safetensors","phi3_v","text-generation","nlp","code","vision","conversational","custom_code","multilingual","license:mit","region:us"]', 'text-generation', 970, 17715, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Phi-3-vision-128k-instruct/resolve/main/LICENSE\n\nlanguage:\n- multilingual\npipeline_tag: text-generation\ntags:\n- nlp\n- code\n- vision\ninference:\n  parameters:\n    temperature: 0.7\nwidget:\n  - messages:\n      - role: user\n        content: <|image_1|>Can you describe what you see in the image?\n---\n **Phi-3.5**: [[mini-instruct]](https://huggingface.co/microsoft/Phi-3.5-mini-instruct); [[MoE-instruct]](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct) ; [[vision-instruct]](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)\n\n## Model Summary\n\nThe Phi-3-Vision-128K-Instruct is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision.  The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\nResources and Technical Documentation:\n\n+ [Phi-3 Microsoft Blog](https://aka.ms/Phi-3Build2024)\n+ [Phi-3 Technical Report](https://aka.ms/phi3-tech-report)\n+ [Phi-3 on Azure AI Studio](https://aka.ms/try-phi3vision)\n+ [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook)\n\n\n|         | Short Context | Long Context |\n| ------- | ------------- | ------------ |\n| Mini    | 4K [[HF]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-onnx) ; [[GGUF]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx)|\n| Small   | 8K [[HF]](https://huggingface.co/microsoft/Phi-3-small-8k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-small-8k-instruct-onnx-cuda) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-small-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-small-128k-instruct-onnx-cuda)|\n| Medium  | 4K [[HF]](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct-onnx-cuda) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct-onnx-cuda)|\n| Vision  |  | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct-onnx-cuda)|\n\n## Intended Uses\n\n**Primary use cases**\n\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications with visual and text input capabilities which require \n\n1) memory/compute constrained environments;\n2) latency bound scenarios;\n3) general image understanding;\n4) OCR;\n5) chart and table understanding.\n\nOur model is designed to accelerate research on efficient language and multimodal models, for use as a building block for generative AI powered features.\n\n**Use case considerations**\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. \nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case. \n\nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\n\n## How to Use\n\nPhi-3-Vision-128K-Instruct has been integrated in the development version (4.40.2) of `transformers`. Until the official version is released through `pip`, ensure that you are doing one of the following:\n* When loading the model, ensure that `trust_remote_code=True` is passed as an argument of the `from_pretrained()` function.\n\n* Update your local `transformers` to the development version: `pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers`. The previous command is an alternative to cloning and installing from the source.\n\nThe current `transformers` version can be verified with: `pip list | grep transformers`.\n\nExamples of required packages:\n```\nflash_attn==2.5.8\nnumpy==1.24.4\nPillow==10.3.0\nRequests==2.31.0\ntorch==2.3.0\ntorchvision==0.18.0\ntransformers==4.40.2\n```\n\nPhi-3-Vision-128K-Instruct is also available in [Azure AI Studio](https://aka.ms/phi3-azure-ai).\n\n### Chat Format\n\nGiven the nature of the training data, the Phi-3-Vision-128K-Instruct model is best suited for a single image input wih prompts using the chat format as follows. \nYou can provide the prompt as a single image with a generic template as follow:\n```markdown\n<|user|>\n<|image_1|>\n{prompt}<|end|>\n<|assistant|>\n \n```\n\nwhere the model generates the text after `<|assistant|>` . In case of multi-turn conversation, the prompt can be formatted as follows:\n\n```markdown\n<|user|>\n<|image_1|>\n{prompt_1}<|end|>\n<|assistant|>\n{response_1}<|end|>\n<|user|>\n{prompt_2}<|end|>\n<|assistant|>\n \n```\n\n### Sample inference code\n\nThis code snippets show how to get quickly started with running the model on a GPU:\n\n```python\nfrom PIL import Image \nimport requests \nfrom transformers import AutoModelForCausalLM \nfrom transformers import AutoProcessor \n\nmodel_id = "microsoft/Phi-3-vision-128k-instruct" \n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map="cuda", trust_remote_code=True, torch_dtype="auto", _attn_implementation=''flash_attention_2'') # use _attn_implementation=''eager'' to disable flash attention\n\nprocessor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \n\nmessages = [ \n    {"role": "user", "content": "<|image_1|>\nWhat is shown in this image?"}, \n    {"role": "assistant", "content": "The chart displays the percentage of respondents who agree with various statements about their preparedness for meetings. It shows five categories: ''Having clear and pre-defined goals for meetings'', ''Knowing where to find the information I need for a meeting'', ''Understanding my exact role and responsibilities when I''m invited'', ''Having tools to manage admin tasks like note-taking or summarization'', and ''Having more focus time to sufficiently prepare for meetings''. Each category has an associated bar indicating the level of agreement, measured on a scale from 0% to 100%."}, \n    {"role": "user", "content": "Provide insightful questions to spark discussion."} \n] \n\nurl = "https://assets-c4akfrf5b4d3f4b7.z01.azurefd.net/assets/2024/04/BMDataViz_661fb89f3845e.png" \nimage = Image.open(requests.get(url, stream=True).raw) \n\nprompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\ninputs = processor(prompt, [image], return_tensors="pt").to("cuda:0") \n\ngeneration_args = { \n    "max_new_tokens": 500, \n    "temperature": 0.0, \n    "do_sample": False, \n} \n\ngenerate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n\n# remove input tokens \ngenerate_ids = generate_ids[:, inputs[''input_ids''].shape[1]:]\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \n\nprint(response) \n```\n\nAdditional basic examples are provided [here](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct/blob/main/sample_inference.py).\n\n### How to finetune?\nWe recommend user to take a look at the [Phi-3 CookBook finetuning recipe for Vision](https://github.com/microsoft/Phi-3CookBook/blob/main/md/04.Fine-tuning/FineTuning_Vision.md)\n\n\n## Responsible AI Considerations\n\nLike other models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:   \n\n+ Quality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.    \n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.  \n+ Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.  \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.   \n+ Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as "typing, math, random, collections, datetime, itertools". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.      \n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:  \n\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n+ Identification of individuals: models with vision capabilities may have the potential to uniquely identify individuals in images. Safety post-training steers the model to refuse such requests, but developers should consider and implement, as appropriate, additional mitigations or user consent flows as required in their respective jurisdiction, (e.g., building measures to blur faces in image inputs before processing.\n  \n## Training\n\n### Model\n\n* Architecture: Phi-3-Vision-128K-Instruct has 4.2B parameters and contains image encoder, connector, projector, and Phi-3 Mini language model.\n* Inputs: Text and Image. Its best suited for prompts using the chat format. \n* Context length: 128K tokens\n* GPUs: 512 H100-80G\n* Training time: 1.5 days\n* Training data: 500B vision and text tokens\n* Outputs: Generated text in response to the input\n* Dates: Our models were trained between February and April 2024\n* Status: This is a static model trained on an offline text dataset with cutoff date Mar 15, 2024. Future versions of the tuned models may be released as we improve models.\n* Release Type: Open weight release\n* Release dates: The model weight is released on May 21, 2024.\n\n### Datasets\n\nOur training data includes a wide variety of sources, and is a combination of \n\n1) publicly available documents filtered rigorously for quality, selected high-quality educational data and code;\n2) selected high-quality image-text interleave;\n3) newly created synthetic, textbook-like data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.), newly created image data, e.g., chart/table/diagram/slides;\n4) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nThe data collection process involved sourcing information from publicly available documents, with a meticulous approach to filtering out undesirable documents and images. To safeguard privacy, we carefully filtered various image and text data sources to remove or scrub any potentially personal data from the training data.\n \nMore details can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).\n\n## Benchmarks\n\nTo understand the capabilities, we compare Phi-3-Vision-128K-Instruct with a set of models over a variety of zero-shot benchmarks using our internal benchmark platform.\n\n|Benchmark|Phi-3 Vision-128K-In|LlaVA-1.6 Vicuna-7B|QWEN-VL Chat|Llama3-Llava-Next-8B|Claude-3 Haiku|Gemini 1.0 Pro V|GPT-4V-Turbo|\n|---------|---------------------|------------------|------------|--------------------|--------------|----------------|------------|\n|MMMU|40.4|34.2|39.0|36.4|40.7|42.0|55.5|\n|MMBench|80.5|76.3|75.8|79.4|62.4|80.0|86.1|\n|ScienceQA|90.8|70.6|67.2|73.7|72.0|79.7|75.7|\n|MathVista|44.5|31.5|29.4|34.8|33.2|35.0|47.5|\n|InterGPS|38.1|20.5|22.3|24.6|32.1|28.6|41.0|\n|AI2D|76.7|63.1|59.8|66.9|60.3|62.8|74.7|\n|ChartQA|81.4|55.0|50.9|65.8|59.3|58.0|62.3|\n|TextVQA|70.9|64.6|59.4|55.7|62.7|64.7|68.1|\n|POPE|85.8|87.2|82.6|87.0|74.4|84.2|83.7|\n\n\n## Software\n\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [Transformers](https://github.com/huggingface/transformers)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n## Hardware\nNote that by default, the Phi-3-Vision-128K model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\n* NVIDIA A100\n* NVIDIA A6000\n* NVIDIA H100\n\n## License\n\nThe model is licensed under the [MIT license](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct/resolve/main/LICENSE).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow[Microsofts Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-partys policies.\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":4146621440,"storage_bytes":16586661776,"files_count":20,"spaces_count":47,"gated":false,"private":false,"config":{"architectures":["Phi3VForCausalLM"],"auto_map":{"AutoConfig":"configuration_phi3_v.Phi3VConfig","AutoModelForCausalLM":"modeling_phi3_v.Phi3VForCausalLM"},"model_type":"phi3_v","tokenizer_config":{"bos_token":"<s>","chat_template":"{% for message in messages %}{{''<|'' + message[''role''] + ''|>'' + ''\n'' + message[''content''] + ''<|end|>\n'' }}{% endfor %}{% if add_generation_prompt and messages[-1][''role''] != ''assistant'' %}{{- ''<|assistant|>\n'' -}}{% endif %}","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:microsoft:Phi-3CookBook","source_url":"https://github.com/microsoft/Phi-3CookBook"},{"type":"has_code","target_id":"github:huggingface:transformers`.","source_url":"https://github.com/huggingface/transformers`."},{"type":"has_code","target_id":"github:microsoft:Phi-3CookBook","source_url":"https://github.com/microsoft/Phi-3CookBook"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:HazyResearch:flash-attention","source_url":"https://github.com/HazyResearch/flash-attention"}]', NULL, 'MIT', 'approved', 79.9, '3e8385944f76437cee916a91bce2c7ba', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-CompVis-stable-diffusion', 'huggingface--compvis--stable-diffusion', 'stable-diffusion', 'CompVis', '--- license: creativeml-openrail-m tags: - stable-diffusion - text-to-image inference: false --- Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. This model card gives an overview of all available model checkpoints. For more in-detail model cards, please have a look at the model repositories listed under Model Access. For the first version 4 model checkpoints are released. *Higher* versions have been trained for long...', '["stable-diffusion","text-to-image","arxiv:2207.12598","license:creativeml-openrail-m","region:us"]', 'text-to-image', 967, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/CompVis/stable-diffusion","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- text-to-image\ninference: false\n---\n# Stable Diffusion\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nThis model card gives an overview of all available model checkpoints. For more in-detail model cards, please have a look at the model repositories listed under [Model Access](#model-access).\n\n## Stable Diffusion Version 1\n\nFor the first version 4 model checkpoints are released.\n*Higher* versions have been trained for longer and are thus usually better in terms of image generation quality then *lower* versions. More specifically: \n\n- **stable-diffusion-v1-1**: The checkpoint is randomly initialized and has been trained on 237,000 steps at resolution `256x256` on [laion2B-en](https://huggingface.co/datasets/laion/laion2B-en).\n  194,000 steps at resolution `512x512` on [laion-high-resolution](https://huggingface.co/datasets/laion/laion-high-resolution) (170M examples from LAION-5B with resolution `>= 1024x1024`).\n- **stable-diffusion-v1-2**: The checkpoint resumed training from `stable-diffusion-v1-1`.\n  515,000 steps at resolution `512x512` on "laion-improved-aesthetics" (a subset of laion2B-en,\nfiltered to images with an original size `>= 512x512`, estimated aesthetics score `> 5.0`, and an estimated watermark probability `< 0.5`. The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an [improved aesthetics estimator](https://github.com/christophschuhmann/improved-aesthetic-predictor)).\n- **stable-diffusion-v1-3**: The checkpoint resumed training from `stable-diffusion-v1-2`. 195,000 steps at resolution `512x512` on "laion-improved-aesthetics" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598)\n- **stable-diffusion-v1-4**: The checkpoint resumed training from `stable-diffusion-v1-2`. 195,000 steps at resolution `512x512` on "laion-improved-aesthetics" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n- [**`stable-diffusion-v1-4`**](https://huggingface.co/CompVis/stable-diffusion-v1-4) Resumed from `stable-diffusion-v1-2`.225,000 steps at resolution `512x512` on "laion-aesthetics v2 5+" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\n### Model Access\n\nEach checkpoint can be used both with Hugging Face''s [  Diffusers library](https://github.com/huggingface/diffusers) or the original [Stable Diffusion GitHub repository](https://github.com/CompVis/stable-diffusion). Note that you have to *"click-request"* them on each respective model repository.\n\n| **[''s  Diffusers library](https://github.com/huggingface/diffusers)**     | **[Stable Diffusion GitHub repository](https://github.com/CompVis/stable-diffusion)** |\n| ----------- | ----------- |\n| [`stable-diffusion-v1-1`](https://huggingface.co/CompVis/stable-diffusion-v1-1)      | [`stable-diffusion-v-1-1-original`](https://huggingface.co/CompVis/stable-diffusion-v-1-1-original)       |\n| [`stable-diffusion-v1-2`](https://huggingface.co/CompVis/stable-diffusion-v1-2)   | [`stable-diffusion-v-1-2-original`](https://huggingface.co/CompVis/stable-diffusion-v-1-2-original)        |\n| [`stable-diffusion-v1-3`](https://huggingface.co/CompVis/stable-diffusion-v1-3)   | [`stable-diffusion-v-1-3-original`](https://huggingface.co/CompVis/stable-diffusion-v-1-3-original)        |\n| [`stable-diffusion-v1-4`](https://huggingface.co/CompVis/stable-diffusion-v1-4)   | [`stable-diffusion-v-1-4-original`](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original)        |\n\n### Demo\n\nTo quickly try out the model, you can try out the [Stable Diffusion Space](https://huggingface.co/spaces/stabilityai/stable-diffusion).\n\n### License\n\n[The CreativeML OpenRAIL M license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) is an [Open RAIL M license](https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses), adapted from the work that [BigScience](https://bigscience.huggingface.co/) and [the RAIL Initiative](https://www.licenses.ai/) are jointly carrying in the area of responsible AI licensing. See also [the article about the BLOOM Open RAIL license](https://bigscience.huggingface.co/blog/the-bigscience-rail-license) on which our license is based.\n\n## Citation\n\n```bibtex\n    @InProceedings{Rombach_2022_CVPR,\n        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},\n        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n        month     = {June},\n        year      = {2022},\n        pages     = {10684-10695}\n    }\n```\n\n*This model card was written by: Robin Rombach and Patrick Esser and is based on the [DALL-E Mini model card](https://huggingface.co/dalle-mini/dalle-mini).*\n', '{"pipeline_tag":"text-to-image","library_name":null,"framework":null,"params":null,"storage_bytes":2132781863,"files_count":5,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:christophschuhmann:improved-aesthetic-predictor","source_url":"https://github.com/christophschuhmann/improved-aesthetic-predictor"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:CompVis:stable-diffusion","source_url":"https://github.com/CompVis/stable-diffusion"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:CompVis:stable-diffusion","source_url":"https://github.com/CompVis/stable-diffusion"},{"type":"based_on_paper","target_id":"arxiv:2207.12598","source_url":"https://arxiv.org/abs/2207.12598"}]', NULL, 'creativeml-openrail-m', 'approved', 64.9, 'f561d8df23c833c0c10f2a328aa79254', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-answerdotai-ModernBERT-base', 'huggingface--answerdotai--modernbert-base', 'ModernBERT-base', 'answerdotai', '--- library_name: transformers license: apache-2.0 language: - en tags: - fill-mask - masked-lm - long-context - modernbert pipeline_tag: fill-mask inference: false --- 1. Model Summary 2. Usage 3. Evaluation 4. Limitations 5. Training 6. License 7. Citation ModernBERT is a modernized bidirectional encoder-only Transformer model (BERT-style) pre-trained on 2 trillion tokens of English and code data with a native context length of up to 8,192 tokens. ModernBERT leverages recent architectural i...', '["transformers","pytorch","onnx","safetensors","modernbert","fill-mask","masked-lm","long-context","en","arxiv:2412.13663","license:apache-2.0","deploy:azure","region:us"]', 'fill-mask', 964, 812999, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/answerdotai/ModernBERT-base","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\nlanguage:\n- en\ntags:\n- fill-mask\n- masked-lm\n- long-context\n- modernbert\npipeline_tag: fill-mask\ninference: false\n---\n\n# ModernBERT\n\n## Table of Contents\n1. [Model Summary](#model-summary)\n2. [Usage](#Usage)\n3. [Evaluation](#Evaluation)\n4. [Limitations](#limitations)\n5. [Training](#training)\n6. [License](#license)\n7. [Citation](#citation)\n\n## Model Summary\n\nModernBERT is a modernized bidirectional encoder-only Transformer model (BERT-style) pre-trained on 2 trillion tokens of English and code data with a native context length of up to 8,192 tokens. ModernBERT leverages recent architectural improvements such as:\n\n- **Rotary Positional Embeddings (RoPE)** for long-context support.  \n- **Local-Global Alternating Attention** for efficiency on long inputs.  \n- **Unpadding and Flash Attention** for efficient inference.  \n\nModernBERTs native long context length makes it ideal for tasks that require processing long documents, such as retrieval, classification, and semantic search within large corpora. The model was trained on a large corpus of text and code, making it suitable for a wide range of downstream tasks, including code retrieval and hybrid (text + code) semantic search.\n\nIt is available in the following sizes:\n\n- [ModernBERT-base](https://huggingface.co/answerdotai/ModernBERT-base) - 22 layers, 149 million parameters\n- [ModernBERT-large](https://huggingface.co/answerdotai/ModernBERT-large) - 28 layers, 395 million parameters\n\nFor more information about ModernBERT, we recommend our [release blog post](https://huggingface.co/blog/modernbert) for a high-level overview, and our [arXiv pre-print](https://arxiv.org/abs/2412.13663) for in-depth information.\n\n*ModernBERT is a collaboration between [Answer.AI](https://answer.ai), [LightOn](https://lighton.ai), and friends.*\n\n## Usage\n\nYou can use these models directly with the `transformers` library starting from v4.48.0:\n\n```sh\npip install -U transformers>=4.48.0\n```\n\nSince ModernBERT is a Masked Language Model (MLM), you can use the `fill-mask` pipeline or load it via `AutoModelForMaskedLM`. To use ModernBERT for downstream tasks like classification, retrieval, or QA, fine-tune it following standard BERT fine-tuning recipes.\n\n** If your GPU supports it, we recommend using ModernBERT with Flash Attention 2 to reach the highest efficiency. To do so, install Flash Attention as follows, then use the model as normal:**\n\n```bash\npip install flash-attn\n```\n\nUsing `AutoModelForMaskedLM`:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\nmodel_id = "answerdotai/ModernBERT-base"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForMaskedLM.from_pretrained(model_id)\n\ntext = "The capital of France is [MASK]."\ninputs = tokenizer(text, return_tensors="pt")\noutputs = model(**inputs)\n\n# To get predictions for the mask:\nmasked_index = inputs["input_ids"][0].tolist().index(tokenizer.mask_token_id)\npredicted_token_id = outputs.logits[0, masked_index].argmax(axis=-1)\npredicted_token = tokenizer.decode(predicted_token_id)\nprint("Predicted token:", predicted_token)\n# Predicted token:  Paris\n```\n\nUsing a pipeline:\n\n```python\nimport torch\nfrom transformers import pipeline\nfrom pprint import pprint\n\npipe = pipeline(\n    "fill-mask",\n    model="answerdotai/ModernBERT-base",\n    torch_dtype=torch.bfloat16,\n)\n\ninput_text = "He walked to the [MASK]."\nresults = pipe(input_text)\npprint(results)\n```\n\n**Note:** ModernBERT does not use token type IDs, unlike some earlier BERT models. Most downstream usage is identical to standard BERT models on the Hugging Face Hub, except you can omit the `token_type_ids` parameter.\n\n## Evaluation\n\nWe evaluate ModernBERT across a range of tasks, including natural language understanding (GLUE), general retrieval (BEIR), long-context retrieval (MLDR), and code retrieval (CodeSearchNet and StackQA).\n\n**Key highlights:**\n- On GLUE, ModernBERT-base surpasses other similarly-sized encoder models, and ModernBERT-large is second only to Deberta-v3-large.\n- For general retrieval tasks, ModernBERT performs well on BEIR in both single-vector (DPR-style) and multi-vector (ColBERT-style) settings.\n- Thanks to the inclusion of code data in its training mixture, ModernBERT as a backbone also achieves new state-of-the-art code retrieval results on CodeSearchNet and StackQA.\n\n### Base Models\n\n| Model       | IR (DPR)     | IR (DPR)     | IR (DPR)     | IR (ColBERT)  | IR (ColBERT)  | NLU  | Code | Code |\n|-------------|--------------|--------------|--------------|---------------|---------------|------|------|------|\n|             | BEIR         | MLDR_OOD     | MLDR_ID      | BEIR          | MLDR_OOD      | GLUE | CSN  | SQA  |\n| BERT        | 38.9         | 23.9         | 32.2         | 49.0          | 28.1          | 84.7 | 41.2 | 59.5 |\n| RoBERTa     | 37.7         | 22.9         | 32.8         | 48.7          | 28.2          | 86.4 | 44.3 | 59.6 |\n| DeBERTaV3   | 20.2         | 5.4          | 13.4         | 47.1          | 21.9          | 88.1 | 17.5 | 18.6 |\n| NomicBERT   | 41.0         | 26.7         | 30.3         | 49.9          | 61.3          | 84.0 | 41.6 | 61.4 |\n| GTE-en-MLM  | 41.4         | **34.3**    |**44.4**   | 48.2          | 69.3          | 85.6 | 44.9 | 71.4 |\n| ModernBERT  | **41.6**    | 27.4         | 44.0         | **51.3**    | **80.2**      | **88.4** | **56.4** |**73.6**|\n\n---\n\n### Large Models\n\n| Model       | IR (DPR)     | IR (DPR)     | IR (DPR)     | IR (ColBERT)  | IR (ColBERT)  | NLU  | Code | Code |\n|-------------|--------------|--------------|--------------|---------------|---------------|------|------|------|\n|             | BEIR         | MLDR_OOD     | MLDR_ID      | BEIR          | MLDR_OOD      | GLUE | CSN  | SQA  |\n| BERT        | 38.9         | 23.3         | 31.7         | 49.5          | 28.5          | 85.2 | 41.6 | 60.8 |\n| RoBERTa     | 41.4         | 22.6         | 36.1         | 49.8          | 28.8          | 88.9 | 47.3 | 68.1 |\n| DeBERTaV3   | 25.6         | 7.1          | 19.2         | 46.7          | 23.0          | **91.4**| 21.2 | 19.7 |\n| GTE-en-MLM  | 42.5         | **36.4**    | **48.9**  | 50.7          | 71.3          | 87.6 | 40.5 | 66.9 |\n| ModernBERT  | **44.0**    | 34.3         | 48.6         | **52.4**     | **80.4**     | 90.4 |**59.5** |**83.9**|\n\n*Table 1: Results for all models across an overview of all tasks. CSN refers to CodeSearchNet and SQA to StackQA. MLDRID refers to in-domain (fine-tuned on the training set) evaluation, and MLDR_OOD to out-of-domain.*\n\nModernBERTs strong results, coupled with its efficient runtime on long-context inputs, demonstrate that encoder-only models can be significantly improved through modern architectural choices and extensive pretraining on diversified data sources.\n\n\n## Limitations\n\nModernBERTs training data is primarily English and code, so performance may be lower for other languages. While it can handle long sequences efficiently, using the full 8,192 tokens window may be slower than short-context inference. Like any large language model, ModernBERT may produce representations that reflect biases present in its training data. Verify critical or sensitive outputs before relying on them.\n\n## Training\n\n- Architecture: Encoder-only, Pre-Norm Transformer with GeGLU activations.\n- Sequence Length: Pre-trained up to 1,024 tokens, then extended to 8,192 tokens.\n- Data: 2 trillion tokens of English text and code.\n- Optimizer: StableAdamW with trapezoidal LR scheduling and 1-sqrt decay.\n- Hardware: Trained on 8x H100 GPUs.\n\nSee the paper for more details.\n\n## License\n\nWe release the ModernBERT model architectures, model weights, training codebase under the Apache 2.0 license.\n\n## Citation\n\nIf you use ModernBERT in your work, please cite:\n\n```\n@misc{modernbert,\n      title={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference}, \n      author={Benjamin Warner and Antoine Chaffin and Benjamin Clavi and Orion Weller and Oskar Hallstrm and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},\n      year={2024},\n      eprint={2412.13663},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.13663}, \n}\n```', '{"pipeline_tag":"fill-mask","library_name":"transformers","framework":"transformers","params":149655232,"storage_bytes":5084307056,"files_count":16,"spaces_count":64,"gated":false,"private":false,"config":{"architectures":["ModernBertForMaskedLM"],"model_type":"modernbert","tokenizer_config":{"cls_token":"[CLS]","mask_token":"[MASK]","pad_token":"[PAD]","sep_token":"[SEP]","unk_token":"[UNK]"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2412.13663","source_url":"https://arxiv.org/abs/2412.13663"}]', NULL, 'Apache-2.0', 'approved', 64.8, '4c2898c4d434bfa01bb2712550a1cda9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-black-forest-labs-FLUX.1-Fill-dev', 'huggingface--black-forest-labs--flux.1-fill-dev', 'FLUX.1-Fill-dev', 'black-forest-labs', '', '["diffusers","safetensors","image-generation","flux","diffusion-single-file","en","license:other","diffusers:fluxfillpipeline","region:us"]', 'other', 962, 152935, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":null,"library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":58052037720,"files_count":28,"spaces_count":79,"gated":"auto","private":false,"config":{"diffusers":{"_class_name":"FluxFillPipeline"}}}', '[]', '[]', NULL, 'Other', 'approved', 39.8, '42e7a7f191345963c4f74a32f6b7fdb5', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tencent-SRPO', 'huggingface--tencent--srpo', 'SRPO', 'tencent', '--- library_name: diffusers license: other license_name: tencent-hunyuan-community license_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline_tag: text-to-image --- <div align=center style=font-family: charter;> <h1 align="center">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1> <div align="center"> <a href=''https://arxiv.org/abs/2509.06942''><img src=''https://img.shields.io/badge/ArXiv-red?logo=arxiv''></a> &nbsp; <a href=''h...', '["diffusers","safetensors","text-to-image","arxiv:2509.06942","license:other","region:us"]', 'text-to-image', 961, 3236, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tencent/SRPO","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: diffusers\nlicense: other\nlicense_name: tencent-hunyuan-community\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\npipeline_tag: text-to-image\n---\n\n<div align=center style=font-family: charter;>\n<h1 align="center">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1>\n<div align="center">\n  <a href=''https://arxiv.org/abs/2509.06942''><img src=''https://img.shields.io/badge/ArXiv-red?logo=arxiv''></a>  &nbsp;\n  <a href=''https://github.com/Tencent-Hunyuan/SRPO''><img src=''https://img.shields.io/badge/_Code-SRPO-181717?color=121717&logo=github&logoColor=whitee''></a> &nbsp; \n  <a href=''https://tencent.github.io/srpo-project-page/''><img src=''https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue''></a> &nbsp;\n</div>\n<div align="center">\n  Xiangwei Shen<sup>1,2*</sup>,\n  <a href="https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&hl=zh-CN" target="_blank"><b>Zhimin Li</b></a><sup>1*</sup>,\n  <a href="https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ" target="_blank"><b>Zhantao Yang</b></a><sup>1</sup>, \n  <a href="https://shiyi-zh0408.github.io/" target="_blank"><b>Shiyi Zhang</b></a><sup>3</sup>,\n  Yingfang Zhang<sup>1</sup>,\n  Donghao Li<sup>1</sup>,\n  <br>\n  <a href="https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en" target="_blank"><b>Chunyu Wang</b></a><sup>1</sup>,\n  <a href="https://openreview.net/profile?id=%7EQinglin_Lu2" target="_blank"><b>Qinglin Lu</b></a><sup>1</sup>,\n  <a href="https://andytang15.github.io" target="_blank"><b>Yansong Tang</b></a><sup>3,</sup>\n</div>\n<div align="center">\n  <sup>1</sup>Hunyuan, Tencent\n  <br>\n  <sup>2</sup>School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen\n  <br>\n  <sup>3</sup>Shenzhen International Graduate School, Tsinghua University\n  <br>\n  <sup>*</sup>Equal contribution\n  <sup></sup>Corresponding author\n</div>\n\n\n\n## Abstract\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\n\n## Acknowledgement\n\nWe sincerely appreciate contributions from the research community to this project. Below are quantized versions developed by fellow researchers.\n\n1. 8bit(fp8_e4m3fn/Q8_0) version by wikeeyang: https://huggingface.co/wikeeyang/SRPO-Refine-Quantized-v1.0\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6645835a2b57c619a19cc0c4/BATJ0bW_0QPhkN5WY0Q1H.png)\n\n2. bf16 version by rockerBOO: https://huggingface.co/rockerBOO/flux.1-dev-SRPO\n3. GGUF version by befox: https://huggingface.co/befox/SRPO-GGUF\n\n Note: When loading weights in ComfyUI, avoid direct conversion of FP32 weights to FP8 format, as this may result in incomplete denoising. For official weights in this repository, FP32/BF16 loading is recommended.\n\n\n### Checkpoints\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\n##  Inference\n\n### Using ComfyUI\n\nYou can use it in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).\n\nLoad the following image in ComfyUI to get the workflow, or load the JSON file directly [SRPO-workflow](comfyui/SRPO-workflow.json):\n\nTip: The workflow JSON info was added to the image file.\n\n![Example](comfyui/SRPO-workflow.png)\n\n### Quick start\n```bash\nfrom diffusers import FluxPipeline\nfrom safetensors.torch import load_file\n\nprompt=''The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere''\npipe = FluxPipeline.from_pretrained(''./data/flux'',\n        torch_dtype=torch.bfloat16,\n        use_safetensors=True\n    ).to("cuda")\nstate_dict = load_file("./srpo/diffusion_pytorch_model.safetensors")\npipe.transformer.load_state_dict(state_dict)\nimage = pipe(\n    prompt,\n    guidance_scale=3.5,\n    height=1024,\n    width=1024,\n    num_inference_steps=50,\n    max_sequence_length=512,\n    generator=generator\n).images[0]\n```\n### License\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\n## Citation\nIf you use SRPO for your research, please cite our paper:\n\n```bibtex\n@misc{shen2025directlyaligningdiffusiontrajectory,\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\n      year={2025},\n      eprint={2509.06942},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2509.06942}, \n}\n```', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":47610166557,"files_count":6,"spaces_count":8,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:Tencent-Hunyuan:SRPO","source_url":"https://github.com/Tencent-Hunyuan/SRPO"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:SRPO''><img","source_url":"https://github.com/Tencent-Hunyuan/SRPO''><img"},{"type":"has_code","target_id":"github:tgxs002:HPSv2","source_url":"https://github.com/tgxs002/HPSv2"},{"type":"has_code","target_id":"github:comfyanonymous:ComfyUI","source_url":"https://github.com/comfyanonymous/ComfyUI"},{"type":"based_on_paper","target_id":"arxiv:2509.06942","source_url":"https://arxiv.org/abs/2509.06942"}]', NULL, 'Other', 'approved', 64.8, 'fcb72bddaef09fede8129637059a5bca', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-briaai-RMBG-2.0', 'huggingface--briaai--rmbg-2.0', 'RMBG-2.0', 'briaai', '', '["transformers","pytorch","onnx","safetensors","image-segmentation","remove background","background","background-removal","pytorch","vision","legal liability","transformers.js","custom_code","license:other","region:us"]', 'image-segmentation', 960, 260474, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/briaai/RMBG-2.0","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-segmentation","library_name":"transformers","framework":"transformers","params":220700242,"storage_bytes":5896565704,"files_count":19,"spaces_count":98,"gated":"auto","private":false,"config":{"architectures":["BiRefNet"],"auto_map":{"AutoConfig":"BiRefNet_config.BiRefNetConfig","AutoModelForImageSegmentation":"birefnet.BiRefNet"}}}', '[]', '[]', NULL, 'Other', 'approved', 59.8, 'cce1130538b8c04b0ab88304843b3780', NULL, 'https://huggingface.co/briaai/RMBG-2.0/resolve/main/diagram1.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-briaai-RMBG-2.0 from https://huggingface.co/briaai/RMBG-2.0/resolve/main/diagram1.png
HTTP error: 401 Unauthorized

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-stable-video-diffusion-img2vid-xt-1-1', 'huggingface--stabilityai--stable-video-diffusion-img2vid-xt-1-1', 'stable-video-diffusion-img2vid-xt-1-1', 'stabilityai', '', '["diffusers","safetensors","image-to-video","license:other","diffusers:stablevideodiffusionpipeline","region:us"]', 'image-to-video', 959, 27046, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-to-video","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":18313832368,"files_count":17,"spaces_count":36,"gated":"auto","private":false,"config":{"diffusers":{"_class_name":"StableVideoDiffusionPipeline"}}}', '[]', '[]', NULL, 'Other', 'approved', 39.8, 'cb5de6b7948873cab9b7839af54318f9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nitrosocke-mo-di-diffusion', 'huggingface--nitrosocke--mo-di-diffusion', 'mo-di-diffusion', 'nitrosocke', '--- license: creativeml-openrail-m tags: - stable-diffusion - text-to-image --- **Mo Di Diffusion** This is the fine-tuned Stable Diffusion 1.5 model trained on screenshots from a popular animation studio. Use the tokens **_modern disney style_** in your prompts for the effect. **If you enjoy my work, please consider supporting me** **Videogame Characters rendered with the model:** !Videogame Samples **Animal Characters rendered with the model:** !Animal Samples **Cars and Landscapes rendered...', '["diffusers","stable-diffusion","text-to-image","license:creativeml-openrail-m","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 957, 1235, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nitrosocke/mo-di-diffusion","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- text-to-image\n---\n**Mo Di Diffusion**\n\nThis is the fine-tuned Stable Diffusion 1.5 model trained on screenshots from a popular animation studio.\nUse the tokens **_modern disney style_** in your prompts for the effect.\n\n**If you enjoy my work, please consider supporting me** \n[![Become A Patreon](https://badgen.net/badge/become/a%20patron/F96854)](https://patreon.com/user?u=79196446)\n\n**Videogame Characters rendered with the model:**\n![Videogame Samples](https://huggingface.co/nitrosocke/mo-di-diffusion/resolve/main/modi-samples-01s.jpg)\n**Animal Characters rendered with the model:**\n![Animal Samples](https://huggingface.co/nitrosocke/mo-di-diffusion/resolve/main/modi-samples-02s.jpg)\n**Cars and Landscapes rendered with the model:**\n![Misc. Samples](https://huggingface.co/nitrosocke/mo-di-diffusion/resolve/main/modi-samples-03s.jpg)\n#### Prompt and settings for Lara Croft:\n**modern disney lara croft**\n_Steps: 50, Sampler: Euler a, CFG scale: 7, Seed: 3940025417, Size: 512x768_\n\n#### Prompt and settings for the Lion:\n**modern disney (baby lion) Negative prompt: person human**\n_Steps: 50, Sampler: Euler a, CFG scale: 7, Seed: 1355059992, Size: 512x512_\n\nThis model was trained using the diffusers based dreambooth training by ShivamShrirao using prior-preservation loss and the _train-text-encoder_ flag in 9.000 steps.\n\n###  Diffusers\n\nThis model can be used just like any other Stable Diffusion model. For more information,\nplease have a look at the [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\n\nYou can also export the model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX]().\n\n```python\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = "nitrosocke/mo-di-diffusion"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to("cuda")\n\nprompt = "a magical princess with golden hair, modern disney style"\nimage = pipe(prompt).images[0]\n\nimage.save("./magical_princess.png")\n```\n\n# Gradio & Colab\n\nWe also support a [Gradio](https://github.com/gradio-app/gradio) Web UI and Colab with Diffusers to run fine-tuned Stable Diffusion models:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/anzorq/finetuned_diffusion)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1j5YvfMZoGdDGdj3O3xRU1m4ujKYsElZO?usp=sharing)\n\n## License\n\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies: \n\n1. You can''t use the model to deliberately produce nor share illegal or harmful outputs or content \n2. The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":31105706419,"files_count":21,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"}]', NULL, 'creativeml-openrail-m', 'approved', 64.8, '3f2582c65668ac75a0401d9a0160560c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-control-lora', 'huggingface--stabilityai--control-lora', 'control-lora', 'stabilityai', '--- tags: - text-to-image - stable-diffusion license: other language: - en --- By adding low-rank parameter efficient fine tuning to ControlNet, we introduce ***Control-LoRAs***. This approach offers a more efficient and compact method to bring model control to a wider variety of consumer GPUs. For each model below, you''ll find: - *Rank 256* files (reducing the original ControlNet models down to Control-LoRA models) and experimental - *Rank 128* files (reducing to model down to ) Each Control...', '["text-to-image","stable-diffusion","en","license:other","region:us"]', 'text-to-image', 953, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/control-lora","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- text-to-image\n- stable-diffusion\nlicense: other\nlanguage:\n- en\n---\n\n# Control-LoRA Model Card\n\n\n## Introduction\nBy adding low-rank parameter efficient fine tuning to ControlNet, we introduce ***Control-LoRAs***. This approach offers a more efficient and compact method to bring model control to a wider variety of consumer GPUs.\n\nFor each model below, you''ll find:\n\n- *Rank 256* files (reducing the original `4.7GB` ControlNet models down to `~738MB` Control-LoRA models) and experimental\n- *Rank 128* files (reducing to model down to `~377MB`)\n\nEach Control-LoRA has been trained on a diverse range of image concepts and aspect ratios.\n\n### MiDaS and ClipDrop Depth\n![canny](samples/depth-sample.jpeg)\n\nThis Control-LoRA utilizes a grayscale depth map for guided generation.\n\nDepth estimation is an image processing technique that determines the distance of objects in a scene, providing a depth map that highlights variations in proximity.\n\nThe model was trained on the depth results of `MiDaS dpt_beit_large_512`.\n\nIt was further finetuned on the `Portrait Depth Estimation` model available in the [ClipDrop API by Stability AI](https://clipdrop.co/apis/docs/portrait-depth-estimation).\n\n### Canny Edge\n![canny](samples/canny-sample.jpeg)\nCanny Edge Detection is an image processing technique that identifies abrupt changes in intensity to highlight edges in an image.\n\nThis Control-LoRA uses the edges from an image to generate the final image.\n\n### Photograph and Sketch Colorizer\n![photograph colorizer](samples/colorizer-sample.jpeg)\nThese two Control-LoRAs can be used to colorize images.\n\n*Recolor* is designed to colorize black and white photographs.\n\n*Sketch* is designed to color in drawings input as a white-on-black image (either hand-drawn, or created with a `pidi` edge model).\n\n### Revision\n![photograph colorizer](samples/revision-sample.jpeg)\nRevision is a novel approach of using images to prompt SDXL.\n\nIt uses pooled CLIP embeddings to produce images conceptually similar to the input. It can be used either in addition, or to replace text prompts.\n\nRevision also includes a blending function for combining multiple image or text concepts, as either positive or negative prompts.\n\n\n## Inference\n\nControl-LoRAs have been implemented into [ComfyUI](https://github.com/comfyanonymous/ComfyUI) and [StableSwarmUI](https://github.com/Stability-AI/StableSwarmUI)\n\nBasic ComfyUI workflows (using the base model only) are available in this HF repo. Custom nodes from Stability are [available here](https://github.com/Stability-AI/stability-ComfyUI-nodes).\n\n**Recolor example on ComfyUI:** ![comfyui recolor](samples/comfyui-recolor-example.jpeg)\n\n**Canny edge on StableSwarmUI:** ![swarmui recolor](samples/swarmui-canny-example.jpeg)', '{"pipeline_tag":"text-to-image","library_name":null,"framework":null,"params":null,"storage_bytes":8373480116,"files_count":29,"spaces_count":3,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:comfyanonymous:ComfyUI","source_url":"https://github.com/comfyanonymous/ComfyUI"},{"type":"has_code","target_id":"github:Stability-AI:StableSwarmUI","source_url":"https://github.com/Stability-AI/StableSwarmUI"},{"type":"has_code","target_id":"github:Stability-AI:stability-ComfyUI-nodes","source_url":"https://github.com/Stability-AI/stability-ComfyUI-nodes"}]', NULL, 'Other', 'approved', 64.8, '53f73ae37bbf6d12d945d41e522035b4', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-3.1-405B', 'huggingface--meta-llama--llama-3.1-405b', 'Llama-3.1-405B', 'meta-llama', '', '["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","en","de","fr","it","pt","hi","es","th","arxiv:2204.05149","license:llama3.1","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 948, 212771, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-3.1-405B","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":405853388800,"storage_bytes":4667909255120,"files_count":583,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","eos_token":"<|end_of_text|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2204.05149","source_url":"https://arxiv.org/abs/2204.05149"}]', NULL, 'llama3.1', 'approved', 39.8, '78001d53069f399380b0c895673a1d74', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mistralai-Mistral-Small-24B-Instruct-2501', 'huggingface--mistralai--mistral-small-24b-instruct-2501', 'Mistral-Small-24B-Instruct-2501', 'mistralai', '--- library_name: vllm language: - en - fr - de - es - it - pt - zh - ja - ru - ko license: apache-2.0 inference: false base_model: - mistralai/Mistral-Small-24B-Base-2501 extra_gated_description: >- If you want to learn more about how we process your personal data, please read our <a href="https://mistral.ai/terms/">Privacy Policy</a>. tags: - vllm --- Mistral Small 3 ( 2501 ) sets a new benchmark in the "small" Large Language Models category below 70B, boasting 24B parameters and achieving ...', '["vllm","safetensors","mistral","en","fr","de","es","it","pt","zh","ja","ru","ko","base_model:mistralai/mistral-small-24b-base-2501","license:apache-2.0","region:us"]', 'other', 948, 425939, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: vllm\nlanguage:\n- en\n- fr\n- de\n- es\n- it\n- pt\n- zh\n- ja\n- ru\n- ko\nlicense: apache-2.0\ninference: false\nbase_model:\n- mistralai/Mistral-Small-24B-Base-2501\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href="https://mistral.ai/terms/">Privacy Policy</a>.\ntags:\n- vllm\n---\n\n# Model Card for Mistral-Small-24B-Instruct-2501\n\nMistral Small 3 ( 2501 ) sets a new benchmark in the "small" Large Language Models category below 70B, boasting 24B parameters and achieving state-of-the-art capabilities comparable to larger models!  \nThis model is an instruction-fine-tuned version of the base model: [Mistral-Small-24B-Base-2501](https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501).\n\nMistral Small can be deployed locally and is exceptionally "knowledge-dense", fitting in a single RTX 4090 or a 32GB RAM MacBook once quantized.  \nPerfect for:\n- Fast response conversational agents.\n- Low latency function calling.\n- Subject matter experts via fine-tuning.\n- Local inference for hobbyists and organizations handling sensitive data.\n\nFor enterprises that need specialized capabilities (increased context, particular modalities, domain specific knowledge, etc.), we will be releasing commercial models beyond what Mistral AI contributes to the community.\n\nThis release demonstrates our commitment to open source, serving as a strong base model. \n\nLearn more about Mistral Small in our [blog post](https://mistral.ai/news/mistral-small-3/).\n\nModel developper: Mistral AI Team\n\n## Key Features\n- **Multilingual:** Supports dozens of languages, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch, and Polish.\n- **Agent-Centric:** Offers best-in-class agentic capabilities with native function calling and JSON outputting.\n- **Advanced Reasoning:** State-of-the-art conversational and reasoning capabilities.\n- **Apache 2.0 License:** Open license allowing usage and modification for both commercial and non-commercial purposes.\n- **Context Window:** A 32k context window.\n- **System Prompt:** Maintains strong adherence and support for system prompts.\n- **Tokenizer:** Utilizes a Tekken tokenizer with a 131k vocabulary size.\n\n## Benchmark results\n\n\n### Human evaluated benchmarks\n\n| Category | Gemma-2-27B | Qwen-2.5-32B | Llama-3.3-70B | Gpt4o-mini |\n|----------|-------------|--------------|---------------|------------|\n| Mistral is better | 0.536 | 0.496 | 0.192 | 0.200 |\n| Mistral is slightly better | 0.196 | 0.184 | 0.164 | 0.204 |\n| Ties | 0.052 | 0.060 | 0.236 | 0.160 |\n| Other is slightly better | 0.060 | 0.088 | 0.112 | 0.124 |\n| Other is better | 0.156 | 0.172 | 0.296 | 0.312 |\n\n**Note**:\n\n- We conducted side by side evaluations with an external third-party vendor, on a set of over 1k proprietary coding and generalist prompts.\n- Evaluators were tasked with selecting their preferred model response from anonymized generations produced by Mistral Small 3 vs another model.\n- We are aware that in some cases the benchmarks on human judgement starkly differ from publicly available benchmarks, but have taken extra caution in verifying a fair evaluation. We are confident that the above benchmarks are valid.\n\n### Publicly accesible benchmarks\n\n**Reasoning & Knowledge**\n\n| Evaluation | mistral-small-24B-instruct-2501 | gemma-2b-27b | llama-3.3-70b | qwen2.5-32b | gpt-4o-mini-2024-07-18 |\n|------------|---------------|--------------|---------------|---------------|-------------|\n| mmlu_pro_5shot_cot_instruct | 0.663 | 0.536 | 0.666 | 0.683 | 0.617 |\n| gpqa_main_cot_5shot_instruct | 0.453 | 0.344 | 0.531 | 0.404 | 0.377 |\n\n**Math & Coding**\n\n| Evaluation | mistral-small-24B-instruct-2501 | gemma-2b-27b | llama-3.3-70b | qwen2.5-32b | gpt-4o-mini-2024-07-18 |\n|------------|---------------|--------------|---------------|---------------|-------------|\n| humaneval_instruct_pass@1 | 0.848 | 0.732 | 0.854 | 0.909 | 0.890 |\n| math_instruct | 0.706 | 0.535 | 0.743 | 0.819 | 0.761 |\n\n**Instruction following**\n\n| Evaluation | mistral-small-24B-instruct-2501 | gemma-2b-27b | llama-3.3-70b | qwen2.5-32b | gpt-4o-mini-2024-07-18 |\n|------------|---------------|--------------|---------------|---------------|-------------|\n| mtbench_dev | 8.35 | 7.86 | 7.96 | 8.26 | 8.33 |\n| wildbench | 52.27 | 48.21 | 50.04 | 52.73 | 56.13 |\n| arena_hard | 0.873 | 0.788 | 0.840 | 0.860 | 0.897 |\n| ifeval | 0.829 | 0.8065 | 0.8835 | 0.8401 | 0.8499 |\n\n**Note**:\n\n- Performance accuracy on all benchmarks were obtained through the same internal evaluation pipeline - as such, numbers may vary slightly from previously reported performance\n([Qwen2.5-32B-Instruct](https://qwenlm.github.io/blog/qwen2.5/), [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct), [Gemma-2-27B-IT](https://huggingface.co/google/gemma-2-27b-it)). \n- Judge based evals such as Wildbench, Arena hard and MTBench were based on gpt-4o-2024-05-13.\n\n### Basic Instruct Template (V7-Tekken)\n\n```\n<s>[SYSTEM_PROMPT]<system prompt>[/SYSTEM_PROMPT][INST]<user message>[/INST]<assistant response></s>[INST]<user message>[/INST]\n```\n*`<system_prompt>`, `<user message>` and `<assistant response>` are placeholders.*\n\n***Please make sure to use [mistral-common](https://github.com/mistralai/mistral-common) as the source of truth***\n\n## Usage\n\nThe model can be used with the following frameworks;\n- [`vllm`](https://github.com/vllm-project/vllm): See [here](#vllm)\n- [`transformers`](https://github.com/huggingface/transformers): See [here](#transformers)\n\n### vLLM\n\nWe recommend using this model with the [vLLM library](https://github.com/vllm-project/vllm)\nto implement production-ready inference pipelines.\n\n**Note 1**: We recommond using a relatively low temperature, such as `temperature=0.15`.\n\n**Note 2**: Make sure to add a system prompt to the model to best tailer it for your needs. If you want to use the model as a general assistant, we recommend the following \nsystem prompt:\n\n```\nsystem_prompt = """You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\nYour knowledge base was last updated on 2023-10-01. The current date is 2025-01-30.\nWhen you''re not sure about some information, you say that you don''t have the information and don''t make up anything.\nIf the user''s question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \"What are some good restaurants around me?\" => \"Where are you?\" or \"When is the next flight to Tokyo\" => \"Where do you travel from?\")"""\n```\n\n**_Installation_**\n\nMake sure you install [`vLLM >= 0.6.4`](https://github.com/vllm-project/vllm/releases/tag/v0.6.4):\n\n```\npip install --upgrade vllm\n```\n\nAlso make sure you have [`mistral_common >= 1.5.2`](https://github.com/mistralai/mistral-common/releases/tag/v1.5.2) installed:\n\n```\npip install --upgrade mistral_common\n```\n\nYou can also make use of a ready-to-go [docker image](https://github.com/vllm-project/vllm/blob/main/Dockerfile) or on the [docker hub](https://hub.docker.com/layers/vllm/vllm-openai/latest/images/sha256-de9032a92ffea7b5c007dad80b38fd44aac11eddc31c435f8e52f3b7404bbf39).\n\n#### Server\n\nWe recommand that you use Mistral-Small-24B-Instruct-2501 in a server/client setting. \n\n1. Spin up a server:\n\n```\nvllm serve mistralai/Mistral-Small-24B-Instruct-2501 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice\n```\n\n**Note:** Running Mistral-Small-24B-Instruct-2501 on GPU requires ~55 GB of GPU RAM in bf16 or fp16. \n\n\n2. To ping the client you can use a simple Python snippet.\n\n```py\nimport requests\nimport json\nfrom datetime import datetime, timedelta\n\nurl = "http://<your-server>:8000/v1/chat/completions"\nheaders = {"Content-Type": "application/json", "Authorization": "Bearer token"}\n\nmodel = "mistralai/Mistral-Small-24B-Instruct-2501"\n\nmessages = [\n    {\n        "role": "system",\n        "content": "You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat."\n    },\n    {\n        "role": "user",\n        "content": "Give me 5 non-formal ways to say ''See you later'' in French."\n    },\n]\n\ndata = {"model": model, "messages": messages}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()["choices"][0]["message"]["content"])\n\n# Sure, here are five non-formal ways to say "See you later" in French:\n#\n# 1.  plus tard\n# 2.  plus\n# 3. Salut\n# 4.  toute\n# 5. Bisous\n#\n# ```\n#  /\_/\\n# ( o.o )\n#  > ^ <\n# ```\n```\n\n### Function calling\n\nMistral-Small-24-Instruct-2501 is excellent at function / tool calling tasks via vLLM. *E.g.:*\n\n<details>\n  <summary>Example</summary>\n\n```py\nimport requests\nimport json\nfrom huggingface_hub import hf_hub_download\nfrom datetime import datetime, timedelta\n\nurl = "http://<your-url>:8000/v1/chat/completions"\nheaders = {"Content-Type": "application/json", "Authorization": "Bearer token"}\n\nmodel = "mistralai/Mistral-Small-24B-Instruct-2501"\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, "r") as file:\n        system_prompt = file.read()\n    today = datetime.today().strftime("%Y-%m-%d")\n    yesterday = (datetime.today() - timedelta(days=1)).strftime("%Y-%m-%d")\n    model_name = repo_id.split("/")[-1]\n    return system_prompt.format(name=model_name, today=today, yesterday=yesterday)\n\n\nSYSTEM_PROMPT = load_system_prompt(model, "SYSTEM_PROMPT.txt")\n\n\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "get_current_weather",\n            "description": "Get the current weather in a given location",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "city": {\n                        "type": "string",\n                        "description": "The city to find the weather for, e.g. ''San Francisco''",\n                    },\n                    "state": {\n                        "type": "string",\n                        "description": "The state abbreviation, e.g. ''CA'' for California",\n                    },\n                    "unit": {\n                        "type": "string",\n                        "description": "The unit for temperature",\n                        "enum": ["celsius", "fahrenheit"],\n                    },\n                },\n                "required": ["city", "state", "unit"],\n            },\n        },\n    },\n    {\n        "type": "function",\n        "function": {\n            "name": "rewrite",\n            "description": "Rewrite a given text for improved clarity",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "text": {\n                        "type": "string",\n                        "description": "The input text to rewrite",\n                    }\n                },\n            },\n        },\n    },\n]\n\nmessages = [\n    {"role": "system", "content": SYSTEM_PROMPT},\n    {\n        "role": "user",\n        "content": "Could you please make the below article more concise?\n\nOpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.",\n    },\n    {\n        "role": "assistant",\n        "content": "",\n        "tool_calls": [\n            {\n                "id": "bbc5b7ede",\n                "type": "function",\n                "function": {\n                    "name": "rewrite",\n                    "arguments": ''{"text": "OpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership."}'',\n                },\n            }\n        ],\n    },\n    {\n        "role": "tool",\n        "content": ''{"action":"rewrite","outcome":"OpenAI is a FOR-profit company."}'',\n        "tool_call_id": "bbc5b7ede",\n        "name": "rewrite",\n    },\n    {\n        "role": "assistant",\n        "content": "---\n\nOpenAI is a FOR-profit company.",\n    },\n    {\n        "role": "user",\n        "content": "Can you tell me what the temperature will be in Dallas, in Fahrenheit?",\n    },\n]\n\ndata = {"model": model, "messages": messages, "tools": tools}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nimport ipdb; ipdb.set_trace()\nprint(response.json()["choices"][0]["message"]["tool_calls"])\n# [{''id'': ''8PdihwL6d'', ''type'': ''function'', ''function'': {''name'': ''get_current_weather'', ''arguments'': ''{"city": "Dallas", "state": "TX", "unit": "fahrenheit"}''}}]\n```\n\n</details>\n\n#### Offline\n\n```py\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\nfrom datetime import datetime, timedelta\n\nSYSTEM_PROMPT = "You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat."\n\nuser_prompt = "Give me 5 non-formal ways to say ''See you later'' in French."\n\nmessages = [\n    {\n        "role": "system",\n        "content": SYSTEM_PROMPT\n    },\n    {\n        "role": "user",\n        "content": user_prompt\n    },\n]\n\n# note that running this model on GPU requires over 60 GB of GPU RAM\nllm = LLM(model=model_name, tokenizer_mode="mistral", tensor_parallel_size=8)\n\nsampling_params = SamplingParams(max_tokens=512, temperature=0.15)\noutputs = llm.chat(messages, sampling_params=sampling_params)\n\nprint(outputs[0].outputs[0].text)\n# Sure, here are five non-formal ways to say "See you later" in French:\n#\n# 1.  plus tard\n# 2.  plus\n# 3. Salut\n# 4.  toute\n# 5. Bisous\n#\n# ```\n#  /\_/\\n# ( o.o )\n#  > ^ <\n# ```\n```\n\n### Transformers\n\nIf you want to use Hugging Face transformers to generate text, you can do something like this.\n\n```py\nfrom transformers import pipeline\nimport torch\n\nmessages = [\n    {"role": "user", "content": "Give me 5 non-formal ways to say ''See you later'' in French."},\n]\nchatbot = pipeline("text-generation", model="mistralai/Mistral-Small-24B-Instruct-2501", max_new_tokens=256, torch_dtype=torch.bfloat16)\nchatbot(messages)\n```\n\n\n### Ollama\n\n[Ollama](https://github.com/ollama/ollama) can run this model locally on MacOS, Windows and Linux. \n\n```\nollama run mistral-small\n```\n\n4-bit quantization (aliased to default): \n```\nollama run mistral-small:24b-instruct-2501-q4_K_M\n```\n\n8-bit quantization:\n```\nollama run mistral-small:24b-instruct-2501-q8_0\n```\n\nFP16:\n```\nollama run mistral-small:24b-instruct-2501-fp16\n```', '{"pipeline_tag":null,"library_name":"vllm","framework":"vllm","params":23572403200,"storage_bytes":94321574156,"files_count":22,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","chat_template":"{%- set today = strftime_now(\"%Y-%m-%d\") %}\n{%- set default_system_message = \"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\nYour knowledge base was last updated on 2023-10-01. The current date is \" + today + \".\\n\\nWhen you''re not sure about some information, you say that you don''t have the information and don''t make up anything.\\nIf the user''s question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\"What are some good restaurants around me?\\\" => \\\"Where are you?\\\" or \\\"When is the next flight to Tokyo\\\" => \\\"Where do you travel from?\\\")\" %}\n\n{{- bos_token }}\n\n{%- if messages[0][''role''] == ''system'' %}\n    {%- set system_message = messages[0][''content''] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = default_system_message %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{{- ''[SYSTEM_PROMPT]'' + system_message + ''[/SYSTEM_PROMPT]'' }}\n\n{%- for message in loop_messages %}\n    {%- if message[''role''] == ''user'' %}\n        {{- ''[INST]'' + message[''content''] + ''[/INST]'' }}\n    {%- elif message[''role''] == ''system'' %}\n        {{- ''[SYSTEM_PROMPT]'' + message[''content''] + ''[/SYSTEM_PROMPT]'' }}\n    {%- elif message[''role''] == ''assistant'' %}\n        {{- message[''content''] + eos_token }}\n    {%- else %}\n        {{- raise_exception(''Only user, system and assistant roles are supported!'') }}\n    {%- endif %}\n{%- endfor %}","eos_token":"</s>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:mistralai:mistral-common","source_url":"https://github.com/mistralai/mistral-common"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:mistralai:mistral-common","source_url":"https://github.com/mistralai/mistral-common"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:ollama:ollama","source_url":"https://github.com/ollama/ollama"}]', NULL, 'Apache-2.0', 'approved', 79.8, '152f0ab57b10a0a1b8f1b87dc442fbf5', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-OpenAssistant-oasst-sft-6-llama-30b-xor', 'huggingface--openassistant--oasst-sft-6-llama-30b-xor', 'oasst-sft-6-llama-30b-xor', 'OpenAssistant', '--- license: other --- Due to the license attached to LLaMA models by Meta AI it is not possible to directly distribute LLaMA-based models. Instead we provide XOR weights for the OA models. Thanks to Mick for writing the script which enables this process Note: This process applies to model. The same process can be applied to other models in future, but the checksums will be different.. **This process is tested only on Linux (specifically Ubuntu). Some users have reported that the process does...', '["arxiv:2304.07327","license:other","region:us"]', 'other', 943, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor","fetched_at":"2025-12-08T10:39:52.037Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\n---\n\n# OpenAssistant LLaMa 30B SFT 6\n\nDue to the license attached to LLaMA models by Meta AI it is not possible to directly distribute LLaMA-based models. Instead we provide XOR weights for the OA models.\n\nThanks to Mick for writing the `xor_codec.py` script which enables this process\n\n## The Process\n\nNote: This process applies to `oasst-sft-6-llama-30b` model. The same process can be applied to other models in future, but the checksums will be different..\n\n**This process is tested only on Linux (specifically Ubuntu). Some users have reported that the process does not work on Windows. We recommend using WSL if you only have a Windows machine.**\n\nTo use OpenAssistant LLaMA-Based Models, you should have a copy of the original LLaMA model weights and add them to a `llama` subdirectory here. If you cannot obtain the original LLaMA, see the note in italic below for a possible alternative.\n\nEnsure your LLaMA 30B checkpoint matches the correct md5sums:\n\n```\nf856e9d99c30855d6ead4d00cc3a5573  consolidated.00.pth\nd9dbfbea61309dc1e087f5081e98331a  consolidated.01.pth\n2b2bed47912ceb828c0a37aac4b99073  consolidated.02.pth\nea0405cdb5bc638fee12de614f729ebc  consolidated.03.pth\n4babdbd05b8923226a9e9622492054b6  params.json\n```\n\n*If you do not have a copy of the original LLaMA weights and cannot obtain one, you may still be able to complete this process. Some users have reported that [this model](https://huggingface.co/elinas/llama-30b-hf-transformers-4.29) can be used as a base for the XOR conversion. This will also allow you to skip to Step 7. However, we only support conversion starting from LLaMA original checkpoint and cannot provide support if you experience issues with this alternative approach.*\n\n**Important: Follow these exact steps to convert your original LLaMA checkpoint to a HuggingFace Transformers-compatible format. If you use the wrong versions of any dependency, you risk ending up with weights which are not compatible with the XOR files.**\n\n1. Create a clean Python **3.10** virtual environment & activate it:\n\n```\npython3.10 -m venv xor_venv\nsource xor_venv/bin/activate\n```\n\n2. Clone transformers repo and switch to tested version:\n\n```\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\ngit checkout d04ec99bec8a0b432fc03ed60cea9a1a20ebaf3c\npip install .\n```\n\n3. Install **exactly** these dependency versions:\n\n```\npip install torch==1.13.1 accelerate==0.18.0 sentencepiece==0.1.98 protobuf==3.20.1\n```\n\n4. Check `pip freeze` output:\n\n```\naccelerate==0.18.0\ncertifi==2022.12.7\ncharset-normalizer==3.1.0\nfilelock==3.12.0\nhuggingface-hub==0.13.4\nidna==3.4\nnumpy==1.24.2\nnvidia-cublas-cu11==11.10.3.66\nnvidia-cuda-nvrtc-cu11==11.7.99\nnvidia-cuda-runtime-cu11==11.7.99\nnvidia-cudnn-cu11==8.5.0.96\npackaging==23.1\nprotobuf==3.20.1\npsutil==5.9.5\nPyYAML==6.0\nregex==2023.3.23\nrequests==2.28.2\nsentencepiece==0.1.98\ntokenizers==0.13.3\ntorch==1.13.1\ntqdm==4.65.0\ntransformers @ file:///mnt/data/koepf/transformers\ntyping_extensions==4.5.0\nurllib3==1.26.15\n```\n\n5. While in `transformers` repo root, run HF LLaMA conversion script:\n\n```\npython src/transformers/models/llama/convert_llama_weights_to_hf.py --input_dir <input_path_llama_base>  --output_dir <output_path_llama30b_hf> --model_size 30B\n```\n\n6. Run `find . -type f -exec md5sum "{}" +` in the conversion target directory (`output_dir`). This should produce exactly the following checksums if your files are correct:\n\n```\n462a2d07f65776f27c0facfa2affb9f9  ./pytorch_model-00007-of-00007.bin\ne1dc8c48a65279fb1fbccff14562e6a3  ./pytorch_model-00003-of-00007.bin\n9cffb1aeba11b16da84b56abb773d099  ./pytorch_model-00001-of-00007.bin\naee09e21813368c49baaece120125ae3  ./generation_config.json\n92754d6c6f291819ffc3dfcaf470f541  ./pytorch_model-00005-of-00007.bin\n3eddc6fc02c0172d38727e5826181adb  ./pytorch_model-00004-of-00007.bin\neeec4125e9c7560836b4873b6f8e3025  ./tokenizer.model\n99762d59efa6b96599e863893cf2da02  ./pytorch_model-00006-of-00007.bin\n598538f18fed1877b41f77de034c0c8a  ./config.json\nfdb311c39b8659a5d5c1991339bafc09  ./tokenizer.json\nfecfda4fba7bfd911e187a85db5fa2ef  ./pytorch_model.bin.index.json\nedd1a5897748864768b1fab645b31491  ./tokenizer_config.json\n6b2e0a735969660e720c27061ef3f3d3  ./special_tokens_map.json\n5cfcb78b908ffa02e681cce69dbe4303  ./pytorch_model-00002-of-00007.bin\n```\n\n**Important: You should now have the correct LLaMA weights and be ready to apply the XORs. If the checksums above do not match yours, there is a problem.**\n\n7. Once you have LLaMA weights in the correct format, you can apply the XOR decoding:\n\n```\npython xor_codec.py oasst-sft-6-llama-30b/ oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b-xor/ llama30b_hf/\n```\n\nYou should **expect to see one warning message** during execution:\n\n`Exception when processing ''added_tokens.json''`\n\nThis is normal. **If similar messages appear for other files, something has gone wrong**.\n\n8. Now run `find . -type f -exec md5sum "{}" +` in the output directory (here `oasst-sft-6-llama-30b`). You should get a file with exactly these checksums:\n\n```\n970e99665d66ba3fad6fdf9b4910acc5  ./pytorch_model-00007-of-00007.bin\n659fcb7598dcd22e7d008189ecb2bb42  ./pytorch_model-00003-of-00007.bin\nff6e4cf43ddf02fb5d3960f850af1220  ./pytorch_model-00001-of-00007.bin\n27b0dc092f99aa2efaf467b2d8026c3f  ./added_tokens.json\n2917a1cafb895cf57e746cfd7696bfe5  ./generation_config.json\n740c324ae65b1ec25976643cda79e479  ./pytorch_model-00005-of-00007.bin\nf7aefb4c63be2ac512fd905b45295235  ./pytorch_model-00004-of-00007.bin\neeec4125e9c7560836b4873b6f8e3025  ./tokenizer.model\n369df2f0e38bda0d9629a12a77c10dfc  ./pytorch_model-00006-of-00007.bin\ncc9dbf56b68b68a585cc7367696e06a7  ./config.json\n76d47e4f51a8df1d703c6f594981fcab  ./pytorch_model.bin.index.json\nfd9452959d711be29ccf04a97598e8d1  ./tokenizer_config.json\n785905630a0fe583122a8446a5abe287  ./special_tokens_map.json\nae48c4c68e4e171d502dd0896aa19a84  ./pytorch_model-00002-of-00007.bin\n```\n\nIf so you have successfully decoded the weights and should be able to use the model with HuggingFace Transformers. **If your checksums do not match those above, there is a problem.**\n\n### Configuration\n\n```\nllama-30b-sft-6:\n  dtype: fp16\n  log_dir: "llama_log_30b"\n  learning_rate: 1e-5\n  model_name: /home/ubuntu/Open-Assistant/model/model_training/.saved/llama-30b-super-pretrain/checkpoint-3500\n  output_dir: llama_model_30b\n  deepspeed_config: configs/zero3_config_sft.json\n  weight_decay: 0.0\n  residual_dropout: 0.0\n  max_length: 2048\n  use_flash_attention: true\n  warmup_steps: 20\n  gradient_checkpointing: true\n  gradient_accumulation_steps: 16\n  per_device_train_batch_size: 2\n  per_device_eval_batch_size: 3\n  eval_steps: 101\n  save_steps: 292\n  num_train_epochs: 8\n  save_total_limit: 3\n  use_custom_sampler: true\n  sort_by_length: false\n  save_strategy: steps\n  datasets:\n    - oasst_export:\n        lang: "bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk"\n        input_file_path: 2023-04-12_oasst_release_ready_synth.jsonl.gz\n        val_split: 0.05\n    - vicuna:\n        val_split: 0.05\n        max_val_set: 800\n        fraction: 0.8\n    - dolly15k:\n        val_split: 0.05\n        max_val_set: 300\n    - grade_school_math_instructions:\n        val_split: 0.05\n    - code_alpaca:\n        val_split: 0.05\n        max_val_set: 250\n```\n\n- **OASST dataset paper:** https://arxiv.org/abs/2304.07327', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":65059086164,"files_count":17,"spaces_count":20,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers.git","source_url":"https://github.com/huggingface/transformers.git"},{"type":"based_on_paper","target_id":"arxiv:2304.07327","source_url":"https://arxiv.org/abs/2304.07327"}]', NULL, 'Other', 'approved', 64.7, '6149fc4070a7fe3d8c35fda6fcc69ebf', NULL, NULL, CURRENT_TIMESTAMP);
