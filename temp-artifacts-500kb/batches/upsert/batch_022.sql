/* LOGS:
Downloading image for github-deezer-spleeter from https://github.com/deezer.png
Image converted to WebP: data/images/github-deezer-spleeter.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-svc-develop-team-so-vits-svc', 'github--svc-develop-team--so-vits-svc', 'so-vits-svc', 'svc-develop-team', '<div align="center"> <img alt="LOGO" src="https://avatars.githubusercontent.com/u/127122328?s=400&u=5395a98a4f945a3a50cb0cc96c2747505d190dbc&v=4" width="300" height="300" /> **English** | **ä¸­æ–‡ç®€ä½“** This round of limited time update is coming to an end, the warehouse will enter the Archieve state, please know </div> > âœ¨ A studio that contains visible f0 editor, speaker mix timeline editor and other features (Where the Onnx models are used) : MoeVoiceStudio > âœ¨ A fork with a greatly improved use...', '["ai","audio-analysis","deep-learning","flow","generative-adversarial-network","pytorch","singing-voice-conversion","so-vits-svc","sovits","speech","variational-inference","vc","vits","voice","voice-changer","voice-conversion","voiceconversion","python"]', 'other', 27842, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/svc-develop-team/so-vits-svc","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n<img alt="LOGO" src="https://avatars.githubusercontent.com/u/127122328?s=400&u=5395a98a4f945a3a50cb0cc96c2747505d190dbc&v=4" width="300" height="300" />\n  \n# SoftVC VITS Singing Voice Conversion\n\n[**English**](./README.md) | [**ä¸­æ–‡ç®€ä½“**](./README_zh_CN.md)\n\n[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/svc-develop-team/so-vits-svc/blob/4.1-Stable/sovits4_for_colab.ipynb)\n[![Licence](https://img.shields.io/badge/LICENSE-AGPL3.0-green.svg?style=for-the-badge)](https://github.com/svc-develop-team/so-vits-svc/blob/4.1-Stable/LICENSE)\n\nThis round of limited time update is coming to an end, the warehouse will enter the Archieve state, please know\n\n</div>\n\n> âœ¨ A studio that contains visible f0 editor, speaker mix timeline editor and other features (Where the Onnx models are used) : [MoeVoiceStudio](https://github.com/NaruseMioShirakana/MoeVoiceStudio)\n\n> âœ¨ A fork with a greatly improved user interface: [34j/so-vits-svc-fork](https://github.com/34j/so-vits-svc-fork)\n\n> âœ¨ A client supports real-time conversion: [w-okada/voice-changer](https://github.com/w-okada/voice-changer)\n\n**This project differs fundamentally from VITS, as it focuses on Singing Voice Conversion (SVC) rather than Text-to-Speech (TTS). In this project, TTS functionality is not supported, and VITS is incapable of performing SVC tasks. It''s important to note that the models used in these two projects are not interchangeable or universally applicable.**\n\n## Announcement\n\nThe purpose of this project was to enable developers to have their beloved anime characters perform singing tasks. The developers'' intention was to focus solely on fictional characters and avoid any involvement of real individuals, anything related to real individuals deviates from the developer''s original intention.\n\n## Disclaimer\n\nThis project is an open-source, offline endeavor, and all members of SvcDevelopTeam, as well as other developers and maintainers involved (hereinafter referred to as contributors), have no control over the project. The contributors have never provided any form of assistance to any organization or individual, including but not limited to dataset extraction, dataset processing, computing support, training support, inference, and so on. The contributors do not and cannot be aware of the purposes for which users utilize the project. Therefore, any AI models and synthesized audio produced through the training of this project are unrelated to the contributors. Any issues or consequences arising from their use are the sole responsibility of the user.\n\nThis project is run completely offline and does not collect any user information or gather user input data. Therefore, contributors to this project are not aware of all user input and models and therefore are not responsible for any user input.\n\nThis project serves as a framework only and does not possess speech synthesis functionality by itself. All functionalities require users to train the models independently. Furthermore, this project does not come bundled with any models, and any secondary distributed projects are independent of the contributors of this project.\n\n## ğŸ“ Terms of Use\n\n# Warning: Please ensure that you address any authorization issues related to the dataset on your own. You bear full responsibility for any problems arising from the usage of non-authorized datasets for training, as well as any resulting consequences. The repository and its maintainer, svc develop team, disclaim any association with or liability for the consequences. \n\n1. This project is exclusively established for academic purposes, aiming to facilitate communication and learning. It is not intended for deployment in production environments.\n2. Any sovits-based video posted to a video platform must clearly specify in the introduction the input source vocals and audio used for the voice changer conversion, e.g., if you use someone else''s video/audio and convert it by separating the vocals as the input source, you must give a clear link to the original video or music; if you use your own vocals or a voice synthesized by another voice synthesis engine as the input source, you must also state this in your introduction.\n3. You are solely responsible for any infringement issues caused by the input source and all consequences. When using other commercial vocal synthesis software as an input source, please ensure that you comply with the regulations of that software, noting that the regulations of many vocal synthesis engines explicitly state that they cannot be used to convert input sources!\n4. Engaging in illegal activities, as well as religious and political activities, is strictly prohibited when using this project. The project developers vehemently oppose the aforementioned activities. If you disagree with this provision, the usage of the project is prohibited.\n5. If you continue to use the program, you will be deemed to have agreed to the terms and conditions set forth in README and README has discouraged you and is not responsible for any subsequent problems.\n6. If you intend to employ this project for any other purposes, kindly contact and inform the maintainers of this repository in advance.\n\n## ğŸ“ Model Introduction\n\nThe singing voice conversion model uses SoftVC content encoder to extract speech features from the source audio. These feature vectors are directly fed into VITS without the need for conversion to a text-based intermediate representation. As a result, the pitch and intonations of the original audio are preserved. Meanwhile, the vocoder was replaced with [NSF HiFiGAN](https://github.com/openvpi/DiffSinger/tree/refactor/modules/nsf_hifigan) to solve the problem of sound interruption.\n\n### ğŸ†• 4.1-Stable Version Update Content\n\n- Feature input is changed to the 12th Layer of [Content Vec](https://github.com/auspicious3000/contentvec) Transformer output, And compatible with 4.0 branches.\n- Update the shallow diffusion, you can use the shallow diffusion model to improve the sound quality.\n- Added Whisper-PPG encoder support\n- Added static/dynamic sound fusion\n- Added loudness embedding\n- Added Functionality of feature retrieval from [RVC](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI)\n  \n### ğŸ†• Questions about compatibility with the 4.0 model\n\n- To support the 4.0 model and incorporate the speech encoder, you can make modifications to the `config.json` file. Add the `speech_encoder` field to the "model" section as shown below:\n\n```\n  "model": {\n    .........\n    "ssl_dim": 256,\n    "n_speakers": 200,\n    "speech_encoder":"vec256l9"\n  }\n```\n\n### ğŸ†• Shallow diffusion\n![Diagram](shadowdiffusion.png)\n\n## ğŸ’¬ Python Version\n\nBased on our testing, we have determined that the project runs stable on `Python 3.8.9`.\n\n## ğŸ“¥ Pre-trained Model Files\n\n#### **Required**\n\n**You need to select one encoder from the list below**\n\n##### **1. If using contentvec as speech encoder(recommended)**\n\n`vec768l12` and `vec256l9` require the encoder\n\n- ContentVec: [checkpoint_best_legacy_500.pt](https://ibm.box.com/s/z1wgl1stco8ffooyatzdwsqn2psd9lrr)\n  - Place it under the `pretrain` directory\n\nOr download the following ContentVec, which is only 199MB in size but has the same effect:\n- ContentVec: [hubert_base.pt](https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/hubert_base.pt)\n  - Change the file name to `checkpoint_best_legacy_500.pt` and place it in the `pretrain` directory\n\n```shell\n# contentvec\nwget -P pretrain/ https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/hubert_base.pt -O checkpoint_best_legacy_500.pt\n# Alternatively, you can manually download and place it in the hubert directory\n```\n\n##### **2. If hubertsoft is used as the speech encoder**\n- soft vc hubert: [hubert-soft-0d54a1f4.pt](https://github.com/bshall/hubert/releases/download/v0.1/hubert-soft-0d54a1f4.pt)\n  - Place it under the `pretrain` directory\n\n##### **3. If whisper-ppg as the encoder**\n- download model at [medium.pt](https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt), the model fits `whisper-ppg`\n- or download model at [large-v2.pt](https://openaipublic.azureedge.net/main/whisper/models/81f7c96c852ee8fc832187b0132e569d6c3065a3252ed18e56effd0b6a73e524/large-v2.pt), the model fits `whisper-ppg-large`\n  - Place it under the `pretrain` directory\n  \n##### **4. If cnhubertlarge as the encoder**\n- download model at [chinese-hubert-large-fairseq-ckpt.pt](https://huggingface.co/TencentGameMate/chinese-hubert-large/resolve/main/chinese-hubert-large-fairseq-ckpt.pt)\n  - Place it under the `pretrain` directory\n\n##### **5. If dphubert as the encoder**\n- download model at [DPHuBERT-sp0.75.pth](https://huggingface.co/pyf98/DPHuBERT/resolve/main/DPHuBERT-sp0.75.pth)\n  - Place it under the `pretrain` directory\n\n##### **6. If WavLM is used as the encoder**\n- download model at  [WavLM-Base+.pt](https://valle.blob.core.windows.net/share/wavlm/WavLM-Base+.pt?sv=2020-08-04&st=2023-03-01T07%3A51%3A05Z&se=2033-03-02T07%3A51%3A00Z&sr=c&sp=rl&sig=QJXmSJG9DbMKf48UDIU1MfzIro8HQOf3sqlNXiflY1I%3D), the model fits `wavlmbase+`\n  - Place it under the `pretrain` directory\n\n##### **7. If OnnxHubert/ContentVec as the encoder**\n- download model at [MoeSS-SUBModel](https://huggingface.co/NaruseMioShirakana/MoeSS-SUBModel/tree/main)\n  - Place it under the `pretrain` directory\n\n#### **List of Encoders**\n- "vec768l12"\n- "vec256l9"\n- "vec256l9-onnx"\n- "vec256l12-onnx"\n- "vec768l9-onnx"\n- "vec768l12-onnx"\n- "hubertsoft-onnx"\n- "hubertsoft"\n- "whisper-ppg"\n- "cnhubertlarge"\n- "dphubert"\n- "whisper-ppg-large"\n- "wavlmbase+"\n\n#### **Optional(Strongly recommend)**\n\n- Pre-trained model files: `G_0.pth` `D_0.pth`\n  - Place them under the `logs/44k` directory\n\n- Diffusion model pretraining base model file: `model_0.pt`\n  - Put it in the `logs/44k/diffusion` directory\n\nGet Sovits Pre-trained model from svc-develop-team(TBD) or anywhere else.\n\nDiffusion model references [Diffusion-SVC](https://github.com/CNChTu/Diffusion-SVC) diffusion model. The pre-trained diffusion model is universal with the DDSP-SVC''s. You can go to [Diffusion-SVC](https://github.com/CNChTu/Diffusion-SVC)''s repo to get the pre-trained diffusion model.\n\nWhile the pretrained model typically does not pose copyright concerns, it is essential to remain vigilant. It is advisable to consult with the author beforehand or carefully review the description to ascertain the permissible usage of the model. This helps ensure compliance with any specified guidelines or restrictions regarding its utilization.\n\n#### **Optional(Select as Required)**\n\n##### NSF-HIFIGAN\n\nIf you are using the `NSF-HIFIGAN enhancer` or `shallow diffusion`, you will need to download the pre-trained NSF-HIFIGAN model.\n\n- Pre-trained NSF-HIFIGAN Vocoder: [nsf_hifigan_20221211.zip](https://github.com/openvpi/vocoders/releases/download/nsf-hifigan-v1/nsf_hifigan_20221211.zip)\n  - Unzip and place the four files under the `pretrain/nsf_hifigan` directory\n\n```shell\n# nsf_hifigan\nwget -P pretrain/ https://github.com/openvpi/vocoders/releases/download/nsf-hifigan-v1/nsf_hifigan_20221211.zip\nunzip -od pretrain/nsf_hifigan pretrain/nsf_hifigan_20221211.zip\n# Alternatively, you can manually download and place it in the pretrain/nsf_hifigan directory\n# URL: https://github.com/openvpi/vocoders/releases/tag/nsf-hifigan-v1\n```\n\n##### RMVPE\n\nIf you are using the `rmvpe` F0 Predictor, you will need to download the pre-trained RMVPE model.\n\n+ download model at [rmvpe.zip](https://github.com/yxlllc/RMVPE/releases/download/230917/rmvpe.zip), this weight is recommended.\n  + unzip `rmvpe.zip`ï¼Œand rename the `model.pt` file to `rmvpe.pt` and place it under the `pretrain` directory.\n\n- ~~download model at [rmvpe.pt](https://huggingface.co/datasets/ylzz1997/rmvpe_pretrain_model/resolve/main/rmvpe.pt)~~\n  - ~~Place it under the `pretrain` directory~~\n\n##### FCPE(Preview version)\n\n[FCPE(Fast Context-base Pitch Estimator)](https://github.com/CNChTu/MelPE) is a dedicated F0 predictor designed for real-time voice conversion and will become the preferred F0 predictor for sovits real-time voice conversion in the future.(The paper is being written)\n\nIf you are using the `fcpe` F0 Predictor, you will need to download the pre-trained FCPE model.\n\n- download model at [fcpe.pt](https://huggingface.co/datasets/ylzz1997/rmvpe_pretrain_model/resolve/main/fcpe.pt)\n  - Place it under the `pretrain` directory\n\n## ğŸ“Š Dataset Preparation\n\nSimply place the dataset in the `dataset_raw` directory with the following file structure:\n\n```\ndataset_raw\nâ”œâ”€â”€â”€speaker0\nâ”‚   â”œâ”€â”€â”€xxx1-xxx1.wav\nâ”‚   â”œâ”€â”€â”€...\nâ”‚   â””â”€â”€â”€Lxx-0xx8.wav\nâ””â”€â”€â”€speaker1\n    â”œâ”€â”€â”€xx2-0xxx2.wav\n    â”œâ”€â”€â”€...\n    â””â”€â”€â”€xxx7-xxx007.wav\n```\nThere are no specific restrictions on the format of the name for each audio file (naming conventions such as `000001.wav` to `999999.wav` are also valid), but the file type must be `WAV``.\n\nYou can customize the speaker''s name as showed below:\n\n```\ndataset_raw\nâ””â”€â”€â”€suijiSUI\n    â”œâ”€â”€â”€1.wav\n    â”œâ”€â”€â”€...\n    â””â”€â”€â”€25788785-20221210-200143-856_01_(Vocals)_0_0.wav\n```\n\n## ğŸ› ï¸ Preprocessing\n\n### 0. Slice audio\n\nTo avoid video memory overflow during training or pre-processing, it is recommended to limit the length of audio clips. Cutting the audio to a length of "5s - 15s" is more recommended. Slightly longer times are acceptable, however, excessively long clips may cause problems such as `torch.cuda.OutOfMemoryError`.\n\nTo facilitate the slicing process, you can use [audio-slicer-GUI](https://github.com/flutydeer/audio-slicer) or [audio-slicer-CLI](https://github.com/openvpi/audio-slicer)\n\nIn general, only the `Minimum Interval` needs to be adjusted. For spoken audio, the default value usually suffices, while for singing audio, it can be adjusted to around `100` or even `50`, depending on the specific requirements.\n\nAfter slicing, it is recommended to remove any audio clips that are excessively long or too short.\n\n**If you are using whisper-ppg encoder for training, the audio clips must shorter than 30s.**\n\n### 1. Resample to 44100Hz and mono\n\n```shell\npython resample.py\n```\n\n#### Cautions\n\nAlthough this project has resample.py scripts for resampling, mono and loudness matching, the default loudness matching is to match to 0db. This can cause damage to the sound quality. While python''s loudness matching package pyloudnorm does not limit the level, this can lead to sonic boom. Therefore, it is recommended to consider using professional sound processing software, such as `adobe audition` for loudness matching. If you are already using other software for loudness matching, add the parameter `-skip_loudnorm` to the run command:\n\n```shell\npython resample.py --skip_loudnorm\n```\n\n### 2. Automatically split the dataset into training and validation sets, and generate configuration files.\n\n```shell\npython preprocess_flist_config.py --speech_encoder vec768l12\n```\n\nspeech_encoder has the following options\n\n```\nvec768l12\nvec256l9\nhubertsoft\nwhisper-ppg\ncnhubertlarge\ndphubert\nwhisper-ppg-large\nwavlmbase+\n```\n\nIf the speech_encoder argument is omitted, the default value is `vec768l12`\n\n**Use loudness embedding**\n\nAdd `--vol_aug` if you want to enable loudness embedding:\n\n```shell\npython preprocess_flist_config.py --speech_encoder vec768l12 --vol_aug\n```\n\nAfter enabling loudness embedding, the trained model will match the loudness of the input source; otherwise, it will match the loudness of the training set.\n\n#### You can modify some parameters in the generated config.json and diffusion.yaml\n\n* `keep_ckpts`: Keep the the the number of previous models during training. Set to `0` to keep them all. Default is `3`.\n\n* `all_in_mem`: Load all dataset to RAM. It can be enabled when the disk IO of some platforms is too low and the system memory is **much larger** than your dataset.\n  \n* `batch_size`: The amount of data loaded to the GPU for a single training session can be adjusted to a size lower than the GPU memory capacity.\n\n* `vocoder_name`: Select a vocoder. The default is `nsf-hifigan`.\n\n##### diffusion.yaml\n\n* `cache_all_data`: Load all dataset to RAM. It can be enabled when the disk IO of some platforms is too low and the system memory is **much larger** than your dataset.\n\n* `duration`: The duration of the audio slicing during training, can be adjusted according to the size of the video memory, **Note: this value must be less than the minimum time of the audio in the training set!**\n\n* `batch_size`: The amount of data loaded to the GPU for a single training session can be adjusted to a size lower than the video memory capacity.\n\n* `timesteps`: The total number of steps in the diffusion model, which defaults to 1000.\n\n* `k_step_max`: Training can only train `k_step_max` step diffusion to save training time, note that the value must be less than `timesteps`, 0 is to train the entire diffusion model, **Note: if you do not train the entire diffusion model will not be able to use only_diffusion!**\n\n##### **List of Vocoders**\n\n```\nnsf-hifigan\nnsf-snake-hifigan\n```\n\n### 3. Generate hubert and f0\n\n```shell\npython preprocess_hubert_f0.py --f0_predictor dio\n```\n\nf0_predictor has the following options\n\n```\ncrepe\ndio\npm\nharvest\nrmvpe\nfcpe\n```\n\nIf the training set is too noisy,it is recommended to use `crepe` to handle f0\n\nIf the f0_predictor parameter is omitted, the default value is `rmvpe`\n\nIf you want shallow diffusion (optional), you need to add the `--use_diff` parameter, for example:\n\n```shell\npython preprocess_hubert_f0.py --f0_predictor dio --use_diff\n```\n\n**Speed Up preprocess**\n\nIf your dataset is pretty large,you can increase the param `--num_processes` like that:\n\n```shell\npython preprocess_hubert_f0.py --f0_predictor dio --num_processes 8\n```\nAll the worker will be assigned to different GPU if you have more than one GPUs.\n\nAfter completing the above steps, the dataset directory will contain the preprocessed data, and the dataset_raw folder can be deleted.\n\n## ğŸ‹ï¸â€ Training\n\n### Sovits Model\n\n```shell\npython train.py -c configs/config.json -m 44k\n```\n\n### Diffusion Model (optional)\n\nIf the shallow diffusion function is needed, the diffusion model needs to be trained. The diffusion model training method is as follows:\n\n```shell\npython train_diff.py -c configs/diffusion.yaml\n```\n\nDuring training, the model files will be saved to `logs/44k`, and the diffusion model will be saved to `logs/44k/diffusion`\n\n## ğŸ¤– Inference\n\nUse [inference_main.py](https://github.com/svc-develop-team/so-vits-svc/blob/4.0/inference_main.py)\n\n```shell\n# Example\npython inference_main.py -m "logs/44k/G_30400.pth" -c "configs/config.json" -n "å›ã®çŸ¥ã‚‰ãªã„ç‰©èª-src.wav" -t 0 -s "nen"\n```\n\nRequired parameters:\n- `-m` | `--model_path`: path to the model.\n- `-c` | `--config_path`: path to the configuration file.\n- `-n` | `--clean_names`: a list of wav file names located in the `raw` folder.\n- `-t` | `--trans`: pitch shift, supports positive and negative (semitone) values.\n- `-s` | `--spk_list`: Select the speaker ID to use for conversion.\n- `-cl` | `--clip`: Forced audio clipping, set to 0 to disable(default), setting it to a non-zero value (duration in seconds) to enable.\n\nOptional parameters: see the next section\n- `-lg` | `--linear_gradient`: The cross fade length of two audio slices in seconds. If there is a discontinuous voice after forced slicing, you can adjust this value. Otherwise, it is recommended to use the default value of 0.\n- `-f0p` | `--f0_predictor`: Select a F0 predictor, options are `crepe`, `pm`, `dio`, `harvest`, `rmvpe`,`fcpe`, default value is `pm`(note: f0 mean pooling will be enable when using `crepe`)\n- `-a` | `--auto_predict_f0`: automatic pitch prediction, do not enable this when converting singing voices as it can cause serious pitch issues.\n- `-cm` | `--cluster_model_path`: Cluster model or feature retrieval index path, if left blank, it will be automatically set as the default path of these models. If there is no training cluster or feature retrieval, fill in at will.\n- `-cr` | `--cluster_infer_ratio`: The proportion of clustering scheme or feature retrieval ranges from 0 to 1. If there is no training clustering model or feature retrieval, the default is 0.\n- `-eh` | `--enhance`: Whether to use NSF_HIFIGAN enhancer, this option has certain effect on sound quality enhancement for some models with few training sets, but has negative effect on well-trained models, so it is disabled by default.\n- `-shd` | `--shallow_diffusion`: Whether to use shallow diffusion, which can solve some electrical sound problems after use. This option is disabled by default. When this option is enabled, NSF_HIFIGAN enhancer will be disabled\n- `-usm` | `--use_spk_mix`: whether to use dynamic voice fusion\n- `-lea` | `--loudness_envelope_adjustment`ï¼šThe adjustment of the input source''s loudness envelope in relation to the fusion ratio of the output loudness envelope. The closer to 1, the more the output loudness envelope is used\n- `-fr` | `--feature_retrieval`ï¼šWhether to use feature retrieval If clustering model is used, it will be disabled, and `cm` and `cr` parameters will become the index path and mixing ratio of feature retrieval\n  \nShallow diffusion settings:\n- `-dm` | `--diffusion_model_path`: Diffusion model path\n- `-dc` | `--diffusion_config_path`: Diffusion config file path\n- `-ks` | `--k_step`: The larger the number of k_steps, the closer it is to the result of the diffusion model. The default is 100\n- `-od` | `--only_diffusion`: Whether to use Only diffusion mode, which does not load the sovits model to only use diffusion model inference\n- `-se` | `--second_encoding`ï¼šwhich involves applying an additional encoding to the original audio before shallow diffusion. This option can yield varying results - sometimes positive and sometimes negative.\n\n### Cautions\n\nIf inferencing using `whisper-ppg` speech encoder, you need to set `--clip` to 25 and `-lg` to 1. Otherwise it will fail to infer properly.\n\n## ğŸ¤” Optional Settings\n\nIf you are satisfied with the previous results, or if you do not feel you understand what follows, you can skip it and it will have no effect on the use of the model. The impact of these optional settings mentioned is relatively small, and while they may have some impact on specific datasets, in most cases the difference may not be significant.\n\n### Automatic f0 prediction\n\nDuring the training of the 4.0 model, an f0 predictor is also trained, which enables automatic pitch prediction during voice conversion. However, if the results are not satisfactory, manual pitch prediction can be used instead. Please note that when converting singing voices, it is advised not to enable this feature as it may cause significant pitch shifting.\n\n- Set `auto_predict_f0` to `true` in `inference_main.py`.\n\n### Cluster-based timbre leakage control\n\nIntroduction: The clustering scheme implemented in this model aims to reduce timbre leakage and enhance the similarity of the trained model to the target''s timbre, although the effect may not be very pronounced. However, relying solely on clustering can reduce the model''s clarity and make it sound less distinct. Therefore, a fusion method is adopted in this model to control the balance between the clustering and non-clustering approaches. This allows manual adjustment of the trade-off between "sounding like the target''s timbre" and "have clear enunciation" to find an optimal balance.\n\nNo changes are required in the existing steps. Simply train an additional clustering model, which incurs relatively low training costs.\n\n- Training process:\n  - Train on a machine with good CPU performance. According to extant experience, it takes about 4 minutes to train each speaker on a Tencent Cloud machine with 6-core CPU.\n  - Execute `python cluster/train_cluster.py`. The output model will be saved in `logs/44k/kmeans_10000.pt`.\n  - The clustering model can currently be trained using the gpu by executing `python cluster/train_cluster.py --gpu`\n- Inference process:\n  - Specify `cluster_model_path` in `inference_main.py`. If not specified, the default is `logs/44k/kmeans_10000.pt`.\n  - Specify `cluster_infer_ratio` in `inference_main.py`, where `0` means not using clustering at all, `1` means only using clustering, and usually `0.5` is sufficient.\n\n### Feature retrieval\n\nIntroduction: As with the clustering scheme, the timbre leakage can be reduced, the enunciation is slightly better than clustering, but it will reduce the inference speed. By employing the fusion method, it becomes possible to linearly control the balance between feature retrieval and non-feature retrieval, allowing for fine-tuning of the desired proportion.\n\n- Training process: \n  First, it needs to be executed after generating hubert and f0: \n\n```shell\npython train_index.py -c configs/config.json\n```\n\nThe output of the model will be in `logs/44k/feature_and_index.pkl`\n\n- Inference process: \n  - The `--feature_retrieval` needs to be formulated first, and the clustering mode automatically switches to the feature retrieval mode.\n  - Specify `cluster_model_path` in `inference_main.py`. If not specified, the default is `logs/44k/feature_and_index.pkl`.\n  - Specify `cluster_infer_ratio` in `inference_main.py`, where `0` means not using feature retrieval at all, `1` means only using feature retrieval, and usually `0.5` is sufficient.\n\n## ğŸ—œï¸ Model compression\n\nThe generated model contains data that is needed for further training. If you confirm that the model is final and not be used in further training, it is safe to remove these data to get smaller file size (about 1/3).\n\n```shell\n# Example\npython compress_model.py -c="configs/config.json" -i="logs/44k/G_30400.pth" -o="logs/44k/release.pth"\n```\n\n## ğŸ‘¨â€ğŸ”§ Timbre mixing\n\n### Static Tone Mixing\n\n**Refer to `webUI.py` file for stable Timbre mixing of the gadget/lab feature.**\n\nIntroduction: This function can combine multiple models into one model (convex combination or linear combination of multiple model parameters) to create mixed voice that do not exist in reality\n\n**Note:**\n1. This feature is only supported for single-speaker models\n2. If you force a multi-speaker model, it is critical to make sure there are the same number of speakers in each model. This will ensure that sounds with the same SpeakerID can be mixed correctly.\n3. Ensure that the `model` fields in config.json of all models to be mixed are the same\n4. The mixed model can use any config.json file from the models being synthesized. However, the clustering model will not be functional after mixed.\n5. When batch uploading models, it is best to put the models into a folder and upload them together after selecting them\n6. It is suggested to adjust the mixing ratio between 0 and 100, or to other numbers, but unknown effects will occur in the linear combination mode\n7. After mixing, the file named output.pth will be saved in the root directory of the project\n8. Convex combination mode will perform Softmax to add the mix ratio to 1, while linear combination mode will not\n\n### Dynamic timbre mixing\n\n**Refer to the `spkmix.py` file for an introduction to dynamic timbre mixing**\n\nCharacter mix track writing rules:\n\nRole ID: \[\[Start time 1, end time 1, start value 1, start value 1], [Start time 2, end time 2, start value 2]]\n\nThe start time must be the same as the end time of the previous one. The first start time must be 0, and the last end time must be 1 (time ranges from 0 to 1).\n\nAll roles must be filled in. For unused roles, fill \[\[0., 1., 0., 0.]]\n\nThe fusion value can be filled in arbitrarily, and the linear change from the start value to the end value within the specified period of time. The \n\ninternal linear combination will be automatically guaranteed to be 1 (convex combination condition), so it can be used safely\n\nUse the `--use_spk_mix` parameter when reasoning to enable dynamic timbre mixing\n\n## ğŸ“¤ Exporting to Onnx\n\nUse [onnx_export.py](https://github.com/svc-develop-team/so-vits-svc/blob/4.0/onnx_export.py)\n\n- Create a folder named `checkpoints` and open it\n- Create a folder in the `checkpoints` folder as your project folder, naming it after your project, for example `aziplayer`\n- Rename your model as `model.pth`, the configuration file as `config.json`, and place them in the `aziplayer` folder you just created\n- Modify `"NyaruTaffy"` in `path = "NyaruTaffy"` in [onnx_export.py](https://github.com/svc-develop-team/so-vits-svc/blob/4.0/onnx_export.py) to your project name, `path = "aziplayer"`ï¼ˆonnx_export_speaker_mix makes you can mix speaker''s voiceï¼‰\n- Run [onnx_export.py](https://github.com/svc-develop-team/so-vits-svc/blob/4.0/onnx_export.py)\n- Wait for it to finish running. A `model.onnx` will be generated in your project folder, which is the exported model.\n\nNote: For Hubert Onnx models, please use the models provided by MoeSS. Currently, they cannot be exported on their own (Hubert in fairseq has many unsupported operators and things involving constants that can cause errors or result in problems with the input/output shape and results when exported.)\n\n\n## ğŸ“ Reference\n\n| URL | Designation | Title | Implementation Source |\n| --- | ----------- | ----- | --------------------- |\n|[2106.06103](https://arxiv.org/abs/2106.06103) | VITS (Synthesizer)| Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech | [jaywalnut310/vits](https://github.com/jaywalnut310/vits) |\n|[2111.02392](https://arxiv.org/abs/2111.02392) | SoftVC (Speech Encoder)| A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion | [bshall/hubert](https://github.com/bshall/hubert) |\n|[2204.09224](https://arxiv.org/abs/2204.09224) | ContentVec (Speech Encoder)| ContentVec: An Improved Self-Supervised Speech Representation by Disentangling Speakers | [auspicious3000/contentvec](https://github.com/auspicious3000/contentvec) |\n|[2212.04356](https://arxiv.org/abs/2212.04356) | Whisper (Speech Encoder) | Robust Speech Recognition via Large-Scale Weak Supervision | [openai/whisper](https://github.com/openai/whisper) |\n|[2110.13900](https://arxiv.org/abs/2110.13900) | WavLM (Speech Encoder) | WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing | [microsoft/unilm/wavlm](https://github.com/microsoft/unilm/tree/master/wavlm) |\n|[2305.17651](https://arxiv.org/abs/2305.17651) | DPHubert (Speech Encoder) | DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models | [pyf98/DPHuBERT](https://github.com/pyf98/DPHuBERT) |\n|[DOI:10.21437/Interspeech.2017-68](http://dx.doi.org/10.21437/Interspeech.2017-68) | Harvest (F0 Predictor) | Harvest: A high-performance fundamental frequency estimator from speech signals | [mmorise/World/harvest](https://github.com/mmorise/World/blob/master/src/harvest.cpp) |\n|[aes35-000039](https://www.aes.org/e-lib/online/browse.cfm?elib=15165) | Dio (F0 Predictor) | Fast and reliable F0 estimation method based on the period extraction of vocal fold vibration of singing voice and speech | [mmorise/World/dio](https://github.com/mmorise/World/blob/master/src/dio.cpp) |\n|[8461329](https://ieeexplore.ieee.org/document/8461329) | Crepe (F0 Predictor) | Crepe: A Convolutional Representation for Pitch Estimation | [maxrmorrison/torchcrepe](https://github.com/maxrmorrison/torchcrepe) |\n|[DOI:10.1016/j.wocn.2018.07.001](https://doi.org/10.1016/j.wocn.2018.07.001) | Parselmouth (F0 Predictor) | Introducing Parselmouth: A Python interface to Praat | [YannickJadoul/Parselmouth](https://github.com/YannickJadoul/Parselmouth) |\n|[2306.15412v2](https://arxiv.org/abs/2306.15412v2) | RMVPE (F0 Predictor) | RMVPE: A Robust Model for Vocal Pitch Estimation in Polyphonic Music | [Dream-High/RMVPE](https://github.com/Dream-High/RMVPE) |\n|[2010.05646](https://arxiv.org/abs/2010.05646) | HIFIGAN (Vocoder) | HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis | [jik876/hifi-gan](https://github.com/jik876/hifi-gan) |\n|[1810.11946](https://arxiv.org/abs/1810.11946.pdf) | NSF (Vocoder) | Neural source-filter-based waveform model for statistical parametric speech synthesis | [openvpi/DiffSinger/modules/nsf_hifigan](https://github.com/openvpi/DiffSinger/tree/refactor/modules/nsf_hifigan)\n|[2006.08195](https://arxiv.org/abs/2006.08195) | Snake (Vocoder) | Neural Networks Fail to Learn Periodic Functions and How to Fix It | [EdwardDixon/snake](https://github.com/EdwardDixon/snake)\n|[2105.02446v3](https://arxiv.org/abs/2105.02446v3) | Shallow Diffusion (PostProcessing)| DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism | [CNChTu/Diffusion-SVC](https://github.com/CNChTu/Diffusion-SVC) |\n|[K-means](https://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=01D65490BADCC216F350D06F84D721AD?doi=10.1.1.308.8619&rep=rep1&type=pdf) | Feature K-means Clustering (PreProcessing)| Some methods for classification and analysis of multivariate observations | This repo |\n| | Feature TopK Retrieval (PreProcessing)| Retrieval based Voice Conversion | [RVC-Project/Retrieval-based-Voice-Conversion-WebUI](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI) |\n| | whisper ppg| whisper ppg | [PlayVoice/whisper_ppg](https://github.com/PlayVoice/whisper_ppg) |\n| | bigvgan| bigvgan | [PlayVoice/so-vits-svc-5.0](https://github.com/PlayVoice/so-vits-svc-5.0/tree/bigvgan-mix-v2/vits_decoder/alias) |\n\n\n## â˜€ï¸ Previous contributors\n\nFor some reason the author deleted the original repository. Because of the negligence of the organization members, the contributor list was cleared because all files were directly reuploaded to this repository at the beginning of the reconstruction of this repository. Now add a previous contributor list to README.md.\n\n*Some members have not listed according to their personal wishes.*\n\n<table>\n  <tr>\n    <td align="center"><a href="https://github.com/MistEO"><img src="https://avatars.githubusercontent.com/u/18511905?v=4" width="100px;" alt=""/><br /><sub><b>MistEO</b></sub></a><br /></td>\n    <td align="center"><a href="https://github.com/XiaoMiku01"><img src="https://avatars.githubusercontent.com/u/54094119?v=4" width="100px;" alt=""/><br /><sub><b>XiaoMiku01</b></sub></a><br /></td>\n    <td align="center"><a href="https://github.com/ForsakenRei"><img src="https://avatars.githubusercontent.com/u/23041178?v=4" width="100px;" alt=""/><br /><sub><b>ã—ãã‚Œ</b></sub></a><br /></td>\n    <td align="center"><a href="https://github.com/TomoGaSukunai"><img src="https://avatars.githubusercontent.com/u/25863522?v=4" width="100px;" alt=""/><br /><sub><b>TomoGaSukunai</b></sub></a><br /></td>\n    <td align="center"><a href="https://github.com/Plachtaa"><img src="https://avatars.githubusercontent.com/u/112609742?v=4" width="100px;" alt=""/><br /><sub><b>Plachtaa</b></sub></a><br /></td>\n    <td align="center"><a href="https://github.com/zdxiaoda"><img src="https://avatars.githubusercontent.com/u/45501959?v=4" width="100px;" alt=""/><br /><sub><b>zdå°è¾¾</b></sub></a><br /></td>\n    <td align="center"><a href="https://github.com/Archivoice"><img src="https://avatars.githubusercontent.com/u/107520869?v=4" width="100px;" alt=""/><br /><sub><b>å‡è²éŸ¿ä¸–</b></sub></a><br /></td>\n  </tr>\n</table>\n\n## ğŸ“š Some legal provisions for reference\n\n#### Any country, region, organization, or individual using this project must comply with the following laws.\n\n#### ã€Šæ°‘æ³•å…¸ã€‹\n\n##### ç¬¬ä¸€åƒé›¶ä¸€åä¹æ¡ \n\nä»»ä½•ç»„ç»‡æˆ–è€…ä¸ªäººä¸å¾—ä»¥ä¸‘åŒ–ã€æ±¡æŸï¼Œæˆ–è€…åˆ©ç”¨ä¿¡æ¯æŠ€æœ¯æ‰‹æ®µä¼ªé€ ç­‰æ–¹å¼ä¾µå®³ä»–äººçš„è‚–åƒæƒã€‚æœªç»è‚–åƒæƒäººåŒæ„ï¼Œä¸å¾—åˆ¶ä½œã€ä½¿ç”¨ã€å…¬å¼€è‚–åƒæƒäººçš„è‚–åƒï¼Œä½†æ˜¯æ³•å¾‹å¦æœ‰è§„å®šçš„é™¤å¤–ã€‚æœªç»è‚–åƒæƒäººåŒæ„ï¼Œè‚–åƒä½œå“æƒåˆ©äººä¸å¾—ä»¥å‘è¡¨ã€å¤åˆ¶ã€å‘è¡Œã€å‡ºç§Ÿã€å±•è§ˆç­‰æ–¹å¼ä½¿ç”¨æˆ–è€…å…¬å¼€è‚–åƒæƒäººçš„è‚–åƒã€‚å¯¹è‡ªç„¶äººå£°éŸ³çš„ä¿æŠ¤ï¼Œå‚ç…§é€‚ç”¨è‚–åƒæƒä¿æŠ¤çš„æœ‰å…³è§„å®šã€‚\n\n#####  ç¬¬ä¸€åƒé›¶äºŒåå››æ¡ \n\nã€åèª‰æƒã€‘æ°‘äº‹ä¸»ä½“äº«æœ‰åèª‰æƒã€‚ä»»ä½•ç»„ç»‡æˆ–è€…ä¸ªäººä¸å¾—ä»¥ä¾®è¾±ã€è¯½è°¤ç­‰æ–¹å¼ä¾µå®³ä»–äººçš„åèª‰æƒã€‚  \n\n#####  ç¬¬ä¸€åƒé›¶äºŒåä¸ƒæ¡\n\nã€ä½œå“ä¾µå®³åèª‰æƒã€‘è¡Œä¸ºäººå‘è¡¨çš„æ–‡å­¦ã€è‰ºæœ¯ä½œå“ä»¥çœŸäººçœŸäº‹æˆ–è€…ç‰¹å®šäººä¸ºæè¿°å¯¹è±¡ï¼Œå«æœ‰ä¾®è¾±ã€è¯½è°¤å†…å®¹ï¼Œä¾µå®³ä»–äººåèª‰æƒçš„ï¼Œå—å®³äººæœ‰æƒä¾æ³•è¯·æ±‚è¯¥è¡Œä¸ºäººæ‰¿æ‹…æ°‘äº‹è´£ä»»ã€‚è¡Œä¸ºäººå‘è¡¨çš„æ–‡å­¦ã€è‰ºæœ¯ä½œå“ä¸ä»¥ç‰¹å®šäººä¸ºæè¿°å¯¹è±¡ï¼Œä»…å…¶ä¸­çš„æƒ…èŠ‚ä¸è¯¥ç‰¹å®šäººçš„æƒ…å†µç›¸ä¼¼çš„ï¼Œä¸æ‰¿æ‹…æ°‘äº‹è´£ä»»ã€‚  \n\n#### ã€Š[ä¸­åäººæ°‘å…±å’Œå›½å®ªæ³•](http://www.gov.cn/guoqing/2018-03/22/content_5276318.htm)ã€‹\n\n#### ã€Š[ä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•](http://gongbao.court.gov.cn/Details/f8e30d0689b23f57bfc782d21035c3.html?sw=%E4%B8%AD%E5%8D%8E%E4%BA%BA%E6%B0%91%E5%85%B1%E5%92%8C%E5%9B%BD%E5%88%91%E6%B3%95)ã€‹\n\n#### ã€Š[ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸](http://gongbao.court.gov.cn/Details/51eb6750b8361f79be8f90d09bc202.html)ã€‹\n\n#### ã€Š[ä¸­åäººæ°‘å…±å’Œå›½åˆåŒæ³•](http://www.npc.gov.cn/zgrdw/npc/lfzt/rlyw/2016-07/01/content_1992739.htm)ã€‹\n\n## ğŸ’ª Thanks to all contributors for their efforts\n<a href="https://github.com/svc-develop-team/so-vits-svc/graphs/contributors" target="_blank">\n  <img src="https://contrib.rocks/image?repo=svc-develop-team/so-vits-svc" />\n</a>\n', '{"language":"Python","stars":27842,"forks":5070,"watchers":27842,"open_issues":28,"topics":["ai","audio-analysis","deep-learning","flow","generative-adversarial-network","pytorch","singing-voice-conversion","so-vits-svc","sovits","speech","variational-inference","vc","vits","voice","voice-changer","voice-conversion","voiceconversion"],"default_branch":"4.1-Stable","size_kb":11145,"archived":true,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:svc-develop-team:so-vits-svc","source_url":"https://github.com/svc-develop-team/so-vits-svc"},{"type":"has_code","target_id":"github:NaruseMioShirakana:MoeVoiceStudio","source_url":"https://github.com/NaruseMioShirakana/MoeVoiceStudio"},{"type":"has_code","target_id":"github:34j:so-vits-svc-fork","source_url":"https://github.com/34j/so-vits-svc-fork"},{"type":"has_code","target_id":"github:w-okada:voice-changer","source_url":"https://github.com/w-okada/voice-changer"},{"type":"has_code","target_id":"github:openvpi:DiffSinger","source_url":"https://github.com/openvpi/DiffSinger"},{"type":"has_code","target_id":"github:auspicious3000:contentvec","source_url":"https://github.com/auspicious3000/contentvec"},{"type":"has_code","target_id":"github:RVC-Project:Retrieval-based-Voice-Conversion-WebUI","source_url":"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI"},{"type":"has_code","target_id":"github:bshall:hubert","source_url":"https://github.com/bshall/hubert"},{"type":"has_code","target_id":"github:CNChTu:Diffusion-SVC","source_url":"https://github.com/CNChTu/Diffusion-SVC"},{"type":"has_code","target_id":"github:CNChTu:Diffusion-SVC","source_url":"https://github.com/CNChTu/Diffusion-SVC"},{"type":"has_code","target_id":"github:openvpi:vocoders","source_url":"https://github.com/openvpi/vocoders"},{"type":"has_code","target_id":"github:openvpi:vocoders","source_url":"https://github.com/openvpi/vocoders"},{"type":"has_code","target_id":"github:openvpi:vocoders","source_url":"https://github.com/openvpi/vocoders"},{"type":"has_code","target_id":"github:yxlllc:RMVPE","source_url":"https://github.com/yxlllc/RMVPE"},{"type":"has_code","target_id":"github:CNChTu:MelPE","source_url":"https://github.com/CNChTu/MelPE"},{"type":"has_code","target_id":"github:flutydeer:audio-slicer","source_url":"https://github.com/flutydeer/audio-slicer"},{"type":"has_code","target_id":"github:openvpi:audio-slicer","source_url":"https://github.com/openvpi/audio-slicer"},{"type":"has_code","target_id":"github:svc-develop-team:so-vits-svc","source_url":"https://github.com/svc-develop-team/so-vits-svc"},{"type":"has_code","target_id":"github:svc-develop-team:so-vits-svc","source_url":"https://github.com/svc-develop-team/so-vits-svc"},{"type":"has_code","target_id":"github:svc-develop-team:so-vits-svc","source_url":"https://github.com/svc-develop-team/so-vits-svc"},{"type":"has_code","target_id":"github:svc-develop-team:so-vits-svc","source_url":"https://github.com/svc-develop-team/so-vits-svc"},{"type":"has_code","target_id":"github:jaywalnut310:vits","source_url":"https://github.com/jaywalnut310/vits"},{"type":"has_code","target_id":"github:bshall:hubert","source_url":"https://github.com/bshall/hubert"},{"type":"has_code","target_id":"github:auspicious3000:contentvec","source_url":"https://github.com/auspicious3000/contentvec"},{"type":"has_code","target_id":"github:openai:whisper","source_url":"https://github.com/openai/whisper"},{"type":"has_code","target_id":"github:microsoft:unilm","source_url":"https://github.com/microsoft/unilm"},{"type":"has_code","target_id":"github:pyf98:DPHuBERT","source_url":"https://github.com/pyf98/DPHuBERT"},{"type":"has_code","target_id":"github:mmorise:World","source_url":"https://github.com/mmorise/World"},{"type":"has_code","target_id":"github:mmorise:World","source_url":"https://github.com/mmorise/World"},{"type":"has_code","target_id":"github:maxrmorrison:torchcrepe","source_url":"https://github.com/maxrmorrison/torchcrepe"},{"type":"has_code","target_id":"github:YannickJadoul:Parselmouth","source_url":"https://github.com/YannickJadoul/Parselmouth"},{"type":"has_code","target_id":"github:Dream-High:RMVPE","source_url":"https://github.com/Dream-High/RMVPE"},{"type":"has_code","target_id":"github:jik876:hifi-gan","source_url":"https://github.com/jik876/hifi-gan"},{"type":"has_code","target_id":"github:openvpi:DiffSinger","source_url":"https://github.com/openvpi/DiffSinger"},{"type":"has_code","target_id":"github:EdwardDixon:snake","source_url":"https://github.com/EdwardDixon/snake"},{"type":"has_code","target_id":"github:CNChTu:Diffusion-SVC","source_url":"https://github.com/CNChTu/Diffusion-SVC"},{"type":"has_code","target_id":"github:RVC-Project:Retrieval-based-Voice-Conversion-WebUI","source_url":"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI"},{"type":"has_code","target_id":"github:PlayVoice:whisper_ppg","source_url":"https://github.com/PlayVoice/whisper_ppg"},{"type":"has_code","target_id":"github:PlayVoice:so-vits-svc-5.0","source_url":"https://github.com/PlayVoice/so-vits-svc-5.0"},{"type":"has_code","target_id":"github:svc-develop-team:so-vits-svc","source_url":"https://github.com/svc-develop-team/so-vits-svc"}]', NULL, 'AGPL-3.0', 'approved', 80, '0e5d18d10c1f3ec6cb1a1a377f0ca841', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-svc-develop-team-so-vits-svc from https://github.com/svc-develop-team.png
Image converted to WebP: data/images/github-svc-develop-team-so-vits-svc.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-WZMIAOMIAO-deep-learning-for-image-processing', 'github--wzmiaomiao--deep-learning-for-image-processing', 'deep-learning-for-image-processing', 'WZMIAOMIAO', '* æœ¬æ•™ç¨‹æ˜¯å¯¹æœ¬äººç ”ç©¶ç”ŸæœŸé—´çš„ç ”ç©¶å†…å®¹è¿›è¡Œæ•´ç†æ€»ç»“ï¼Œæ€»ç»“çš„åŒæ—¶ä¹Ÿå¸Œæœ›èƒ½å¤Ÿå¸®åŠ©æ›´å¤šçš„å°ä¼™ä¼´ã€‚åæœŸå¦‚æœæœ‰å­¦ä¹ åˆ°æ–°çš„çŸ¥è¯†ä¹Ÿä¼šä¸å¤§å®¶ä¸€èµ·åˆ†äº«ã€‚ * æœ¬æ•™ç¨‹ä¼šä»¥è§†é¢‘çš„æ–¹å¼è¿›è¡Œåˆ†äº«ï¼Œæ•™å­¦æµç¨‹å¦‚ä¸‹ï¼š 1ï¼‰ä»‹ç»ç½‘ç»œçš„ç»“æ„ä¸åˆ›æ–°ç‚¹ 2ï¼‰ä½¿ç”¨Pytorchè¿›è¡Œç½‘ç»œçš„æ­å»ºä¸è®­ç»ƒ 3ï¼‰ä½¿ç”¨Tensorflowï¼ˆå†…éƒ¨çš„kerasæ¨¡å—ï¼‰è¿›è¡Œç½‘ç»œçš„æ­å»ºä¸è®­ç»ƒ * è¯¾ç¨‹ä¸­æ‰€æœ‰PPTéƒ½æ”¾åœ¨æ–‡ä»¶å¤¹ä¸‹ï¼Œéœ€è¦çš„è‡ªè¡Œä¸‹è½½ã€‚ * å›¾åƒåˆ†ç±» * LeNetï¼ˆå·²å®Œæˆï¼‰ * Pytorchå®˜æ–¹demo(Lenet) * Tensorflow2å®˜æ–¹demo * AlexNetï¼ˆå·²å®Œæˆï¼‰ * AlexNetç½‘ç»œè®²è§£ * Pytorchæ­å»ºAlexNet * Tensorflow2æ­å»ºAlexnet * VggNetï¼ˆå·²å®Œæˆï¼‰ * VggNetç½‘ç»œè®²è§£ * Pytorchæ­å»ºVGGç½‘ç»œ * Tensorflow2æ­å»ºVGGç½‘ç»œ * GoogLeNetï¼ˆå·²å®Œæˆï¼‰ * GoogLeNetç½‘ç»œè®²è§£ * Pytorchæ­å»ºGoogLeNetç½‘ç»œ * Tensorflow2æ­å»ºGoogLeNetç½‘ç»œ * ResNetï¼ˆå·²å®Œæˆï¼‰ * ResNet...', '["bilibili","classification","deep-learning","object-detection","pytorch","segmentation","tensorflow2","python"]', 'other', 25863, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/WZMIAOMIAO/deep-learning-for-image-processing","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# æ·±åº¦å­¦ä¹ åœ¨å›¾åƒå¤„ç†ä¸­çš„åº”ç”¨æ•™ç¨‹\n\n## å‰è¨€\n* æœ¬æ•™ç¨‹æ˜¯å¯¹æœ¬äººç ”ç©¶ç”ŸæœŸé—´çš„ç ”ç©¶å†…å®¹è¿›è¡Œæ•´ç†æ€»ç»“ï¼Œæ€»ç»“çš„åŒæ—¶ä¹Ÿå¸Œæœ›èƒ½å¤Ÿå¸®åŠ©æ›´å¤šçš„å°ä¼™ä¼´ã€‚åæœŸå¦‚æœæœ‰å­¦ä¹ åˆ°æ–°çš„çŸ¥è¯†ä¹Ÿä¼šä¸å¤§å®¶ä¸€èµ·åˆ†äº«ã€‚\n* æœ¬æ•™ç¨‹ä¼šä»¥è§†é¢‘çš„æ–¹å¼è¿›è¡Œåˆ†äº«ï¼Œæ•™å­¦æµç¨‹å¦‚ä¸‹ï¼š  \n1ï¼‰ä»‹ç»ç½‘ç»œçš„ç»“æ„ä¸åˆ›æ–°ç‚¹  \n2ï¼‰ä½¿ç”¨Pytorchè¿›è¡Œç½‘ç»œçš„æ­å»ºä¸è®­ç»ƒ  \n3ï¼‰ä½¿ç”¨Tensorflowï¼ˆå†…éƒ¨çš„kerasæ¨¡å—ï¼‰è¿›è¡Œç½‘ç»œçš„æ­å»ºä¸è®­ç»ƒ \n* è¯¾ç¨‹ä¸­æ‰€æœ‰PPTéƒ½æ”¾åœ¨`course_ppt`æ–‡ä»¶å¤¹ä¸‹ï¼Œéœ€è¦çš„è‡ªè¡Œä¸‹è½½ã€‚\n\n\n## æ•™ç¨‹ç›®å½•ï¼Œç‚¹å‡»è·³è½¬ç›¸åº”è§†é¢‘ï¼ˆåæœŸä¼šæ ¹æ®å­¦ä¹ å†…å®¹å¢åŠ ï¼‰\n\n* å›¾åƒåˆ†ç±»\n  * LeNetï¼ˆå·²å®Œæˆï¼‰\n    * [Pytorchå®˜æ–¹demo(Lenet)](https://www.bilibili.com/video/BV187411T7Ye)\n    * [Tensorflow2å®˜æ–¹demo](https://www.bilibili.com/video/BV1n7411T7o6)\n\n  * AlexNetï¼ˆå·²å®Œæˆï¼‰\n    * [AlexNetç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1p7411T7Pc)\n    * [Pytorchæ­å»ºAlexNet](https://www.bilibili.com/video/BV1W7411T7qc)\n    * [Tensorflow2æ­å»ºAlexnet](https://www.bilibili.com/video/BV1s7411T7vs)\n\n  * VggNetï¼ˆå·²å®Œæˆï¼‰\n    * [VggNetç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1q7411T7Y6)\n    * [Pytorchæ­å»ºVGGç½‘ç»œ](https://www.bilibili.com/video/BV1i7411T7ZN)\n    * [Tensorflow2æ­å»ºVGGç½‘ç»œ](https://www.bilibili.com/video/BV1q7411T76b)\n\n  * GoogLeNetï¼ˆå·²å®Œæˆï¼‰\n    * [GoogLeNetç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1z7411T7ie)\n    * [Pytorchæ­å»ºGoogLeNetç½‘ç»œ](https://www.bilibili.com/video/BV1r7411T7M5)\n    * [Tensorflow2æ­å»ºGoogLeNetç½‘ç»œ](https://www.bilibili.com/video/BV1a7411T7Ht)\n\n  * ResNetï¼ˆå·²å®Œæˆï¼‰\n    * [ResNetç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1T7411T7wa)\n    * [Pytorchæ­å»ºResNetç½‘ç»œ](https://www.bilibili.com/video/BV14E411H7Uw)\n    * [Tensorflow2æ­å»ºResNetç½‘ç»œ](https://www.bilibili.com/video/BV1WE41177Ya)\n\n  * ResNeXt (å·²å®Œæˆ)\n    * [ResNeXtç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1Ap4y1p71v/)\n    * [Pytorchæ­å»ºResNeXtç½‘ç»œ](https://www.bilibili.com/video/BV1rX4y1N7tE)\n\n  * MobileNet_V1_V2ï¼ˆå·²å®Œæˆï¼‰\n    * [MobileNet_V1_V2ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1yE411p7L7)\n    * [Pytorchæ­å»ºMobileNetV2ç½‘ç»œ](https://www.bilibili.com/video/BV1qE411T7qZ)\n    * [Tensorflow2æ­å»ºMobileNetV2ç½‘ç»œ](https://www.bilibili.com/video/BV1NE411K7tX)\n\n  * MobileNet_V3ï¼ˆå·²å®Œæˆï¼‰\n    * [MobileNet_V3ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1GK4y1p7uE)\n    * [Pytorchæ­å»ºMobileNetV3ç½‘ç»œ](https://www.bilibili.com/video/BV1zT4y1P7pd)\n    * [Tensorflow2æ­å»ºMobileNetV3ç½‘ç»œ](https://www.bilibili.com/video/BV1KA411g7wX)\n\n  * ShuffleNet_V1_V2 (å·²å®Œæˆ)\n    * [ShuffleNet_V1_V2ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV15y4y1Y7SY)\n    * [ä½¿ç”¨Pytorchæ­å»ºShuffleNetV2](https://www.bilibili.com/video/BV1dh411r76X)\n    * [ä½¿ç”¨Tensorflow2æ­å»ºShuffleNetV2](https://www.bilibili.com/video/BV1kr4y1N7bh)\n\n  * EfficientNet_V1ï¼ˆå·²å®Œæˆï¼‰\n    * [EfficientNetç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1XK4y1U7PX)\n    * [ä½¿ç”¨Pytorchæ­å»ºEfficientNet](https://www.bilibili.com/video/BV19z4y1179h/)\n    * [ä½¿ç”¨Tensorflow2æ­å»ºEfficientNet](https://www.bilibili.com/video/BV1PK4y1S7Jf)\n\n  * EfficientNet_V2 (å·²å®Œæˆ)\n    * [EfficientNetV2ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV19v41157AU)\n    * [ä½¿ç”¨Pytorchæ­å»ºEfficientNetV2](https://www.bilibili.com/video/BV1Xy4y1g74u)\n    * [ä½¿ç”¨Tensorflowæ­å»ºEfficientNetV2](https://www.bilibili.com/video/BV19K4y1g7m4)\n  \n  * RepVGGï¼ˆå·²å®Œæˆï¼‰\n    * [RepVGGç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV15f4y1o7QR)\n\n  * Vision Transformer(å·²å®Œæˆ)\n    * [Multi-Head Attentionè®²è§£](https://www.bilibili.com/video/BV15v411W78M)\n    * [Vision Transformerç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1Jh411Y7WQ)\n    * [ä½¿ç”¨Pytorchæ­å»ºVision Transformer](https://www.bilibili.com/video/BV1AL411W7dT)\n    * [ä½¿ç”¨tensorflow2æ­å»ºVision Transformer](https://www.bilibili.com/video/BV1q64y1X7GY)\n\n  * Swin Transformer(å·²å®Œæˆ)\n    * [Swin Transformerç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1pL4y1v7jC)\n    * [ä½¿ç”¨Pytorchæ­å»ºSwin Transformer](https://www.bilibili.com/video/BV1yg411K7Yc)\n    * [ä½¿ç”¨Tensorflow2æ­å»ºSwin Transformer](https://www.bilibili.com/video/BV1bR4y1t7qT)\n\n  * ConvNeXt(å·²å®Œæˆ)\n    * [ConvNeXtç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1SS4y157fu)\n    * [ä½¿ç”¨Pytorchæ­å»ºConvNeXt](https://www.bilibili.com/video/BV14S4y1L791)\n    * [ä½¿ç”¨Tensorflow2æ­å»ºConvNeXt](https://www.bilibili.com/video/BV1TS4y1V7Gz)\n\n  * MobileViT(å·²å®Œæˆ)\n    * [MobileViTç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1TG41137sb)\n    * [ä½¿ç”¨Pytorchæ­å»ºMobileViT](https://www.bilibili.com/video/BV1ae411L7Ki)\n\n* ç›®æ ‡æ£€æµ‹\n  * Faster-RCNN/FPNï¼ˆå·²å®Œæˆï¼‰\n    * [Faster-RCNNç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1af4y1m7iL)\n    * [FPNç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1dh411U7D9)\n    * [Faster-RCNNæºç è§£æ(Pytorch)](https://www.bilibili.com/video/BV1of4y1m7nj)\n\n  * SSD/RetinaNet (å·²å®Œæˆ)\n    * [SSDç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1fT4y1L7Gi)\n    * [RetinaNetç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1Q54y1L7sM)\n    * [SSDæºç è§£æ(Pytorch)](https://www.bilibili.com/video/BV1vK411H771)\n\n  * YOLO Series (å·²å®Œæˆ)\n    * [YOLOç³»åˆ—ç½‘ç»œè®²è§£(V1~V3)](https://www.bilibili.com/video/BV1yi4y1g7ro)\n    * [YOLOv3 SPPæºç è§£æ(Pytorchç‰ˆ)](https://www.bilibili.com/video/BV1t54y1C7ra)\n    * [YOLOV4ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1NF41147So)\n    * [YOLOV5ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1T3411p7zR)\n    * [YOLOX ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1JW4y1k76c)\n  \n  * FCOSï¼ˆå·²å®Œæˆï¼‰\n    * [FCOSç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1G5411X7jw)\n\n* è¯­ä¹‰åˆ†å‰² \n  * FCN (å·²å®Œæˆ)\n    * [FCNç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1J3411C7zd)\n    * [FCNæºç è§£æ(Pytorchç‰ˆ)](https://www.bilibili.com/video/BV19q4y1971Q)\n\n  * DeepLabV3 (å·²å®Œæˆ)\n    * [DeepLabV1ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1SU4y1N7Ao)\n    * [DeepLabV2ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1gP4y1G7TC)\n    * [DeepLabV3ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1Jb4y1q7j7)\n    * [DeepLabV3æºç è§£æ(Pytorchç‰ˆ)](https://www.bilibili.com/video/BV1TD4y1c7Wx)\n\n  * LR-ASPP (å·²å®Œæˆ)\n    * [LR-ASPPç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1LS4y1M76E)\n    * [LR-ASPPæºç è§£æ(Pytorchç‰ˆ)](https://www.bilibili.com/video/bv13D4y1F7ML)\n  \n  * U-Net (å·²å®Œæˆ)\n    * [U-Netç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1Vq4y127fB/)\n    * [U-Netæºç è§£æ(Pytorchç‰ˆ)](https://www.bilibili.com/video/BV1Vq4y127fB)\n  \n  * U2Net (å·²å®Œæˆ)\n    * [U2Netç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1yB4y1z7mj)\n    * [U2Netæºç è§£æ(Pytorchç‰ˆ)](https://www.bilibili.com/video/BV1Kt4y137iS)\n\n* å®ä¾‹åˆ†å‰²\n  * Mask R-CNNï¼ˆå·²å®Œæˆï¼‰\n    * [Mask R-CNNç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1ZY411774T)\n    * [Mask R-CNNæºç è§£æ(Pytorchç‰ˆ)](https://www.bilibili.com/video/BV1hY411E7wD)\n\n* å…³é”®ç‚¹æ£€æµ‹\n  * DeepPoseï¼ˆå·²å®Œæˆï¼‰\n    * [DeepPoseç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1bm421g7aJ)\n    * [DeepPoseæºç è§£æ(Pytorchç‰ˆ)](https://www.bilibili.com/video/BV1bm421g7aJ)\n\n  * HRNetï¼ˆå·²å®Œæˆï¼‰\n    * [HRNetç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1bB4y1y7qP)\n    * [HRNetæºç è§£æ(Pytorchç‰ˆ)](https://www.bilibili.com/video/BV1ar4y157JM)\n\n**[æ›´å¤šç›¸å…³è§†é¢‘è¯·è¿›å…¥æˆ‘çš„bilibilié¢‘é“æŸ¥çœ‹](https://space.bilibili.com/18161609/channel/index)**\n\n---\n\næ¬¢è¿å¤§å®¶å…³æ³¨ä¸‹æˆ‘çš„å¾®ä¿¡å…¬ä¼—å·ï¼ˆ**é˜¿å–†å­¦ä¹ å°è®°**ï¼‰ï¼Œå¹³æ—¶ä¼šæ€»ç»“äº›ç›¸å…³å­¦ä¹ åšæ–‡ã€‚    \n\nå¦‚æœæœ‰ä»€ä¹ˆé—®é¢˜ï¼Œä¹Ÿå¯ä»¥åˆ°æˆ‘çš„CSDNä¸­ä¸€èµ·è®¨è®ºã€‚\n[https://blog.csdn.net/qq_37541097/article/details/103482003](https://blog.csdn.net/qq_37541097/article/details/103482003)\n\næˆ‘çš„bilibilié¢‘é“ï¼š\n[https://space.bilibili.com/18161609/channel/index](https://space.bilibili.com/18161609/channel/index)\n', '{"language":"Python","stars":25863,"forks":8245,"watchers":25863,"open_issues":102,"topics":["bilibili","classification","deep-learning","object-detection","pytorch","segmentation","tensorflow2"],"default_branch":"master","size_kb":25636,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[]', NULL, 'GPL-3.0', 'approved', 65, '70af8da5616d56cd31e714af5f66f91c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-WZMIAOMIAO-deep-learning-for-image-processing from https://github.com/WZMIAOMIAO.png
Image converted to WebP: data/images/github-WZMIAOMIAO-deep-learning-for-image-processing.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-HumanSignal-label-studio', 'github--humansignal--label-studio', 'label-studio', 'HumanSignal', '<img src="https://user-images.githubusercontent.com/12534576/192582340-4c9e4401-1fe6-4dbb-95bb-fdbba5493f61.png"/> !GitHub !label-studio:build !GitHub release Website â€¢ Docs â€¢ Join Slack Community <img src="https://app.heartex.ai/docs/images/slack-mini.png" width="18px"/> <!-- <a href="https://labelstud.io/blog/release-130.html"><img src="https://github.com/HumanSignal/label-studio/raw/master/docs/themes/htx/source/images/release-130/LS-Hits-v1.3.png" align="right" /></a> --> Label Studio is ...', '["annotation","annotation-tool","annotations","boundingbox","computer-vision","data-labeling","dataset","datasets","deep-learning","image-annotation","image-classification","image-labeling","image-labelling-tool","label-studio","labeling","labeling-tool","mlops","semantic-segmentation","text-annotation","yolo","typescript"]', 'other', 25735, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/HumanSignal/label-studio","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<img src="https://user-images.githubusercontent.com/12534576/192582340-4c9e4401-1fe6-4dbb-95bb-fdbba5493f61.png"/>\n\n![GitHub](https://img.shields.io/github/license/heartexlabs/label-studio?logo=heartex) ![label-studio:build](https://github.com/HumanSignal/label-studio/workflows/label-studio:build/badge.svg) ![GitHub release](https://img.shields.io/github/v/release/heartexlabs/label-studio?include_prereleases)\n\n[Website](https://labelstud.io/) â€¢ [Docs](https://labelstud.io/guide/) â€¢ [Join Slack Community <img src="https://app.heartex.ai/docs/images/slack-mini.png" width="18px"/>](https://slack.labelstud.io/?source=github-1)\n\n\n## What is Label Studio?\n\n<!-- <a href="https://labelstud.io/blog/release-130.html"><img src="https://github.com/HumanSignal/label-studio/raw/master/docs/themes/htx/source/images/release-130/LS-Hits-v1.3.png" align="right" /></a> -->\n\nLabel Studio is an open source data labeling tool. It lets you label data types like audio, text, images, videos, and time series with a simple and straightforward UI and export to various model formats. It can be used to prepare raw data or improve existing training data to get more accurate ML models.\n\n- [Try out Label Studio](#try-out-label-studio)\n- [What you get from Label Studio](#what-you-get-from-label-studio)\n- [Included templates for labeling data in Label Studio](#included-templates-for-labeling-data-in-label-studio)\n- [Set up machine learning models with Label Studio](#set-up-machine-learning-models-with-Label-Studio)\n- [Integrate Label Studio with your existing tools](#integrate-label-studio-with-your-existing-tools)\n\n![Gif of Label Studio annotating different types of data](/images/annotation_examples.gif)\n\nHave a custom dataset? You can customize Label Studio to fit your needs. Read an [introductory blog post](https://towardsdatascience.com/introducing-label-studio-a-swiss-army-knife-of-data-labeling-140c1be92881) to learn more. \n\n## Try out Label Studio\n\nInstall Label Studio locally or deploy it in a cloud instance. [Or sign up for a free trial of our Starter Cloud edition!](https://humansignal.com/platform/starter-cloud/) You can learn more about what each edition offers [here](https://labelstud.io/guide/label_studio_compare). \n\n- [Install locally with Docker](#install-locally-with-docker)\n- [Run with Docker Compose (Label Studio + Nginx + PostgreSQL)](#run-with-docker-compose)\n- [Install locally with pip](#install-locally-with-pip)\n- [Install locally with poetry](#install-locally-with-poetry)\n- [Install locally with Anaconda](#install-locally-with-anaconda)\n- [Install for local development](#install-for-local-development)\n- [Deploy in a cloud instance](#deploy-in-a-cloud-instance)\n\n### Install locally with Docker\nOfficial Label Studio docker image is [here](https://hub.docker.com/r/heartexlabs/label-studio) and it can be downloaded with `docker pull`. \nRun Label Studio in a Docker container and access it at `http://localhost:8080`.\n\n\n```bash\ndocker pull heartexlabs/label-studio:latest\ndocker run -it -p 8080:8080 -v $(pwd)/mydata:/label-studio/data heartexlabs/label-studio:latest\n```\nYou can find all the generated assets, including SQLite3 database storage `label_studio.sqlite3` and uploaded files, in the `./mydata` directory.\n\n#### Override default Docker install\nYou can override the default launch command by appending the new arguments:\n```bash\ndocker run -it -p 8080:8080 -v $(pwd)/mydata:/label-studio/data heartexlabs/label-studio:latest label-studio --log-level DEBUG\n```\n\n#### Build a local image with Docker\nIf you want to build a local image, run:\n```bash\ndocker build -t heartexlabs/label-studio:latest .\n```\n\n### Run with Docker Compose\nDocker Compose script provides production-ready stack consisting of the following components:\n\n- Label Studio\n- [Nginx](https://www.nginx.com/) - proxy web server used to load various static data, including uploaded audio, images, etc.\n- [PostgreSQL](https://www.postgresql.org/) - production-ready database that replaces less performant SQLite3.\n\nTo start using the app from `http://localhost` run this command:\n```bash\ndocker-compose up\n```\n\n### Run with Docker Compose + MinIO\nYou can also run it with an additional MinIO server for local S3 storage. This is particularly useful when you want to \ntest the behavior with S3 storage on your local system. To start Label Studio in this way, you need to run the following command:\n````bash\n# Add sudo on Linux if you are not a member of the docker group\ndocker compose -f docker-compose.yml -f docker-compose.minio.yml up -d\n````\nIf you do not have a static IP address, you must create an entry in your hosts file so that both Label Studio and your \nbrowser can access the MinIO server. For more detailed instructions, please refer to [our guide on storing data](docs/source/guide/storedata.md).\n\n\n### Install locally with pip\n\n```bash\n# Requires Python >=3.8\npip install label-studio\n\n# Start the server at http://localhost:8080\nlabel-studio\n```\n\n### Install locally with poetry\n\n```bash\n### install poetry\npip install poetry\n\n### set poetry environment\npoetry new my-label-studio\ncd my-label-studio\npoetry add label-studio\n\n### activate poetry environment\npoetry shell\n\n### Start the server at http://localhost:8080\nlabel-studio\n```\n\n### Install locally with Anaconda\n\n```bash\nconda create --name label-studio\nconda activate label-studio\nconda install psycopg2\npip install label-studio\n```\n\n### Install for local development\n\nYou can run the latest Label Studio version locally without installing the package from pypi. \n\n```bash\n# Install all package dependencies\npip install poetry\npoetry install\n# Run database migrations\npython label_studio/manage.py migrate\npython label_studio/manage.py collectstatic\n# Start the server in development mode at http://localhost:8080\npython label_studio/manage.py runserver\n```\n\n### Deploy in a cloud instance\n\nYou can deploy Label Studio with one click in Heroku, Microsoft Azure, or Google Cloud Platform: \n\n<a href="https://www.heroku.com/deploy?template=https://github.com/HumanSignal/label-studio/tree/heroku-persistent-pg"><img src="https://www.herokucdn.com/deploy/button.svg" alt="Deploy" height="30px"></a>\n[<img src="https://aka.ms/deploytoazurebutton" height="30px">](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fhumansignal%2Flabel-studio%2Fdevelop%2Fazuredeploy.json)\n[<img src="https://deploy.cloud.run/button.svg" height="30px">](https://deploy.cloud.run)\n\n\n#### Apply frontend changes\n\nFor information about updating the frontend, see [label-studio/web/README.md](https://github.com/HumanSignal/label-studio/blob/develop/web/README.md#installation-instructions).\n\n\n#### Install dependencies on Windows \nTo run Label Studio on Windows, download and install the following wheel packages from [Gohlke builds](https://www.lfd.uci.edu/~gohlke/pythonlibs) to ensure you''re using the correct version of Python:\n- [lxml](https://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml)\n\n```bash\n# Upgrade pip \npip install -U pip\n\n# If you''re running Win64 with Python 3.8, install the packages downloaded from Gohlke:\npip install lxmlâ€‘4.5.0â€‘cp38â€‘cp38â€‘win_amd64.whl\n\n# Install label studio\npip install label-studio\n```\n\n### Run test suite\nTo add the tests'' dependencies to your local install:\n\n```bash\npoetry install --with test\n```\n\nAlternatively, it is possible to run the unit tests from a Docker container in which the test dependencies are installed:\n\n\n```bash\nmake build-testing-image\nmake docker-testing-shell\n```\n\nIn either case, to run the unit tests:\n\n```bash\ncd label_studio\n\n# sqlite3\nDJANGO_DB=sqlite DJANGO_SETTINGS_MODULE=core.settings.label_studio pytest -vv\n\n# postgres (assumes default postgres user,db,pass. Will not work in Docker\n# testing container without additional configuration)\nDJANGO_DB=default DJANGO_SETTINGS_MODULE=core.settings.label_studio pytest -vv\n```\n \n## What you get from Label Studio\n\nhttps://github.com/user-attachments/assets/525ad5ff-6904-4398-b507-7e8954268d69\n\n- **Multi-user labeling** sign up and login, when you create an annotation it''s tied to your account.\n- **Multiple projects** to work on all your datasets in one instance.\n- **Streamlined design** helps you focus on your task, not how to use the software.\n- **Configurable label formats** let you customize the visual interface to meet your specific labeling needs.\n- **Support for multiple data types** including images, audio, text, HTML, time-series, and video. \n- **Import from files or from cloud storage** in Amazon AWS S3, Google Cloud Storage, or JSON, CSV, TSV, RAR, and ZIP archives. \n- **Integration with machine learning models** so that you can visualize and compare predictions from different models and perform pre-labeling.\n- **Embed it in your data pipeline** REST API makes it easy to make it a part of your pipeline\n\n## Included templates for labeling data in Label Studio \n\nLabel Studio includes a variety of templates to help you label your data, or you can create your own using specifically designed configuration language. The most common templates and use cases for labeling include the following cases:\n\n<img src="/images/template-types.png" />\n\n## Set up machine learning models with Label Studio\n\nConnect your favorite machine learning model using the Label Studio Machine Learning SDK. Follow these steps:\n\n1. Start your own machine learning backend server. See [more detailed instructions](https://github.com/HumanSignal/label-studio-ml-backend).\n2. Connect Label Studio to the server on the model page found in project settings.\n\nThis lets you:\n\n- **Pre-label** your data using model predictions. \n- Do **online learning** and retrain your model while new annotations are being created. \n- Do **active learning** by labeling only the most complex examples in your data.\n\n## Integrate Label Studio with your existing tools\n\nYou can use Label Studio as an independent part of your machine learning workflow or integrate the frontend or backend into your existing tools.  \n\n## Ecosystem\n\n| Project | Description |\n|-|-|\n| label-studio | Server, distributed as a pip package |\n| [Frontend library](web/libs/editor/) | The Label Studio frontend library. This uses React to build the UI and mobx-state-tree for state management. |  \n| [Data Manager library](web/libs/datamanager/) | A library for the Data Manager, our data exploration tool. | \n| [label-studio-converter](https://github.com/HumanSignal/label-studio-sdk/tree/master/src/label_studio_sdk/converter) | Encode labels in the format of your favorite machine learning library |\n| [label-studio-transformers](https://github.com/HumanSignal/label-studio-transformers) | Transformers library connected and configured for use with Label Studio |\n\n## Citation\n\nInclude a citation for Label Studio in the **References** section of your articles:\n\n```tex\n@misc{Label Studio,\n  title={{Label Studio}: Data labeling software},\n  url={https://github.com/HumanSignal/label-studio},\n  note={Open source software available from https://github.com/HumanSignal/label-studio},\n  author={\n    Maxim Tkachenko and\n    Mikhail Malyuk and\n    Andrey Holmanyuk and\n    Nikolai Liubimov},\n  year={2020-2025},\n}\n```\n\n## License\n\nThis software is licensed under the [Apache 2.0 LICENSE](/LICENSE) Â© [Heartex](https://www.heartex.com/). 2020-2025\n\n<img src="https://user-images.githubusercontent.com/12534576/192582529-cf628f58-abc5-479b-a0d4-8a3542a4b35e.png" title="Hey everyone!" width="180" />\n', '{"language":"TypeScript","stars":25735,"forks":3240,"watchers":25735,"open_issues":948,"topics":["annotation","annotation-tool","annotations","boundingbox","computer-vision","data-labeling","dataset","datasets","deep-learning","image-annotation","image-classification","image-labeling","image-labelling-tool","label-studio","labeling","labeling-tool","mlops","semantic-segmentation","text-annotation","yolo"],"default_branch":"develop","size_kb":2950135,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:HumanSignal:label-studio","source_url":"https://github.com/HumanSignal/label-studio"},{"type":"has_code","target_id":"github:HumanSignal:label-studio","source_url":"https://github.com/HumanSignal/label-studio"},{"type":"has_code","target_id":"github:HumanSignal:label-studio","source_url":"https://github.com/HumanSignal/label-studio"},{"type":"has_code","target_id":"github:HumanSignal:label-studio","source_url":"https://github.com/HumanSignal/label-studio"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:HumanSignal:label-studio-ml-backend","source_url":"https://github.com/HumanSignal/label-studio-ml-backend"},{"type":"has_code","target_id":"github:HumanSignal:label-studio-sdk","source_url":"https://github.com/HumanSignal/label-studio-sdk"},{"type":"has_code","target_id":"github:HumanSignal:label-studio-transformers","source_url":"https://github.com/HumanSignal/label-studio-transformers"},{"type":"has_code","target_id":"github:HumanSignal:label-studio},","source_url":"https://github.com/HumanSignal/label-studio},"},{"type":"has_code","target_id":"github:HumanSignal:label-studio},","source_url":"https://github.com/HumanSignal/label-studio},"}]', NULL, 'Apache-2.0', 'approved', 80, 'dd1cdcc1d05b052b7a5d7d295c400ad5', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-HumanSignal-label-studio from https://github.com/HumanSignal.png
Image converted to WebP: data/images/github-HumanSignal-label-studio.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-junyanz-pytorch-CycleGAN-and-pix2pix', 'github--junyanz--pytorch-cyclegan-and-pix2pix', 'pytorch-CycleGAN-and-pix2pix', 'junyanz', '<img src=''imgs/horse2zebra.gif'' align="right" width=384> <br><br><br> **Udpate in 2025**: we recently updated the code to support Python 3.11 and PyTorch 2.4. It also supports DDP for single-machine multiple-GPU training. (Please use ) **New**: Please check out img2img-turbo repo that includes both pix2pix-turbo and CycleGAN-Turbo. Our new one-step image-to-image translation methods can support both paired and unpaired training and produce better results by leveraging the pre-trained StableDi...', '["computer-graphics","computer-vision","cyclegan","deep-learning","gan","gans","generative-adversarial-network","image-generation","image-manipulation","pix2pix","pytorch","python"]', 'other', 24769, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<img src=''imgs/horse2zebra.gif'' align="right" width=384>\n\n<br><br><br>\n\n# CycleGAN and pix2pix in PyTorch\n\n**Udpate in 2025**: we recently updated the code to support Python 3.11 and PyTorch 2.4. It also supports DDP for single-machine multiple-GPU training. (Please use `torchrun --nproc_per_node=4 train.py ...`)\n\n**New**: Please check out [img2img-turbo](https://github.com/GaParmar/img2img-turbo) repo that includes both pix2pix-turbo and CycleGAN-Turbo. Our new one-step image-to-image translation methods can support both paired and unpaired training and produce better results by leveraging the pre-trained StableDiffusion-Turbo model. The inference time for 512x512 image is 0.29 sec on A6000 and 0.11 sec on A100.\n\nPlease check out [contrastive-unpaired-translation](https://github.com/taesungp/contrastive-unpaired-translation) (CUT), our new unpaired image-to-image translation model that enables fast and memory-efficient training.\n\nWe provide PyTorch implementations for both unpaired and paired image-to-image translation.\n\nThe code was written by [Jun-Yan Zhu](https://github.com/junyanz) and [Taesung Park](https://github.com/taesungp), and supported by [Tongzhou Wang](https://github.com/SsnL).\n\nThis PyTorch implementation produces results comparable to or better than our original Torch software. If you would like to reproduce the same results as in the papers, check out the original [CycleGAN Torch](https://github.com/junyanz/CycleGAN) and [pix2pix Torch](https://github.com/phillipi/pix2pix) code in Lua/Torch.\n\n**Note**: The current software works well with PyTorch 2.4+. Check out the older [branch](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/pytorch0.3.1) that supports PyTorch 0.1-0.3.\n\nYou may find useful information in [training/test tips](docs/tips.md) and [frequently asked questions](docs/qa.md). To implement custom models and datasets, check out our [templates](#custom-model-and-dataset). To help users better understand and adapt our codebase, we provide an [overview](docs/overview.md) of the code structure of this repository.\n\n**CycleGAN: [Project](https://junyanz.github.io/CycleGAN/) | [Paper](https://arxiv.org/pdf/1703.10593.pdf) | [Torch](https://github.com/junyanz/CycleGAN) |\n[Tensorflow Core Tutorial](https://www.tensorflow.org/tutorials/generative/cyclegan) | [PyTorch Colab](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb)**\n\n<img src="https://junyanz.github.io/CycleGAN/images/teaser_high_res.jpg" width="800"/>\n\n**Pix2pix: [Project](https://phillipi.github.io/pix2pix/) | [Paper](https://arxiv.org/pdf/1611.07004.pdf) | [Torch](https://github.com/phillipi/pix2pix) |\n[Tensorflow Core Tutorial](https://www.tensorflow.org/tutorials/generative/pix2pix) | [PyTorch Colab](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb)**\n\n<img src="https://phillipi.github.io/pix2pix/images/teaser_v3.png" width="800px"/>\n\n**[EdgesCats Demo](https://affinelayer.com/pixsrv/) | [pix2pix-tensorflow](https://github.com/affinelayer/pix2pix-tensorflow) | by [Christopher Hesse](https://twitter.com/christophrhesse)**\n\n<img src=''imgs/edges2cats.jpg'' width="400px"/>\n\nIf you use this code for your research, please cite:\n\nUnpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.<br>\n[Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/)\*, [Taesung Park](https://taesung.me/)\*, [Phillip Isola](https://people.eecs.berkeley.edu/~isola/), [Alexei A. Efros](https://people.eecs.berkeley.edu/~efros). In ICCV 2017. (\* equal contributions) [[Bibtex]](https://junyanz.github.io/CycleGAN/CycleGAN.txt)\n\nImage-to-Image Translation with Conditional Adversarial Networks.<br>\n[Phillip Isola](https://people.eecs.berkeley.edu/~isola), [Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/), [Tinghui Zhou](https://people.eecs.berkeley.edu/~tinghuiz), [Alexei A. Efros](https://people.eecs.berkeley.edu/~efros). In CVPR 2017. [[Bibtex]](https://www.cs.cmu.edu/~junyanz/projects/pix2pix/pix2pix.bib)\n\n## Talks and Course\n\npix2pix slides: [keynote](http://efrosgans.eecs.berkeley.edu/CVPR18_slides/pix2pix.key) | [pdf](http://efrosgans.eecs.berkeley.edu/CVPR18_slides/pix2pix.pdf),\nCycleGAN slides: [pptx](http://efrosgans.eecs.berkeley.edu/CVPR18_slides/CycleGAN.pptx) | [pdf](http://efrosgans.eecs.berkeley.edu/CVPR18_slides/CycleGAN.pdf)\n\nCycleGAN course assignment [code](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-code.zip) and [handout](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-handout.pdf) designed by Prof. [Roger Grosse](http://www.cs.toronto.edu/~rgrosse/) for [CSC321](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/) "Intro to Neural Networks and Machine Learning" at University of Toronto. Please contact the instructor if you would like to adopt it in your course.\n\n## Colab Notebook\n\nTensorFlow Core CycleGAN Tutorial: [Google Colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb) | [Code](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb)\n\nTensorFlow Core pix2pix Tutorial: [Google Colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb) | [Code](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb)\n\nPyTorch Colab notebook: [CycleGAN](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb) and [pix2pix](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb)\n\nZeroCostDL4Mic Colab notebook: [CycleGAN](https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks_Beta/CycleGAN_ZeroCostDL4Mic.ipynb) and [pix2pix](https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks_Beta/pix2pix_ZeroCostDL4Mic.ipynb)\n\n## Other implementations\n\n### CycleGAN\n\n<p><a href="https://github.com/leehomyc/cyclegan-1"> [Tensorflow]</a> (by Harry Yang),\n<a href="https://github.com/architrathore/CycleGAN/">[Tensorflow]</a> (by Archit Rathore),\n<a href="https://github.com/vanhuyz/CycleGAN-TensorFlow">[Tensorflow]</a> (by Van Huy),\n<a href="https://github.com/XHUJOY/CycleGAN-tensorflow">[Tensorflow]</a> (by Xiaowei Hu),\n<a href="https://github.com/LynnHo/CycleGAN-Tensorflow-2"> [Tensorflow2]</a> (by Zhenliang He),\n<a href="https://github.com/luoxier/CycleGAN_Tensorlayer"> [TensorLayer1.0]</a> (by luoxier),\n<a href="https://github.com/tensorlayer/cyclegan"> [TensorLayer2.0]</a> (by zsdonghao),\n<a href="https://github.com/Aixile/chainer-cyclegan">[Chainer]</a> (by Yanghua Jin),\n<a href="https://github.com/yunjey/mnist-svhn-transfer">[Minimal PyTorch]</a> (by yunjey),\n<a href="https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/CycleGAN">[Mxnet]</a> (by Ldpe2G),\n<a href="https://github.com/tjwei/GANotebooks">[lasagne/Keras]</a> (by tjwei),\n<a href="https://github.com/simontomaskarlsson/CycleGAN-Keras">[Keras]</a> (by Simon Karlsson),\n<a href="https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Oneflow-Python/CycleGAN">[OneFlow]</a> (by Ldpe2G)\n</p>\n</ul>\n\n### pix2pix\n\n<p><a href="https://github.com/affinelayer/pix2pix-tensorflow"> [Tensorflow]</a> (by Christopher Hesse),\n<a href="https://github.com/Eyyub/tensorflow-pix2pix">[Tensorflow]</a> (by EyyÃ¼b Sariu),\n<a href="https://github.com/datitran/face2face-demo"> [Tensorflow (face2face)]</a> (by Dat Tran),\n<a href="https://github.com/awjuliani/Pix2Pix-Film"> [Tensorflow (film)]</a> (by Arthur Juliani),\n<a href="https://github.com/kaonashi-tyc/zi2zi">[Tensorflow (zi2zi)]</a> (by Yuchen Tian),\n<a href="https://github.com/pfnet-research/chainer-pix2pix">[Chainer]</a> (by mattya),\n<a href="https://github.com/tjwei/GANotebooks">[tf/torch/keras/lasagne]</a> (by tjwei),\n<a href="https://github.com/taey16/pix2pixBEGAN.pytorch">[Pytorch]</a> (by taey16)\n</p>\n</ul>\n\n## Prerequisites\n\n- Linux or macOS\n- Python 3\n- CPU or NVIDIA GPU + CUDA CuDNN\n\n## Getting Started\n\n### Installation\n\n- Clone this repo:\n\n```bash\ngit clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\ncd pytorch-CycleGAN-and-pix2pix\n```\n\n- Install [PyTorch](http://pytorch.org) and other dependencies. For Conda users, you can create a new Conda environment by\n\n```bash\nconda env create -f environment.yml\n```\n\nand then activate the environment by\n\n```bash\nconda activate pytorch-img2img\n```\n\n- For Docker users, we provide the pre-built Docker image and Dockerfile. Please refer to our [Docker](docs/docker.md) page.\n- For Repl users, please click [![Run on Repl.it](https://repl.it/badge/github/junyanz/pytorch-CycleGAN-and-pix2pix)](https://repl.it/github/junyanz/pytorch-CycleGAN-and-pix2pix).\n\n### CycleGAN train/test\n\n- Download a CycleGAN dataset (e.g. maps):\n\n```bash\nbash ./datasets/download_cyclegan_dataset.sh maps\n```\n\n- To log training progress and test images to W&B dashboard, set the `--use_wandb` flag with training script\n- Train a model:\n\n```bash\n#!./scripts/train_cyclegan.sh\npython train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --use_wandb\n```\n\nTo see more intermediate results, check out `./checkpoints/maps_cyclegan/web/index.html`.\n\n- Test the model:\n\n```bash\n#!./scripts/test_cyclegan.sh\npython test.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan\n```\n\n- The test results will be saved to a html file here: `./results/maps_cyclegan/latest_test/index.html`.\n\n### pix2pix train/test\n\n- Download a pix2pix dataset (e.g.[facades](http://cmp.felk.cvut.cz/~tylecr1/facade/)):\n\n```bash\nbash ./datasets/download_pix2pix_dataset.sh facades\n```\n\n- To log training progress and test images to W&B dashboard, set the `--use_wandb` flag with training script\n- Train a model:\n\n```bash\n#!./scripts/train_pix2pix.sh\npython train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA  --use_wandb\n```\n\nTo see more intermediate results, check out `./checkpoints/facades_pix2pix/web/index.html`.\n\n- Test the model (`bash ./scripts/test_pix2pix.sh`):\n\n```bash\n#!./scripts/test_pix2pix.sh\npython test.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA\n```\n\n- The test results will be saved to a html file here: `./results/facades_pix2pix/test_latest/index.html`. You can find more scripts at `scripts` directory.\n- To train and test pix2pix-based colorization models, please add `--model colorization` and `--dataset_mode colorization`. See our training [tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#notes-on-colorization) for more details.\n\n### Apply a pre-trained model (CycleGAN)\n\n- You can download a pretrained model (e.g. horse2zebra) with the following script:\n\n```bash\nbash ./scripts/download_cyclegan_model.sh horse2zebra\n```\n\n- The pretrained model is saved at `./checkpoints/{name}_pretrained/latest_net_G.pth`. Check [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/scripts/download_cyclegan_model.sh#L3) for all the available CycleGAN models.\n- To test the model, you also need to download the horse2zebra dataset:\n\n```bash\nbash ./datasets/download_cyclegan_dataset.sh horse2zebra\n```\n\n- Then generate the results using\n\n```bash\npython test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout\n```\n\n- The option `--model test` is used for generating results of CycleGAN only for one side. This option will automatically set `--dataset_mode single`, which only loads the images from one set. On the contrary, using `--model cycle_gan` requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at `./results/`. Use `--results_dir {directory_path_to_save_result}` to specify the results directory.\n\n- For pix2pix and your own models, you need to explicitly specify `--netG`, `--norm`, `--no_dropout` to match the generator architecture of the trained model. See this [FAQ](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#runtimeerror-errors-in-loading-state_dict-812-671461-296) for more details.\n\n### Apply a pre-trained model (pix2pix)\n\nDownload a pre-trained model with `./scripts/download_pix2pix_model.sh`.\n\n- Check [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/scripts/download_pix2pix_model.sh#L3) for all the available pix2pix models. For example, if you would like to download label2photo model on the Facades dataset,\n\n```bash\nbash ./scripts/download_pix2pix_model.sh facades_label2photo\n```\n\n- Download the pix2pix facades datasets:\n\n```bash\nbash ./datasets/download_pix2pix_dataset.sh facades\n```\n\n- Then generate the results using\n\n```bash\npython test.py --dataroot ./datasets/facades/ --direction BtoA --model pix2pix --name facades_label2photo_pretrained\n```\n\n- Note that we specified `--direction BtoA` as Facades dataset''s A to B direction is photos to labels.\n\n- If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use `--model test` option. See `./scripts/test_single.sh` for how to apply a model to Facade label maps (stored in the directory `facades/testB`).\n\n- See a list of currently available models at `./scripts/download_pix2pix_model.sh`\n\n### Multi-GPU training\n\nTo train a model on multiple GPUs, please use `torchrun --nproc_per_node=4 train.py ...` instead of `python train.py ...`. We also need to use synchronized batchnorm by setting `--norm sync_batch` (or `--norm sync_instance` for instance normgalization). The `--norm batch` is not compatible with DDP.\n\n## [Docker](docs/docker.md)\n\nWe provide the pre-built Docker image and Dockerfile that can run this code repo. See [docker](docs/docker.md).\n\n## [Datasets](docs/datasets.md)\n\nDownload pix2pix/CycleGAN datasets and create your own datasets.\n\n## [Training/Test Tips](docs/tips.md)\n\nBest practice for training and testing your models.\n\n## [Frequently Asked Questions](docs/qa.md)\n\nBefore you post a new question, please first look at the above Q & A and existing GitHub issues.\n\n## Custom Model and Dataset\n\nIf you plan to implement custom models and dataset for your new applications, we provide a dataset [template](data/template_dataset.py) and a model [template](models/template_model.py) as a starting point.\n\n## [Code structure](docs/overview.md)\n\nTo help users better understand and use our code, we briefly overview the functionality and implementation of each package and each module.\n\n## Pull Request\n\nYou are always welcome to contribute to this repository by sending a [pull request](https://help.github.com/articles/about-pull-requests/).\nPlease run `flake8 --ignore E501 .` and `pytest scripts/test_before_push.py -v` before you commit the code. Please also update the code structure [overview](docs/overview.md) accordingly if you add or remove files.\n\n## Citation\n\nIf you use this code for your research, please cite our papers.\n\n```\n@inproceedings{CycleGAN2017,\n  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},\n  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},\n  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},\n  year={2017}\n}\n\n\n@inproceedings{isola2017image,\n  title={Image-to-Image Translation with Conditional Adversarial Networks},\n  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},\n  booktitle={Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on},\n  year={2017}\n}\n```\n\n## Other Languages\n\n[Spanish](docs/README_es.md)\n\n## Related Projects\n\n[img2img-turbo](https://github.com/GaParmar/img2img-turbo)<br>\n[contrastive-unpaired-translation](https://github.com/taesungp/contrastive-unpaired-translation) (CUT)<br>\n[CycleGAN-Torch](https://github.com/junyanz/CycleGAN) |\n[pix2pix-Torch](https://github.com/phillipi/pix2pix) | [pix2pixHD](https://github.com/NVIDIA/pix2pixHD)|\n[BicycleGAN](https://github.com/junyanz/BicycleGAN) | [vid2vid](https://tcwang0509.github.io/vid2vid/) | [SPADE/GauGAN](https://github.com/NVlabs/SPADE)<br>\n[iGAN](https://github.com/junyanz/iGAN) | [GAN Dissection](https://github.com/CSAILVision/GANDissect) | [GAN Paint](http://ganpaint.io/)\n\n## Cat Paper Collection\n\nIf you love cats, and love reading cool graphics, vision, and learning papers, please check out the Cat Paper [Collection](https://github.com/junyanz/CatPapers).\n\n## Acknowledgments\n\nOur code is inspired by [pytorch-DCGAN](https://github.com/pytorch/examples/tree/master/dcgan).\n', '{"language":"Python","stars":24769,"forks":6546,"watchers":24769,"open_issues":586,"topics":["computer-graphics","computer-vision","cyclegan","deep-learning","gan","gans","generative-adversarial-network","image-generation","image-manipulation","pix2pix","pytorch"],"default_branch":"master","size_kb":8464,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:GaParmar:img2img-turbo","source_url":"https://github.com/GaParmar/img2img-turbo"},{"type":"has_code","target_id":"github:taesungp:contrastive-unpaired-translation","source_url":"https://github.com/taesungp/contrastive-unpaired-translation"},{"type":"has_code","target_id":"github:junyanz:CycleGAN","source_url":"https://github.com/junyanz/CycleGAN"},{"type":"has_code","target_id":"github:phillipi:pix2pix","source_url":"https://github.com/phillipi/pix2pix"},{"type":"has_code","target_id":"github:junyanz:pytorch-CycleGAN-and-pix2pix","source_url":"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"},{"type":"has_code","target_id":"github:junyanz:CycleGAN","source_url":"https://github.com/junyanz/CycleGAN"},{"type":"has_code","target_id":"github:phillipi:pix2pix","source_url":"https://github.com/phillipi/pix2pix"},{"type":"has_code","target_id":"github:affinelayer:pix2pix-tensorflow","source_url":"https://github.com/affinelayer/pix2pix-tensorflow"},{"type":"has_code","target_id":"github:tensorflow:docs","source_url":"https://github.com/tensorflow/docs"},{"type":"has_code","target_id":"github:tensorflow:docs","source_url":"https://github.com/tensorflow/docs"},{"type":"has_code","target_id":"github:leehomyc:cyclegan-1\">","source_url":"https://github.com/leehomyc/cyclegan-1\">"},{"type":"has_code","target_id":"github:architrathore:CycleGAN","source_url":"https://github.com/architrathore/CycleGAN"},{"type":"has_code","target_id":"github:vanhuyz:CycleGAN-TensorFlow\">[Tensorflow]<","source_url":"https://github.com/vanhuyz/CycleGAN-TensorFlow\">[Tensorflow]<"},{"type":"has_code","target_id":"github:XHUJOY:CycleGAN-tensorflow\">[Tensorflow]<","source_url":"https://github.com/XHUJOY/CycleGAN-tensorflow\">[Tensorflow]<"},{"type":"has_code","target_id":"github:LynnHo:CycleGAN-Tensorflow-2\">","source_url":"https://github.com/LynnHo/CycleGAN-Tensorflow-2\">"},{"type":"has_code","target_id":"github:luoxier:CycleGAN_Tensorlayer\">","source_url":"https://github.com/luoxier/CycleGAN_Tensorlayer\">"},{"type":"has_code","target_id":"github:tensorlayer:cyclegan\">","source_url":"https://github.com/tensorlayer/cyclegan\">"},{"type":"has_code","target_id":"github:Aixile:chainer-cyclegan\">[Chainer]<","source_url":"https://github.com/Aixile/chainer-cyclegan\">[Chainer]<"},{"type":"has_code","target_id":"github:yunjey:mnist-svhn-transfer\">[Minimal","source_url":"https://github.com/yunjey/mnist-svhn-transfer\">[Minimal"},{"type":"has_code","target_id":"github:Ldpe2G:DeepLearningForFun","source_url":"https://github.com/Ldpe2G/DeepLearningForFun"},{"type":"has_code","target_id":"github:tjwei:GANotebooks\">[lasagne","source_url":"https://github.com/tjwei/GANotebooks\">[lasagne"},{"type":"has_code","target_id":"github:simontomaskarlsson:CycleGAN-Keras\">[Keras]<","source_url":"https://github.com/simontomaskarlsson/CycleGAN-Keras\">[Keras]<"},{"type":"has_code","target_id":"github:Ldpe2G:DeepLearningForFun","source_url":"https://github.com/Ldpe2G/DeepLearningForFun"},{"type":"has_code","target_id":"github:affinelayer:pix2pix-tensorflow\">","source_url":"https://github.com/affinelayer/pix2pix-tensorflow\">"},{"type":"has_code","target_id":"github:Eyyub:tensorflow-pix2pix\">[Tensorflow]<","source_url":"https://github.com/Eyyub/tensorflow-pix2pix\">[Tensorflow]<"},{"type":"has_code","target_id":"github:datitran:face2face-demo\">","source_url":"https://github.com/datitran/face2face-demo\">"},{"type":"has_code","target_id":"github:awjuliani:Pix2Pix-Film\">","source_url":"https://github.com/awjuliani/Pix2Pix-Film\">"},{"type":"has_code","target_id":"github:kaonashi-tyc:zi2zi\">[Tensorflow","source_url":"https://github.com/kaonashi-tyc/zi2zi\">[Tensorflow"},{"type":"has_code","target_id":"github:pfnet-research:chainer-pix2pix\">[Chainer]<","source_url":"https://github.com/pfnet-research/chainer-pix2pix\">[Chainer]<"},{"type":"has_code","target_id":"github:tjwei:GANotebooks\">[tf","source_url":"https://github.com/tjwei/GANotebooks\">[tf"},{"type":"has_code","target_id":"github:taey16:pix2pixBEGAN.pytorch\">[Pytorch]<","source_url":"https://github.com/taey16/pix2pixBEGAN.pytorch\">[Pytorch]<"},{"type":"has_code","target_id":"github:junyanz:pytorch-CycleGAN-and-pix2pix","source_url":"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"},{"type":"has_code","target_id":"github:junyanz:pytorch-CycleGAN-and-pix2pix","source_url":"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"},{"type":"has_code","target_id":"github:junyanz:pytorch-CycleGAN-and-pix2pix","source_url":"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"},{"type":"has_code","target_id":"github:junyanz:pytorch-CycleGAN-and-pix2pix","source_url":"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"},{"type":"has_code","target_id":"github:junyanz:pytorch-CycleGAN-and-pix2pix","source_url":"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"},{"type":"has_code","target_id":"github:GaParmar:img2img-turbo","source_url":"https://github.com/GaParmar/img2img-turbo"},{"type":"has_code","target_id":"github:taesungp:contrastive-unpaired-translation","source_url":"https://github.com/taesungp/contrastive-unpaired-translation"},{"type":"has_code","target_id":"github:junyanz:CycleGAN","source_url":"https://github.com/junyanz/CycleGAN"},{"type":"has_code","target_id":"github:phillipi:pix2pix","source_url":"https://github.com/phillipi/pix2pix"},{"type":"has_code","target_id":"github:NVIDIA:pix2pixHD","source_url":"https://github.com/NVIDIA/pix2pixHD"},{"type":"has_code","target_id":"github:junyanz:BicycleGAN","source_url":"https://github.com/junyanz/BicycleGAN"},{"type":"has_code","target_id":"github:NVlabs:SPADE","source_url":"https://github.com/NVlabs/SPADE"},{"type":"has_code","target_id":"github:junyanz:iGAN","source_url":"https://github.com/junyanz/iGAN"},{"type":"has_code","target_id":"github:CSAILVision:GANDissect","source_url":"https://github.com/CSAILVision/GANDissect"},{"type":"has_code","target_id":"github:junyanz:CatPapers","source_url":"https://github.com/junyanz/CatPapers"},{"type":"has_code","target_id":"github:pytorch:examples","source_url":"https://github.com/pytorch/examples"}]', NULL, 'NOASSERTION', 'approved', 80, 'c5fe41bb2002943a730c061ed28f4e04', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-junyanz-pytorch-CycleGAN-and-pix2pix from https://github.com/junyanz.png
Image converted to WebP: data/images/github-junyanz-pytorch-CycleGAN-and-pix2pix.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-HumanSignal-labelImg', 'github--humansignal--labelimg', 'labelImg', 'HumanSignal', '.. image:: /readme/images/labelimg.png :target: https://github.com/heartexlabs/label-studio Label Studio is a modern, multi-modal data annotation tool ======= LabelImg, the popular image annotation tool created by Tzutalin with the help of dozens contributors, is no longer actively being developed and has become part of the Label Studio community. Check out __, the most flexible open source data labeling tool for images, text, hypertext, audio, video and time-series data. __ Label Studio and ...', '["annotations","deep-learning","detection","image-classification","imagenet","python2","python3","recognition","tools","python"]', 'other', 24551, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/HumanSignal/labelImg","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '.. image:: /readme/images/labelimg.png\n        :target: https://github.com/heartexlabs/label-studio\n\nLabel Studio is a modern, multi-modal data annotation tool\n=======\n\nLabelImg, the popular image annotation tool created by Tzutalin with the help of dozens contributors, is no longer actively being developed and has become part of the Label Studio community. Check out `Label Studio <https://github.com/heartexlabs/label-studio>`__, the most flexible open source data labeling tool for images, text, hypertext, audio, video and time-series data. `Install <https://labelstud.io/guide/install.html>`__ Label Studio and join the `slack community <https://label-studio.slack.com/>`__ to get started.\n\n.. image:: /readme/images/label-studio-1-6-player-screenshot.png\n        :target: https://github.com/heartexlabs/label-studio\n\nAbout LabelImg\n========\n\n.. image:: https://img.shields.io/pypi/v/labelimg.svg\n        :target: https://pypi.python.org/pypi/labelimg\n\n.. image:: https://img.shields.io/github/workflow/status/tzutalin/labelImg/Package?style=for-the-badge\n        :alt: GitHub Workflow Status\n\n.. image:: https://img.shields.io/badge/lang-en-blue.svg\n        :target: https://github.com/tzutalin/labelImg\n\n.. image:: https://img.shields.io/badge/lang-zh-green.svg\n        :target: https://github.com/tzutalin/labelImg/blob/master/readme/README.zh.rst\n\n.. image:: https://img.shields.io/badge/lang-jp-green.svg\n        :target: https://github.com/tzutalin/labelImg/blob/master/readme/README.jp.rst\n\nLabelImg is a graphical image annotation tool.\n\nIt is written in Python and uses Qt for its graphical interface.\n\nAnnotations are saved as XML files in PASCAL VOC format, the format used\nby `ImageNet <http://www.image-net.org/>`__.  Besides, it also supports YOLO and CreateML formats.\n\n.. image:: https://raw.githubusercontent.com/tzutalin/labelImg/master/demo/demo3.jpg\n     :alt: Demo Image\n\n.. image:: https://raw.githubusercontent.com/tzutalin/labelImg/master/demo/demo.jpg\n     :alt: Demo Image\n\n`Watch a demo video <https://youtu.be/p0nR2YsCY_U>`__\n\nInstallation\n------------------\n\nGet from PyPI but only python3.0 or above\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThis is the simplest (one-command) install method on modern Linux distributions such as Ubuntu and Fedora.\n\n.. code:: shell\n\n    pip3 install labelImg\n    labelImg\n    labelImg [IMAGE_PATH] [PRE-DEFINED CLASS FILE]\n\n\nBuild from source\n~~~~~~~~~~~~~~~~~\n\nLinux/Ubuntu/Mac requires at least `Python\n2.6 <https://www.python.org/getit/>`__ and has been tested with `PyQt\n4.8 <https://www.riverbankcomputing.com/software/pyqt/intro>`__. However, `Python\n3 or above <https://www.python.org/getit/>`__ and  `PyQt5 <https://pypi.org/project/PyQt5/>`__ are strongly recommended.\n\n\nUbuntu Linux\n^^^^^^^^^^^^\n\nPython 3 + Qt5\n\n.. code:: shell\n\n    sudo apt-get install pyqt5-dev-tools\n    sudo pip3 install -r requirements/requirements-linux-python3.txt\n    make qt5py3\n    python3 labelImg.py\n    python3 labelImg.py [IMAGE_PATH] [PRE-DEFINED CLASS FILE]\n\nmacOS\n^^^^^\n\nPython 3 + Qt5\n\n.. code:: shell\n\n    brew install qt  # Install qt-5.x.x by Homebrew\n    brew install libxml2\n\n    or using pip\n\n    pip3 install pyqt5 lxml # Install qt and lxml by pip\n\n    make qt5py3\n    python3 labelImg.py\n    python3 labelImg.py [IMAGE_PATH] [PRE-DEFINED CLASS FILE]\n\n\nPython 3 Virtualenv (Recommended)\n\nVirtualenv can avoid a lot of the QT / Python version issues\n\n.. code:: shell\n\n    brew install python3\n    pip3 install pipenv\n    pipenv run pip install pyqt5==5.15.2 lxml\n    pipenv run make qt5py3\n    pipenv run python3 labelImg.py\n    [Optional] rm -rf build dist; pipenv run python setup.py py2app -A;mv "dist/labelImg.app" /Applications\n\nNote: The Last command gives you a nice .app file with a new SVG Icon in your /Applications folder. You can consider using the script: build-tools/build-for-macos.sh\n\n\nWindows\n^^^^^^^\n\nInstall `Python <https://www.python.org/downloads/windows/>`__,\n`PyQt5 <https://www.riverbankcomputing.com/software/pyqt/download5>`__\nand `install lxml <http://lxml.de/installation.html>`__.\n\nOpen cmd and go to the `labelImg <#labelimg>`__ directory\n\n.. code:: shell\n\n    pyrcc4 -o libs/resources.py resources.qrc\n    For pyqt5, pyrcc5 -o libs/resources.py resources.qrc\n\n    python labelImg.py\n    python labelImg.py [IMAGE_PATH] [PRE-DEFINED CLASS FILE]\n\nIf you want to package it into a separate EXE file\n\n.. code:: shell\n\n    Install pyinstaller and execute:\n\n    pip install pyinstaller\n    pyinstaller --hidden-import=pyqt5 --hidden-import=lxml -F -n "labelImg" -c labelImg.py -p ./libs -p ./\n\nWindows + Anaconda\n^^^^^^^^^^^^^^^^^^\n\nDownload and install `Anaconda <https://www.anaconda.com/download/#download>`__ (Python 3+)\n\nOpen the Anaconda Prompt and go to the `labelImg <#labelimg>`__ directory\n\n.. code:: shell\n\n    conda install pyqt=5\n    conda install -c anaconda lxml\n    pyrcc5 -o libs/resources.py resources.qrc\n    python labelImg.py\n    python labelImg.py [IMAGE_PATH] [PRE-DEFINED CLASS FILE]\n\nUse Docker\n~~~~~~~~~~~~~~~~~\n.. code:: shell\n\n    docker run -it \\n    --user $(id -u) \\n    -e DISPLAY=unix$DISPLAY \\n    --workdir=$(pwd) \\n    --volume="/home/$USER:/home/$USER" \\n    --volume="/etc/group:/etc/group:ro" \\n    --volume="/etc/passwd:/etc/passwd:ro" \\n    --volume="/etc/shadow:/etc/shadow:ro" \\n    --volume="/etc/sudoers.d:/etc/sudoers.d:ro" \\n    -v /tmp/.X11-unix:/tmp/.X11-unix \\n    tzutalin/py2qt4\n\n    make qt4py2;./labelImg.py\n\nYou can pull the image which has all of the installed and required dependencies. `Watch a demo video <https://youtu.be/nw1GexJzbCI>`__\n\n\nUsage\n-----\n\nSteps (PascalVOC)\n~~~~~~~~~~~~~~~~~\n\n1. Build and launch using the instructions above.\n2. Click ''Change default saved annotation folder'' in Menu/File\n3. Click ''Open Dir''\n4. Click ''Create RectBox''\n5. Click and release left mouse to select a region to annotate the rect\n   box\n6. You can use right mouse to drag the rect box to copy or move it\n\nThe annotation will be saved to the folder you specify.\n\nYou can refer to the below hotkeys to speed up your workflow.\n\nSteps (YOLO)\n~~~~~~~~~~~~\n\n1. In ``data/predefined_classes.txt`` define the list of classes that will be used for your training.\n\n2. Build and launch using the instructions above.\n\n3. Right below "Save" button in the toolbar, click "PascalVOC" button to switch to YOLO format.\n\n4. You may use Open/OpenDIR to process single or multiple images. When finished with a single image, click save.\n\nA txt file of YOLO format will be saved in the same folder as your image with same name. A file named "classes.txt" is saved to that folder too. "classes.txt" defines the list of class names that your YOLO label refers to.\n\nNote:\n\n- Your label list shall not change in the middle of processing a list of images. When you save an image, classes.txt will also get updated, while previous annotations will not be updated.\n\n- You shouldn''t use "default class" function when saving to YOLO format, it will not be referred.\n\n- When saving as YOLO format, "difficult" flag is discarded.\n\nCreate pre-defined classes\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can edit the\n`data/predefined\_classes.txt <https://github.com/tzutalin/labelImg/blob/master/data/predefined_classes.txt>`__\nto load pre-defined classes\n\nAnnotation visualization\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n1. Copy the existing lables file to same folder with the images. The labels file name must be same with image file name.\n\n2. Click File and choose ''Open Dir'' then Open the image folder.\n\n3. Select image in File List, it will appear the bounding box and label for all objects in that image.\n\n(Choose Display Labels mode in View to show/hide lablels)\n\n\nHotkeys\n~~~~~~~\n\n+--------------------+--------------------------------------------+\n| Ctrl + u           | Load all of the images from a directory    |\n+--------------------+--------------------------------------------+\n| Ctrl + r           | Change the default annotation target dir   |\n+--------------------+--------------------------------------------+\n| Ctrl + s           | Save                                       |\n+--------------------+--------------------------------------------+\n| Ctrl + d           | Copy the current label and rect box        |\n+--------------------+--------------------------------------------+\n| Ctrl + Shift + d   | Delete the current image                   |\n+--------------------+--------------------------------------------+\n| Space              | Flag the current image as verified         |\n+--------------------+--------------------------------------------+\n| w                  | Create a rect box                          |\n+--------------------+--------------------------------------------+\n| d                  | Next image                                 |\n+--------------------+--------------------------------------------+\n| a                  | Previous image                             |\n+--------------------+--------------------------------------------+\n| del                | Delete the selected rect box               |\n+--------------------+--------------------------------------------+\n| Ctrl++             | Zoom in                                    |\n+--------------------+--------------------------------------------+\n| Ctrl--             | Zoom out                                   |\n+--------------------+--------------------------------------------+\n| â†‘â†’â†“â†               | Keyboard arrows to move selected rect box  |\n+--------------------+--------------------------------------------+\n\n**Verify Image:**\n\nWhen pressing space, the user can flag the image as verified, a green background will appear.\nThis is used when creating a dataset automatically, the user can then through all the pictures and flag them instead of annotate them.\n\n**Difficult:**\n\nThe difficult field is set to 1 indicates that the object has been annotated as "difficult", for example, an object which is clearly visible but difficult to recognize without substantial use of context.\nAccording to your deep neural network implementation, you can include or exclude difficult objects during training.\n\nHow to reset the settings\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn case there are issues with loading the classes, you can either:\n\n1. From the top menu of the labelimg click on Menu/File/Reset All\n2. Remove the `.labelImgSettings.pkl` from your home directory. In Linux and Mac you can do:\n    `rm ~/.labelImgSettings.pkl`\n\n\nHow to contribute\n~~~~~~~~~~~~~~~~~\n\nSend a pull request\n\nLicense\n~~~~~~~\n`Free software: MIT license <https://github.com/tzutalin/labelImg/blob/master/LICENSE>`_\n\nCitation: Tzutalin. LabelImg. Git code (2015). https://github.com/tzutalin/labelImg\n\nRelated and additional tools\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n1. `Label Studio <https://github.com/heartexlabs/label-studio>`__ to label images, text, audio, video and time-series data for machine learning and AI\n2. `ImageNet Utils <https://github.com/tzutalin/ImageNet_Utils>`__ to\n   download image, create a label text for machine learning, etc\n3. `Use Docker to run labelImg <https://hub.docker.com/r/tzutalin/py2qt4>`__\n4. `Generating the PASCAL VOC TFRecord files <https://github.com/tensorflow/models/blob/4f32535fe7040bb1e429ad0e3c948a492a89482d/research/object_detection/g3doc/preparing_inputs.md#generating-the-pascal-voc-tfrecord-files>`__\n5. `App Icon based on Icon by Nick Roach (GPL) <https://www.elegantthemes.com/>`__\n6. `Setup python development in vscode <https://tzutalin.blogspot.com/2019/04/set-up-visual-studio-code-for-python-in.html>`__\n7. `The link of this project on iHub platform <https://code.ihub.org.cn/projects/260/repository/labelImg>`__\n8. `Convert annotation files to CSV format or format for Google Cloud AutoML <https://github.com/tzutalin/labelImg/tree/master/tools>`__\n\n\n\nStargazers over time\n~~~~~~~~~~~~~~~~~~~~\n\n.. image:: https://starchart.cc/tzutalin/labelImg.svg\n\n', '{"language":"Python","stars":24551,"forks":6561,"watchers":24551,"open_issues":451,"topics":["annotations","deep-learning","detection","image-classification","imagenet","python2","python3","recognition","tools"],"default_branch":"master","size_kb":242833,"archived":true,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:heartexlabs:label-studio","source_url":"https://github.com/heartexlabs/label-studio"},{"type":"has_code","target_id":"github:heartexlabs:label-studio>`__,","source_url":"https://github.com/heartexlabs/label-studio>`__,"},{"type":"has_code","target_id":"github:heartexlabs:label-studio","source_url":"https://github.com/heartexlabs/label-studio"},{"type":"has_code","target_id":"github:tzutalin:labelImg","source_url":"https://github.com/tzutalin/labelImg"},{"type":"has_code","target_id":"github:tzutalin:labelImg","source_url":"https://github.com/tzutalin/labelImg"},{"type":"has_code","target_id":"github:tzutalin:labelImg","source_url":"https://github.com/tzutalin/labelImg"},{"type":"has_code","target_id":"github:tzutalin:labelImg","source_url":"https://github.com/tzutalin/labelImg"},{"type":"has_code","target_id":"github:tzutalin:labelImg","source_url":"https://github.com/tzutalin/labelImg"},{"type":"has_code","target_id":"github:tzutalin:labelImg","source_url":"https://github.com/tzutalin/labelImg"},{"type":"has_code","target_id":"github:heartexlabs:label-studio>`__","source_url":"https://github.com/heartexlabs/label-studio>`__"},{"type":"has_code","target_id":"github:tzutalin:ImageNet_Utils>`__","source_url":"https://github.com/tzutalin/ImageNet_Utils>`__"},{"type":"has_code","target_id":"github:tensorflow:models","source_url":"https://github.com/tensorflow/models"},{"type":"has_code","target_id":"github:tzutalin:labelImg","source_url":"https://github.com/tzutalin/labelImg"}]', NULL, 'MIT', 'approved', 80, 'e6a3a6a105688e7afcc5b58686204df6', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-HumanSignal-labelImg from https://github.com/HumanSignal.png
Image converted to WebP: data/images/github-HumanSignal-labelImg.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-microsoft-JARVIS', 'github--microsoft--jarvis', 'JARVIS', 'microsoft', 'The mission of JARVIS is to explore artificial general intelligence (AGI) and deliver cutting-edge research to the whole community. + [2024.01.15] We release Easytool for easier tool usage. + The code and datasets are available at EasyTool. + The paper is available at EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction. + [2023.11.30] We release TaskBench for evaluating task automation capability of LLMs. + The code and datasets are available at TaskBench. + The paper is availa...', '["deep-learning","platform","pytorch","python"]', 'other', 24485, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/microsoft/JARVIS","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# JARVIS\n\n\n[![arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/abs/2303.17580)\n[![Open in Spaces](https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/microsoft/HuggingGPT)\n\nThe mission of JARVIS is to explore artificial general intelligence (AGI) and deliver cutting-edge research to the whole community.\n\n## What''s New\n\n+  [2024.01.15] We release Easytool for easier tool usage.\n   + The code and datasets are available at [EasyTool](/easytool).\n   + The paper is available at [EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction](https://arxiv.org/abs/2401.06201).\n+  [2023.11.30] We release TaskBench for evaluating task automation capability of LLMs.\n   + The code and datasets are available at [TaskBench](/taskbench).\n   + The paper is available at [TaskBench: Benchmarking Large Language Models for Task Automation](https://arxiv.org/abs/2311.18760).\n+  [2023.07.28] We are now in the process of planning evaluation and project rebuilding. We will release a new version of Jarvis in the near future.\n+  [2023.07.24] We released a light langchain version of Jarvis. See <a href="https://github.com/langchain-ai/langchain/tree/master/libs/experimental/langchain_experimental/autonomous_agents/hugginggpt">here</a>.\n+  [2023.04.16] Jarvis now supports the OpenAI service on the Azure platform and the GPT-4 model.\n+  [2023.04.06] We added the Gradio demo and built the web API for `/tasks` and `/results` in `server` mode.\n   +  The Gradio demo is now hosted on Hugging Face Space. (Build with `inference_mode=hybrid` and `local_deployment=standard`)\n   +  The Web API `/tasks` and `/results` access intermediate results for `Stage #1`: task planning and `Stage #1-3`: model selection with execution results. See <a href="#Server">here</a>.\n+  [2023.04.03] We added the CLI mode and provided parameters for configuring the scale of local endpoints.\n   +  You can enjoy a lightweight experience with Jarvis without deploying the models locally. See <a href="#Configuration">here</a>.\n   +  Just run `python awesome_chat.py --config configs/config.lite.yaml` to experience it.\n+  [2023.04.01] We updated a version of code for building.\n\n\n### Overview\n\nLanguage serves as an interface for LLMs to connect numerous AI models for solving complicated AI tasks!\n\n<p align="center">\n<img width="100%" alt="image" src="./hugginggpt/assets/intro.png">    \n</p>\n\n\nSee our paper: [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace](http://arxiv.org/abs/2303.17580), Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu and Yueting Zhuang (the first two authors contribute equally)\n\nWe introduce a collaborative system that consists of **an LLM as the controller** and **numerous expert models as collaborative executors** (from HuggingFace Hub). The workflow of our system consists of four stages:\n+ **Task Planning**: Using ChatGPT to analyze the requests of users to understand their intention, and disassemble them into possible solvable tasks.\n+ **Model Selection**: To solve the planned tasks, ChatGPT selects expert models hosted on Hugging Face based on their descriptions.\n+ **Task Execution**: Invokes and executes each selected model, and return the results to ChatGPT.\n+ **Response Generation**: Finally, using ChatGPT to integrate the prediction of all models, and generate responses.\n\n<p align="center"><img src="./hugginggpt/assets/overview.jpg"></p>\n\n### System Requirements\n\n#### Default (Recommended)\n\nFor `configs/config.default.yaml`:\n\n+ Ubuntu 16.04 LTS\n+ VRAM >= 24GB\n+ RAM > 12GB (minimal), 16GB (standard), 80GB (full)\n+ Disk > 284GB \n  + 42GB for `damo-vilab/text-to-video-ms-1.7b`\n  + 126GB for `ControlNet`\n  + 66GB for `stable-diffusion-v1-5`\n  + 50GB for others\n  \n#### Minimum (Lite)\n\nFor `configs/config.lite.yaml`:\n\n+ Ubuntu 16.04 LTS\n+ Nothing else\n\nThe configuration `configs/config.lite.yaml` does not require any expert models to be downloaded and deployed locally. However, it means that Jarvis is restricted to models running stably on HuggingFace Inference Endpoints.\n\n### Quick Start\n\nFirst replace `openai.key` and `huggingface.token` in `server/configs/config.default.yaml` with **your personal OpenAI Key** and **your Hugging Face Token**, or put them in the environment variables `OPENAI_API_KEY` and `HUGGINGFACE_ACCESS_TOKEN` respectively. Then run the following commands:\n\n<span id="Server"></span>\n\n#### For Server:\n\n```bash\n# setup env\ncd server\nconda create -n jarvis python=3.8\nconda activate jarvis\nconda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\npip install -r requirements.txt\n\n# download models. Make sure that `git-lfs` is installed.\ncd models\nbash download.sh # required when `inference_mode` is `local` or `hybrid`. \n\n# run server\ncd ..\npython models_server.py --config configs/config.default.yaml # required when `inference_mode` is `local` or `hybrid`\npython awesome_chat.py --config configs/config.default.yaml --mode server # for text-davinci-003\n```\n\nNow you can access Jarvis'' services by the Web API. \n\n+ `/hugginggpt` --method `POST`, access the full service.\n+ `/tasks` --method `POST`, access intermediate results for Stage #1.\n+ `/results` --method `POST`, access intermediate results for Stage #1-3.\n\nFor example:\n\n```bash\n# request\ncurl --location ''http://localhost:8004/tasks'' \\n--header ''Content-Type: application/json'' \\n--data ''{\n    "messages": [\n        {\n            "role": "user",\n            "content": "based on pose of /examples/d.jpg and content of /examples/e.jpg, please show me a new image"\n        }\n    ]\n}''\n\n# response\n[{"args":{"image":"/examples/d.jpg"},"dep":[-1],"id":0,"task":"openpose-control"},{"args":{"image":"/examples/e.jpg"},"dep":[-1],"id":1,"task":"image-to-text"},{"args":{"image":"<GENERATED>-0","text":"<GENERATED>-1"},"dep":[1,0],"id":2,"task":"openpose-text-to-image"}]\n```\n\n\n#### For Web:\n\nWe provide a user-friendly web page. After starting `awesome_chat.py` in a server mode, you can run the commands to communicate with Jarvis in your browser:\n \n- you need to install `nodejs` and `npm` first.\n- [ IMPORTANT ] if you are running the web client on another machine, you need set `http://{LAN_IP_of_the_server}:{port}/` to `HUGGINGGPT_BASE_URL` of `web/src/config/index.ts`.\n- if you want to use the video generation feature, you need to compile `ffmpeg` manually with H.264.\n- you can switch to ChatGPT by `double click` on the setting icon!\n\n```bash\ncd web\nnpm install\nnpm run dev\n```\n\n```bash\n# Optional: Install ffmpeg\n# This command need be executed without errors.\nLD_LIBRARY_PATH=/usr/local/lib /usr/local/bin/ffmpeg -i input.mp4 -vcodec libx264 output.mp4\n```\n\n<span id="Gradio"></span>\n\n#### For Gradio\n\nThe Gradio demo is now hosted on Hugging Face Space. You can also run the following commands to start the demo locally:\n\n```bash\npython models_server.py --config configs/config.gradio.yaml\npython run_gradio_demo.py --config configs/config.gradio.yaml\n\n# or run the HF Space as a Docker image (Build with `inference_mode=hibrid` and `local_deployment=standard`)\ndocker run -it -p 7860:7860 --platform=linux/amd64 registry.hf.space/microsoft-hugginggpt:latest python app.py\n```\n\n#### For CLI:\n\nYou can also run Jarvis more easily in CLI mode:\n\n```bash\ncd server\npython awesome_chat.py --config configs/config.default.yaml --mode cli\n```\n\nExamples of CLI mode:\n\n```\nWelcome to Jarvis! A collaborative system that consists of an LLM as the controller and numerous expert models as collaborative executors. Jarvis can plan tasks, schedule Hugging Face models, generate friendly responses based on your requests, and help you with many things. Please enter your request (`exit` to exit).\n[ User ]: Given a collection of image A: /examples/a.jpg, B: /examples/b.jpg, C: /examples/c.jpg, please tell me how many zebras in these picture?\n[ Jarvis ]: Based on your request, I have used several models to process the images you provided. For the image A: /examples/a.jpg, I used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text "a cat sitting on a window sill looking out". Then I used the object-detection model facebook/detr-resnet-50 to generate the image with predicted box ''/images/a3dc.jpg'', which contains two objects with labels ''potted plant'' and ''cat''. For the image B: /examples/b.jpg, I used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text "two zebras standing in a field of dry grass". Then I used the object-detection model facebook/detr-resnet-50 to generate the image with predicted box ''/images/f5df.jpg'', which contains three objects with labels ''zebra''. For the image C: /examples/c.jpg, I used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text "a zebra standing in a field of tall grass". Then I used the object-detection model facebook/detr-resnet-50 to generate the image with predicted box ''/images/69c7.jpg'', which contains one object with label ''zebra''. Finally, I used the canny-control model canny-control to generate the image ''/images/7e18.png'' based on the image /examples/f.jpg. Therefore, there are four zebras in these pictures. Is there anything else I can help you with?\n[ User ]: Please answer all the named entities in the sentence: Iron Man is a superhero appearing in American comic books published by Marvel Comics. The character was co-created by writer and editor Stan Lee, developed by scripter Larry Lieber, and designed by artists Don Heck and Jack Kirby.\n[ Jarvis ]: Sure, I understand your request. Based on the inference results of the models, I will answer your request step by step. Firstly, I used the token-classification model dslim/bert-base-NER to recognize the named entities in the sentence. The inference result is that the named entities are Iron Man, Stan Lee, Larry Lieber, Don Heck and Jack Kirby. Then, I used the question-answering model deepset/roberta-base-squad2 to answer your request. The inference result is that there is no output since the context cannot be empty. Therefore, I cannot make it. I hope this answer your request. Is there anything else I can help you with?\n[ User ]: Please generate a canny image based on /examples/f.jpg\n[ Jarvis ]: Sure. I have generated a canny image based on /examples/f.jpg. To do this, I first used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text description of the image, which is "a herd of giraffes and zebras grazing in a field". Then I used the canny-control model to generate the canny image of the original image, which is stored in the path "/images/1ea2.png". Finally, I used the canny-text-to-image model lllyasviel/sd-controlnet-canny to generate the canny image based on the text description and the canny image, which is stored in the path "/images/ba66.png". Do you need anything else?\n```\n\n\n### Configuration\n\nThe server-side configuration file is `server/configs/config.default.yaml`, and some parameters are presented as follows:\n\n+ `model`: LLM, currently supports `text-davinci-003`. We are working on integrating more open-source LLMs.\n+ `inference_mode`: mode of inference endpoints\n  + `local`: only use the local inference endpoints\n  + `huggingface`: only use the Hugging Face Inference Endpoints **(free of local inference endpoints)**\n  + `hybrid`: both of `local` and `huggingface`\n+ `local_deployment`: scale of locally deployed models, works under `local` or `hybrid` inference mode:\n  +  `minimal` (RAM>12GB, ControlNet only)\n  +  `standard` (RAM>16GB, ControlNet + Standard Pipelines)\n  +  `full` (RAM>42GB, All registered models)\n\nOn a personal laptop, we recommend the configuration of `inference_mode: hybrid `and `local_deployment: minimal`. But the available models under this setting may be limited due to the instability of remote Hugging Face Inference Endpoints.\n\n### NVIDIA Jetson Embedded Device Support\nA [Dockerfile](./Dockerfile.jetson) is included that provides experimental support for [NVIDIA Jetson embedded devices](https://developer.nvidia.com/embedded-computing).  This image provides accelerated ffmpeg, pytorch, torchaudio, and torchvision dependencies.  To build the docker image, [ensure that the default docker runtime is set to ''nvidia''](https://github.com/NVIDIA/nvidia-docker/wiki/Advanced-topics#default-runtime).  A pre-built image is provided at https://hub.docker.com/r/toolboc/nv-jarvis.\n\n```bash\n#Build the docker image\ndocker build --pull --rm -f "Dockerfile.jetson" -t toolboc/nv-jarvis:r35.2.1 \n```\n\nDue to to memory requirements, JARVIS is required to run on Jetson AGX Orin family devices (64G on-board RAM device preferred) with config options set to:\n* `inference_mode: local` \n* `local_deployment: standard`  \n\nModels and configs are recommended to be provided through a volume mount from the host to the container as shown in the `docker run` step below.  It is possible to uncomment the `# Download local models` section of the [Dockerfile](./Dockerfile.jetson) to build a container with models included.\n\n#### Start the model server, awesomechat, and web app on Jetson Orin AGX\n\n```bash\n# run the container which will automatically start the model server\ndocker run --name jarvis --net=host --gpus all -v ~/jarvis/configs:/app/server/configs -v ~/src/JARVIS/server/models:/app/server/models toolboc/nv-jarvis:r35.2.1\n\n# (wait for model server to complete initialization)\n\n# start awesome_chat.py \ndocker exec jarvis python3 awesome_chat.py --config configs/config.default.yaml --mode server\n\n#start the web application (application will be acessible at http://localhost:9999)\ndocker exec jarvis npm run dev --prefix=/app/web\n```\n\n### Screenshots\n\n<p align="center"><img src="./hugginggpt/assets/screenshot_q.jpg"><img src="./hugginggpt/assets/screenshot_a.jpg"></p>\n\n\n\n\n## Citation\nIf you find this work useful in your method, you can cite the paper as below:\n\n    @inproceedings{shen2023hugginggpt,\n      author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},\n      booktitle = {Advances in Neural Information Processing Systems},\n      title = {HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace},\n      year = {2023}\n    }\n\n>\n    @article{shen2023taskbench,\n      title   = {TaskBench: Benchmarking Large Language Models for Task Automation},\n      author  = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Zhang, Wenqi and Ren, Kan and Yuan, Siyu and Lu, Weiming and Li, Dongsheng and Zhuang, Yueting},\n      journal = {arXiv preprint arXiv:2311.18760},\n      year    = {2023}\n    }\n\n>\n    @article{yuan2024easytool,\n      title   = {EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction},\n      author  = {Siyu Yuan and Kaitao Song and Jiangjie Chen and Xu Tan and Yongliang Shen and Ren Kan and Dongsheng Li and Deqing Yang},\n      journal = {arXiv preprint arXiv:2401.06201},\n      year    = {2024}\n    }\n', '{"language":"Python","stars":24485,"forks":2057,"watchers":24485,"open_issues":521,"topics":["deep-learning","platform","pytorch"],"default_branch":"main","size_kb":41674,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:langchain-ai:langchain","source_url":"https://github.com/langchain-ai/langchain"},{"type":"has_code","target_id":"github:NVIDIA:nvidia-docker","source_url":"https://github.com/NVIDIA/nvidia-docker"}]', NULL, 'MIT', 'approved', 80, '369e288e7826121d407a77ded6f13ddb', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-microsoft-JARVIS from https://github.com/microsoft.png
Image converted to WebP: data/images/github-microsoft-JARVIS.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-pyg-team-pytorch-geometric', 'github--pyg-team--pytorch-geometric', 'pytorch_geometric', 'pyg-team', '<p align="center"> <img height="150" src="https://raw.githubusercontent.com/pyg-team/pyg_sphinx_theme/master/pyg_sphinx_theme/static/img/pyg_logo_text.svg?sanitize=true" /> </p> ______________________________________________________________________ <div align="center"> [![PyPI Version][pypi-image]][pypi-url] [![PyPI Download][pypi-download-image]][pypi-download-url] [![Slack][slack-image]][slack-url] [![Contributing][contributing-image]][contributing-url] **Documentation** | **PyG 1.0 Paper**...', '["deep-learning","geometric-deep-learning","graph-convolutional-networks","graph-neural-networks","pytorch","python"]', 'other', 23243, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/pyg-team/pytorch_geometric","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <img height="150" src="https://raw.githubusercontent.com/pyg-team/pyg_sphinx_theme/master/pyg_sphinx_theme/static/img/pyg_logo_text.svg?sanitize=true" />\n</p>\n\n______________________________________________________________________\n\n<div align="center">\n\n[![PyPI Version][pypi-image]][pypi-url]\n[![PyPI Download][pypi-download-image]][pypi-download-url]\n[![Slack][slack-image]][slack-url]\n[![Contributing][contributing-image]][contributing-url]\n\n**[Documentation](https://pytorch-geometric.readthedocs.io)** |\n**[PyG 1.0 Paper](https://arxiv.org/abs/1903.02428)** |\n**[PyG 2.0 Paper](https://arxiv.org/abs/2507.16991)** |\n**[Colab Notebooks](https://pytorch-geometric.readthedocs.io/en/latest/get_started/colabs.html)** |\n**[External Resources](https://pytorch-geometric.readthedocs.io/en/latest/external/resources.html)** |\n**[OGB Examples](https://github.com/snap-stanford/ogb/tree/master/examples)**\n\n</div>\n\n**PyG** *(PyTorch Geometric)* is a library built upon [PyTorch](https://pytorch.org/) to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data.\n\nIt consists of various methods for deep learning on graphs and other irregular structures, also known as *[geometric deep learning](http://geometricdeeplearning.com/)*, from a variety of published papers.\nIn addition, it consists of easy-to-use mini-batch loaders for operating on many small and single giant graphs, [multi GPU-support](https://github.com/pyg-team/pytorch_geometric/tree/master/examples/multi_gpu), [`torch.compile`](https://pytorch-geometric.readthedocs.io/en/latest/advanced/compile.html) support, [`DataPipe`](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/datapipe.py) support, a large number of common benchmark datasets (based on simple interfaces to create your own), and helpful transforms, both for learning on arbitrary graphs as well as on 3D meshes or point clouds.\n\n**[Click here to join our Slack community!][slack-url]**\n\n<p align="center">\n  <a href="https://medium.com/stanford-cs224w"><img style="max-width: 941px" src="https://data.pyg.org/img/cs224w_tutorials.png" /></a>\n</p>\n\n______________________________________________________________________\n\n- [Library Highlights](#library-highlights)\n- [Quick Tour for New Users](#quick-tour-for-new-users)\n- [Architecture Overview](#architecture-overview)\n- [Implemented GNN Models](#implemented-gnn-models)\n- [Installation](#installation)\n\n## Library Highlights\n\nWhether you are a machine learning researcher or first-time user of machine learning toolkits, here are some reasons to try out PyG for machine learning on graph-structured data.\n\n- **Easy-to-use and unified API**:\n  All it takes is 10-20 lines of code to get started with training a GNN model (see the next section for a [quick tour](#quick-tour-for-new-users)).\n  PyG is *PyTorch-on-the-rocks*: It utilizes a tensor-centric API and keeps design principles close to vanilla PyTorch.\n  If you are already familiar with PyTorch, utilizing PyG is straightforward.\n- **Comprehensive and well-maintained GNN models**:\n  Most of the state-of-the-art Graph Neural Network architectures have been implemented by library developers or authors of research papers and are ready to be applied.\n- **Great flexibility**:\n  Existing PyG models can easily be extended for conducting your own research with GNNs.\n  Making modifications to existing models or creating new architectures is simple, thanks to its easy-to-use message passing API, and a variety of operators and utility functions.\n- **Large-scale real-world GNN models**:\n  We focus on the need of GNN applications in challenging real-world scenarios, and support learning on diverse types of graphs, including but not limited to: scalable GNNs for graphs with millions of nodes; dynamic GNNs for node predictions over time; heterogeneous GNNs with multiple node types and edge types.\n\n## Quick Tour for New Users\n\nIn this quick tour, we highlight the ease of creating and training a GNN model with only a few lines of code.\n\n### Train your own GNN model\n\nIn the first glimpse of PyG, we implement the training of a GNN for classifying papers in a citation graph.\nFor this, we load the [Cora](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html) dataset, and create a simple 2-layer GCN model using the pre-defined [`GCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html):\n\n```python\nimport torch\nfrom torch import Tensor\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.datasets import Planetoid\n\ndataset = Planetoid(root=''.'', name=''Cora'')\n\nclass GCN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels):\n        super().__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.conv2 = GCNConv(hidden_channels, out_channels)\n\n    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n        # x: Node feature matrix of shape [num_nodes, in_channels]\n        # edge_index: Graph connectivity matrix of shape [2, num_edges]\n        x = self.conv1(x, edge_index).relu()\n        x = self.conv2(x, edge_index)\n        return x\n\nmodel = GCN(dataset.num_features, 16, dataset.num_classes)\n```\n\n<details>\n<summary>We can now optimize the model in a training loop, similar to the <a href="https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#full-implementation">standard PyTorch training procedure</a>.</summary>\n\n```python\nimport torch.nn.functional as F\n\ndata = dataset[0]\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nfor epoch in range(200):\n    pred = model(data.x, data.edge_index)\n    loss = F.cross_entropy(pred[data.train_mask], data.y[data.train_mask])\n\n    # Backpropagation\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\n</details>\n\nMore information about evaluating final model performance can be found in the corresponding [example](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn.py).\n\n### Create your own GNN layer\n\nIn addition to the easy application of existing GNNs, PyG makes it simple to implement custom Graph Neural Networks (see [here](https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_gnn.html) for the accompanying tutorial).\nFor example, this is all it takes to implement the [edge convolutional layer](https://arxiv.org/abs/1801.07829) from Wang *et al.*:\n\n$$x_i^{\\prime} ~ = ~ \\max\_{j \\in \\mathcal{N}(i)} ~ \\textrm{MLP}\_{\\theta} \\left( [ ~ x_i, ~ x_j - x_i ~ ] \\right)$$\n\n```python\nimport torch\nfrom torch import Tensor\nfrom torch.nn import Sequential, Linear, ReLU\nfrom torch_geometric.nn import MessagePassing\n\nclass EdgeConv(MessagePassing):\n    def __init__(self, in_channels, out_channels):\n        super().__init__(aggr="max")  # "Max" aggregation.\n        self.mlp = Sequential(\n            Linear(2 * in_channels, out_channels),\n            ReLU(),\n            Linear(out_channels, out_channels),\n        )\n\n    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n        # x: Node feature matrix of shape [num_nodes, in_channels]\n        # edge_index: Graph connectivity matrix of shape [2, num_edges]\n        return self.propagate(edge_index, x=x)  # shape [num_nodes, out_channels]\n\n    def message(self, x_j: Tensor, x_i: Tensor) -> Tensor:\n        # x_j: Source node features of shape [num_edges, in_channels]\n        # x_i: Target node features of shape [num_edges, in_channels]\n        edge_features = torch.cat([x_i, x_j - x_i], dim=-1)\n        return self.mlp(edge_features)  # shape [num_edges, out_channels]\n```\n\n## Architecture Overview\n\nPyG provides a multi-layer framework that enables users to build Graph Neural Network solutions on both low and high levels.\nIt comprises of the following components:\n\n- The PyG **engine** utilizes the powerful PyTorch deep learning framework with full [`torch.compile`](https://pytorch-geometric.readthedocs.io/en/latest/advanced/compile.html) and [TorchScript](https://pytorch-geometric.readthedocs.io/en/latest/advanced/jit.html) support, as well as additions of efficient CPU/CUDA libraries for operating on sparse data, *e.g.*, [`pyg-lib`](https://github.com/pyg-team/pyg-lib).\n- The PyG **storage** handles data processing, transformation and loading pipelines. It is capable of handling and processing large-scale graph datasets, and provides effective solutions for heterogeneous graphs. It further provides a variety of sampling solutions, which enable training of GNNs on large-scale graphs.\n- The PyG **operators** bundle essential functionalities for implementing Graph Neural Networks. PyG supports important GNN building blocks that can be combined and applied to various parts of a GNN model, ensuring rich flexibility of GNN design.\n- Finally, PyG provides an abundant set of GNN **models**, and examples that showcase GNN models on standard graph benchmarks. Thanks to its flexibility, users can easily build and modify custom GNN models to fit their specific needs.\n\n<p align="center">\n  <img width="100%" src="https://raw.githubusercontent.com/pyg-team/pytorch_geometric/master/docs/source/_figures/architecture.svg?sanitize=true" />\n</p>\n\n## Implemented GNN Models\n\nWe list currently supported PyG models, layers and operators according to category:\n\n**GNN layers:**\nAll Graph Neural Network layers are implemented via the **[`nn.MessagePassing`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.MessagePassing.html)** interface.\nA GNN layer specifies how to perform message passing, *i.e.* by designing different message, aggregation and update functions as defined [here](https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_gnn.html).\nThese GNN layers can be stacked together to create Graph Neural Network models.\n\n- **[GCNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html)** from Kipf and Welling: [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907) (ICLR 2017) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn.py)\]\n- **[ChebConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.ChebConv.html)** from Defferrard *et al.*: [Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering](https://arxiv.org/abs/1606.09375) (NIPS 2016) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn.py#L36-L37)\]\n- **[GATConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html)** from VeliÄkoviÄ‡ *et al.*: [Graph Attention Networks](https://arxiv.org/abs/1710.10903) (ICLR 2018) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gat.py)\]\n\n<details>\n<summary><b>Expand to see all implemented GNN layers...</b></summary>\n\n- **[GCN2Conv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCN2Conv.html)** from Chen *et al.*: [Simple and Deep Graph Convolutional Networks](https://arxiv.org/abs/2007.02133) (ICML 2020) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn2_cora.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn2_ppi.py)\]\n- **[SplineConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SplineConv.html)** from Fey *et al.*: [SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels](https://arxiv.org/abs/1711.08920) (CVPR 2018) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/cora.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/faust.py)\]\n- **[NNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.NNConv.html)** from Gilmer *et al.*: [Neural Message Passing for Quantum Chemistry](https://arxiv.org/abs/1704.01212) (ICML 2017) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/qm9_nn_conv.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/mnist_nn_conv.py)\]\n- **[CGConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.CGConv.html)** from Xie and Grossman: [Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.145301) (Physical Review Letters 120, 2018)\n- **[ECConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.ECConv.html)** from Simonovsky and Komodakis: [Edge-Conditioned Convolution on Graphs](https://arxiv.org/abs/1704.02901) (CVPR 2017)\n- **[EGConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.EGConv.html)** from Tailor *et al.*: [Adaptive Filters and Aggregator Fusion for Efficient Graph Convolutions](https://arxiv.org/abs/2104.01481) (GNNSys 2021) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/egc.py)\]\n- **[GATv2Conv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATv2Conv.html)** from Brody *et al.*: [How Attentive are Graph Attention Networks?](https://arxiv.org/abs/2105.14491) (ICLR 2022)\n- **[TransformerConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.TransformerConv.html)** from Shi *et al.*: [Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification](https://arxiv.org/abs/2009.03509) (CoRR 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/unimp_arxiv.py)\]\n- **[SAGEConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SAGEConv.html)** from Hamilton *et al.*: [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216) (NIPS 2017) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/reddit.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/ogbn_train.py), [**Example3**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_sage_unsup.py), [**Example4**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_sage_unsup_ppi.py)\]\n- **[GraphConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GraphConv.html)** from, *e.g.*, Morris *et al.*: [Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks](https://arxiv.org/abs/1810.02244) (AAAI 2019)\n- **[GatedGraphConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GatedGraphConv.html)** from Li *et al.*: [Gated Graph Sequence Neural Networks](https://arxiv.org/abs/1511.05493) (ICLR 2016)\n- **[ResGatedGraphConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.ResGatedGraphConv.html)** from Bresson and Laurent: [Residual Gated Graph ConvNets](https://arxiv.org/abs/1711.07553) (CoRR 2017)\n- **[GINConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GINConv.html)** from Xu *et al.*: [How Powerful are Graph Neural Networks?](https://arxiv.org/abs/1810.00826) (ICLR 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/mutag_gin.py)\]\n- **[GINEConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GINEConv.html)** from Hu *et al.*: [Strategies for Pre-training Graph Neural Networks](https://arxiv.org/abs/1905.12265) (ICLR 2020)\n- **[ARMAConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.ARMAConv.html)** from Bianchi *et al.*: [Graph Neural Networks with Convolutional ARMA Filters](https://arxiv.org/abs/1901.01343) (CoRR 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/arma.py)\]\n- **[SGConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SGConv.html)** from Wu *et al.*: [Simplifying Graph Convolutional Networks](https://arxiv.org/abs/1902.07153) (CoRR 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/sgc.py)\]\n- **[APPNP](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.APPNP.html)** from Klicpera *et al.*: [Predict then Propagate: Graph Neural Networks meet Personalized PageRank](https://arxiv.org/abs/1810.05997) (ICLR 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/citation/appnp.py)\]\n- **[MFConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.MFConv.html)** from Duvenaud *et al.*: [Convolutional Networks on Graphs for Learning Molecular Fingerprints](https://arxiv.org/abs/1509.09292) (NIPS 2015)\n- **[AGNNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.AGNNConv.html)** from Thekumparampil *et al.*: [Attention-based Graph Neural Network for Semi-Supervised Learning](https://arxiv.org/abs/1803.03735) (CoRR 2017) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/agnn.py)\]\n- **[TAGConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.TAGConv.html)** from Du *et al.*: [Topology Adaptive Graph Convolutional Networks](https://arxiv.org/abs/1710.10370) (CoRR 2017) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/tagcn.py)\]\n- **[PNAConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.PNAConv.html)** from Corso *et al.*: [Principal Neighbourhood Aggregation for Graph Nets](https://arxiv.org/abs/2004.05718) (CoRR 2020) \[**[Example](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/pna.py)**\]\n- **[FAConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.FAConv.html)** from Bo *et al.*: [Beyond Low-Frequency Information in Graph Convolutional Networks](https://arxiv.org/abs/2101.00797) (AAAI 2021)\n- **[PDNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.nn.conv.PDNConv.html)** from Rozemberczki *et al.*: [Pathfinder Discovery Networks for Neural Message Passing](https://arxiv.org/abs/2010.12878) (WWW 2021)\n- **[RGCNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.RGCNConv.html)** from Schlichtkrull *et al.*: [Modeling Relational Data with Graph Convolutional Networks](https://arxiv.org/abs/1703.06103) (ESWC 2018) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/rgcn.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/rgcn_link_pred.py)\]\n- **[RGATConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.RGATConv.html)** from Busbridge *et al.*: [Relational Graph Attention Networks](https://arxiv.org/abs/1904.05811) (CoRR 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/rgat.py)\]\n- **[FiLMConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.FiLMConv.html)** from Brockschmidt: [GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation](https://arxiv.org/abs/1906.12192) (ICML 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/film.py)\]\n- **[SignedConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SignedConv.html)** from Derr *et al.*: [Signed Graph Convolutional Network](https://arxiv.org/abs/1808.06354) (ICDM 2018) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/signed_gcn.py)\]\n- **[DNAConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.DNAConv.html)** from Fey: [Just Jump: Dynamic Neighborhood Aggregation in Graph Neural Networks](https://arxiv.org/abs/1904.04849) (ICLR-W 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/dna.py)\]\n- **[PANConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.PANConv.html)** from Ma *et al.*: [Path Integral Based Convolution and Pooling for Graph Neural Networks](https://arxiv.org/abs/2006.16811) (NeurIPS 2020)\n- **[PointNetConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.PointNetConv.html)** (including **[Iterative Farthest Point Sampling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.fps.html)**, dynamic graph generation based on **[nearest neighbor](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.knn_graph.html)** or **[maximum distance](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.radius_graph.html)**, and **[k-NN interpolation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.unpool.knn_interpolate.html)** for upsampling) from Qi *et al.*: [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593) (CVPR 2017) and [PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space](https://arxiv.org/abs/1706.02413) (NIPS 2017) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/pointnet2_classification.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/pointnet2_segmentation.py)\]\n- **[EdgeConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.EdgeConv.html)** from Wang *et al.*: [Dynamic Graph CNN for Learning on Point Clouds](https://arxiv.org/abs/1801.07829) (CoRR, 2018) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/dgcnn_classification.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/dgcnn_segmentation.py)\]\n- **[XConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.XConv.html)** from Li *et al.*: [PointCNN: Convolution On X-Transformed Points](https://arxiv.org/abs/1801.07791) (NeurIPS 2018) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/points/point_cnn.py)\]\n- **[PPFConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.PPFConv.html)** from Deng *et al.*: [PPFNet: Global Context Aware Local Features for Robust 3D Point Matching](https://arxiv.org/abs/1802.02669) (CVPR 2018)\n- **[GMMConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GMMConv.html)** from Monti *et al.*: [Geometric Deep Learning on Graphs and Manifolds using Mixture Model CNNs](https://arxiv.org/abs/1611.08402) (CVPR 2017)\n- **[FeaStConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.FeaStConv.html)** from Verma *et al.*: [FeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis](https://arxiv.org/abs/1706.05206) (CVPR 2018)\n- **[PointTransformerConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.PointTransformerConv.html)** from Zhao *et al.*: [Point Transformer](https://arxiv.org/abs/2012.09164) (2020)\n- **[HypergraphConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.HypergraphConv.html)** from Bai *et al.*: [Hypergraph Convolution and Hypergraph Attention](https://arxiv.org/abs/1901.08150) (CoRR 2019)\n- **[GravNetConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GravNetConv.html)** from Qasim *et al.*: [Learning Representations of Irregular Particle-detector Geometry with Distance-weighted Graph Networks](https://arxiv.org/abs/1902.07987) (European Physics Journal C, 2019)\n- **[SuperGAT](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SuperGATConv.html)** from Kim and Oh: [How To Find Your Friendly Neighborhood: Graph Attention Design With Self-Supervision](https://openreview.net/forum?id=Wi5KUNlqWty) (ICLR 2021) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/super_gat.py)\]\n- **[HGTConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.HGTConv.html)** from Hu *et al.*: [Heterogeneous Graph Transformer](https://arxiv.org/abs/2003.01332) (WWW 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/hgt_dblp.py)\]\n- **[HEATConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.HEATonv.html)** from Mo *et al.*: [Heterogeneous Edge-Enhanced Graph Attention Network For Multi-Agent Trajectory Prediction](https://arxiv.org/abs/2106.07161) (CoRR 2021)\n- **[SSGConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SSGConv.html)** from Zhu *et al.*: [Simple Spectral Graph Convolution](https://openreview.net/forum?id=CYO5T-YjWZV) (ICLR 2021)\n- **[FusedGATConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.FusedGATConv.html)** from Zhang *et al.*: [Understanding GNN Computational Graph: A Coordinated Computation, IO, and Memory Perspective](https://proceedings.mlsys.org/paper/2022/file/9a1158154dfa42caddbd0694a4e9bdc8-Paper.pdf) (MLSys 2022)\n- **[GPSConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GPSConv.html)** from RampÃ¡Å¡ek *et al.*: [Recipe for a General, Powerful, Scalable Graph Transformer](https://arxiv.org/abs/2205.12454) (NeurIPS 2022) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_gps.py)\]\n\n</details>\n\n**Pooling layers:**\nGraph pooling layers combine the vectorial representations of a set of nodes in a graph (or a subgraph) into a single vector representation that summarizes its properties of nodes.\nIt is commonly applied to graph-level tasks, which require combining node features into a single graph representation.\n\n- **[Top-K Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.TopKPooling.html)** from Gao and Ji: [Graph U-Nets](https://arxiv.org/abs/1905.05178) (ICML 2019), Cangea *et al.*: [Towards Sparse Hierarchical Graph Classifiers](https://arxiv.org/abs/1811.01287) (NeurIPS-W 2018) and Knyazev *et al.*: [Understanding Attention and Generalization in Graph Neural Networks](https://arxiv.org/abs/1905.02850) (ICLR-W 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/proteins_topk_pool.py)\]\n- **[DiffPool](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.dense.dense_diff_pool.html)** from Ying *et al.*: [Hierarchical Graph Representation Learning with Differentiable Pooling](https://arxiv.org/abs/1806.08804) (NeurIPS 2018) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/proteins_diff_pool.py)\]\n\n<details>\n<summary><b>Expand to see all implemented pooling layers...</b></summary>\n\n- **[Attentional Aggregation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.aggr.AttentionalAggregation.html)** from Li *et al.*: [Graph Matching Networks for Learning the Similarity of Graph Structured Objects](https://arxiv.org/abs/1904.12787) (ICML 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/global_attention.py)\]\n- **[Set2Set](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.aggr.Set2Set.html)** from Vinyals *et al.*: [Order Matters: Sequence to Sequence for Sets](https://arxiv.org/abs/1511.06391) (ICLR 2016) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/set2set.py)\]\n- **[Sort Aggregation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.aggr.SortAggregation.html)** from Zhang *et al.*: [An End-to-End Deep Learning Architecture for Graph Classification](https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf) (AAAI 2018) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/sort_pool.py)\]\n- **[MinCut Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.dense.dense_mincut_pool.html)** from Bianchi *et al.*: [Spectral Clustering with Graph Neural Networks for Graph Pooling](https://arxiv.org/abs/1907.00481) (ICML 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/proteins_mincut_pool.py)\]\n- **[DMoN Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.dense.DMoNPooling.html)** from Tsitsulin *et al.*: [Graph Clustering with Graph Neural Networks](https://arxiv.org/abs/2006.16904) (CoRR 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/proteins_dmon_pool.py)\]\n- **[Graclus Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.graclus.html)** from Dhillon *et al.*: [Weighted Graph Cuts without Eigenvectors: A Multilevel Approach](http://www.cs.utexas.edu/users/inderjit/public_papers/multilevel_pami.pdf) (PAMI 2007) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/mnist_graclus.py)\]\n- **[Voxel Grid Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.voxel_grid.html)** from, *e.g.*, Simonovsky and Komodakis: [Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs](https://arxiv.org/abs/1704.02901) (CVPR 2017) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/mnist_voxel_grid.py)\]\n- **[SAG Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.SAGPooling.html)** from Lee *et al.*: [Self-Attention Graph Pooling](https://arxiv.org/abs/1904.08082) (ICML 2019) and Knyazev *et al.*: [Understanding Attention and Generalization in Graph Neural Networks](https://arxiv.org/abs/1905.02850) (ICLR-W 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/sag_pool.py)\]\n- **[Edge Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.EdgePooling.html)** from Diehl *et al.*: [Towards Graph Pooling by Edge Contraction](https://graphreason.github.io/papers/17.pdf) (ICML-W 2019) and Diehl: [Edge Contraction Pooling for Graph Neural Networks](https://arxiv.org/abs/1905.10990) (CoRR 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/edge_pool.py)\]\n- **[ASAPooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.ASAPooling.html)** from Ranjan *et al.*: [ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations](https://arxiv.org/abs/1911.07979) (AAAI 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/asap.py)\]\n- **[PANPooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.PANPooling.html)** from Ma *et al.*: [Path Integral Based Convolution and Pooling for Graph Neural Networks](https://arxiv.org/abs/2006.16811) (NeurIPS 2020)\n- **[MemPooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.MemPooling.html)** from Khasahmadi *et al.*: [Memory-Based Graph Networks](https://arxiv.org/abs/2002.09518) (ICLR 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/mem_pool.py)\]\n- **[Graph Multiset Transformer](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.aggr.GraphMultisetTransformer.html)** from Baek *et al.*: [Accurate Learning of Graph Representations with Graph Multiset Pooling](https://arxiv.org/abs/2102.11533) (ICLR 2021) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/proteins_gmt.py)\]\n- **[Equilibrium Aggregation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.aggr.EquilibriumAggregation.html)** from Bartunov *et al.*: [](https://arxiv.org/abs/2202.12795) (UAI 2022) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/equilibrium_median.py)\]\n\n</details>\n\n**GNN models:**\nOur supported GNN models incorporate multiple message passing layers, and users can directly use these pre-defined models to make predictions on graphs.\nUnlike simple stacking of GNN layers, these models could involve pre-processing, additional learnable parameters, skip connections, graph coarsening, etc.\n\n- **[SchNet](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.SchNet.html)** from SchÃ¼tt *et al.*: [SchNet: A Continuous-filter Convolutional Neural Network for Modeling Quantum Interactions](https://arxiv.org/abs/1706.08566) (NIPS 2017) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/qm9_pretrained_schnet.py)\]\n- **[DimeNet](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.DimeNet.html)** and **[DimeNetPlusPlus](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.DimeNetPlusPlus.html)** from Klicpera *et al.*: [Directional Message Passing for Molecular Graphs](https://arxiv.org/abs/2003.03123) (ICLR 2020) and [Fast and Uncertainty-Aware Directional Message Passing for Non-Equilibrium Molecules](https://arxiv.org/abs/2011.14115) (NeurIPS-W 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/qm9_pretrained_dimenet.py)\]\n- **[Node2Vec](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.Node2Vec.html)** from Grover and Leskovec: [node2vec: Scalable Feature Learning for Networks](https://arxiv.org/abs/1607.00653) (KDD 2016) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/node2vec.py)\]\n- **[Deep Graph Infomax](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.DeepGraphInfomax.html)** from VeliÄkoviÄ‡ *et al.*: [Deep Graph Infomax](https://arxiv.org/abs/1809.10341) (ICLR 2019) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/infomax_transductive.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/infomax_inductive.py)\]\n- **Deep Multiplex Graph Infomax** from Park *et al.*: [Unsupervised Attributed Multiplex Network Embedding](https://arxiv.org/abs/1911.06750) (AAAI 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/dmgi_unsup.py)\]\n- **[Masked Label Prediction](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.MaskLabel.html)** from Shi *et al.*: [Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification](https://arxiv.org/abs/2009.03509) (CoRR 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/unimp_arxiv.py)\]\n- **[PMLP](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.PMLP.html)** from Yang *et al.*: [Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs](https://arxiv.org/abs/2212.09034) (ICLR 2023)\n\n<details>\n<summary><b>Expand to see all implemented GNN models...</b></summary>\n\n- **[Jumping Knowledge](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.JumpingKnowledge.html)** from Xu *et al.*: [Representation Learning on Graphs with Jumping Knowledge Networks](https://arxiv.org/abs/1806.03536) (ICML 2018) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/gin.py#L54-L106)\]\n- A **[MetaLayer](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.MetaLayer.html)** for building any kind of graph network similar to the [TensorFlow Graph Nets library](https://github.com/deepmind/graph_nets) from Battaglia *et al.*: [Relational Inductive Biases, Deep Learning, and Graph Networks](https://arxiv.org/abs/1806.01261) (CoRR 2018)\n- **[MetaPath2Vec](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.MetaPath2Vec.html)** from Dong *et al.*: [metapath2vec: Scalable Representation Learning for Heterogeneous Networks](https://ericdongyx.github.io/papers/KDD17-dong-chawla-swami-metapath2vec.pdf) (KDD 2017) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/metapath2vec.py)\]\n- All variants of **[Graph Autoencoders](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GAE.html)** and **[Variational Autoencoders](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.VGAE.html)** from:\n  - [Variational Graph Auto-Encoders](https://arxiv.org/abs/1611.07308) from Kipf and Welling (NIPS-W 2016) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/autoencoder.py)\]\n  - [Adversarially Regularized Graph Autoencoder for Graph Embedding](https://arxiv.org/abs/1802.04407) from Pan *et al.* (IJCAI 2018) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/argva_node_clustering.py)\]\n  - [Simple and Effective Graph Autoencoders with One-Hop Linear Models](https://arxiv.org/abs/2001.07614) from Salha *et al.* (ECML 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/autoencoder.py)\]\n- **[SEAL](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/seal_link_pred.py)** from Zhang and Chen: [Link Prediction Based on Graph Neural Networks](https://arxiv.org/pdf/1802.09691.pdf) (NeurIPS 2018) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/seal_link_pred.py)\]\n- **[RENet](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.RENet.html)** from Jin *et al.*: [Recurrent Event Network for Reasoning over Temporal Knowledge Graphs](https://arxiv.org/abs/1904.05530) (ICLR-W 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/renet.py)\]\n- **[GraphUNet](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GraphUNet.html)** from Gao and Ji: [Graph U-Nets](https://arxiv.org/abs/1905.05178) (ICML 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_unet.py)\]\n- **[AttentiveFP](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.AttentiveFP.html)** from Xiong *et al.*: [Pushing the Boundaries of Molecular Representation for Drug Discovery with the Graph Attention Mechanism](https://pubs.acs.org/doi/10.1021/acs.jmedchem.9b00959) (J. Med. Chem. 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/attentive_fp.py)\]\n- **[DeepGCN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.DeepGCNLayer.html)** and the **[GENConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GENConv.html)** from Li *et al.*: [DeepGCNs: Can GCNs Go as Deep as CNNs?](https://arxiv.org/abs/1904.03751) (ICCV 2019) and [DeeperGCN: All You Need to Train Deeper GCNs](https://arxiv.org/abs/2006.07739) (CoRR 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/ogbn_proteins_deepgcn.py)\]\n- **[RECT](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.RECT_L.html)** from Wang *et al.*: [Network Embedding with Completely-imbalanced Labels](https://ieeexplore.ieee.org/document/8979355) (TKDE 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/rect.py)\]\n- **[GNNExplainer](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.explain.algorithm.GNNExplainer.html)** from Ying *et al.*: [GNNExplainer: Generating Explanations for Graph Neural Networks](https://arxiv.org/abs/1903.03894) (NeurIPS 2019) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/explain/gnn_explainer.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/explain/gnn_explainer_ba_shapes.py), [**Example3**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/explain/gnn_explainer_link_pred.py)\]\n- **Graph-less Neural Networks** from Zhang *et al.*: [Graph-less Neural Networks: Teaching Old MLPs New Tricks via Distillation](https://arxiv.org/abs/2110.08727) (CoRR 2021) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/glnn.py)\]\n- **[LINKX](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.LINKX.html)** from Lim *et al.*: [Large Scale Learning on Non-Homophilous Graphs:\n  New Benchmarks and Strong Simple Methods](https://arxiv.org/abs/2110.14446) (NeurIPS 2021) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/linkx.py)\]\n- **[RevGNN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GroupAddRev.html)** from Li *et al.*: [Training Graph Neural with 1000 Layers](https://arxiv.org/abs/2106.07476) (ICML 2021) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/rev_gnn.py)\]\n- **[TransE](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.kge.TransE.html)** from Bordes *et al.*: [Translating Embeddings for Modeling Multi-Relational Data](https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf) (NIPS 2013) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/kge_fb15k_237.py)\]\n- **[ComplEx](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.kge.ComplEx.html)** from Trouillon *et al.*: [Complex Embeddings for Simple Link Prediction](https://arxiv.org/abs/1606.06357) (ICML 2016) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/kge_fb15k_237.py)\]\n- **[DistMult](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.kge.DistMult.html)** from Yang *et al.*: [Embedding Entities and Relations for Learning and Inference in Knowledge Bases](https://arxiv.org/abs/1412.6575) (ICLR 2015) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/kge_fb15k_237.py)\]\n- **[RotatE](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.kge.RotatE.html)** from Sun *et al.*: [RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space](https://arxiv.org/abs/1902.10197) (ICLR 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/kge_fb15k_237.py)\]\n\n</details>\n\n**GNN operators and utilities:**\nPyG comes with a rich set of neural network operators that are commonly used in many GNN models.\nThey follow an extensible design: It is easy to apply these operators and graph utilities to existing GNN layers and models to further enhance model performance.\n\n- **[DropEdge](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.dropout_edge)** from Rong *et al.*: [DropEdge: Towards Deep Graph Convolutional Networks on Node Classification](https://openreview.net/forum?id=Hkx1qkrKPr) (ICLR 2020)\n- **[DropNode](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.dropout_node)**, **[MaskFeature](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.mask_feature)** and **[AddRandomEdge](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.add_random_edge)** from You *et al.*: [Graph Contrastive Learning with Augmentations](https://arxiv.org/abs/2010.13902) (NeurIPS 2020)\n- **[DropPath](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.dropout_path)** from Li *et al.*: [MaskGAE: Masked Graph Modeling Meets Graph Autoencoders](https://arxiv.org/abs/2205.10053) (arXiv 2022)\n- **[ShuffleNode](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.shuffle_node)** from VeliÄkoviÄ‡ *et al.*: [Deep Graph Infomax](https://arxiv.org/abs/1809.10341) (ICLR 2019)\n- **[GraphNorm](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.norm.GraphNorm.html)** from Cai *et al.*: [GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training](https://proceedings.mlr.press/v139/cai21e.html) (ICML 2021)\n- **[GDC](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.transforms.GDC.html)** from Klicpera *et al.*: [Diffusion Improves Graph Learning](https://arxiv.org/abs/1911.05485) (NeurIPS 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn.py)\]\n\n<details>\n<summary><b>Expand to see all implemented GNN operators and utilities...</b></summary>\n\n- **[GraphSizeNorm](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.norm.GraphSizeNorm.html)** from Dwivedi *et al.*: [Benchmarking Graph Neural Networks](https://arxiv.org/abs/2003.00982) (CoRR 2020)\n- **[PairNorm](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.norm.PairNorm.html)** from Zhao and Akoglu: [PairNorm: Tackling Oversmoothing in GNNs](https://arxiv.org/abs/1909.12223) (ICLR 2020)\n- **[MeanSubtractionNorm](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.norm.MeanSubtractionNorm.html)** from Yang *et al.*: [Revisiting "Over-smoothing" in Deep GCNs](https://arxiv.org/abs/2003.13663) (CoRR 2020)\n- **[DiffGroupNorm](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.norm.DiffGroupNorm.html)** from Zhou *et al.*: [Towards Deeper Graph Neural Networks with Differentiable Group Normalization](https://arxiv.org/abs/2006.06972) (NeurIPS 2020)\n- **[Tree Decomposition](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.tree_decomposition)** from Jin *et al.*: [Junction Tree Variational Autoencoder for Molecular Graph Generation](https://arxiv.org/abs/1802.04364) (ICML 2018)\n- **[TGN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.TGNMemory.html)** from Rossi *et al.*: [Temporal Graph Networks for Deep Learning on Dynamic Graphs](https://arxiv.org/abs/2006.10637) (GRL+ 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/tgn.py)\]\n- **[Weisfeiler Lehman Operator](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.WLConv.html)** from Weisfeiler and Lehman: [A Reduction of a Graph to a Canonical Form and an Algebra Arising During this Reduction](https://www.iti.zcu.cz/wl2018/pdf/wl_paper_translation.pdf) (Nauchno-Technicheskaya Informatsia 1968) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/wl_kernel.py)\]\n- **[Continuous Weisfeiler Lehman Operator](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.WLConvContinuous.html)** from Togninalli *et al.*: [Wasserstein Weisfeiler-Lehman Graph Kernels](https://arxiv.org/abs/1906.01277) (NeurIPS 2019)\n- **[Label Propagation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.LabelPropagation.html)** from Zhu and Ghahramani: [Learning from Labeled and Unlabeled Data with Label Propagation](http://mlg.eng.cam.ac.uk/zoubin/papers/CMU-CALD-02-107.pdf) (CMU-CALD 2002) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/label_prop.py)\]\n- **[Local Degree Profile](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.transforms.LocalDegreeProfile)** from Cai and Wang: [A Simple yet Effective Baseline for Non-attribute Graph Classification](https://arxiv.org/abs/1811.03508) (CoRR 2018)\n- **[CorrectAndSmooth](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.CorrectAndSmooth.html)** from Huang *et al.*: [Combining Label Propagation And Simple Models Out-performs Graph Neural Networks](https://arxiv.org/abs/2010.13993) (CoRR 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/correct_and_smooth.py)\]\n- **[Gini](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.functional.gini.html)** and **[BRO](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.functional.bro.html)** regularization from Henderson *et al.*: [Improving Molecular Graph Neural Network Explainability with Orthonormalization and Induced Sparsity](https://arxiv.org/abs/2105.04854) (ICML 2021)\n- **[RootedEgoNets](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.transforms.RootedEgoNets)** and **[RootedRWSubgraph](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.transforms.RootedRWSubgraph)** from Zhao *et al.*: [From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness](https://arxiv.org/abs/2110.03753) (ICLR 2022)\n- **[FeaturePropagation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.transforms.FeaturePropagation)** from Rossi *et al.*: [On the Unreasonable Effectiveness of Feature Propagation in Learning on Graphs with Missing Node Features](https://arxiv.org/abs/2111.12128) (CoRR 2021)\n\n</details>\n\n**Scalable GNNs:**\nPyG supports the implementation of Graph Neural Networks that can scale to large-scale graphs.\nSuch application is challenging since the entire graph, its associated features and the GNN parameters cannot fit into GPU memory.\nMany state-of-the-art scalability approaches tackle this challenge by sampling neighborhoods for mini-batch training, graph clustering and partitioning, or by using simplified GNN models.\nThese approaches have been implemented in PyG, and can benefit from the above GNN layers, operators and models.\n\n- **[NeighborLoader](https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.NeighborLoader)** from Hamilton *et al.*: [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216) (NIPS 2017) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/reddit.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/ogbn_train.py), [**Example3**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/to_hetero_mag.py)\]\n- **[ClusterGCN](https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.ClusterLoader)** from Chiang *et al.*: [Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks](https://arxiv.org/abs/1905.07953) (KDD 2019) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/cluster_gcn_reddit.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/cluster_gcn_ppi.py)\]\n- **[GraphSAINT](https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.GraphSAINTSampler)** from Zeng *et al.*: [GraphSAINT: Graph Sampling Based Inductive Learning Method](https://arxiv.org/abs/1907.04931) (ICLR 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_saint.py)\]\n\n<details>\n<summary><b>Expand to see all implemented scalable GNNs...</b></summary>\n\n- **[ShaDow](https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.ShaDowKHopSampler)** from Zeng *et al.*: [Decoupling the Depth and Scope of Graph Neural Networks](https://arxiv.org/abs/2201.07858) (NeurIPS 2021) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/shadow.py)\]\n- **[SIGN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.transforms.SIGN.html)** from Rossi *et al.*: [SIGN: Scalable Inception Graph Neural Networks](https://arxiv.org/abs/2004.11198) (CoRR 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/sign.py)\]\n- **[HGTLoader](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.loader.HGTLoader.html)** from Hu *et al.*: [Heterogeneous Graph Transformer](https://arxiv.org/abs/2003.01332) (WWW 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/to_hetero_mag.py)\]\n\n</details>\n\n## Installation\n\nPyG is available for Python 3.10 to Python 3.13.\n\nFrom **PyG 2.3** onwards, you can install and use PyG **without any external library** required except for PyTorch.\nFor this, simply run\n\n```\npip install torch_geometric\n```\n\n### Additional Libraries\n\nIf you want to utilize the full set of features from PyG, there exists several additional libraries you may want to install:\n\n- **[`pyg-lib`](https://github.com/pyg-team/pyg-lib)**: Heterogeneous GNN operators and graph sampling routines\n- **[`torch-scatter`](https://github.com/rusty1s/pytorch_scatter)**: Accelerated and efficient sparse reductions\n- **[`torch-sparse`](https://github.com/rusty1s/pytorch_sparse)**: [`SparseTensor`](https://pytorch-geometric.readthedocs.io/en/latest/advanced/sparse_tensor.html) support\n- **[`torch-cluster`](https://github.com/rusty1s/pytorch_cluster)**: Graph clustering routines\n- **[`torch-spline-conv`](https://github.com/rusty1s/pytorch_spline_conv)**: [`SplineConv`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SplineConv.html) support\n\nThese packages come with their own CPU and GPU kernel implementations based on the [PyTorch C++/CUDA/hip(ROCm) extension interface](https://github.com/pytorch/extension-cpp).\nFor a basic usage of PyG, these dependencies are **fully optional**.\nWe recommend to start with a minimal installation, and install additional dependencies once you start to actually need them.\n\nFor ease of installation of these extensions, we provide `pip` wheels for all major OS/PyTorch/CUDA combinations, see [here](https://data.pyg.org/whl).\n\n#### PyTorch 2.8\n\nTo install the binaries for PyTorch 2.8.0, simply run\n\n```\npip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.8.0+${CUDA}.html\n```\n\nwhere `${CUDA}` should be replaced by either `cpu`, `cu126`, `cu128`, or `cu129` depending on your PyTorch installation.\n\n|             | `cpu` | `cu126` | `cu128` | `cu129` |\n| ----------- | ----- | ------- | ------- | ------- |\n| **Linux**   | âœ…    | âœ…      | âœ…      | âœ…      |\n| **Windows** | âœ…    | âœ…      | âœ…      | âœ…      |\n| **macOS**   | âœ…    |         |         |         |\n\n#### PyTorch 2.7\n\nTo install the binaries for PyTorch 2.7.0, simply run\n\n```\npip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.7.0+${CUDA}.html\n```\n\nwhere `${CUDA}` should be replaced by either `cpu`, `cu118`, `cu126`, or `cu128` depending on your PyTorch installation.\n\n|             | `cpu` | `cu118` | `cu126` | `cu128` |\n| ----------- | ----- | ------- | ------- | ------- |\n| **Linux**   | âœ…    | âœ…      | âœ…      | âœ…      |\n| **Windows** | âœ…    | âœ…      | âœ…      | âœ…      |\n| **macOS**   | âœ…    |         |         |         |\n\n#### PyTorch 2.6\n\nTo install the binaries for PyTorch 2.6.0, simply run\n\n```\npip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+${CUDA}.html\n```\n\nwhere `${CUDA}` should be replaced by either `cpu`, `cu118`, `cu124`, or `cu126` depending on your PyTorch installation.\n\n|             | `cpu` | `cu118` | `cu124` | `cu126` |\n| ----------- | ----- | ------- | ------- | ------- |\n| **Linux**   | âœ…    | âœ…      | âœ…      | âœ…      |\n| **Windows** | âœ…    | âœ…      | âœ…      | âœ…      |\n| **macOS**   | âœ…    |         |         |         |\n\n**Note:** Binaries of older versions are also provided for PyTorch 1.4.0, PyTorch 1.5.0, PyTorch 1.6.0, PyTorch 1.7.0/1.7.1, PyTorch 1.8.0/1.8.1, PyTorch 1.9.0, PyTorch 1.10.0/1.10.1/1.10.2, PyTorch 1.11.0, PyTorch 1.12.0/1.12.1, PyTorch 1.13.0/1.13.1, PyTorch 2.0.0/2.0.1, PyTorch 2.1.0/2.1.1/2.1.2, PyTorch 2.2.0/2.2.1/2.2.2, PyTorch 2.3.0/2.3.1, PyTorch 2.4.0/2.4.1, and PyTorch 2.5.0/2.5.1 (following the same procedure).\n**For older versions, you might need to explicitly specify the latest supported version number** or install via `pip install --no-index` in order to prevent a manual installation from source.\nYou can look up the latest supported version number [here](https://data.pyg.org/whl).\n\n### NVIDIA PyG Container\n\nNVIDIA provides a PyG docker container for effortlessly training and deploying GPU accelerated GNNs with PyG, see [here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pyg).\n\n### Nightly and Master\n\nIn case you want to experiment with the latest PyG features which are not fully released yet, either install the **nightly version** of PyG via\n\n```\npip install pyg-nightly\n```\n\nor install PyG **from master** via\n\n```\npip install git+https://github.com/pyg-team/pytorch_geometric.git\n```\n\n### ROCm Wheels\n\nThe external [`pyg-rocm-build` repository](https://github.com/Looong01/pyg-rocm-build) provides wheels and detailed instructions on how to install PyG for ROCm.\nIf you have any questions about it, please open an issue [here](https://github.com/Looong01/pyg-rocm-build/issues).\n\n## Cite\n\nPlease cite our [PyG 1.0](https://arxiv.org/abs/1903.02428) and [PyG 2.0](https://www.arxiv.org/abs/2507.16991) papers if you use this code in your own work:\n\n```\n@inproceedings{Fey/Lenssen/2019,\n  title={Fast Graph Representation Learning with {PyTorch Geometric}},\n  author={Fey, Matthias and Lenssen, Jan E.},\n  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},\n  year={2019},\n}\n\n@inproceedings{Fey/etal/2025,\n  title={{PyG} 2.0: Scalable Learning on Real World Graphs},\n  author={Fey, Matthias and Sunil, Jinu and Nitta, Akihiro and Puri, Rishi and Shah, Manan, and Stojanovi{\v{c}, Bla{\v{z} and Bendias, Ramona and Alexandria, Barghi and Kocijan, Vid and Zhang, Zecheng and He, Xinwei and Lenssen, Jan E. and Leskovec, Jure},\n  booktitle={Temporal Graph Learning Workshop @Â KDD},\n  year={2025},\n}\n```\n\nFeel free to [email us](mailto:matthias.fey@tu-dortmund.de) if you wish your work to be listed in the [external resources](https://pytorch-geometric.readthedocs.io/en/latest/external/resources.html).\nIf you notice anything unexpected, please open an [issue](https://github.com/pyg-team/pytorch_geometric/issues) and let us know.\nIf you have any questions or are missing a specific feature, feel free [to discuss them with us](https://github.com/pyg-team/pytorch_geometric/discussions).\nWe are motivated to constantly make PyG even better.\n\n[contributing-image]: https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat&color=4B26A4\n[contributing-url]: https://github.com/pyg-team/pytorch_geometric/blob/master/.github/CONTRIBUTING.md\n[pypi-download-image]: https://img.shields.io/pypi/dm/torch_geometric?color=4B26A4\n[pypi-download-url]: https://pepy.tech/projects/torch_geometric\n[pypi-image]: https://img.shields.io/pypi/pyversions/torch-geometric?color=4B26A4\n[pypi-url]: https://pypi.python.org/pypi/torch-geometric\n[slack-image]: https://img.shields.io/badge/slack-join-white.svg?logo=slack&color=4B26A4\n[slack-url]: https://data.pyg.org/slack.html\n', '{"language":"Python","stars":23243,"forks":3925,"watchers":23243,"open_issues":1242,"topics":["deep-learning","geometric-deep-learning","graph-convolutional-networks","graph-neural-networks","pytorch"],"default_branch":"master","size_kb":24632,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:snap-stanford:ogb","source_url":"https://github.com/snap-stanford/ogb"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pyg-lib","source_url":"https://github.com/pyg-team/pyg-lib"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:deepmind:graph_nets","source_url":"https://github.com/deepmind/graph_nets"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pyg-lib","source_url":"https://github.com/pyg-team/pyg-lib"},{"type":"has_code","target_id":"github:rusty1s:pytorch_scatter","source_url":"https://github.com/rusty1s/pytorch_scatter"},{"type":"has_code","target_id":"github:rusty1s:pytorch_sparse","source_url":"https://github.com/rusty1s/pytorch_sparse"},{"type":"has_code","target_id":"github:rusty1s:pytorch_cluster","source_url":"https://github.com/rusty1s/pytorch_cluster"},{"type":"has_code","target_id":"github:rusty1s:pytorch_spline_conv","source_url":"https://github.com/rusty1s/pytorch_spline_conv"},{"type":"has_code","target_id":"github:pytorch:extension-cpp","source_url":"https://github.com/pytorch/extension-cpp"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric.git","source_url":"https://github.com/pyg-team/pytorch_geometric.git"},{"type":"has_code","target_id":"github:Looong01:pyg-rocm-build","source_url":"https://github.com/Looong01/pyg-rocm-build"},{"type":"has_code","target_id":"github:Looong01:pyg-rocm-build","source_url":"https://github.com/Looong01/pyg-rocm-build"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"}]', NULL, 'MIT', 'approved', 80, '47e74c61b452909de05596692041ee34', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-pyg-team-pytorch-geometric from https://github.com/pyg-team.png
Image converted to WebP: data/images/github-pyg-team-pytorch-geometric.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-Tencent-ncnn', 'github--tencent--ncnn', 'ncnn', 'Tencent', '!ncnn ncnn is a high-performance neural network inference computing framework optimized for mobile platforms. ncnn is deeply considerate about deployment and uses on mobile phones from the beginning of design. ncnn does not have third-party dependencies. It is cross-platform and runs faster than all known open-source frameworks on mobile phone cpu. Developers can easily deploy deep learning algorithm models to the mobile platform by using efficient ncnn implementation, creating intelligent AP...', '["android","arm-neon","artificial-intelligence","caffe","darknet","deep-learning","high-preformance","inference","ios","keras","mlir","mxnet","ncnn","neural-network","onnx","pytorch","riscv","simd","tensorflow","vulkan","c++"]', 'other', 22381, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/Tencent/ncnn","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '![ncnn](https://raw.githubusercontent.com/Tencent/ncnn/master/images/256-ncnn.png)\n\n# ncnn\n\n[![License](https://img.shields.io/badge/license-BSD_3_Clause-blue.svg?style=for-the-badge)](LICENSE.txt)\n[![Download Total Count](https://img.shields.io/github/downloads/Tencent/ncnn/total.svg?style=for-the-badge)](https://github.com/Tencent/ncnn/releases)\n[![codecov](https://img.shields.io/codecov/c/github/Tencent/ncnn/master?style=for-the-badge)](https://codecov.io/gh/Tencent/ncnn)\n\nncnn is a high-performance neural network inference computing framework optimized for mobile platforms.\nncnn is deeply considerate about deployment and uses on mobile phones from the beginning of design.\nncnn does not have third-party dependencies.\nIt is cross-platform and runs faster than all known open-source frameworks on mobile phone cpu.\nDevelopers can easily deploy deep learning algorithm models to the mobile platform by using efficient ncnn implementation, creating intelligent APPs, and bringing artificial intelligence to your fingertips.\nncnn is currently being used in many Tencent applications, such as QQ, Qzone, WeChat, Pitu, and so on.\n\nncnn æ˜¯ä¸€ä¸ªä¸ºæ‰‹æœºç«¯æè‡´ä¼˜åŒ–çš„é«˜æ€§èƒ½ç¥ç»ç½‘ç»œå‰å‘è®¡ç®—æ¡†æ¶ã€‚\nncnn ä»è®¾è®¡ä¹‹åˆæ·±åˆ»è€ƒè™‘æ‰‹æœºç«¯çš„éƒ¨ç½²å’Œä½¿ç”¨ã€‚\næ— ç¬¬ä¸‰æ–¹ä¾èµ–ï¼Œè·¨å¹³å°ï¼Œæ‰‹æœºç«¯ cpu çš„é€Ÿåº¦å¿«äºç›®å‰æ‰€æœ‰å·²çŸ¥çš„å¼€æºæ¡†æ¶ã€‚\nåŸºäº ncnnï¼Œå¼€å‘è€…èƒ½å¤Ÿå°†æ·±åº¦å­¦ä¹ ç®—æ³•è½»æ¾ç§»æ¤åˆ°æ‰‹æœºç«¯é«˜æ•ˆæ‰§è¡Œï¼Œ\nå¼€å‘å‡ºäººå·¥æ™ºèƒ½ APPï¼Œå°† AI å¸¦åˆ°ä½ çš„æŒ‡å°–ã€‚\nncnn ç›®å‰å·²åœ¨è…¾è®¯å¤šæ¬¾åº”ç”¨ä¸­ä½¿ç”¨ï¼Œå¦‚ï¼šQQï¼ŒQzoneï¼Œå¾®ä¿¡ï¼Œå¤©å¤© P å›¾ç­‰ã€‚\n\n---\n\n<table>\n<tr>\n<td>\n<b>æŠ€æœ¯äº¤æµ QQ ç¾¤</b><br />\n637093648 (è¶…å¤šå¤§ä½¬)<br />\nç­”æ¡ˆï¼šå·å·å·å·å·ï¼ˆå·²æ»¡ï¼‰\n</td>\n<td rowspan=3>\n<b>Telegram Group</b>\n\n<https://t.me/ncnnyes>\n</td>\n<td rowspan=3>\n<b>Discord Channel</b>\n\n<https://discord.gg/YRsxgmF>\n</td>\n</tr>\n<tr>\n<td>\n<b>Pocky QQ ç¾¤ï¼ˆMLIR YES!ï¼‰</b><br />\n677104663 (è¶…å¤šå¤§ä½¬)<br />\nç­”æ¡ˆï¼šmulti-level intermediate representation\n</td>\n</tr>\n<tr>\n<td>\n<b>ä»–ä»¬éƒ½ä¸çŸ¥é“ pnnx æœ‰å¤šå¥½ç”¨ç¾¤</b><br />\n818998520 (æ–°ç¾¤ï¼)\n</td>\n</tr>\n</table>\n\n---\n\n## Download & Build status\n\nhttps://github.com/Tencent/ncnn/releases/latest\n\n\n<table>\n<tr>\n<td rowspan=2>\n  <img src="https://user-images.githubusercontent.com/25181517/192108372-f71d70ac-7ae6-4c0d-8395-51d8870c2ef0.png" width="120" height="auto">\n</td>\n<td colspan=3>\n\n  **[how to build ncnn library](https://github.com/Tencent/ncnn/wiki/how-to-build) on Linux / Windows / macOS / Raspberry Pi3, Pi4 / POWER / Android / NVIDIA Jetson / iOS / WebAssembly / AllWinner D1 / Loongson 2K1000**\n\n</td>\n</tr>\n<tr>\n<td>Source</td>\n<td colspan=2>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-full-source.zip)\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=3>\n  <img src="https://user-images.githubusercontent.com/25181517/117269608-b7dcfb80-ae58-11eb-8e66-6cc8753553f0.png" width="120" height="auto">\n</td>\n<td colspan=3>\n\n- [Build for Android](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-android)\n- [Build for Termux on Android](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-termux-on-android)\n\n</td>\n</tr>\n<tr>\n<td>Android</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-android-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-android.zip)\n\n</td>\n<td rowspan=2>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/android.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Aandroid)\n\n</td>\n</tr>\n<tr>\n<td>Android shared</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-android-vulkan-shared.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-android-shared.zip)\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=3>\n  <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/HMOS_Logo_Icon.svg/240px-HMOS_Logo_Icon.svg.png" width="120" height="auto">\n</td>\n<td colspan=3>\n\n- [Build for HarmonyOS with cross-compiling](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-harmonyos-with-cross-compiling)\n\n</td>\n</tr>\n<tr>\n<td>HarmonyOS</td>\n<td>\n\n</td>\n<td rowspan=2>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/harmonyos.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Aharmonyos)\n\n</td>\n</tr>\n<tr>\n<td>HarmonyOS shared</td>\n<td>\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=3>\n  <img src="https://user-images.githubusercontent.com/25181517/121406611-a8246b80-c95e-11eb-9b11-b771486377f6.png" width="120" height="auto">\n</td>\n<td colspan=3>\n\n- [Build for iOS on macOS with xcode](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-ios-on-macos-with-xcode)\n\n</td>\n</tr>\n<tr>\n<td>iOS</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ios-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ios.zip)\n\n</td>\n<td rowspan=2>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/ios.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Aios)\n\n</td>\n</tr>\n<tr>\n<td>iOS-Simulator</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ios-simulator-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ios-simulator.zip)\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=10>\n  <img src="https://user-images.githubusercontent.com/25181517/186884152-ae609cca-8cf1-4175-8d60-1ce1fa078ca2.png" width="120" height="auto">\n</td>\n<td colspan=3>\n\n- [Build for macOS](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-macos)\n\n</td>\n</tr>\n<tr>\n<td>macOS</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-macos-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-macos.zip)\n\n</td>\n<td rowspan=1>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/macos.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Amacos)\n\n</td>\n</tr>\n<tr>\n<td>Mac-Catalyst</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-mac-catalyst-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-mac-catalyst.zip)\n\n</td>\n<td rowspan=1>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/mac-catalyst.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Amac-catalyst)\n\n</td>\n</tr>\n<tr>\n<td>watchOS</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-watchos.zip)\n\n</td>\n<td rowspan=2>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/watchos.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Awatchos)\n\n</td>\n</tr>\n<tr>\n<td>watchOS-Simulator</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-watchos-simulator.zip)\n\n</td>\n</tr>\n<tr>\n<td>tvOS</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-tvos-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-tvos.zip)\n\n</td>\n<td rowspan=2>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/tvos.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Atvos)\n\n</td>\n</tr>\n<tr>\n<td>tvOS-Simulator</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-tvos-simulator-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-tvos-simulator.zip)\n\n</td>\n</tr>\n<tr>\n<td>visionOS</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-visionos-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-visionos.zip)\n\n</td>\n<td rowspan=2>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/visionos.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Avisionos)\n\n</td>\n</tr>\n<tr>\n<td>visionOS-Simulator</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-visionos-simulator-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-visionos-simulator.zip)\n\n</td>\n</tr>\n<tr>\n<td>Apple xcframework</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-apple-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-apple.zip)\n\n</td>\n<td rowspan=1>\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=3>\n  <img src="https://user-images.githubusercontent.com/25181517/186884153-99edc188-e4aa-4c84-91b0-e2df260ebc33.png" width="120" height="auto">\n</td>\n<td colspan=3>\n\n- [Build for Linux / NVIDIA Jetson / Raspberry Pi3, Pi4 / POWER](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-linux)\n\n</td>\n</tr>\n<tr>\n<td>Ubuntu 22.04</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ubuntu-2204.zip)\n  [<img src="https://img.shields.io/badge/+shared-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ubuntu-2204-shared.zip)\n\n</td>\n<td rowspan=2>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-x64-gpu-gcc.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-x64-gpu-gcc)\n\n</td>\n</tr>\n<tr>\n<td>Ubuntu 24.04</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ubuntu-2404.zip)\n  [<img src="https://img.shields.io/badge/+shared-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ubuntu-2404-shared.zip)\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=5>\n  <img alt="windows" src="https://user-images.githubusercontent.com/25181517/186884150-05e9ff6d-340e-4802-9533-2c3f02363ee3.png" width="120" height="auto">\n</td>\n<td colspan=3>\n\n- [Build for Windows x64 using VS2017](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-windows-x64-using-visual-studio-community-2017)\n- [Build for Windows x64 using MinGW-w64](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-windows-x64-using-mingw-w64)\n\n</td>\n</tr>\n<tr>\n<td>VS2015</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2015.zip)\n  [<img src="https://img.shields.io/badge/+shared-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2015-shared.zip)\n\n</td>\n<td rowspan=4>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/windows.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Awindows)\n\n</td>\n</tr>\n<tr>\n<td>VS2017</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2017.zip)\n  [<img src="https://img.shields.io/badge/+shared-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2017-shared.zip)\n\n</td>\n</tr>\n<tr>\n<td>VS2019</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2019.zip)\n  [<img src="https://img.shields.io/badge/+shared-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2019-shared.zip)\n\n</td>\n</tr>\n<tr>\n<td>VS2022</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2022.zip)\n  [<img src="https://img.shields.io/badge/+shared-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2022-shared.zip)\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=2>\n  <img src="https://user-images.githubusercontent.com/25181517/188324036-d704ac9a-6e61-4722-b978-254b25b61bed.png" width="120" height="auto">\n</td>\n<td colspan=3>\n\n- [Build for WebAssembly](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-webassembly)\n\n</td>\n</tr>\n<tr>\n<td>WebAssembly</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-webassembly.zip)\n\n</td>\n<td>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/web-assembly.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Aweb-assembly)\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=8>\n  <img src="https://github.com/marwin1991/profile-technology-icons/assets/76662862/2481dc48-be6b-4ebb-9e8c-3b957efe69fa" width="120" height="auto">\n</td>\n<td colspan=3>\n\n- [Build for ARM Cortex-A family with cross-compiling](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-arm-cortex-a-family-with-cross-compiling)\n- [Build for Hisilicon platform with cross-compiling](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-hisilicon-platform-with-cross-compiling)\n- [Build for AllWinner D1](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-allwinner-d1)\n- [Build for Loongson 2K1000](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-loongson-2k1000)\n- [Build for QNX](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-qnx)\n\n</td>\n</tr>\n<tr>\n<td>Linux (arm)</td>\n<td></td>\n<td>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-arm.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-arm)\n\n</td>\n</tr>\n<tr>\n<td>Linux (aarch64)</td>\n<td></td>\n<td>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-aarch64.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-aarch64)\n\n</td>\n</tr>\n<tr>\n<td>Linux (mips)</td>\n<td></td>\n<td>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-mips.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-mips)\n\n</td>\n</tr>\n<tr>\n<td>Linux (mips64)</td>\n<td></td>\n<td>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-mips64.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-mips64)\n\n</td>\n</tr>\n<tr>\n<td>Linux (ppc64)</td>\n<td></td>\n<td>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-ppc64.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-ppc64)\n\n</td>\n</tr>\n<tr>\n<td>Linux (riscv64)</td>\n<td></td>\n<td>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-riscv64.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-riscv64)\n\n</td>\n</tr>\n<tr>\n<td>Linux (loongarch64)</td>\n<td></td>\n<td>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-loongarch64.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-loongarch64)\n\n</td>\n</tr>\n\n</table>\n\n\n---\n\n## Support most commonly used CNN network\n\n## æ”¯æŒå¤§éƒ¨åˆ†å¸¸ç”¨çš„ CNN ç½‘ç»œ\n\n- Classical CNN:\n  [VGG](https://github.com/BVLC/caffe/wiki/Model-Zoo#models-used-by-the-vgg-team-in-ilsvrc-2014)\n  [AlexNet](https://github.com/BVLC/caffe/tree/9b891540183ddc834a02b2bd81b31afae71b2153/models/bvlc_alexnet)\n  [GoogleNet](https://github.com/BVLC/caffe/tree/9b891540183ddc834a02b2bd81b31afae71b2153/models/bvlc_googlenet)\n  Inception\n  ...\n- Practical CNN:\n  [ResNet](https://github.com/tornadomeet/ResNet)\n  [DenseNet](https://github.com/liuzhuang13/DenseNet)\n  [SENet](https://github.com/hujie-frank/SENet)\n  [FPN](https://github.com/unsky/FPN)\n  ...\n- Light-weight CNN:\n  [SqueezeNet](https://github.com/forresti/SqueezeNet)\n  [MobileNetV1](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md)\n  [MobileNetV2/V3](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md)\n  [ShuffleNetV1](https://github.com/farmingyard/ShuffleNet)\n  [ShuffleNetV2](https://github.com/opconty/keras-shufflenetV2)\n  [MNasNet](https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet)\n  ...\n- Face Detection:\n  [MTCNN](https://github.com/ipazc/mtcnn)\n  [RetinaFace](https://github.com/biubug6/Pytorch_Retinaface)\n  [scrfd](https://github.com/nihui/ncnn-android-scrfd)\n  ...\n- Detection:\n  [VGG-SSD](https://github.com/lzx1413/CAFFE_SSD)\n  [MobileNet-SSD](https://github.com/chuanqi305/MobileNet-SSD)\n  [SqueezeNet-SSD](https://github.com/chuanqi305/SqueezeNet-SSD)\n  [MobileNetV2-SSDLite](https://github.com/chuanqi305/MobileNetv2-SSDLite)\n  [MobileNetV3-SSDLite](https://github.com/XiaoyuHuang96/MobilenetV3SSDLite-tfkeras)\n  ...\n- Detection:\n  [Faster-RCNN](https://github.com/rbgirshick/py-faster-rcnn)\n  [R-FCN](https://github.com/daijifeng001/R-FCN)\n  ...\n- Detection:\n  [YOLOv2](https://github.com/longcw/yolo2-pytorch)\n  [YOLOv3](https://github.com/ultralytics/yolov3)\n  [MobileNet-YOLOv3](https://github.com/eric612/MobileNet-YOLO)\n  [YOLOv4](https://github.com/Tianxiaomo/pytorch-YOLOv4)\n  [YOLOv5](https://github.com/ultralytics/yolov5)\n  [YOLOv7](https://github.com/WongKinYiu/yolov7)\n  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)\n  [YOLOv8](https://github.com/nihui/ncnn-android-yolov8)\n  ...\n- Detection:\n  [NanoDet](https://github.com/RangiLyu/nanodet)\n- Segmentation:\n  [FCN](https://github.com/unsky/FPN)\n  [PSPNet](https://github.com/hszhao/PSPNet)\n  [UNet](https://github.com/zhixuhao/unet)\n  [YOLACT](https://github.com/dbolya/yolact)\n  ...\n- Pose Estimation:\n  [SimplePose](https://github.com/dog-qiuqiu/Ultralight-SimplePose)\n  ...\n\n---\n\n## HowTo\n\n**[use ncnn with alexnet](https://github.com/Tencent/ncnn/wiki/use-ncnn-with-alexnet) with detailed steps, recommended for beginners :)**\n\n**[ncnn ç»„ä»¶ä½¿ç”¨æŒ‡åŒ— alexnet](https://github.com/Tencent/ncnn/wiki/use-ncnn-with-alexnet.zh) é™„å¸¦è¯¦ç»†æ­¥éª¤ï¼Œæ–°äººå¼ºçƒˆæ¨è :)**\n\n**[use netron for ncnn model visualization](https://netron.app)**\n\n**[use ncnn with pytorch or onnx](https://github.com/Tencent/ncnn/wiki/use-ncnn-with-pytorch-or-onnx)**\n\n[ncnn low-level operation api](https://github.com/Tencent/ncnn/wiki/low-level-operation-api)\n\n[ncnn param and model file spec](https://github.com/Tencent/ncnn/wiki/param-and-model-file-structure)\n\n[ncnn operation param weight table](https://github.com/Tencent/ncnn/wiki/operation-param-weight-table)\n\n[how to implement custom layer step by step](https://github.com/Tencent/ncnn/wiki/how-to-implement-custom-layer-step-by-step)\n\n---\n\n## FAQ\n\n**[ncnn deepwiki](https://deepwiki.com/Tencent/ncnn) LLM Answering Questions ;)** \n\n**[ncnn throw error](https://github.com/Tencent/ncnn/wiki/FAQ-ncnn-throw-error)**\n\n**[ncnn produce wrong result](https://github.com/Tencent/ncnn/wiki/FAQ-ncnn-produce-wrong-result)**\n\n**[ncnn vulkan](https://github.com/Tencent/ncnn/wiki/FAQ-ncnn-vulkan)**\n\n---\n\n## Features\n\n- Supports convolutional neural networks, supports multiple input and multi-branch structure, can calculate part of the branch\n- No third-party library dependencies, does not rely on BLAS / NNPACK or any other computing framework\n- Pure C++ implementation, cross-platform, supports Android, iOS and so on\n- ARM NEON assembly level of careful optimization, calculation speed is extremely high\n- Sophisticated memory management and data structure design, very low memory footprint\n- Supports multi-core parallel computing acceleration, ARM big.LITTLE CPU scheduling optimization\n- Supports GPU acceleration via the next-generation low-overhead Vulkan API\n- Extensible model design, supports 8bit quantization and half-precision floating point storage, can import caffe/pytorch/mxnet/onnx/darknet/keras/tensorflow(mlir) models\n- Support direct memory zero copy reference load network model\n- Can be registered with custom layer implementation and extended\n- Well, it is strong, not afraid of being stuffed with å· QvQ\n\n## åŠŸèƒ½æ¦‚è¿°\n\n- æ”¯æŒå·ç§¯ç¥ç»ç½‘ç»œï¼Œæ”¯æŒå¤šè¾“å…¥å’Œå¤šåˆ†æ”¯ç»“æ„ï¼Œå¯è®¡ç®—éƒ¨åˆ†åˆ†æ”¯\n- æ— ä»»ä½•ç¬¬ä¸‰æ–¹åº“ä¾èµ–ï¼Œä¸ä¾èµ– BLAS/NNPACK ç­‰è®¡ç®—æ¡†æ¶\n- çº¯ C++ å®ç°ï¼Œè·¨å¹³å°ï¼Œæ”¯æŒ Android / iOS ç­‰\n- ARM Neon æ±‡ç¼–çº§è‰¯å¿ƒä¼˜åŒ–ï¼Œè®¡ç®—é€Ÿåº¦æå¿«\n- ç²¾ç»†çš„å†…å­˜ç®¡ç†å’Œæ•°æ®ç»“æ„è®¾è®¡ï¼Œå†…å­˜å ç”¨æä½\n- æ”¯æŒå¤šæ ¸å¹¶è¡Œè®¡ç®—åŠ é€Ÿï¼ŒARM big.LITTLE CPU è°ƒåº¦ä¼˜åŒ–\n- æ”¯æŒåŸºäºå…¨æ–°ä½æ¶ˆè€—çš„ Vulkan API GPU åŠ é€Ÿ\n- å¯æ‰©å±•çš„æ¨¡å‹è®¾è®¡ï¼Œæ”¯æŒ 8bit [é‡åŒ–](tools/quantize) å’ŒåŠç²¾åº¦æµ®ç‚¹å­˜å‚¨ï¼Œå¯å¯¼å…¥ caffe/pytorch/mxnet/onnx/darknet/keras/tensorflow(mlir) æ¨¡å‹\n- æ”¯æŒç›´æ¥å†…å­˜é›¶æ‹·è´å¼•ç”¨åŠ è½½ç½‘ç»œæ¨¡å‹\n- å¯æ³¨å†Œè‡ªå®šä¹‰å±‚å®ç°å¹¶æ‰©å±•\n- æ©ï¼Œå¾ˆå¼ºå°±æ˜¯äº†ï¼Œä¸æ€•è¢«å¡å· QvQ\n\n---\n\n## supported platform matrix\n\n- âœ… = known work and runs fast with good optimization\n- âœ”ï¸ = known work, but speed may not be fast enough\n- â” = shall work, not confirmed\n- / = not applied\n\n|            | Windows | Linux | Android | macOS | iOS |\n| ---------- | ------- | ----- | ------- | ----- | --- |\n| intel-cpu  | âœ”ï¸      | âœ”ï¸    | âœ”ï¸      | âœ”ï¸    | /   |\n| intel-gpu  | âœ”ï¸      | âœ”ï¸    | âœ”ï¸      | âœ”ï¸    | /   |\n| amd-cpu    | âœ”ï¸      | âœ”ï¸    | âœ”ï¸      | âœ”ï¸    | /   |\n| amd-gpu    | âœ”ï¸      | âœ”ï¸    | âœ”ï¸      | âœ”ï¸    | /   |\n| nvidia-gpu | âœ”ï¸      | âœ”ï¸    | âœ”ï¸      | âœ”ï¸    | /   |\n| qcom-cpu   | âœ…      | âœ…    | âœ…      | /     | /   |\n| qcom-gpu   | âœ”ï¸      | âœ”ï¸    | âœ”ï¸      | /     | /   |\n| arm-cpu    | âœ…      | âœ…    | âœ…      | /     | /   |\n| arm-gpu    | â”      | âœ”ï¸    | âœ”ï¸      | /     | /   |\n| apple-cpu  | /       | /     | /       | âœ”ï¸    | âœ…  |\n| apple-gpu  | /       | /     | /       | âœ”ï¸    | âœ”ï¸  |\n| ibm-cpu    | /       | âœ”ï¸     | /       | /    | /  |\n\n---\n\n## Project examples\n\n- <https://github.com/nihui/ncnn-android-squeezenet>\n- <https://github.com/nihui/ncnn-android-styletransfer>\n- <https://github.com/nihui/ncnn-android-mobilenetssd>\n- <https://github.com/moli232777144/mtcnn_ncnn>\n- <https://github.com/nihui/ncnn-android-yolov5>\n- <https://github.com/xiang-wuu/ncnn-android-yolov7>\n- <https://github.com/nihui/ncnn-android-scrfd> ğŸ¤©\n- <https://github.com/shaoshengsong/qt_android_ncnn_lib_encrypt_example>\n\n<img src="https://github.com/nihui/ncnn-assets/raw/master/20181217/ncnn-2.jpg" height ="230"/><img src="https://github.com/nihui/ncnn-assets/raw/master/20181217/4.jpg" height ="230"/><img src="https://github.com/nihui/ncnn-assets/raw/master/20181217/ncnn-33.jpg" height ="230"/><img src="https://github.com/nihui/ncnn-assets/raw/master/20181217/ncnn-m.png" height ="230"/><img src="https://github.com/nihui/ncnn-android-yolov5/raw/master/screenshot.jpg" height ="230"/><img src="https://github.com/nihui/ncnn-android-scrfd/raw/master/screenshot.jpg" height ="230"/><br>\n\n- <https://github.com/magicse/ncnn-colorization-siggraph17><br>\n<img src="https://user-images.githubusercontent.com/13585785/189326958-f5a8d6f8-caef-49bf-88da-ae494371195d.jpg" width ="700"/>\n\n- <https://github.com/mizu-bai/ncnn-fortran> Call ncnn from Fortran\n\n- <https://github.com/k2-fsa/sherpa> Use ncnn for real-time speech\n  recognition (i.e., speech-to-text); also support embedded devices and provide\n  mobile Apps (e.g., Android App)\n\n---\n\n## License\n\n[BSD 3 Clause](LICENSE.txt)\n', '{"language":"C++","stars":22381,"forks":4355,"watchers":22381,"open_issues":1143,"topics":["android","arm-neon","artificial-intelligence","caffe","darknet","deep-learning","high-preformance","inference","ios","keras","mlir","mxnet","ncnn","neural-network","onnx","pytorch","riscv","simd","tensorflow","vulkan"],"default_branch":"master","size_kb":33028,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:marwin1991:profile-technology-icons","source_url":"https://github.com/marwin1991/profile-technology-icons"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:BVLC:caffe","source_url":"https://github.com/BVLC/caffe"},{"type":"has_code","target_id":"github:BVLC:caffe","source_url":"https://github.com/BVLC/caffe"},{"type":"has_code","target_id":"github:BVLC:caffe","source_url":"https://github.com/BVLC/caffe"},{"type":"has_code","target_id":"github:tornadomeet:ResNet","source_url":"https://github.com/tornadomeet/ResNet"},{"type":"has_code","target_id":"github:liuzhuang13:DenseNet","source_url":"https://github.com/liuzhuang13/DenseNet"},{"type":"has_code","target_id":"github:hujie-frank:SENet","source_url":"https://github.com/hujie-frank/SENet"},{"type":"has_code","target_id":"github:unsky:FPN","source_url":"https://github.com/unsky/FPN"},{"type":"has_code","target_id":"github:forresti:SqueezeNet","source_url":"https://github.com/forresti/SqueezeNet"},{"type":"has_code","target_id":"github:tensorflow:models","source_url":"https://github.com/tensorflow/models"},{"type":"has_code","target_id":"github:tensorflow:models","source_url":"https://github.com/tensorflow/models"},{"type":"has_code","target_id":"github:farmingyard:ShuffleNet","source_url":"https://github.com/farmingyard/ShuffleNet"},{"type":"has_code","target_id":"github:opconty:keras-shufflenetV2","source_url":"https://github.com/opconty/keras-shufflenetV2"},{"type":"has_code","target_id":"github:tensorflow:models","source_url":"https://github.com/tensorflow/models"},{"type":"has_code","target_id":"github:ipazc:mtcnn","source_url":"https://github.com/ipazc/mtcnn"},{"type":"has_code","target_id":"github:biubug6:Pytorch_Retinaface","source_url":"https://github.com/biubug6/Pytorch_Retinaface"},{"type":"has_code","target_id":"github:nihui:ncnn-android-scrfd","source_url":"https://github.com/nihui/ncnn-android-scrfd"},{"type":"has_code","target_id":"github:lzx1413:CAFFE_SSD","source_url":"https://github.com/lzx1413/CAFFE_SSD"},{"type":"has_code","target_id":"github:chuanqi305:MobileNet-SSD","source_url":"https://github.com/chuanqi305/MobileNet-SSD"},{"type":"has_code","target_id":"github:chuanqi305:SqueezeNet-SSD","source_url":"https://github.com/chuanqi305/SqueezeNet-SSD"},{"type":"has_code","target_id":"github:chuanqi305:MobileNetv2-SSDLite","source_url":"https://github.com/chuanqi305/MobileNetv2-SSDLite"},{"type":"has_code","target_id":"github:XiaoyuHuang96:MobilenetV3SSDLite-tfkeras","source_url":"https://github.com/XiaoyuHuang96/MobilenetV3SSDLite-tfkeras"},{"type":"has_code","target_id":"github:rbgirshick:py-faster-rcnn","source_url":"https://github.com/rbgirshick/py-faster-rcnn"},{"type":"has_code","target_id":"github:daijifeng001:R-FCN","source_url":"https://github.com/daijifeng001/R-FCN"},{"type":"has_code","target_id":"github:longcw:yolo2-pytorch","source_url":"https://github.com/longcw/yolo2-pytorch"},{"type":"has_code","target_id":"github:ultralytics:yolov3","source_url":"https://github.com/ultralytics/yolov3"},{"type":"has_code","target_id":"github:eric612:MobileNet-YOLO","source_url":"https://github.com/eric612/MobileNet-YOLO"},{"type":"has_code","target_id":"github:Tianxiaomo:pytorch-YOLOv4","source_url":"https://github.com/Tianxiaomo/pytorch-YOLOv4"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:WongKinYiu:yolov7","source_url":"https://github.com/WongKinYiu/yolov7"},{"type":"has_code","target_id":"github:Megvii-BaseDetection:YOLOX","source_url":"https://github.com/Megvii-BaseDetection/YOLOX"},{"type":"has_code","target_id":"github:nihui:ncnn-android-yolov8","source_url":"https://github.com/nihui/ncnn-android-yolov8"},{"type":"has_code","target_id":"github:RangiLyu:nanodet","source_url":"https://github.com/RangiLyu/nanodet"},{"type":"has_code","target_id":"github:unsky:FPN","source_url":"https://github.com/unsky/FPN"},{"type":"has_code","target_id":"github:hszhao:PSPNet","source_url":"https://github.com/hszhao/PSPNet"},{"type":"has_code","target_id":"github:zhixuhao:unet","source_url":"https://github.com/zhixuhao/unet"},{"type":"has_code","target_id":"github:dbolya:yolact","source_url":"https://github.com/dbolya/yolact"},{"type":"has_code","target_id":"github:dog-qiuqiu:Ultralight-SimplePose","source_url":"https://github.com/dog-qiuqiu/Ultralight-SimplePose"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:nihui:ncnn-android-squeezenet>","source_url":"https://github.com/nihui/ncnn-android-squeezenet>"},{"type":"has_code","target_id":"github:nihui:ncnn-android-styletransfer>","source_url":"https://github.com/nihui/ncnn-android-styletransfer>"},{"type":"has_code","target_id":"github:nihui:ncnn-android-mobilenetssd>","source_url":"https://github.com/nihui/ncnn-android-mobilenetssd>"},{"type":"has_code","target_id":"github:moli232777144:mtcnn_ncnn>","source_url":"https://github.com/moli232777144/mtcnn_ncnn>"},{"type":"has_code","target_id":"github:nihui:ncnn-android-yolov5>","source_url":"https://github.com/nihui/ncnn-android-yolov5>"},{"type":"has_code","target_id":"github:xiang-wuu:ncnn-android-yolov7>","source_url":"https://github.com/xiang-wuu/ncnn-android-yolov7>"},{"type":"has_code","target_id":"github:nihui:ncnn-android-scrfd>","source_url":"https://github.com/nihui/ncnn-android-scrfd>"},{"type":"has_code","target_id":"github:shaoshengsong:qt_android_ncnn_lib_encrypt_example>","source_url":"https://github.com/shaoshengsong/qt_android_ncnn_lib_encrypt_example>"},{"type":"has_code","target_id":"github:nihui:ncnn-assets","source_url":"https://github.com/nihui/ncnn-assets"},{"type":"has_code","target_id":"github:nihui:ncnn-assets","source_url":"https://github.com/nihui/ncnn-assets"},{"type":"has_code","target_id":"github:nihui:ncnn-assets","source_url":"https://github.com/nihui/ncnn-assets"},{"type":"has_code","target_id":"github:nihui:ncnn-assets","source_url":"https://github.com/nihui/ncnn-assets"},{"type":"has_code","target_id":"github:nihui:ncnn-android-yolov5","source_url":"https://github.com/nihui/ncnn-android-yolov5"},{"type":"has_code","target_id":"github:nihui:ncnn-android-scrfd","source_url":"https://github.com/nihui/ncnn-android-scrfd"},{"type":"has_code","target_id":"github:magicse:ncnn-colorization-siggraph17><br>","source_url":"https://github.com/magicse/ncnn-colorization-siggraph17><br>"},{"type":"has_code","target_id":"github:mizu-bai:ncnn-fortran>","source_url":"https://github.com/mizu-bai/ncnn-fortran>"},{"type":"has_code","target_id":"github:k2-fsa:sherpa>","source_url":"https://github.com/k2-fsa/sherpa>"}]', NULL, 'NOASSERTION', 'approved', 80, '9e2a1f04c32e99099ce2adfa6a63cb1c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-Tencent-ncnn from https://github.com/Tencent.png
Image converted to WebP: data/images/github-Tencent-ncnn.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-SYSTRAN-faster-whisper', 'github--systran--faster-whisper', 'faster-whisper', 'SYSTRAN', '**faster-whisper** is a reimplementation of OpenAI''s Whisper model using CTranslate2, which is a fast inference engine for Transformer models. This implementation is up to 4 times faster than openai/whisper for the same accuracy while using less memory. The efficiency can be further improved with 8-bit quantization on both CPU and GPU. For reference, here''s the time and memory usage that are required to transcribe **13 minutes** of audio using different implementations: * openai/whisper@v2024...', '["deep-learning","inference","openai","quantization","speech-recognition","speech-to-text","transformer","whisper","python"]', 'other', 19366, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/SYSTRAN/faster-whisper","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '[![CI](https://github.com/SYSTRAN/faster-whisper/workflows/CI/badge.svg)](https://github.com/SYSTRAN/faster-whisper/actions?query=workflow%3ACI) [![PyPI version](https://badge.fury.io/py/faster-whisper.svg)](https://badge.fury.io/py/faster-whisper)\n\n# Faster Whisper transcription with CTranslate2\n\n**faster-whisper** is a reimplementation of OpenAI''s Whisper model using [CTranslate2](https://github.com/OpenNMT/CTranslate2/), which is a fast inference engine for Transformer models.\n\nThis implementation is up to 4 times faster than [openai/whisper](https://github.com/openai/whisper) for the same accuracy while using less memory. The efficiency can be further improved with 8-bit quantization on both CPU and GPU.\n\n## Benchmark\n\n### Whisper\n\nFor reference, here''s the time and memory usage that are required to transcribe [**13 minutes**](https://www.youtube.com/watch?v=0u7tTptBo9I) of audio using different implementations:\n\n* [openai/whisper](https://github.com/openai/whisper)@[v20240930](https://github.com/openai/whisper/tree/v20240930)\n* [whisper.cpp](https://github.com/ggerganov/whisper.cpp)@[v1.7.2](https://github.com/ggerganov/whisper.cpp/tree/v1.7.2)\n* [transformers](https://github.com/huggingface/transformers)@[v4.46.3](https://github.com/huggingface/transformers/tree/v4.46.3)\n* [faster-whisper](https://github.com/SYSTRAN/faster-whisper)@[v1.1.0](https://github.com/SYSTRAN/faster-whisper/tree/v1.1.0)\n\n### Large-v2 model on GPU\n\n| Implementation | Precision | Beam size | Time | VRAM Usage |\n| --- | --- | --- | --- | --- |\n| openai/whisper | fp16 | 5 | 2m23s | 4708MB |\n| whisper.cpp (Flash Attention) | fp16 | 5 | 1m05s | 4127MB |\n| transformers (SDPA)[^1] | fp16 | 5 | 1m52s | 4960MB |\n| faster-whisper | fp16 | 5 | 1m03s | 4525MB |\n| faster-whisper (`batch_size=8`) | fp16 | 5 | 17s | 6090MB |\n| faster-whisper | int8 | 5 | 59s | 2926MB |\n| faster-whisper (`batch_size=8`) | int8 | 5 | 16s | 4500MB |\n\n### distil-whisper-large-v3 model on GPU\n\n| Implementation | Precision | Beam size | Time | YT Commons WER |\n| --- | --- | --- | --- | --- |\n| transformers (SDPA) (`batch_size=16`) | fp16 | 5 | 46m12s | 14.801 |\n| faster-whisper (`batch_size=16`) | fp16 | 5 | 25m50s | 13.527 |\n\n*GPU Benchmarks are Executed with CUDA 12.4 on a NVIDIA RTX 3070 Ti 8GB.*\n[^1]: transformers OOM for any batch size > 1\n\n### Small model on CPU\n\n| Implementation | Precision | Beam size | Time | RAM Usage |\n| --- | --- | --- | --- | --- |\n| openai/whisper | fp32 | 5 | 6m58s | 2335MB |\n| whisper.cpp | fp32 | 5 | 2m05s | 1049MB |\n| whisper.cpp (OpenVINO) | fp32 | 5 | 1m45s | 1642MB |\n| faster-whisper | fp32 | 5 | 2m37s | 2257MB |\n| faster-whisper (`batch_size=8`) | fp32 | 5 | 1m06s | 4230MB |\n| faster-whisper | int8 | 5 | 1m42s | 1477MB |\n| faster-whisper (`batch_size=8`) | int8 | 5 | 51s | 3608MB |\n\n*Executed with 8 threads on an Intel Core i7-12700K.*\n\n\n## Requirements\n\n* Python 3.9 or greater\n\nUnlike openai-whisper, FFmpeg does **not** need to be installed on the system. The audio is decoded with the Python library [PyAV](https://github.com/PyAV-Org/PyAV) which bundles the FFmpeg libraries in its package.\n\n### GPU\n\nGPU execution requires the following NVIDIA libraries to be installed:\n\n* [cuBLAS for CUDA 12](https://developer.nvidia.com/cublas)\n* [cuDNN 9 for CUDA 12](https://developer.nvidia.com/cudnn)\n\n**Note**: The latest versions of `ctranslate2` only support CUDA 12 and cuDNN 9. For CUDA 11 and cuDNN 8, the current workaround is downgrading to the `3.24.0` version of `ctranslate2`, for CUDA 12 and cuDNN 8, downgrade to the `4.4.0` version of `ctranslate2`, (This can be done with `pip install --force-reinstall ctranslate2==4.4.0` or specifying the version in a `requirements.txt`).\n\nThere are multiple ways to install the NVIDIA libraries mentioned above. The recommended way is described in the official NVIDIA documentation, but we also suggest other installation methods below. \n\n<details>\n<summary>Other installation methods (click to expand)</summary>\n\n\n**Note:** For all these methods below, keep in mind the above note regarding CUDA versions. Depending on your setup, you may need to install the _CUDA 11_ versions of libraries that correspond to the CUDA 12 libraries listed in the instructions below.\n\n#### Use Docker\n\nThe libraries (cuBLAS, cuDNN) are installed in this official NVIDIA CUDA Docker images: `nvidia/cuda:12.3.2-cudnn9-runtime-ubuntu22.04`.\n\n#### Install with `pip` (Linux only)\n\nOn Linux these libraries can be installed with `pip`. Note that `LD_LIBRARY_PATH` must be set before launching Python.\n\n```bash\npip install nvidia-cublas-cu12 nvidia-cudnn-cu12==9.*\n\nexport LD_LIBRARY_PATH=`python3 -c ''import os; import nvidia.cublas.lib; import nvidia.cudnn.lib; print(os.path.dirname(nvidia.cublas.lib.__file__) + ":" + os.path.dirname(nvidia.cudnn.lib.__file__))''`\n```\n\n#### Download the libraries from Purfview''s repository (Windows & Linux)\n\nPurfview''s [whisper-standalone-win](https://github.com/Purfview/whisper-standalone-win) provides the required NVIDIA libraries for Windows & Linux in a [single archive](https://github.com/Purfview/whisper-standalone-win/releases/tag/libs). Decompress the archive and place the libraries in a directory included in the `PATH`.\n\n</details>\n\n## Installation\n\nThe module can be installed from [PyPI](https://pypi.org/project/faster-whisper/):\n\n```bash\npip install faster-whisper\n```\n\n<details>\n<summary>Other installation methods (click to expand)</summary>\n\n### Install the master branch\n\n```bash\npip install --force-reinstall "faster-whisper @ https://github.com/SYSTRAN/faster-whisper/archive/refs/heads/master.tar.gz"\n```\n\n### Install a specific commit\n\n```bash\npip install --force-reinstall "faster-whisper @ https://github.com/SYSTRAN/faster-whisper/archive/a4f1cc8f11433e454c3934442b5e1a4ed5e865c3.tar.gz"\n```\n\n</details>\n\n## Usage\n\n### Faster-whisper\n\n```python\nfrom faster_whisper import WhisperModel\n\nmodel_size = "large-v3"\n\n# Run on GPU with FP16\nmodel = WhisperModel(model_size, device="cuda", compute_type="float16")\n\n# or run on GPU with INT8\n# model = WhisperModel(model_size, device="cuda", compute_type="int8_float16")\n# or run on CPU with INT8\n# model = WhisperModel(model_size, device="cpu", compute_type="int8")\n\nsegments, info = model.transcribe("audio.mp3", beam_size=5)\n\nprint("Detected language ''%s'' with probability %f" % (info.language, info.language_probability))\n\nfor segment in segments:\n    print("[%.2fs -> %.2fs] %s" % (segment.start, segment.end, segment.text))\n```\n\n**Warning:** `segments` is a *generator* so the transcription only starts when you iterate over it. The transcription can be run to completion by gathering the segments in a list or a `for` loop:\n\n```python\nsegments, _ = model.transcribe("audio.mp3")\nsegments = list(segments)  # The transcription will actually run here.\n```\n\n### Batched Transcription\nThe following code snippet illustrates how to run batched transcription on an example audio file. `BatchedInferencePipeline.transcribe` is a drop-in replacement for `WhisperModel.transcribe`\n\n```python\nfrom faster_whisper import WhisperModel, BatchedInferencePipeline\n\nmodel = WhisperModel("turbo", device="cuda", compute_type="float16")\nbatched_model = BatchedInferencePipeline(model=model)\nsegments, info = batched_model.transcribe("audio.mp3", batch_size=16)\n\nfor segment in segments:\n    print("[%.2fs -> %.2fs] %s" % (segment.start, segment.end, segment.text))\n```\n\n### Faster Distil-Whisper\n\nThe Distil-Whisper checkpoints are compatible with the Faster-Whisper package. In particular, the latest [distil-large-v3](https://huggingface.co/distil-whisper/distil-large-v3)\ncheckpoint is intrinsically designed to work with the Faster-Whisper transcription algorithm. The following code snippet \ndemonstrates how to run inference with distil-large-v3 on a specified audio file:\n\n```python\nfrom faster_whisper import WhisperModel\n\nmodel_size = "distil-large-v3"\n\nmodel = WhisperModel(model_size, device="cuda", compute_type="float16")\nsegments, info = model.transcribe("audio.mp3", beam_size=5, language="en", condition_on_previous_text=False)\n\nfor segment in segments:\n    print("[%.2fs -> %.2fs] %s" % (segment.start, segment.end, segment.text))\n```\n\nFor more information about the distil-large-v3 model, refer to the original [model card](https://huggingface.co/distil-whisper/distil-large-v3).\n\n### Word-level timestamps\n\n```python\nsegments, _ = model.transcribe("audio.mp3", word_timestamps=True)\n\nfor segment in segments:\n    for word in segment.words:\n        print("[%.2fs -> %.2fs] %s" % (word.start, word.end, word.word))\n```\n\n### VAD filter\n\nThe library integrates the [Silero VAD](https://github.com/snakers4/silero-vad) model to filter out parts of the audio without speech:\n\n```python\nsegments, _ = model.transcribe("audio.mp3", vad_filter=True)\n```\n\nThe default behavior is conservative and only removes silence longer than 2 seconds. See the available VAD parameters and default values in the [source code](https://github.com/SYSTRAN/faster-whisper/blob/master/faster_whisper/vad.py). They can be customized with the dictionary argument `vad_parameters`:\n\n```python\nsegments, _ = model.transcribe(\n    "audio.mp3",\n    vad_filter=True,\n    vad_parameters=dict(min_silence_duration_ms=500),\n)\n```\nVad filter is enabled by default for batched transcription.\n\n### Logging\n\nThe library logging level can be configured like this:\n\n```python\nimport logging\n\nlogging.basicConfig()\nlogging.getLogger("faster_whisper").setLevel(logging.DEBUG)\n```\n\n### Going further\n\nSee more model and transcription options in the [`WhisperModel`](https://github.com/SYSTRAN/faster-whisper/blob/master/faster_whisper/transcribe.py) class implementation.\n\n## Community integrations\n\nHere is a non exhaustive list of open-source projects using faster-whisper. Feel free to add your project to the list!\n\n\n* [speaches](https://github.com/speaches-ai/speaches) is an OpenAI compatible server using `faster-whisper`. It''s easily deployable with Docker, works with OpenAI SDKs/CLI, supports streaming, and live transcription.\n* [WhisperX](https://github.com/m-bain/whisperX) is an award-winning Python library that offers speaker diarization and accurate word-level timestamps using wav2vec2 alignment\n* [whisper-ctranslate2](https://github.com/Softcatala/whisper-ctranslate2) is a command line client based on faster-whisper and compatible with the original client from openai/whisper.\n* [whisper-diarize](https://github.com/MahmoudAshraf97/whisper-diarization) is a speaker diarization tool that is based on faster-whisper and NVIDIA NeMo.\n* [whisper-standalone-win](https://github.com/Purfview/whisper-standalone-win) Standalone CLI executables of faster-whisper for Windows, Linux & macOS. \n* [asr-sd-pipeline](https://github.com/hedrergudene/asr-sd-pipeline) provides a scalable, modular, end to end multi-speaker speech to text solution implemented using AzureML pipelines.\n* [Open-Lyrics](https://github.com/zh-plus/Open-Lyrics) is a Python library that transcribes voice files using faster-whisper, and translates/polishes the resulting text into `.lrc` files in the desired language using OpenAI-GPT.\n* [wscribe](https://github.com/geekodour/wscribe) is a flexible transcript generation tool supporting faster-whisper, it can export word level transcript and the exported transcript then can be edited with [wscribe-editor](https://github.com/geekodour/wscribe-editor)\n* [aTrain](https://github.com/BANDAS-Center/aTrain) is a graphical user interface implementation of faster-whisper developed at the BANDAS-Center at the University of Graz for transcription and diarization in Windows ([Windows Store App](https://apps.microsoft.com/detail/atrain/9N15Q44SZNS2)) and Linux.\n* [Whisper-Streaming](https://github.com/ufal/whisper_streaming) implements real-time mode for offline Whisper-like speech-to-text models with faster-whisper as the most recommended back-end. It implements a streaming policy with self-adaptive latency based on the actual source complexity, and demonstrates the state of the art.\n* [WhisperLive](https://github.com/collabora/WhisperLive) is a nearly-live implementation of OpenAI''s Whisper which uses faster-whisper as the backend to transcribe audio in real-time.\n* [Faster-Whisper-Transcriber](https://github.com/BBC-Esq/ctranslate2-faster-whisper-transcriber) is a simple but reliable voice transcriber that provides a user-friendly interface.\n* [Open-dubbing](https://github.com/softcatala/open-dubbing) is open dubbing is an AI dubbing system which uses machine learning models to automatically translate and synchronize audio dialogue into different languages.\n* [Whisper-FastAPI](https://github.com/heimoshuiyu/whisper-fastapi) whisper-fastapi is a very simple script that provides an API backend compatible with OpenAI, HomeAssistant, and Konele (Android voice typing) formats.\n\n## Model conversion\n\nWhen loading a model from its size such as `WhisperModel("large-v3")`, the corresponding CTranslate2 model is automatically downloaded from the [Hugging Face Hub](https://huggingface.co/Systran).\n\nWe also provide a script to convert any Whisper models compatible with the Transformers library. They could be the original OpenAI models or user fine-tuned models.\n\nFor example the command below converts the [original "large-v3" Whisper model](https://huggingface.co/openai/whisper-large-v3) and saves the weights in FP16:\n\n```bash\npip install transformers[torch]>=4.23\n\nct2-transformers-converter --model openai/whisper-large-v3 --output_dir whisper-large-v3-ct2\n--copy_files tokenizer.json preprocessor_config.json --quantization float16\n```\n\n* The option `--model` accepts a model name on the Hub or a path to a model directory.\n* If the option `--copy_files tokenizer.json` is not used, the tokenizer configuration is automatically downloaded when the model is loaded later.\n\nModels can also be converted from the code. See the [conversion API](https://opennmt.net/CTranslate2/python/ctranslate2.converters.TransformersConverter.html).\n\n### Load a converted model\n\n1. Directly load the model from a local directory:\n```python\nmodel = faster_whisper.WhisperModel("whisper-large-v3-ct2")\n```\n\n2. [Upload your model to the Hugging Face Hub](https://huggingface.co/docs/transformers/model_sharing#upload-with-the-web-interface) and load it from its name:\n```python\nmodel = faster_whisper.WhisperModel("username/whisper-large-v3-ct2")\n```\n\n## Comparing performance against other implementations\n\nIf you are comparing the performance against other Whisper implementations, you should make sure to run the comparison with similar settings. In particular:\n\n* Verify that the same transcription options are used, especially the same beam size. For example in openai/whisper, `model.transcribe` uses a default beam size of 1 but here we use a default beam size of 5.\n* Transcription speed is closely affected by the number of words in the transcript, so ensure that other implementations have a similar WER (Word Error Rate) to this one.\n* When running on CPU, make sure to set the same number of threads. Many frameworks will read the environment variable `OMP_NUM_THREADS`, which can be set when running your script:\n\n```bash\nOMP_NUM_THREADS=4 python3 my_script.py\n```\n', '{"language":"Python","stars":19366,"forks":1610,"watchers":19366,"open_issues":294,"topics":["deep-learning","inference","openai","quantization","speech-recognition","speech-to-text","transformer","whisper"],"default_branch":"master","size_kb":40514,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:SYSTRAN:faster-whisper","source_url":"https://github.com/SYSTRAN/faster-whisper"},{"type":"has_code","target_id":"github:SYSTRAN:faster-whisper","source_url":"https://github.com/SYSTRAN/faster-whisper"},{"type":"has_code","target_id":"github:OpenNMT:CTranslate2","source_url":"https://github.com/OpenNMT/CTranslate2"},{"type":"has_code","target_id":"github:openai:whisper","source_url":"https://github.com/openai/whisper"},{"type":"has_code","target_id":"github:openai:whisper","source_url":"https://github.com/openai/whisper"},{"type":"has_code","target_id":"github:openai:whisper","source_url":"https://github.com/openai/whisper"},{"type":"has_code","target_id":"github:ggerganov:whisper.cpp","source_url":"https://github.com/ggerganov/whisper.cpp"},{"type":"has_code","target_id":"github:ggerganov:whisper.cpp","source_url":"https://github.com/ggerganov/whisper.cpp"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:SYSTRAN:faster-whisper","source_url":"https://github.com/SYSTRAN/faster-whisper"},{"type":"has_code","target_id":"github:SYSTRAN:faster-whisper","source_url":"https://github.com/SYSTRAN/faster-whisper"},{"type":"has_code","target_id":"github:PyAV-Org:PyAV","source_url":"https://github.com/PyAV-Org/PyAV"},{"type":"has_code","target_id":"github:Purfview:whisper-standalone-win","source_url":"https://github.com/Purfview/whisper-standalone-win"},{"type":"has_code","target_id":"github:Purfview:whisper-standalone-win","source_url":"https://github.com/Purfview/whisper-standalone-win"},{"type":"has_code","target_id":"github:SYSTRAN:faster-whisper","source_url":"https://github.com/SYSTRAN/faster-whisper"},{"type":"has_code","target_id":"github:SYSTRAN:faster-whisper","source_url":"https://github.com/SYSTRAN/faster-whisper"},{"type":"has_code","target_id":"github:snakers4:silero-vad","source_url":"https://github.com/snakers4/silero-vad"},{"type":"has_code","target_id":"github:SYSTRAN:faster-whisper","source_url":"https://github.com/SYSTRAN/faster-whisper"},{"type":"has_code","target_id":"github:SYSTRAN:faster-whisper","source_url":"https://github.com/SYSTRAN/faster-whisper"},{"type":"has_code","target_id":"github:speaches-ai:speaches","source_url":"https://github.com/speaches-ai/speaches"},{"type":"has_code","target_id":"github:m-bain:whisperX","source_url":"https://github.com/m-bain/whisperX"},{"type":"has_code","target_id":"github:Softcatala:whisper-ctranslate2","source_url":"https://github.com/Softcatala/whisper-ctranslate2"},{"type":"has_code","target_id":"github:MahmoudAshraf97:whisper-diarization","source_url":"https://github.com/MahmoudAshraf97/whisper-diarization"},{"type":"has_code","target_id":"github:Purfview:whisper-standalone-win","source_url":"https://github.com/Purfview/whisper-standalone-win"},{"type":"has_code","target_id":"github:hedrergudene:asr-sd-pipeline","source_url":"https://github.com/hedrergudene/asr-sd-pipeline"},{"type":"has_code","target_id":"github:zh-plus:Open-Lyrics","source_url":"https://github.com/zh-plus/Open-Lyrics"},{"type":"has_code","target_id":"github:geekodour:wscribe","source_url":"https://github.com/geekodour/wscribe"},{"type":"has_code","target_id":"github:geekodour:wscribe-editor","source_url":"https://github.com/geekodour/wscribe-editor"},{"type":"has_code","target_id":"github:BANDAS-Center:aTrain","source_url":"https://github.com/BANDAS-Center/aTrain"},{"type":"has_code","target_id":"github:ufal:whisper_streaming","source_url":"https://github.com/ufal/whisper_streaming"},{"type":"has_code","target_id":"github:collabora:WhisperLive","source_url":"https://github.com/collabora/WhisperLive"},{"type":"has_code","target_id":"github:BBC-Esq:ctranslate2-faster-whisper-transcriber","source_url":"https://github.com/BBC-Esq/ctranslate2-faster-whisper-transcriber"},{"type":"has_code","target_id":"github:softcatala:open-dubbing","source_url":"https://github.com/softcatala/open-dubbing"},{"type":"has_code","target_id":"github:heimoshuiyu:whisper-fastapi","source_url":"https://github.com/heimoshuiyu/whisper-fastapi"}]', NULL, 'MIT', 'approved', 80, 'd320e66d3ef8f69fbf01257efd04ad5d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-SYSTRAN-faster-whisper from https://github.com/SYSTRAN.png
Image converted to WebP: data/images/github-SYSTRAN-faster-whisper.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-ShusenTang-Dive-into-DL-PyTorch', 'github--shusentang--dive-into-dl-pytorch', 'Dive-into-DL-PyTorch', 'ShusenTang', '<div align=center> <img width="500" src="img/cover.png" alt="å°é¢"/> </div> æœ¬é¡¹ç›®å°†ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹ åŸä¹¦ä¸­MXNetä»£ç å®ç°æ”¹ä¸ºPyTorchå®ç°ã€‚åŸä¹¦ä½œè€…ï¼šé˜¿æ–¯é¡¿Â·å¼ ã€ææ²ã€æ‰å¡é‡Œ C. ç«‹é¡¿ã€äºšå†å±±å¤§ J. æ–¯è«æ‹‰ä»¥åŠå…¶ä»–ç¤¾åŒºè´¡çŒ®è€…ï¼ŒGitHubåœ°å€ï¼šhttps://github.com/d2l-ai/d2l-zh æ­¤ä¹¦çš„ä¸­è‹±ç‰ˆæœ¬å­˜åœ¨ä¸€äº›ä¸åŒï¼Œé’ˆå¯¹æ­¤ä¹¦è‹±æ–‡ç‰ˆçš„PyTorché‡æ„å¯å‚è€ƒè¿™ä¸ªé¡¹ç›®ã€‚ There are some differences between the Chinese and English versions of this book. For the PyTorch modifying of the English version, you can refer to this repo. æœ¬ä»“åº“ä¸»è¦åŒ…å«codeå’Œdocsä¸¤ä¸ªæ–‡ä»¶å¤¹ï¼ˆå¤–åŠ ä¸€äº›æ•°æ®å­˜æ”¾åœ¨dataä¸­ï¼‰ã€‚å…¶ä¸­codeæ–‡ä»¶å¤¹å°±æ˜¯æ¯ç« ç›¸å…³jupyter notebookä»£ç ï¼ˆåŸºäºPyTorchï¼‰ï¼›docsæ–‡ä»¶å¤¹å°±æ˜¯markdo...', '["computer-vision","d2l","deep-learning","deep-learning-tutorial","natural-language-processing","pytorch","pytorch-tutorial","jupyter notebook"]', 'other', 19200, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/ShusenTang/Dive-into-DL-PyTorch","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '\n<div align=center>\n<img width="500" src="img/cover.png" alt="å°é¢"/>\n</div>\n\n[æœ¬é¡¹ç›®](https://tangshusen.me/Dive-into-DL-PyTorch)å°†[ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹](http://zh.d2l.ai/)Â åŸä¹¦ä¸­MXNetä»£ç å®ç°æ”¹ä¸ºPyTorchå®ç°ã€‚åŸä¹¦ä½œè€…ï¼šé˜¿æ–¯é¡¿Â·å¼ ã€ææ²ã€æ‰å¡é‡Œ C. ç«‹é¡¿ã€äºšå†å±±å¤§ J. æ–¯è«æ‹‰ä»¥åŠå…¶ä»–ç¤¾åŒºè´¡çŒ®è€…ï¼ŒGitHubåœ°å€ï¼šhttps://github.com/d2l-ai/d2l-zh\n\næ­¤ä¹¦çš„[ä¸­](https://zh.d2l.ai/)[è‹±](https://d2l.ai/)ç‰ˆæœ¬å­˜åœ¨ä¸€äº›ä¸åŒï¼Œé’ˆå¯¹æ­¤ä¹¦è‹±æ–‡ç‰ˆçš„PyTorché‡æ„å¯å‚è€ƒ[è¿™ä¸ªé¡¹ç›®](https://github.com/dsgiitr/d2l-pytorch)ã€‚\nThere are some differences between the [Chinese](https://zh.d2l.ai/) and [English](https://d2l.ai/) versions of this book. For the PyTorch modifying of the English version, you can refer to [this repo](https://github.com/dsgiitr/d2l-pytorch).\n\n\n## ç®€ä»‹\næœ¬ä»“åº“ä¸»è¦åŒ…å«codeå’Œdocsä¸¤ä¸ªæ–‡ä»¶å¤¹ï¼ˆå¤–åŠ ä¸€äº›æ•°æ®å­˜æ”¾åœ¨dataä¸­ï¼‰ã€‚å…¶ä¸­codeæ–‡ä»¶å¤¹å°±æ˜¯æ¯ç« ç›¸å…³jupyter notebookä»£ç ï¼ˆåŸºäºPyTorchï¼‰ï¼›docsæ–‡ä»¶å¤¹å°±æ˜¯markdownæ ¼å¼çš„ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹ä¹¦ä¸­çš„ç›¸å…³å†…å®¹ï¼Œç„¶ååˆ©ç”¨[docsify](https://docsify.js.org/#/zh-cn/)å°†ç½‘é¡µæ–‡æ¡£éƒ¨ç½²åˆ°GitHub Pagesä¸Šï¼Œç”±äºåŸä¹¦ä½¿ç”¨çš„æ˜¯MXNetæ¡†æ¶ï¼Œæ‰€ä»¥docså†…å®¹å¯èƒ½ä¸åŸä¹¦ç•¥æœ‰ä¸åŒï¼Œä½†æ˜¯æ•´ä½“å†…å®¹æ˜¯ä¸€æ ·çš„ã€‚æ¬¢è¿å¯¹æœ¬é¡¹ç›®åšå‡ºè´¡çŒ®æˆ–æå‡ºissueã€‚\n\n## é¢å‘äººç¾¤\næœ¬é¡¹ç›®é¢å‘å¯¹æ·±åº¦å­¦ä¹ æ„Ÿå…´è¶£ï¼Œå°¤å…¶æ˜¯æƒ³ä½¿ç”¨PyTorchè¿›è¡Œæ·±åº¦å­¦ä¹ çš„ç«¥é‹ã€‚æœ¬é¡¹ç›®å¹¶ä¸è¦æ±‚ä½ æœ‰ä»»ä½•æ·±åº¦å­¦ä¹ æˆ–è€…æœºå™¨å­¦ä¹ çš„èƒŒæ™¯çŸ¥è¯†ï¼Œä½ åªéœ€äº†è§£åŸºç¡€çš„æ•°å­¦å’Œç¼–ç¨‹ï¼Œå¦‚åŸºç¡€çš„çº¿æ€§ä»£æ•°ã€å¾®åˆ†å’Œæ¦‚ç‡ï¼Œä»¥åŠåŸºç¡€çš„Pythonç¼–ç¨‹ã€‚\n\n## é£Ÿç”¨æ–¹æ³• \n### æ–¹æ³•ä¸€\næœ¬ä»“åº“åŒ…å«ä¸€äº›latexå…¬å¼ï¼Œä½†githubçš„markdownåŸç”Ÿæ˜¯ä¸æ”¯æŒå…¬å¼æ˜¾ç¤ºçš„ï¼Œè€Œdocsæ–‡ä»¶å¤¹å·²ç»åˆ©ç”¨[docsify](https://docsify.js.org/#/zh-cn/)è¢«éƒ¨ç½²åˆ°äº†GitHub Pagesä¸Šï¼Œæ‰€ä»¥æŸ¥çœ‹æ–‡æ¡£æœ€ç®€ä¾¿çš„æ–¹æ³•å°±æ˜¯ç›´æ¥è®¿é—®[æœ¬é¡¹ç›®ç½‘é¡µç‰ˆ](https://tangshusen.me/Dive-into-DL-PyTorch)ã€‚å½“ç„¶å¦‚æœä½ è¿˜æƒ³è·‘ä¸€ä¸‹è¿è¡Œç›¸å…³ä»£ç çš„è¯è¿˜æ˜¯å¾—æŠŠæœ¬é¡¹ç›®cloneä¸‹æ¥ï¼Œç„¶åè¿è¡Œcodeæ–‡ä»¶å¤¹ä¸‹ç›¸å…³ä»£ç ã€‚\n\n### æ–¹æ³•äºŒ\nä½ è¿˜å¯ä»¥åœ¨æœ¬åœ°è®¿é—®æ–‡æ¡£ï¼Œå…ˆå®‰è£…`docsify-cli`å·¥å…·:\n``` shell\nnpm i docsify-cli -g\n```\nç„¶åå°†æœ¬é¡¹ç›®cloneåˆ°æœ¬åœ°:\n``` shell\ngit clone https://github.com/ShusenTang/Dive-into-DL-PyTorch.git\ncd Dive-into-DL-PyTorch\n```\nç„¶åè¿è¡Œä¸€ä¸ªæœ¬åœ°æœåŠ¡å™¨ï¼Œè¿™æ ·å°±å¯ä»¥å¾ˆæ–¹ä¾¿çš„åœ¨`http://localhost:3000`å®æ—¶è®¿é—®æ–‡æ¡£ç½‘é¡µæ¸²æŸ“æ•ˆæœã€‚\n``` shell\ndocsify serve docs\n```\n\n### æ–¹æ³•ä¸‰\nå¦‚æœä½ ä¸æƒ³å®‰è£…`docsify-cli`å·¥å…·ï¼Œç”šè‡³ä½ çš„ç”µè„‘ä¸Šéƒ½æ²¡æœ‰å®‰è£…`Node.js`ï¼Œè€Œå‡ºäºæŸäº›åŸå› ä½ åˆæƒ³åœ¨æœ¬åœ°æµè§ˆæ–‡æ¡£ï¼Œé‚£ä¹ˆä½ å¯ä»¥åœ¨`docker`å®¹å™¨ä¸­è¿è¡Œç½‘é¡µæœåŠ¡ã€‚\n\né¦–å…ˆå°†æœ¬é¡¹ç›®cloneåˆ°æœ¬åœ°:\n``` shell\ngit clone https://github.com/ShusenTang/Dive-into-DL-PyTorch.git\ncd Dive-into-DL-PyTorch\n```\nä¹‹åä½¿ç”¨å¦‚ä¸‹å‘½ä»¤åˆ›å»ºä¸€ä¸ªåç§°ä¸ºã€Œd2dlã€çš„`docker`é•œåƒï¼š\n``` shell\ndocker build -t d2dl .\n```\né•œåƒåˆ›å»ºå¥½åï¼Œè¿è¡Œå¦‚ä¸‹å‘½ä»¤åˆ›å»ºä¸€ä¸ªæ–°çš„å®¹å™¨ï¼š\n``` shell\ndocker run -dp 3000:3000 d2dl\n```\næœ€ååœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¿™ä¸ªåœ°å€`http://localhost:3000/#/`ï¼Œå°±èƒ½æ„‰å¿«åœ°è®¿é—®æ–‡æ¡£äº†ã€‚é€‚åˆé‚£äº›ä¸æƒ³åœ¨ç”µè„‘ä¸Šè£…å¤ªå¤šå·¥å…·çš„å°ä¼™ä¼´ã€‚\n\n\n## ç›®å½•\n* [ç®€ä»‹]()\n* [é˜…è¯»æŒ‡å—](read_guide.md)\n* [1. æ·±åº¦å­¦ä¹ ç®€ä»‹](chapter01_DL-intro/deep-learning-intro.md)\n* 2\. é¢„å¤‡çŸ¥è¯†\n   * [2.1 ç¯å¢ƒé…ç½®](chapter02_prerequisite/2.1_install.md)\n   * [2.2 æ•°æ®æ“ä½œ](chapter02_prerequisite/2.2_tensor.md)\n   * [2.3 è‡ªåŠ¨æ±‚æ¢¯åº¦](chapter02_prerequisite/2.3_autograd.md)\n* 3\. æ·±åº¦å­¦ä¹ åŸºç¡€\n   * [3.1 çº¿æ€§å›å½’](chapter03_DL-basics/3.1_linear-regression.md)\n   * [3.2 çº¿æ€§å›å½’çš„ä»é›¶å¼€å§‹å®ç°](chapter03_DL-basics/3.2_linear-regression-scratch.md)\n   * [3.3 çº¿æ€§å›å½’çš„ç®€æ´å®ç°](chapter03_DL-basics/3.3_linear-regression-pytorch.md)\n   * [3.4 softmaxå›å½’](chapter03_DL-basics/3.4_softmax-regression.md)\n   * [3.5 å›¾åƒåˆ†ç±»æ•°æ®é›†ï¼ˆFashion-MNISTï¼‰](chapter03_DL-basics/3.5_fashion-mnist.md)\n   * [3.6 softmaxå›å½’çš„ä»é›¶å¼€å§‹å®ç°](chapter03_DL-basics/3.6_softmax-regression-scratch.md)\n   * [3.7 softmaxå›å½’çš„ç®€æ´å®ç°](chapter03_DL-basics/3.7_softmax-regression-pytorch.md)\n   * [3.8 å¤šå±‚æ„ŸçŸ¥æœº](chapter03_DL-basics/3.8_mlp.md)\n   * [3.9 å¤šå±‚æ„ŸçŸ¥æœºçš„ä»é›¶å¼€å§‹å®ç°](chapter03_DL-basics/3.9_mlp-scratch.md)\n   * [3.10 å¤šå±‚æ„ŸçŸ¥æœºçš„ç®€æ´å®ç°](chapter03_DL-basics/3.10_mlp-pytorch.md)\n   * [3.11 æ¨¡å‹é€‰æ‹©ã€æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆ](chapter03_DL-basics/3.11_underfit-overfit.md)\n   * [3.12 æƒé‡è¡°å‡](chapter03_DL-basics/3.12_weight-decay.md)\n   * [3.13 ä¸¢å¼ƒæ³•](chapter03_DL-basics/3.13_dropout.md)\n   * [3.14 æ­£å‘ä¼ æ’­ã€åå‘ä¼ æ’­å’Œè®¡ç®—å›¾](chapter03_DL-basics/3.14_backprop.md)\n   * [3.15 æ•°å€¼ç¨³å®šæ€§å’Œæ¨¡å‹åˆå§‹åŒ–](chapter03_DL-basics/3.15_numerical-stability-and-init.md)\n   * [3.16 å®æˆ˜Kaggleæ¯”èµ›ï¼šæˆ¿ä»·é¢„æµ‹](chapter03_DL-basics/3.16_kaggle-house-price.md)\n* 4\. æ·±åº¦å­¦ä¹ è®¡ç®—\n   * [4.1 æ¨¡å‹æ„é€ ](chapter04_DL_computation/4.1_model-construction.md)\n   * [4.2 æ¨¡å‹å‚æ•°çš„è®¿é—®ã€åˆå§‹åŒ–å’Œå…±äº«](chapter04_DL_computation/4.2_parameters.md)\n   * [4.3 æ¨¡å‹å‚æ•°çš„å»¶ååˆå§‹åŒ–](chapter04_DL_computation/4.3_deferred-init.md)\n   * [4.4 è‡ªå®šä¹‰å±‚](chapter04_DL_computation/4.4_custom-layer.md)\n   * [4.5 è¯»å–å’Œå­˜å‚¨](chapter04_DL_computation/4.5_read-write.md)\n   * [4.6 GPUè®¡ç®—](chapter04_DL_computation/4.6_use-gpu.md)\n* 5\. å·ç§¯ç¥ç»ç½‘ç»œ\n   * [5.1 äºŒç»´å·ç§¯å±‚](chapter05_CNN/5.1_conv-layer.md)\n   * [5.2 å¡«å……å’Œæ­¥å¹…](chapter05_CNN/5.2_padding-and-strides.md)\n   * [5.3 å¤šè¾“å…¥é€šé“å’Œå¤šè¾“å‡ºé€šé“](chapter05_CNN/5.3_channels.md)\n   * [5.4 æ± åŒ–å±‚](chapter05_CNN/5.4_pooling.md)\n   * [5.5 å·ç§¯ç¥ç»ç½‘ç»œï¼ˆLeNetï¼‰](chapter05_CNN/5.5_lenet.md)\n   * [5.6 æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆAlexNetï¼‰](chapter05_CNN/5.6_alexnet.md)\n   * [5.7 ä½¿ç”¨é‡å¤å…ƒç´ çš„ç½‘ç»œï¼ˆVGGï¼‰](chapter05_CNN/5.7_vgg.md)\n   * [5.8 ç½‘ç»œä¸­çš„ç½‘ç»œï¼ˆNiNï¼‰](chapter05_CNN/5.8_nin.md)\n   * [5.9 å«å¹¶è¡Œè¿ç»“çš„ç½‘ç»œï¼ˆGoogLeNetï¼‰](chapter05_CNN/5.9_googlenet.md)\n   * [5.10 æ‰¹é‡å½’ä¸€åŒ–](chapter05_CNN/5.10_batch-norm.md)\n   * [5.11 æ®‹å·®ç½‘ç»œï¼ˆResNetï¼‰](chapter05_CNN/5.11_resnet.md)\n   * [5.12 ç¨ å¯†è¿æ¥ç½‘ç»œï¼ˆDenseNetï¼‰](chapter05_CNN/5.12_densenet.md)\n* 6\. å¾ªç¯ç¥ç»ç½‘ç»œ\n   * [6.1 è¯­è¨€æ¨¡å‹](chapter06_RNN/6.1_lang-model.md)\n   * [6.2 å¾ªç¯ç¥ç»ç½‘ç»œ](chapter06_RNN/6.2_rnn.md)\n   * [6.3 è¯­è¨€æ¨¡å‹æ•°æ®é›†ï¼ˆå‘¨æ°ä¼¦ä¸“è¾‘æ­Œè¯ï¼‰](chapter06_RNN/6.3_lang-model-dataset.md)\n   * [6.4 å¾ªç¯ç¥ç»ç½‘ç»œçš„ä»é›¶å¼€å§‹å®ç°](chapter06_RNN/6.4_rnn-scratch.md)\n   * [6.5 å¾ªç¯ç¥ç»ç½‘ç»œçš„ç®€æ´å®ç°](chapter06_RNN/6.5_rnn-pytorch.md)\n   * [6.6 é€šè¿‡æ—¶é—´åå‘ä¼ æ’­](chapter06_RNN/6.6_bptt.md)\n   * [6.7 é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰](chapter06_RNN/6.7_gru.md)\n   * [6.8 é•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰](chapter06_RNN/6.8_lstm.md)\n   * [6.9 æ·±åº¦å¾ªç¯ç¥ç»ç½‘ç»œ](chapter06_RNN/6.9_deep-rnn.md)\n   * [6.10 åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ](chapter06_RNN/6.10_bi-rnn.md)\n* 7\. ä¼˜åŒ–ç®—æ³•\n   * [7.1 ä¼˜åŒ–ä¸æ·±åº¦å­¦ä¹ ](chapter07_optimization/7.1_optimization-intro.md)\n   * [7.2 æ¢¯åº¦ä¸‹é™å’Œéšæœºæ¢¯åº¦ä¸‹é™](chapter07_optimization/7.2_gd-sgd.md)\n   * [7.3 å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™](chapter07_optimization/7.3_minibatch-sgd.md)\n   * [7.4 åŠ¨é‡æ³•](chapter07_optimization/7.4_momentum.md)\n   * [7.5 AdaGradç®—æ³•](chapter07_optimization/7.5_adagrad.md)\n   * [7.6 RMSPropç®—æ³•](chapter07_optimization/7.6_rmsprop.md)\n   * [7.7 AdaDeltaç®—æ³•](chapter07_optimization/7.7_adadelta.md)\n   * [7.8 Adamç®—æ³•](chapter07_optimization/7.8_adam.md)\n* 8\. è®¡ç®—æ€§èƒ½\n   * [8.1 å‘½ä»¤å¼å’Œç¬¦å·å¼æ··åˆç¼–ç¨‹](chapter08_computational-performance/8.1_hybridize.md)\n   * [8.2 å¼‚æ­¥è®¡ç®—](chapter08_computational-performance/8.2_async-computation.md)\n   * [8.3 è‡ªåŠ¨å¹¶è¡Œè®¡ç®—](chapter08_computational-performance/8.3_auto-parallelism.md)\n   * [8.4 å¤šGPUè®¡ç®—](chapter08_computational-performance/8.4_multiple-gpus.md)\n* 9\. è®¡ç®—æœºè§†è§‰\n   * [9.1 å›¾åƒå¢å¹¿](chapter09_computer-vision/9.1_image-augmentation.md)\n   * [9.2 å¾®è°ƒ](chapter09_computer-vision/9.2_fine-tuning.md)\n   * [9.3 ç›®æ ‡æ£€æµ‹å’Œè¾¹ç•Œæ¡†](chapter09_computer-vision/9.3_bounding-box.md)\n   * [9.4 é”šæ¡†](chapter09_computer-vision/9.4_anchor.md)\n   * [9.5 å¤šå°ºåº¦ç›®æ ‡æ£€æµ‹](chapter09_computer-vision/9.5_multiscale-object-detection.md)\n   * [9.6 ç›®æ ‡æ£€æµ‹æ•°æ®é›†ï¼ˆçš®å¡ä¸˜ï¼‰](chapter09_computer-vision/9.6_object-detection-dataset.md)\n   - [ ] 9.7 å•å‘å¤šæ¡†æ£€æµ‹ï¼ˆSSDï¼‰\n   * [9.8 åŒºåŸŸå·ç§¯ç¥ç»ç½‘ç»œï¼ˆR-CNNï¼‰ç³»åˆ—](chapter09_computer-vision/9.8_rcnn.md)\n   * [9.9 è¯­ä¹‰åˆ†å‰²å’Œæ•°æ®é›†](chapter09_computer-vision/9.9_semantic-segmentation-and-dataset.md)\n   - [ ] 9.10 å…¨å·ç§¯ç½‘ç»œï¼ˆFCNï¼‰\n   * [9.11 æ ·å¼è¿ç§»](chapter09_computer-vision/9.11_neural-style.md)\n   - [ ] 9.12 å®æˆ˜Kaggleæ¯”èµ›ï¼šå›¾åƒåˆ†ç±»ï¼ˆCIFAR-10ï¼‰\n   - [ ] 9.13 å®æˆ˜Kaggleæ¯”èµ›ï¼šç‹—çš„å“ç§è¯†åˆ«ï¼ˆImageNet Dogsï¼‰\n* 10\. è‡ªç„¶è¯­è¨€å¤„ç†\n   * [10.1 è¯åµŒå…¥ï¼ˆword2vecï¼‰](chapter10_natural-language-processing/10.1_word2vec.md)\n   * [10.2 è¿‘ä¼¼è®­ç»ƒ](chapter10_natural-language-processing/10.2_approx-training.md)\n   * [10.3 word2vecçš„å®ç°](chapter10_natural-language-processing/10.3_word2vec-pytorch.md)\n   * [10.4 å­è¯åµŒå…¥ï¼ˆfastTextï¼‰](chapter10_natural-language-processing/10.4_fasttext.md)\n   * [10.5 å…¨å±€å‘é‡çš„è¯åµŒå…¥ï¼ˆGloVeï¼‰](chapter10_natural-language-processing/10.5_glove.md)\n   * [10.6 æ±‚è¿‘ä¹‰è¯å’Œç±»æ¯”è¯](chapter10_natural-language-processing/10.6_similarity-analogy.md)\n   * [10.7 æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ï¼šä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œ](chapter10_natural-language-processing/10.7_sentiment-analysis-rnn.md)\n   * [10.8 æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ï¼šä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆtextCNNï¼‰](chapter10_natural-language-processing/10.8_sentiment-analysis-cnn.md)\n   * [10.9 ç¼–ç å™¨â€”è§£ç å™¨ï¼ˆseq2seqï¼‰](chapter10_natural-language-processing/10.9_seq2seq.md)\n   * [10.10 æŸæœç´¢](chapter10_natural-language-processing/10.10_beam-search.md)\n   * [10.11 æ³¨æ„åŠ›æœºåˆ¶](chapter10_natural-language-processing/10.11_attention.md)\n   * [10.12 æœºå™¨ç¿»è¯‘](chapter10_natural-language-processing/10.12_machine-translation.md)\n\n\n\næŒç»­æ›´æ–°ä¸­......\n\n\n\n\n## åŸä¹¦åœ°å€\nä¸­æ–‡ç‰ˆï¼š[åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ](https://zh.d2l.ai/) | [Githubä»“åº“](https://github.com/d2l-ai/d2l-zh)       \nEnglish Version: [Dive into Deep Learning](https://d2l.ai/) | [Github Repo](https://github.com/d2l-ai/d2l-en)\n\n\n## å¼•ç”¨\nå¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†è¿™ä¸ªé¡¹ç›®è¯·å¼•ç”¨åŸä¹¦:\n```\n@book{zhang2019dive,\n    title={Dive into Deep Learning},\n    author={Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola},\n    note={\url{http://www.d2l.ai}},\n    year={2020}\n}\n```\n', '{"language":"Jupyter Notebook","stars":19200,"forks":5432,"watchers":19200,"open_issues":78,"topics":["computer-vision","d2l","deep-learning","deep-learning-tutorial","natural-language-processing","pytorch","pytorch-tutorial"],"default_branch":"master","size_kb":35157,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:d2l-ai:d2l-zh","source_url":"https://github.com/d2l-ai/d2l-zh"},{"type":"has_code","target_id":"github:dsgiitr:d2l-pytorch","source_url":"https://github.com/dsgiitr/d2l-pytorch"},{"type":"has_code","target_id":"github:dsgiitr:d2l-pytorch","source_url":"https://github.com/dsgiitr/d2l-pytorch"},{"type":"has_code","target_id":"github:ShusenTang:Dive-into-DL-PyTorch.git","source_url":"https://github.com/ShusenTang/Dive-into-DL-PyTorch.git"},{"type":"has_code","target_id":"github:ShusenTang:Dive-into-DL-PyTorch.git","source_url":"https://github.com/ShusenTang/Dive-into-DL-PyTorch.git"},{"type":"has_code","target_id":"github:d2l-ai:d2l-zh","source_url":"https://github.com/d2l-ai/d2l-zh"},{"type":"has_code","target_id":"github:d2l-ai:d2l-en","source_url":"https://github.com/d2l-ai/d2l-en"}]', NULL, 'Apache-2.0', 'approved', 65, 'c8e3194c3e886650a4cb348193680386', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-ShusenTang-Dive-into-DL-PyTorch from https://github.com/ShusenTang.png
Image converted to WebP: data/images/github-ShusenTang-Dive-into-DL-PyTorch.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-microsoft-CNTK', 'github--microsoft--cntk', 'CNTK', 'microsoft', '| **Chat** | **Windows build status** | **Linux build status** | |-------------|-------------|---------------| | | | | The Microsoft Cognitive Toolkit (https://cntk.ai) is a unified deep learning toolkit that describes neural networks as a series of computational steps via a directed graph. In this directed graph, leaf nodes represent input values or network parameters, while other nodes represent matrix operations upon their inputs. CNTK allows users to easily realize and combine popular mod...', '["c-plus-plus","c-sharp","cntk","cognitive-toolkit","deep-learning","deep-neural-networks","distributed","java","machine-learning","neural-network","python","c++"]', 'other', 17601, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/microsoft/CNTK","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '## CNTK\n\n| **Chat** | **Windows build status** | **Linux build status** |\n|-------------|-------------|---------------|\n| [![Join the chat at https://gitter.im/Microsoft/CNTK](https://badges.gitter.im/Microsoft/CNTK.svg)](https://gitter.im/Microsoft/CNTK?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) | [![Build Status](https://aiinfra.visualstudio.com/_apis/public/build/definitions/a95b3960-90bb-440b-bd18-d3ec5d1cf8c3/126/badge)](https://cntk.ai/nightly-windows.html) | [![Build Status](https://aiinfra.visualstudio.com/_apis/public/build/definitions/a95b3960-90bb-440b-bd18-d3ec5d1cf8c3/127/badge)](https://cntk.ai/nightly-linux.html) |\n\nThe Microsoft Cognitive Toolkit (https://cntk.ai) is a unified deep learning toolkit that describes neural networks as a series of computational steps via a directed graph. In this directed graph, leaf nodes represent input values or network parameters, while other nodes represent matrix operations upon their inputs. CNTK allows users to easily realize and combine popular model types such as feed-forward DNNs, convolutional nets (CNNs), and recurrent networks (RNNs/LSTMs). It implements stochastic gradient descent (SGD, error backpropagation) learning with automatic differentiation and parallelization across multiple GPUs and servers. CNTK has been available under an open-source license since April 2015. It is our hope that the community will take advantage of CNTK to share ideas more quickly through the exchange of open source working code.\n\n## Installation\n\n* [Setup CNTK](https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-CNTK-on-your-machine)\n    * Windows ([Python-only](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-windows-python) / [Script-driven](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-windows-binary-script) / [Manual](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-windows-binary-manual))\n    * Linux ([Python-only](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-linux-python) / [Script-driven](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-linux-binary-script) / [Manual](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-linux-binary-manual) / [Docker](https://docs.microsoft.com/en-us/cognitive-toolkit/cntk-docker-containers))\n* [CNTK backend for Keras](https://docs.microsoft.com/en-us/cognitive-toolkit/using-cntk-with-keras)\n* [Setup CNTK development environment](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-development-environment)\n    * Windows ([Script-driven](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-cntk-with-script-on-windows) / [Manual](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-cntk-on-windows))\n    * Linux ([Manual](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-cntk-on-linux))\n    \n### Installing nightly packages\n\nIf you prefer to use latest CNTK bits from master, use one of the CNTK nightly packages:\n\n* [Nightly packages for Windows](https://cntk.ai/nightly-windows.html)\n* [Nightly packages for Linux](https://cntk.ai/nightly-linux.html)\n\n## Learning CNTK\n\nYou can learn more about using and contributing to CNTK with the following resources:\n\n* [General documentation](https://docs.microsoft.com/en-us/cognitive-toolkit/)\n* [Python API documentation](https://cntk.ai/pythondocs/)\n* [Evaluation documentation (C++, C#/.NET, Python, Java)](https://docs.microsoft.com/en-us/cognitive-toolkit/CNTK-Evaluation-Overview)\n* [Manual](https://github.com/Microsoft/CNTK/tree/master/Manual)\n* [Tutorials](https://docs.microsoft.com/en-us/cognitive-toolkit/tutorials)\n* [Examples](https://docs.microsoft.com/en-us/cognitive-toolkit/Examples)\n* [Pretrained models](./PretrainedModels)\n* [Blog](https://www.microsoft.com/en-us/cognitive-toolkit/blog/)\n* [Presentations](https://docs.microsoft.com/en-us/cognitive-toolkit/Presentations)\n* [License](./LICENSE.md)\n\n## More information\n\n* [Contribute to CNTK](https://docs.microsoft.com/en-us/cognitive-toolkit/Contributing-to-CNTK)\n* [FAQ](https://docs.microsoft.com/en-us/cognitive-toolkit/CNTK-FAQ)\n* [Feedback](https://docs.microsoft.com/en-us/cognitive-toolkit/Feedback-Channels)\n\n## Disclaimer\n\nDear community, \n\nWith our ongoing contributions to ONNX and the ONNX Runtime, we have made it easier to interoperate within the AI framework ecosystem and to access high performance, cross-platform inferencing capabilities for both traditional ML models and deep neural networks. Over the last few years we have been privileged to develop such key open-source machine learning projects, including the Microsoft Cognitive Toolkit, which has enabled its users to leverage industry-wide advancements in deep learning at scale. \n\nTodayâ€™s 2.7 release will be the last main release of CNTK. We may have some subsequent minor releases for bug fixes, but these will be evaluated on a case-by-case basis. There are no plans for new feature development post this release. \n\nThe CNTK 2.7 release has full support for ONNX 1.4.1, and we encourage those seeking to operationalize their CNTK models to take advantage of ONNX and the ONNX Runtime. Moving forward, users can continue to leverage evolving ONNX innovations via the number of frameworks that support it. For example, users can natively export ONNX models from PyTorch or convert TensorFlow models to ONNX with the TensorFlow-ONNX converter. \n\nWe are incredibly grateful for all the support we have received from contributors and users over the years since the initial open-source release of CNTK. CNTK has enabled both Microsoft teams and external users to execute complex and large-scale workloads in all manner of deep learning applications, such as historical breakthroughs in speech recognition achieved by Microsoft Speech researchers, the originators of the framework. \n\nAs ONNX is increasingly employed in serving models used across Microsoft products such as Bing and Office, we are dedicated to synthesizing innovations from research with the rigorous demands of production to progress the ecosystem forward. \n\nAbove all, our goal is to make innovations in deep learning across the software and hardware stacks as open and accessible as possible. We will be working hard to bring both the existing strengths of CNTK and new state-of-the-art research into other open-source projects to truly broaden the reach of such technologies. \n\nWith gratitude, \n\n-- The CNTK Team \n\n## Microsoft Open Source Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## News\n\n> You can find more news on [the official project feed](https://docs.microsoft.com/en-us/cognitive-toolkit/news)\n\n***2019-03-29.*** CNTK 2.7.0\n## Highlights of this release\n* Moved to CUDA 10 for both Windows and Linux.\n* Support advance RNN loop in ONNX export.\n* Export larger than 2GB models in ONNX format.\n* Support FP16 in Brain Script train action.\n\n## CNTK support for CUDA 10\n\n### CNTK now supports CUDA 10. This requires an update to build environment to Visual Studio 2017 v15.9 for Windows.\n\nTo setup build and runtime environment on Windows:\n* Install [Visual Studio 2017](https://www.visualstudio.com/downloads/). Note: going forward for CUDA 10 and beyond, it is no longer required to install and run with the specific VC Tools version 14.11.\n* Install [Nvidia CUDA 10](https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64)\n* From PowerShell, run:\n    [DevInstall.ps1](../Tools/devInstall/Windows/DevInstall.ps1)\n* Start Visual Studio 2017 and open [CNTK.sln](./CNTK.sln).\n\nTo setup build and runtime environment on Linux using docker, please build Unbuntu 16.04 docker image using Dockerfiles [here](./Tools/docker). For other Linux systems, please refer to the Dockerfiles to setup dependent libraries for CNTK.\n\n## Support advance RNN loop in ONNX export\nCNTK models with recursive loops can be exported to ONNX models with scan ops.\n\n## Export larger than 2GB models in ONNX format\nTo export models larger than 2GB in ONNX format, use cntk.Function API:\nsave(self, filename, format=ModelFormat.CNTKv2, use_external_files_to_store_parameters=False)\nwith ''format'' set to ModelFormat.ONNX and use_external_files_to_store_parameters set to True.\nIn this case, model parameters are saved in external files. Exported models shall be used with external parameter files when doing model evaluation with onnxruntime.\n\n***2018-11-26.***  \n[Netron](https://github.com/lutzroeder/netron) now supports visualizing CNTK v1 and CNTK v2 `.model` files.\n\n<img src=https://cntk.ai/Images/netron/netron-cntk-dark-1.png alt="NetronCNTKDark1" width="300"> <img src=https://cntk.ai/Images/netron/netron-cntk-light-1.png alt="NetronCNTKLight1" width="300">\n\n\n### Project changelog\n\n***2018-09-17.*** CNTK 2.6.0\n## Efficient group convolution\nThe implementation of group convolution in CNTK has been updated. The updated implementation moves away from creating a sub-graph for group convolution (using slicing and splicing), and instead uses cuDNN7 and MKL2017 APIs directly. This improves the experience both in terms of performance and model size. \n\nAs an example, for a single group convolution op with the following attributes:\n\n- Input tensor (C, H, W) = (32, 128, 128)\n- Number of output channels = 32 (channel multiplier is 1)\n- Groups = 32 (depth wise convolution)\n- Kernel size = (5, 5)\n\nThe comparison numbers for this single node are as follows:\n\n| First Header  | GPU exec. time (in millisec., 1000 run avg.) | CPU exec. time (in millisec., 1000 run avg.) | Model Size (in KB, CNTK format)\n| ------------- | ------------- | ------------- | ------------- |\n| Old implementation  | 9.349  | 41.921  | 38  |\n| New implementation  | 6.581  | 9.963  | 5  |\n| Speedup/savings   Approx.  | 30%  Approx.  | 65-75%   Approx.  | 87% |\n\n## Sequential Convolution\nThe implementation of sequential convolution in CNTK has been updated. The updated implementation creates a separate sequential convolution layer. Different from regular convolution layer, this operation convolves also on the dynamic axis(sequence), and filter_shape[0] is applied to that axis. The updated implementation supports broader cases, such as where stride > 1 for the sequence axis.\n\nFor example, a sequential convolution over a batch of one-channel black-and-white images. The images have the same fixed height of 640, but each with width of variable lengths. The width is then represented by sequential axis. Padding is enabled, and strides for both width and height are 2.\n\n     >>> f = SequentialConvolution((3,3), reduction_rank=0, pad=True, strides=(2,2), activation=C.relu)\n     >>> x = C.input_variable(**Sequence[Tensor[640]])\n     >>> x.shape\n         (640,)\n     >>> h = f(x)\n     >>> h.shape\n         (320,)\n     >>> f.W.shape\n         (1, 1, 3, 3)\n\n## Operators\n### depth_to_space and space_to_depth\nThere is a breaking change in the **depth_to_space** and **space_to_depth** operators. These have been updated to match ONNX specification, specifically\nthe permutation for how the depth dimension is placed as blocks in the spatial dimensions, and vice-versa, has been changed. Please refer to the updated doc\nexamples for these two ops to see the change.\n\n### Tan and Atan\nAdded support for trigonometric ops `Tan` and `Atan`.\n\n### ELU\nAdded support for `alpha` attribute in ELU op.\n\n### Convolution\nUpdated auto padding algorithms of `Convolution` to produce symmetric padding at best effort on CPU, without affecting the final convolution output values. This update increases the range of cases that could be covered by MKL API and improves the performance, E.g. ResNet50.\n\n## Default arguments order\nThere is a breaking change in the **arguments** property in CNTK python API. The default behavior has been updated to return arguments in python order instead of in C++ order. This way it will return arguments in the same order as they are fed into ops. If you wish to still get arguments in C++ order, you can simply override the global option. This change should only affect the following ops: Times, TransposeTimes, and Gemm(internal). \n\n## Bug fixes\n- Updated doc for Convolution layer to include group and dilation arguments.\n- Added improved input validation for group convolution.\n- Updated `LogSoftMax` to use more numerically stable implementation.\n- Fixed Gather op''s incorrect gradient value.\n- Added validation for ''None'' node in python clone substitution.\n- Added validation for padding channel axis in convolution.\n- Added CNTK native default lotusIR logger to fix the "Attempt to use DefaultLogger" error when loading some ONNX models.\n- Added proper initialization for ONNX TypeStrToProtoMap.\n- Updated python doctest to handle different print format for newer version numpy(version >= 1.14).\n- Fixed Pooling(CPU) to produce correct output values when kernel center is on padded input cells.\n\n## ONNX\n### Updates\n- Updated CNTK''s ONNX import/export to use ONNX 1.2 spec.\n- Major update to how batch and sequence axes are handled in export and import. As a result, the complex scenarios and edge cases are handled accurately.\n- Updated CNTK''s ONNX `BatchNormalization` op export/import to latest spec.\n- Added model domain to ONNX model export.\n- Improved error reporting during import and export of ONNX models.\n- Updated `DepthToSpace` and `SpaceToDepth` ops to match ONNX spec on the permutation for how the depth dimension is placed as block dimension.\n- Added support for exporting `alpha` attribute in `ELU` ONNX op.\n- Major overhaul to `Convolution` and `Pooling` export. Unlike before, these ops do not export an explicit `Pad` op in any situation.\n- Major overhaul to `ConvolutionTranspose` export and import. Attributes such as `output_shape`, `output_padding`, and `pads` are fully supported.\n- Added support for CNTK''s `StopGradient` as a no-op.\n- Added ONNX support for TopK op.\n- Added ONNX support for sequence ops: sequence.slice, sequence.first, sequence.last, sequence.reduce_sum, sequence.reduce_max, sequence.softmax. For these ops, there is no need to expand ONNX spec. CNTK ONNX exporter just builds computation equivalent graphs for these sequence ops.\n- Added full support for Softmax op.\n- Made CNTK broadcast ops compatible with ONNX specification.\n- Handle to_batch, to_sequence, unpack_batch, sequence.unpack ops in CNTK ONNX exporter.\n- ONNX tests to export ONNX test cases for other toolkits to run and to validate.\n- Fixed `Hardmax`/`Softmax`/`LogSoftmax` import/export.\n- Added support for `Select` op export.\n- Added import/export support for several trigonometric ops.\n- Updated CNTK support for ONNX `MatMul` op.\n- Updated CNTK support for ONNX `Gemm` op.\n- Updated CNTK''s ONNX `MeanVarianceNormalization` op export/import to latest spec.\n- Updated CNTK''s ONNX `LayerNormalization` op export/import to latest spec.\n- Updated CNTK''s ONNX `PRelu` op export/import to latest spec.\n- Updated CNTK''s ONNX `Gather` op export/import to latest spec.\n- Updated CNTK''s ONNX `ImageScaler` op export/import to latest spec.\n- Updated CNTK''s ONNX `Reduce` ops export/import to latest spec.\n- Updated CNTK''s ONNX `Flatten` op export/import to latest spec.\n- Added CNTK support for ONNX `Unsqueeze` op.\n\n### Bug or minor fixes:\n- Updated LRN op to match ONNX 1.2 spec where the `size` attribute has the semantics of diameter, not radius. Added validation if LRN kernel size is larger than channel size.\n- Updated `Min`/`Max` import implementation to handle variadic inputs.\n- Fixed possible file corruption when resaving on top of existing ONNX model file.\n\n## .Net Support\nThe Cntk.Core.Managed library has officially been converted to .Net Standard and supports .Net Core and .Net Framework applications on both Windows and Linux. Starting from this release, .Net developers should be able to restore CNTK Nuget packages using new .Net SDK style project file with package management format set to PackageReference.\n\nThe following C# code now works on both Windows and Linux:\n\n     >>> var weightParameterName = "weight";\n	 >>> var biasParameterName = "bias";\n	 >>> var inputName = "input";\n	 >>> var outputDim = 2;\n	 >>> var inputDim = 3;\n	 >>> Variable inputVariable = Variable.InputVariable(new int[] { inputDim }, DataType.Float, inputName);\n	 >>> var weightParameter = new Parameter(new int[] { outputDim, inputDim }, DataType.Float, 1, device, weightParameterName);\n	 >>> var biasParameter = new Parameter(new int[] { outputDim }, DataType.Float, 0, device, biasParameterName);\n	 >>> \n     >>> Function modelFunc = CNTKLib.Times(weightParameter, inputVariable) + biasParameter;\n\nFor example, simply adding an ItemGroup clause in the .csproj file of a .Net Core application is sufficient:\n     >>> <Project Sdk="Microsoft.NET.Sdk">\n     >>>\n     >>>   <PropertyGroup>\n     >>>     <TargetFramework>netcoreapp2.1</TargetFramework>\n     >>>     <Platforms>x64</Platforms>\n     >>>   </PropertyGroup>\n     >>>\n     >>>   <ItemGroup>\n     >>>     <PackageReference Include="CNTK.GPU" Version="2.6.0" />\n     >>>   </ItemGroup>\n     >>>\n     >>> </Project>\n\n### Bug or minor fixes:\n- Fixed C# string and char to native wstring and wchar UTF conversion issues on Linux.\n- Fixed multibyte and wide character conversions across the codebase.\n- Fixed Nuget package mechanism to pack for .Net Standard.\n- Fixed a memory leak issue in Value class in C# API where Dispose was not called upon object destruction.\n\n## Misc\n\n\n***2018-04-16.*** CNTK 2.5.1\n\nRepack CNTK 2.5 with third party libraries included in the bundles (Python wheel packages)\n\n---\n\n***2018-03-15.*** CNTK 2.5\n\nChange profiler details output format to be `chrome://tracing`\n\nEnable per-node timing. Working example [here](/Examples/Image/Classification/MLP/Python/SimpleMNIST.py)\n* per-node timing creates items in profiler details when profiler is enabled.\n* usage in Python:\n\n```python\nimport cntk as C\nC.debugging.debug.set_node_timing(True)\nC.debugging.start_profiler() # optional\nC.debugging.enable_profiler() # optional\n#<trainer|evaluator|function> executions\n<trainer|evaluator|function>.print_node_timing()\nC.debugging.stop_profiler()\n```\n\nExample profiler details view in `chrome://tracing`\n![ProfilerDetailWithNodeTiming](https://cntk.ai/Images/ProfilerDetailWithNodeTiming.jpg)\n\nCPU inference performance improvements using MKL\n* Accelerates some common tensor ops in Intel CPU inference for float32, especially for fully connected networks\n* Can be turned on/off by `cntk.cntk_py.enable_cpueval_optimization()/cntk.cntk_py.disable_cpueval_optimization()`\n\n1BitSGD incorporated into CNTK\n* `1BitSGD` source code is now available with CNTK license (MIT license) under `Source/1BitSGD/`\n* `1bitsgd` build target was merged into existing gpu target\n\nNew loss function: hierarchical softmax\n* Thanks @yaochengji for the contribution!\n\nDistributed Training with Multiple Learners\n* Trainer now accepts multiple parameter learners for distributed training. With this change, different parameters of a network can be learned by different learners in a single training session. This also facilitates distributed training for GANs. For more information, please refer to the [Basic_GAN_Distributed.py](/Examples/Image/GAN/Basic_GAN_Distributed.py) and the [cntk.learners.distributed_multi_learner_test.py](/bindings/python/cntk/learners/tests/distributed_multi_learner_test.py)\n\nOperators\n* Added `MeanVarianceNormalization` operator. \n\nBug fixes\n* Fixed convergence issue in Tutorial 201B\n* Fixed pooling/unpooling to support free dimension for sequences\n* Fixed crash in `CNTKBinaryFormat` deserializer when crossing sweep boundary\n* Fixed shape inference bug in RNN step function for scalar broadcasting\n* Fixed a build bug when `mpi=no`\n* Improved distributed training aggregation speed by increasing packing threshold, and expose the knob in V2\n* Fixed a memory leak in MKL layout\n* Fixed a bug in `cntk.convert` API in `misc.converter.py`, which prevents converting complex networks.\n\nONNX\n* Updates\n    * CNTK exported ONNX models are now `ONNX.checker` compliant. \n    * Added ONNX support for CNTKâ€™s `OptimizedRNNStack` operator (LSTM only).\n    * Added support for LSTM and GRU operators\n    * Added support for experimental ONNX op `MeanVarianceNormalization`.\n    * Added support for experimental ONNX op `Identity`.\n    * Added support for exporting CNTKâ€™s `LayerNormalization` layer using ONNX `MeanVarianceNormalization` op.\n* Bug or minor fixes:\n    * Axis attribute is optional in CNTKâ€™s ONNX `Concat` operator.\n    * Bug fix in ONNX broadcasting for scalars.\n    * Bug fix in ONNX ConvTranspose operator. \n    * Backward compatibility bug fix in `LeakyReLu` (argument â€˜alphaâ€™ reverted to type double).\n\nMisc\n* Added a new API `find_by_uid()` under `cntk.logging.graph`. \n\n---\n\n***2018-02-28.*** CNTK supports nightly build\n\nIf you prefer to use latest CNTK bits from master, use one of the CNTK nightly package.\n* [Nightly packages for Windows](https://cntk.ai/nightly-windows.html)\n* [Nightly packages for Linux](https://cntk.ai/nightly-linux.html)\n\nAlternatively, you can also click corresponding build badge to land to nightly build page.\n\n---\n\n***2018-01-31.* CNTK 2.4**\n\nHighlights:\n* Moved to CUDA9, cuDNN 7 and Visual Studio 2017.\n* Removed Python 3.4 support.\n* Added Volta GPU and FP16 support.\n* Better ONNX support.\n* CPU perf improvement.\n* More OPs.\n\nOPs\n* `top_k` operation: in the forward pass it computes the top (largest) k values and corresponding indices along the specified axis. In the backward pass the gradient is scattered to the top k elements (an element not in the top k gets a zero gradient).\n* `gather` operation now supports an axis argument\n* `squeeze` and `expand_dims` operations for easily removing and adding singleton axes\n* `zeros_like` and `ones_like` operations. In many situations you can just rely on CNTK correctly broadcasting a simple 0 or 1 but sometimes you need the actual tensor.\n* `depth_to_space`: Rearranges elements in the input tensor from the depth dimension into spatial blocks. Typical use of this operation is for implementing sub-pixel convolution for some image super-resolution models.\n* `space_to_depth`: Rearranges elements in the input tensor from the spatial dimensions to the depth dimension. It is largely the inverse of DepthToSpace.\n* `sum` operation: Create a new Function instance that computes element-wise sum of input tensors.\n* `softsign` operation: Create a new Function instance that computes the element-wise softsign of a input tensor.\n* `asinh` operation: Create a new Function instance that computes the element-wise asinh of a input tensor.\n* `log_softmax` operation: Create a new Function instance that computes the logsoftmax normalized values of a input tensor.\n* `hard_sigmoid` operation: Create a new Function instance that computes the hard_sigmoid normalized values of a input tensor.\n* `element_and`, `element_not`, `element_or`, `element_xor` element-wise logic operations\n* `reduce_l1` operation: Computes the L1 norm of the input tensor''s element along the provided axes.\n* `reduce_l2` operation: Computes the L2 norm of the input tensor''s element along the provided axes.\n* `reduce_sum_square` operation: Computes the sum square of the input tensor''s element along the provided axes.\n* `image_scaler` operation: Alteration of image by scaling its individual values.\n\nONNX\n* There have been several improvements to ONNX support in CNTK.\n* Updates\n  * Updated ONNX `Reshape` op to handle `InferredDimension`.\n  * Adding `producer_name` and `producer_version` fields to ONNX models.\n  * Handling the case when neither `auto_pad` nor `pads` atrribute is specified in ONNX `Conv` op.\n* Bug fixes\n  * Fixed bug in ONNX `Pooling` op serialization\n  * Bug fix to create ONNX `InputVariable` with only one batch axis.\n  * Bug fixes and updates to implementation of ONNX `Transpose` op to match updated spec.\n  * Bug fixes and updates to implementation of ONNX `Conv`, `ConvTranspose`, and `Pooling` ops to match updated spec.\n\nOperators\n* Group convolution\n  * Fixed bug in group convolution. Output of CNTK `Convolution` op will change for groups > 1. More optimized implementation of group convolution is expected in the next release.\n  * Better error reporting for group convolution in `Convolution` layer.\n\nHalide Binary Convolution\n- The CNTK build can now use optional [Halide](http://halide-lang.org/) libraries to build `Cntk.BinaryConvolution.so/dll` library that can be used with the `netopt` module. The library contains optimized binary convolution operators that perform better than the python based binarized convolution operators. To enable Halide in the build, please download [Halide release](https://github.com/halide/Halide/releases) and set `HALIDE_PATH` environment varibale before starting a build. In Linux, you can use `./configure --with-halide[=directory]` to enable it. For more information on how to use this feature, please refer to [How_to_use_network_optimization](https://github.com/Microsoft/CNTK/blob/master/Manual/Manual_How_to_use_network_optimizations.ipynb).\n\nSee more in the [Release Notes](https://docs.microsoft.com/en-us/cognitive-toolkit/ReleaseNotes/CNTK_2_4_Release_Notes).\nGet the Release from the [CNTK Releases page](https://github.com/Microsoft/CNTK/releases).\n', '{"language":"C++","stars":17601,"forks":4262,"watchers":17601,"open_issues":839,"topics":["c-plus-plus","c-sharp","cntk","cognitive-toolkit","deep-learning","deep-neural-networks","distributed","java","machine-learning","neural-network","python"],"default_branch":"master","size_kb":935217,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:Microsoft:CNTK","source_url":"https://github.com/Microsoft/CNTK"},{"type":"has_code","target_id":"github:lutzroeder:netron","source_url":"https://github.com/lutzroeder/netron"},{"type":"has_code","target_id":"github:halide:Halide","source_url":"https://github.com/halide/Halide"},{"type":"has_code","target_id":"github:Microsoft:CNTK","source_url":"https://github.com/Microsoft/CNTK"},{"type":"has_code","target_id":"github:Microsoft:CNTK","source_url":"https://github.com/Microsoft/CNTK"}]', NULL, 'NOASSERTION', 'approved', 80, 'c6ecf367a378350939a2af57379e79b1', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-microsoft-CNTK from https://github.com/microsoft.png
Image converted to WebP: data/images/github-microsoft-CNTK.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-NLP-LOVE-ML-NLP', 'github--nlp-love--ml-nlp', 'ML-NLP', 'NLP-LOVE', '- æ­¤é¡¹ç›®æ˜¯**æœºå™¨å­¦ä¹ ã€NLPé¢è¯•**ä¸­å¸¸è€ƒåˆ°çš„**çŸ¥è¯†ç‚¹å’Œä»£ç å®ç°**ï¼Œä¹Ÿæ˜¯ä½œä¸ºä¸€ä¸ªç®—æ³•å·¥ç¨‹å¸ˆå¿…ä¼šçš„ç†è®ºåŸºç¡€çŸ¥è¯†ã€‚ - æ—¢ç„¶æ˜¯ä»¥é¢è¯•ä¸ºä¸»è¦ç›®çš„ï¼Œäº¦ä¸å¯ä»¥ç¯‡æ¦‚å…¨ï¼Œè¯·è°…è§£ï¼Œæœ‰é—®é¢˜å¯æå‡ºã€‚ - æ­¤é¡¹ç›®ä»¥å„ä¸ªæ¨¡å—ä¸ºåˆ‡å…¥ç‚¹ï¼Œè®©å¤§å®¶æœ‰ä¸€ä¸ªæ¸…æ™°çš„çŸ¥è¯†ä½“ç³»ã€‚ - æ­¤é¡¹ç›®äº¦å¯æ‹¿æ¥å¸¸è¯»ã€å¸¸è®°ä»¥åŠé¢è¯•æ—¶å¤ä¹ ä¹‹ç”¨ã€‚ - æ¯ä¸€ç« é‡Œçš„é—®é¢˜éƒ½æ˜¯é¢è¯•æ—¶æœ‰å¯èƒ½é—®åˆ°çš„çŸ¥è¯†ç‚¹ï¼Œå¦‚æœ‰é—æ¼å¯è”ç³»æˆ‘è¿›è¡Œè¡¥å……ï¼Œç»“å°¾å¤„éƒ½æœ‰ç®—æ³•çš„**å®æˆ˜ä»£ç æ¡ˆä¾‹**ã€‚ - æ€ç»´å¯¼å›¾ï¼Œ**è¯·å…³æ³¨ AIArea å…¬ä¼—å·å¹¶å›å¤ï¼šNLPæ€ç»´å¯¼å›¾** ï¼Œå³èƒ½ä¸‹è½½é«˜æ¸…å¤§å›¾ã€‚ - ------ - **é¡¹ç›®æŒç»­æ›´æ–°ä¸­......** | æ¨¡å— | ç« èŠ‚ | è´Ÿè´£äºº(GitHub) | è”ç³»QQ | | -------- | ------------------------------------------------------------ | --------------------------------------- | --------- | | æœºå™¨å­¦ä¹  | 1. çº¿æ€§å›å½’(Liner Regression) | @mantchs | 448...', '["deep-learning","machine-learning","nlp","jupyter notebook"]', 'other', 17361, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/NLP-LOVE/ML-NLP","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '## é¡¹ç›®ä»‹ç»\n\n- æ­¤é¡¹ç›®æ˜¯**æœºå™¨å­¦ä¹ ã€NLPé¢è¯•**ä¸­å¸¸è€ƒåˆ°çš„**çŸ¥è¯†ç‚¹å’Œä»£ç å®ç°**ï¼Œä¹Ÿæ˜¯ä½œä¸ºä¸€ä¸ªç®—æ³•å·¥ç¨‹å¸ˆå¿…ä¼šçš„ç†è®ºåŸºç¡€çŸ¥è¯†ã€‚\n- æ—¢ç„¶æ˜¯ä»¥é¢è¯•ä¸ºä¸»è¦ç›®çš„ï¼Œäº¦ä¸å¯ä»¥ç¯‡æ¦‚å…¨ï¼Œè¯·è°…è§£ï¼Œæœ‰é—®é¢˜å¯æå‡ºã€‚\n- æ­¤é¡¹ç›®ä»¥å„ä¸ªæ¨¡å—ä¸ºåˆ‡å…¥ç‚¹ï¼Œè®©å¤§å®¶æœ‰ä¸€ä¸ªæ¸…æ™°çš„çŸ¥è¯†ä½“ç³»ã€‚\n- æ­¤é¡¹ç›®äº¦å¯æ‹¿æ¥å¸¸è¯»ã€å¸¸è®°ä»¥åŠé¢è¯•æ—¶å¤ä¹ ä¹‹ç”¨ã€‚\n- æ¯ä¸€ç« é‡Œçš„é—®é¢˜éƒ½æ˜¯é¢è¯•æ—¶æœ‰å¯èƒ½é—®åˆ°çš„çŸ¥è¯†ç‚¹ï¼Œå¦‚æœ‰é—æ¼å¯è”ç³»æˆ‘è¿›è¡Œè¡¥å……ï¼Œç»“å°¾å¤„éƒ½æœ‰ç®—æ³•çš„**å®æˆ˜ä»£ç æ¡ˆä¾‹**ã€‚\n- æ€ç»´å¯¼å›¾ï¼Œ**è¯·å…³æ³¨ AIArea å…¬ä¼—å·å¹¶å›å¤ï¼šNLPæ€ç»´å¯¼å›¾** ï¼Œå³èƒ½ä¸‹è½½é«˜æ¸…å¤§å›¾ã€‚\n- ![](https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-3_16-0-25.png?raw=true)\n\n\n------\n\n\n\n## ç›®å½•\n\n- **é¡¹ç›®æŒç»­æ›´æ–°ä¸­......**\n\n| æ¨¡å—     | ç« èŠ‚                                                         | è´Ÿè´£äºº(GitHub)                          | è”ç³»QQ    |\n| -------- | ------------------------------------------------------------ | --------------------------------------- | --------- |\n| æœºå™¨å­¦ä¹  | [1. çº¿æ€§å›å½’(Liner Regression)](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/Liner%20Regression/1.Liner%20Regression.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [2. é€»è¾‘å›å½’(Logistics Regression)](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/2.Logistics%20Regression/2.Logistics%20Regression.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [3. å†³ç­–æ ‘(Desision Tree)](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.Desition%20Tree/Desition%20Tree.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [3.1 éšæœºæ£®æ—(Random Forest)](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.1%20Random%20Forest/3.1%20Random%20Forest.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [3.2 æ¢¯åº¦æå‡å†³ç­–æ ‘(GBDT)](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.2%20GBDT/3.2%20GBDT.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [3.3 XGBoost](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.3%20XGBoost/3.3%20XGBoost.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [3.4 LightGBM](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.4%20LightGBM/3.4%20LightGBM.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [4. æ”¯æŒå‘é‡æœº(SVM)](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/4.%20SVM/4.%20SVM.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | 5. æ¦‚ç‡å›¾æ¨¡å‹(Probabilistic Graphical Model)                 |                                         |           |\n| æœºå™¨å­¦ä¹  | [5.1 è´å¶æ–¯ç½‘ç»œ(Bayesian Network)](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/5.1%20Bayes%20Network/5.1%20Bayes%20Network.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [5.2 é©¬å°”ç§‘å¤«(Markov)](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/5.2%20Markov/5.2%20Markov.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [5.3 ä¸»é¢˜æ¨¡å‹(Topic Model)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/5.3%20Topic%20Model) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [6.æœ€å¤§æœŸæœ›ç®—æ³•(EM)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/6.%20EM) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [7.èšç±»(Clustering)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/7.%20Clustering) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [8.MLç‰¹å¾å·¥ç¨‹å’Œä¼˜åŒ–æ–¹æ³•](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/8.%20ML%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E5%92%8C%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [9.Kè¿‘é‚»ç®—æ³•(KNN)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/9.%20KNN) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æ·±åº¦å­¦ä¹  | [10.ç¥ç»ç½‘ç»œ(Neural Network)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/10.%20Neural%20Network) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æ·±åº¦å­¦ä¹  | [11. å·ç§¯ç¥ç»ç½‘ç»œ(CNN)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/11.%20CNN) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æ·±åº¦å­¦ä¹  | [12. å¾ªç¯ç¥ç»ç½‘ç»œ(RNN)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/12.%20RNN) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æ·±åº¦å­¦ä¹  | [12.1 é—¨æ§å¾ªç¯å•å…ƒ(GRU)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/12.1%20GRU) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æ·±åº¦å­¦ä¹  | [12.2 é•¿çŸ­æœŸè®°å¿†(LSTM)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/12.2%20LSTM) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æ·±åº¦å­¦ä¹  | [13.è¿ç§»å­¦ä¹ (Transfer)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/13.%20Transfer%20Learning) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æ·±åº¦å­¦ä¹  | [14.å¼ºåŒ–å­¦ä¹ (Reinforcement) & å¤šä»»åŠ¡](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/14.%20Reinforcement%20Learning) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æ·±åº¦å­¦ä¹  | [15. æ·±åº¦å­¦ä¹ çš„ä¼˜åŒ–æ–¹æ³•](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/15.%20DL%20Optimizer) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16. è‡ªç„¶è¯­è¨€å¤„ç†(NLP)](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.%20NLP) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16.1 è¯åµŒå…¥(Word2Vec)](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.1%20Word%20Embedding) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16.2 å­è¯åµŒå…¥(fastText)](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.2%20fastText) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16.3 å…¨å±€å‘é‡è¯åµŒå…¥(GloVe)](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.3%20GloVe) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16.4 textRNN & textCNN](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.4%20textRNN%20%26%20textCNN) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16.5 åºåˆ—åˆ°åºåˆ—æ¨¡å‹(seq2seq)](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.5%20seq2seq) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16.6 æ³¨æ„åŠ›æœºåˆ¶(Attention Mechanism)](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.6%20Attention) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16.7 Transformeræ¨¡å‹](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.7%20Transformer) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16.8 BERTæ¨¡å‹](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.8%20BERT) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16.9 XLNetæ¨¡å‹](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.9%20XLNet) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| é¡¹ç›®     | [17. æ¨èç³»ç»Ÿ(Recommendation System)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Project/17.%20Recommendation%20System) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| é¡¹ç›®     | [18. æ™ºèƒ½å®¢æœ(Intelligent Customer Service)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Project/18.%20Intelligent%20Customer%20Service) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| é¡¹ç›®     | 19. çŸ¥è¯†å›¾è°±(Knowledge Graph)                                |                                         |           |\n| é¡¹ç›®     | 20. è¯„è®ºåˆ†æ                                                 |                                         |           |\n\n\n\n> æ¬¢è¿å¤§å®¶åŠ å…¥ï¼å…±åŒå®Œå–„æ­¤é¡¹ç›®ï¼NLPå­¦ä¹ QQ2ç¾¤ã€207576902ã€‘<a target="_blank" href="http://shang.qq.com/wpa/qunwpa?idkey=1defd70810d9e67ca6ab3a30e1425a8a358139315a186dd2192d82a4c0ca1ce9"><img border="0" src="http://pub.idqqimg.com/wpa/images/group.png" alt="NLPå­¦ä¹ ç¾¤â‘¡" title="NLPå­¦ä¹ ç¾¤â‘¡"></a>\n', '{"language":"Jupyter Notebook","stars":17361,"forks":4647,"watchers":17361,"open_issues":37,"topics":["deep-learning","machine-learning","nlp"],"default_branch":"master","size_kb":11965,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:NLP-LOVE:Introduction-NLP","source_url":"https://github.com/NLP-LOVE/Introduction-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"}]', NULL, NULL, 'pending', 55, '8de718eed45fb1c387dc8800961aaf61', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-NLP-LOVE-ML-NLP from https://github.com/NLP-LOVE.png
Image converted to WebP: data/images/github-NLP-LOVE-ML-NLP.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-ujjwalkarn-Machine-Learning-Tutorials', 'github--ujjwalkarn--machine-learning-tutorials', 'Machine-Learning-Tutorials', 'ujjwalkarn', '- This repository contains a topic-wise curated list of Machine Learning and Deep Learning tutorials, articles and other resources. Other awesome lists can be found in this list. - If you want to contribute to this list, please read Contributing Guidelines. - Curated list of R tutorials for Data Science, NLP and Machine Learning. - Curated list of Python tutorials for Data Science, NLP and Machine Learning. - Introduction - Interview Resources - Artificial Intelligence - Genetic Algorithms - ...', '["awesome","awesome-list","deep-learning","deep-learning-tutorial","deep-neural-networks","deeplearning","list","machine-learning","machinelearning","neural-network","neural-networks"]', 'other', 17250, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/ujjwalkarn/Machine-Learning-Tutorials","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '\n# Machine Learning & Deep Learning Tutorials [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\n- This repository contains a topic-wise curated list of Machine Learning and Deep Learning tutorials, articles and other resources. Other awesome lists can be found in this [list](https://github.com/sindresorhus/awesome).\n\n- If you want to contribute to this list, please read [Contributing Guidelines](https://github.com/ujjwalkarn/Machine-Learning-Tutorials/blob/master/contributing.md).\n\n- [Curated list of R tutorials for Data Science, NLP and Machine Learning](https://github.com/ujjwalkarn/DataScienceR).\n\n- [Curated list of Python tutorials for Data Science, NLP and Machine Learning](https://github.com/ujjwalkarn/DataSciencePython).\n\n\n## Contents\n- [Introduction](#general)\n- [Interview Resources](#interview)\n- [Artificial Intelligence](#ai)\n- [Genetic Algorithms](#ga)\n- [Statistics](#stat)\n- [Useful Blogs](#blogs)\n- [Resources on Quora](#quora)\n- [Resources on Kaggle](#kaggle)\n- [Cheat Sheets](#cs)\n- [Classification](#classification)\n- [Linear Regression](#linear)\n- [Logistic Regression](#logistic)\n- [Model Validation using Resampling](#validation)\n    - [Cross Validation](#cross)\n    - [Bootstraping](#boot)\n- [Deep Learning](#deep)\n    - [Frameworks](#frame)\n    - [Feed Forward Networks](#feed)\n    - [Recurrent Neural Nets, LSTM, GRU](#rnn)\n    - [Restricted Boltzmann Machine, DBNs](#rbm)\n    - [Autoencoders](#auto)\n    - [Convolutional Neural Nets](#cnn)\n    - [Graph Representation Learning](#nrl)\n- [Natural Language Processing](#nlp)\n    - [Topic Modeling, LDA](#topic)\n    - [Word2Vec](#word2vec)\n- [Computer Vision](#vision)\n- [Support Vector Machine](#svm)\n- [Reinforcement Learning](#rl)\n- [Decision Trees](#dt)\n- [Random Forest / Bagging](#rf)\n- [Boosting](#gbm)\n- [Ensembles](#ensem)\n- [Stacking Models](#stack)\n- [VC Dimension](#vc)\n- [Bayesian Machine Learning](#bayes)\n- [Semi Supervised Learning](#semi)\n- [Optimizations](#opt)\n- [Other Useful Tutorials](#other)\n\n<a name="general" />\n\n## Introduction\n\n- [Machine Learning Course by Andrew Ng (Stanford University)](https://www.coursera.org/learn/machine-learning)\n\n- [AI/ML YouTube Courses](https://github.com/dair-ai/ML-YouTube-Courses)\n\n- [Curated List of Machine Learning Resources](https://hackr.io/tutorials/learn-machine-learning-ml)\n\n- [In-depth introduction to machine learning in 15 hours of expert videos](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/)\n\n- [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)\n\n- [List of Machine Learning University Courses](https://github.com/prakhar1989/awesome-courses#machine-learning)\n\n- [Machine Learning for Software Engineers](https://github.com/ZuzooVn/machine-learning-for-software-engineers)\n\n- [Dive into Machine Learning](https://github.com/hangtwenty/dive-into-machine-learning)\n\n- [A curated list of awesome Machine Learning frameworks, libraries and software](https://github.com/josephmisiti/awesome-machine-learning)\n\n- [A curated list of awesome data visualization libraries and resources.](https://github.com/fasouto/awesome-dataviz)\n\n- [An awesome Data Science repository to learn and apply for real world problems](https://github.com/okulbilisim/awesome-datascience)\n\n- [The Open Source Data Science Masters](http://datasciencemasters.org/)\n\n- [Machine Learning FAQs on Cross Validated](http://stats.stackexchange.com/questions/tagged/machine-learning)\n\n- [Machine Learning algorithms that you should always have a strong understanding of](https://www.quora.com/What-are-some-Machine-Learning-algorithms-that-you-should-always-have-a-strong-understanding-of-and-why)\n\n- [Difference between Linearly Independent, Orthogonal, and Uncorrelated Variables](http://terpconnect.umd.edu/~bmomen/BIOM621/LineardepCorrOrthogonal.pdf)\n\n- [List of Machine Learning Concepts](https://en.wikipedia.org/wiki/List_of_machine_learning_concepts)\n\n- [Slides on Several Machine Learning Topics](http://www.slideshare.net/pierluca.lanzi/presentations)\n\n- [MIT Machine Learning Lecture Slides](http://www.ai.mit.edu/courses/6.867-f04/lectures.html)\n\n- [Comparison Supervised Learning Algorithms](http://www.dataschool.io/comparing-supervised-learning-algorithms/)\n\n- [Learning Data Science Fundamentals](http://www.dataschool.io/learning-data-science-fundamentals/)\n\n- [Machine Learning mistakes to avoid](https://medium.com/@nomadic_mind/new-to-machine-learning-avoid-these-three-mistakes-73258b3848a4#.lih061l3l)\n\n- [Statistical Machine Learning Course](http://www.stat.cmu.edu/~larry/=sml/)\n\n- [TheAnalyticsEdge edX Notes and Codes](https://github.com/pedrosan/TheAnalyticsEdge)\n\n- [Have Fun With Machine Learning](https://github.com/humphd/have-fun-with-machine-learning)\n\n- [Twitter''s Most Shared #machineLearning Content From The Past 7 Days](http://theherdlocker.com/tweet/popularity/machinelearning)\n\n- [Grokking Machine Learning](https://www.manning.com/books/grokking-machine-learning)\n\n<a name="interview" />\n\n## Interview Resources\n\n- [41 Essential Machine Learning Interview Questions (with answers)](https://www.springboard.com/blog/machine-learning-interview-questions/)\n\n- [How can a computer science graduate student prepare himself for data scientist interviews?](https://www.quora.com/How-can-a-computer-science-graduate-student-prepare-himself-for-data-scientist-machine-learning-intern-interviews)\n\n- [How do I learn Machine Learning?](https://www.quora.com/How-do-I-learn-machine-learning-1)\n\n- [FAQs about Data Science Interviews](https://www.quora.com/topic/Data-Science-Interviews/faq)\n\n- [What are the key skills of a data scientist?](https://www.quora.com/What-are-the-key-skills-of-a-data-scientist)\n\n- [The Big List of DS/ML Interview Resources](https://towardsdatascience.com/the-big-list-of-ds-ml-interview-resources-2db4f651bd63)\n\n<a name="ai" />\n\n## Artificial Intelligence\n\n- [Awesome Artificial Intelligence (GitHub Repo)](https://github.com/owainlewis/awesome-artificial-intelligence)\n\n- [UC Berkeley CS188 Intro to AI](http://ai.berkeley.edu/home.html), [Lecture Videos](http://ai.berkeley.edu/lecture_videos.html), [2](https://www.youtube.com/watch?v=W1S-HSakPTM)\n\n- [Programming Community Curated Resources for learning Artificial Intelligence](https://hackr.io/tutorials/learn-artificial-intelligence-ai) \n\n- [MIT 6.034 Artificial Intelligence Lecture Videos](https://www.youtube.com/playlist?list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi), [Complete Course](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/)\n\n- [edX course | Klein & Abbeel](https://courses.edx.org/courses/BerkeleyX/CS188x_1/1T2013/info)\n\n- [Udacity Course | Norvig & Thrun](https://www.udacity.com/course/intro-to-artificial-intelligence--cs271)\n\n- [TED talks on AI](http://www.ted.com/playlists/310/talks_on_artificial_intelligen)\n\n<a name="ga" />\n\n## Genetic Algorithms\n\n- [Genetic Algorithms Wikipedia Page](https://en.wikipedia.org/wiki/Genetic_algorithm)\n\n- [Simple Implementation of Genetic Algorithms in Python (Part 1)](http://outlace.com/miniga.html), [Part 2](http://outlace.com/miniga_addendum.html)\n\n- [Genetic Algorithms vs Artificial Neural Networks](http://stackoverflow.com/questions/1402370/when-to-use-genetic-algorithms-vs-when-to-use-neural-networks)\n\n- [Genetic Algorithms Explained in Plain English](http://www.ai-junkie.com/ga/intro/gat1.html)\n\n- [Genetic Programming](https://en.wikipedia.org/wiki/Genetic_programming)\n\n    - [Genetic Programming in Python (GitHub)](https://github.com/trevorstephens/gplearn)\n    \n    - [Genetic Alogorithms vs Genetic Programming (Quora)](https://www.quora.com/Whats-the-difference-between-Genetic-Algorithms-and-Genetic-Programming), [StackOverflow](http://stackoverflow.com/questions/3819977/what-are-the-differences-between-genetic-algorithms-and-genetic-programming)\n\n<a name="stat" />\n\n## Statistics\n\n- [Stat Trek Website](http://stattrek.com/) - A dedicated website to teach yourselves Statistics\n\n- [Learn Statistics Using Python](https://github.com/rouseguy/intro2stats) - Learn Statistics using an application-centric programming approach\n\n- [Statistics for Hackers | Slides | @jakevdp](https://speakerdeck.com/jakevdp/statistics-for-hackers) - Slides by Jake VanderPlas\n\n- [Online Statistics Book](http://onlinestatbook.com/2/index.html) - An Interactive Multimedia Course for Studying Statistics\n\n- [What is a Sampling Distribution?](http://stattrek.com/sampling/sampling-distribution.aspx)\n\n- Tutorials\n\n    - [AP Statistics Tutorial](http://stattrek.com/tutorials/ap-statistics-tutorial.aspx)\n    \n    - [Statistics and Probability Tutorial](http://stattrek.com/tutorials/statistics-tutorial.aspx)\n    \n    - [Matrix Algebra Tutorial](http://stattrek.com/tutorials/matrix-algebra-tutorial.aspx)\n    \n- [What is an Unbiased Estimator?](https://www.physicsforums.com/threads/what-is-an-unbiased-estimator.547728/)\n\n- [Goodness of Fit Explained](https://en.wikipedia.org/wiki/Goodness_of_fit)\n\n- [What are QQ Plots?](http://onlinestatbook.com/2/advanced_graphs/q-q_plots.html)\n\n- [OpenIntro Statistics](https://www.openintro.org/stat/textbook.php?stat_book=os) - Free PDF textbook\n\n<a name="blogs" />\n\n## Useful Blogs\n\n- [Edwin Chen''s Blog](http://blog.echen.me/) - A blog about Math, stats, ML, crowdsourcing, data science\n\n- [The Data School Blog](http://www.dataschool.io/) - Data science for beginners!\n\n- [ML Wave](http://mlwave.com/) - A blog for Learning Machine Learning\n\n- [Andrej Karpathy](http://karpathy.github.io/) - A blog about Deep Learning and Data Science in general\n\n- [Colah''s Blog](http://colah.github.io/) - Awesome Neural Networks Blog\n\n- [Alex Minnaar''s Blog](http://alexminnaar.com/) - A blog about Machine Learning and Software Engineering\n\n- [Statistically Significant](http://andland.github.io/) - Andrew Landgraf''s Data Science Blog\n\n- [Simply Statistics](http://simplystatistics.org/) - A blog by three biostatistics professors\n\n- [Yanir Seroussi''s Blog](https://yanirseroussi.com/) - A blog about Data Science and beyond\n\n- [fastML](http://fastml.com/) - Machine learning made easy\n\n- [Trevor Stephens Blog](http://trevorstephens.com/) - Trevor Stephens Personal Page\n\n- [no free hunch | kaggle](http://blog.kaggle.com/) - The Kaggle Blog about all things Data Science\n\n- [A Quantitative Journey | outlace](http://outlace.com/) -  learning quantitative applications\n\n- [r4stats](http://r4stats.com/) - analyze the world of data science, and to help people learn to use R\n\n- [Variance Explained](http://varianceexplained.org/) - David Robinson''s Blog\n\n- [AI Junkie](http://www.ai-junkie.com/) - a blog about Artificial Intellingence\n\n- [Deep Learning Blog by Tim Dettmers](http://timdettmers.com/) - Making deep learning accessible\n\n- [J Alammar''s Blog](http://jalammar.github.io/)- Blog posts about Machine Learning and Neural Nets\n\n- [Adam Geitgey](https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471#.f7vwrtfne) - Easiest Introduction to machine learning\n\n- [Ethen''s Notebook Collection](https://github.com/ethen8181/machine-learning) - Continuously updated machine learning documentations (mainly in Python3). Contents include educational implementation of machine learning algorithms from scratch and open-source library usage\n\n<a name="quora" />\n\n## Resources on Quora\n\n- [Most Viewed Machine Learning writers](https://www.quora.com/topic/Machine-Learning/writers)\n\n- [Data Science Topic on Quora](https://www.quora.com/Data-Science)\n\n- [William Chen''s Answers](https://www.quora.com/William-Chen-6/answers)\n\n- [Michael Hochster''s Answers](https://www.quora.com/Michael-Hochster/answers)\n\n- [Ricardo Vladimiro''s Answers](https://www.quora.com/Ricardo-Vladimiro-1/answers)\n\n- [Storytelling with Statistics](https://datastories.quora.com/)\n\n- [Data Science FAQs on Quora](https://www.quora.com/topic/Data-Science/faq)\n\n- [Machine Learning FAQs on Quora](https://www.quora.com/topic/Machine-Learning/faq)\n\n<a name="kaggle" />\n\n## Kaggle Competitions WriteUp\n\n- [How to almost win Kaggle Competitions](https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/)\n\n- [Convolution Neural Networks for EEG detection](http://blog.kaggle.com/2015/10/05/grasp-and-lift-eeg-detection-winners-interview-3rd-place-team-hedj/)\n\n- [Facebook Recruiting III Explained](http://alexminnaar.com/tag/kaggle-competitions.html)\n\n- [Predicting CTR with Online ML](http://mlwave.com/predicting-click-through-rates-with-online-machine-learning/)\n\n- [How to Rank 10% in Your First Kaggle Competition](https://dnc1994.com/2016/05/rank-10-percent-in-first-kaggle-competition-en/)\n\n<a name="cs" />\n\n## Cheat Sheets\n\n- [Probability Cheat Sheet](http://static1.squarespace.com/static/54bf3241e4b0f0d81bf7ff36/t/55e9494fe4b011aed10e48e5/1441352015658/probability_cheatsheet.pdf),\n[Source](http://www.wzchen.com/probability-cheatsheet/)\n\n- [Machine Learning Cheat Sheet](https://github.com/soulmachine/machine-learning-cheat-sheet)\n\n- [ML Compiled](https://ml-compiled.readthedocs.io/en/latest/)\n\n<a name="classification" />\n\n## Classification\n\n- [Does Balancing Classes Improve Classifier Performance?](http://www.win-vector.com/blog/2015/02/does-balancing-classes-improve-classifier-performance/)\n\n- [What is Deviance?](http://stats.stackexchange.com/questions/6581/what-is-deviance-specifically-in-cart-rpart)\n\n- [When to choose which machine learning classifier?](http://stackoverflow.com/questions/2595176/when-to-choose-which-machine-learning-classifier)\n\n- [What are the advantages of different classification algorithms?](https://www.quora.com/What-are-the-advantages-of-different-classification-algorithms)\n\n- [ROC and AUC Explained](http://www.dataschool.io/roc-curves-and-auc-explained/) ([related video](https://youtu.be/OAl6eAyP-yo))\n\n- [An introduction to ROC analysis](https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf)\n\n- [Simple guide to confusion matrix terminology](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)\n\n\n<a name="linear" />\n\n## Linear Regression\n\n- [General](#general-)\n\n    - [Assumptions of Linear Regression](http://pareonline.net/getvn.asp?n=2&v=8), [Stack Exchange](http://stats.stackexchange.com/questions/16381/what-is-a-complete-list-of-the-usual-assumptions-for-linear-regression)\n    \n    - [Linear Regression Comprehensive Resource](http://people.duke.edu/~rnau/regintro.htm)\n    \n    - [Applying and Interpreting Linear Regression](http://www.dataschool.io/applying-and-interpreting-linear-regression/)\n    \n    - [What does having constant variance in a linear regression model mean?](http://stats.stackexchange.com/questions/52089/what-does-having-constant-variance-in-a-linear-regression-model-mean/52107?stw=2#52107)\n    \n    - [Difference between linear regression on y with x and x with y](http://stats.stackexchange.com/questions/22718/what-is-the-difference-between-linear-regression-on-y-with-x-and-x-with-y?lq=1)\n    \n    - [Is linear regression valid when the dependant variable is not normally distributed?](https://www.researchgate.net/post/Is_linear_regression_valid_when_the_outcome_dependant_variable_not_normally_distributed)\n- Multicollinearity and VIF\n\n    - [Dummy Variable Trap | Multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)\n    \n    - [Dealing with multicollinearity using VIFs](https://jonlefcheck.net/2012/12/28/dealing-with-multicollinearity-using-variance-inflation-factors/)\n\n- [Residual Analysis](#residuals-)\n\n    - [Interpreting plot.lm() in R](http://stats.stackexchange.com/questions/58141/interpreting-plot-lm)\n    \n    - [How to interpret a QQ plot?](http://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot?lq=1)\n    \n    - [Interpreting Residuals vs Fitted Plot](http://stats.stackexchange.com/questions/76226/interpreting-the-residuals-vs-fitted-values-plot-for-verifying-the-assumptions)\n\n- [Outliers](#outliers-)\n\n    - [How should outliers be dealt with?](http://stats.stackexchange.com/questions/175/how-should-outliers-be-dealt-with-in-linear-regression-analysis)\n\n- [Elastic Net](https://en.wikipedia.org/wiki/Elastic_net_regularization)\n    - [Regularization and Variable Selection via the\nElastic Net](https://web.stanford.edu/~hastie/Papers/elasticnet.pdf)\n\n<a name="logistic" />\n\n## Logistic Regression\n\n- [Logistic Regression Wiki](https://en.wikipedia.org/wiki/Logistic_regression)\n\n- [Geometric Intuition of Logistic Regression](http://florianhartl.com/logistic-regression-geometric-intuition.html)\n\n- [Obtaining predicted categories (choosing threshold)](http://stats.stackexchange.com/questions/25389/obtaining-predicted-values-y-1-or-0-from-a-logistic-regression-model-fit)\n\n- [Residuals in logistic regression](http://stats.stackexchange.com/questions/1432/what-do-the-residuals-in-a-logistic-regression-mean)\n\n- [Difference between logit and probit models](http://stats.stackexchange.com/questions/20523/difference-between-logit-and-probit-models#30909), [Logistic Regression Wiki](https://en.wikipedia.org/wiki/Logistic_regression), [Probit Model Wiki](https://en.wikipedia.org/wiki/Probit_model)\n\n- [Pseudo R2 for Logistic Regression](http://stats.stackexchange.com/questions/3559/which-pseudo-r2-measure-is-the-one-to-report-for-logistic-regression-cox-s), [How to calculate](http://stats.stackexchange.com/questions/8511/how-to-calculate-pseudo-r2-from-rs-logistic-regression), [Other Details](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/Psuedo_RSquareds.htm)\n\n- [Guide to an in-depth understanding of logistic regression](http://www.dataschool.io/guide-to-logistic-regression/)\n\n<a name="validation" />\n\n## Model Validation using Resampling\n\n- [Resampling Explained](https://en.wikipedia.org/wiki/Resampling_(statistics))\n\n- [Partioning data set in R](http://stackoverflow.com/questions/13536537/partitioning-data-set-in-r-based-on-multiple-classes-of-observations)\n\n- [Implementing hold-out Validaion in R](http://stackoverflow.com/questions/22972854/how-to-implement-a-hold-out-validation-in-r), [2](http://www.gettinggeneticsdone.com/2011/02/split-data-frame-into-testing-and.html)\n\n<a name="cross" />\n\n- [Cross Validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))\n    - [How to use cross-validation in predictive modeling](http://stuartlacy.co.uk/2016/02/04/how-to-correctly-use-cross-validation-in-predictive-modelling/)\n    - [Training with Full dataset after CV?](http://stats.stackexchange.com/questions/11602/training-with-the-full-dataset-after-cross-validation)\n    \n    - [Which CV method is best?](http://stats.stackexchange.com/questions/103459/how-do-i-know-which-method-of-cross-validation-is-best)\n    \n    - [Variance Estimates in k-fold CV](http://stats.stackexchange.com/questions/31190/variance-estimates-in-k-fold-cross-validation)\n    \n    - [Is CV a subsitute for Validation Set?](http://stats.stackexchange.com/questions/18856/is-cross-validation-a-proper-substitute-for-validation-set)\n    \n    - [Choice of k in k-fold CV](http://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation)\n    \n    - [CV for ensemble learning](http://stats.stackexchange.com/questions/102631/k-fold-cross-validation-of-ensemble-learning)\n    \n    - [k-fold CV in R](http://stackoverflow.com/questions/22909197/creating-folds-for-k-fold-cv-in-r-using-caret)\n    \n    - [Good Resources](http://www.chioka.in/tag/cross-validation/)\n    \n    - Overfitting and Cross Validation\n    \n        - [Preventing Overfitting the Cross Validation Data | Andrew Ng](http://ai.stanford.edu/~ang/papers/cv-final.pdf)\n        \n        - [Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation](http://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf)\n\n        - [CV for detecting and preventing Overfitting](http://www.autonlab.org/tutorials/overfit10.pdf)\n        \n        - [How does CV overcome the Overfitting Problem](http://stats.stackexchange.com/questions/9053/how-does-cross-validation-overcome-the-overfitting-problem)\n\n\n<a name="boot" />\n\n- [Bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))\n\n    - [Why Bootstrapping Works?](http://stats.stackexchange.com/questions/26088/explaining-to-laypeople-why-bootstrapping-works)\n    \n    - [Good Animation](https://www.stat.auckland.ac.nz/~wild/BootAnim/)\n    \n    - [Example of Bootstapping](http://statistics.about.com/od/Applications/a/Example-Of-Bootstrapping.htm)\n    \n    - [Understanding Bootstapping for Validation and Model Selection](http://stats.stackexchange.com/questions/14516/understanding-bootstrapping-for-validation-and-model-selection?rq=1)\n    \n    - [Cross Validation vs Bootstrap to estimate prediction error](http://stats.stackexchange.com/questions/18348/differences-between-cross-validation-and-bootstrapping-to-estimate-the-predictio), [Cross-validation vs .632 bootstrapping to evaluate classification performance](http://stats.stackexchange.com/questions/71184/cross-validation-or-bootstrapping-to-evaluate-classification-performance)\n\n\n<a name="deep" />\n\n## Deep Learning\n\n- [fast.ai - Practical Deep Learning For Coders](http://course.fast.ai/)\n\n- [fast.ai - Cutting Edge Deep Learning For Coders](http://course.fast.ai/part2.html)\n\n- [A curated list of awesome Deep Learning tutorials, projects and communities](https://github.com/ChristosChristofidis/awesome-deep-learning)\n\n- **[Deep Learning Papers Reading Roadmap](https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap/blob/master/README.md)**\n\n- [Lots of Deep Learning Resources](http://deeplearning4j.org/documentation.html)\n\n- [Interesting Deep Learning and NLP Projects (Stanford)](http://cs224d.stanford.edu/reports.html), [Website](http://cs224d.stanford.edu/)\n\n- [Core Concepts of Deep Learning](https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/)\n\n- [Understanding Natural Language with Deep Neural Networks Using Torch](https://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/)\n\n- [Stanford Deep Learning Tutorial](http://ufldl.stanford.edu/tutorial/)\n\n- [Deep Learning FAQs on Quora](https://www.quora.com/topic/Deep-Learning/faq)\n\n- [Google+ Deep Learning Page](https://plus.google.com/communities/112866381580457264725)\n\n- [Recent Reddit AMAs related to Deep Learning](http://deeplearning.net/2014/11/22/recent-reddit-amas-about-deep-learning/), [Another AMA](https://www.reddit.com/r/IAmA/comments/3mdk9v/we_are_google_researchers_working_on_deep/)\n\n- [Where to Learn Deep Learning?](http://www.kdnuggets.com/2014/05/learn-deep-learning-courses-tutorials-overviews.html)\n\n- [Deep Learning nvidia concepts](http://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/)\n\n- [Introduction to Deep Learning Using Python (GitHub)](https://github.com/rouseguy/intro2deeplearning), [Good Introduction Slides](https://speakerdeck.com/bargava/introduction-to-deep-learning)\n\n- [Video Lectures Oxford 2015](https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu), [Video Lectures Summer School Montreal](http://videolectures.net/deeplearning2015_montreal/)\n\n- [Deep Learning Software List](http://deeplearning.net/software_links/)\n\n- [Hacker''s guide to Neural Nets](http://karpathy.github.io/neuralnets/)\n\n- [Top arxiv Deep Learning Papers explained](http://www.kdnuggets.com/2015/10/top-arxiv-deep-learning-papers-explained.html)\n\n- [Geoff Hinton Youtube Vidoes on Deep Learning](https://www.youtube.com/watch?v=IcOMKXAw5VA)\n\n- [Awesome Deep Learning Reading List](http://deeplearning.net/reading-list/)\n\n- [Deep Learning Comprehensive Website](http://deeplearning.net/), [Software](http://deeplearning.net/software_links/)\n\n- [deeplearning Tutorials](http://deeplearning4j.org/)\n\n- [AWESOME! Deep Learning Tutorial](https://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks)\n\n- [Deep Learning Basics](http://alexminnaar.com/deep-learning-basics-neural-networks-backpropagation-and-stochastic-gradient-descent.html)\n\n- [Intuition Behind Backpropagation](https://medium.com/spidernitt/breaking-down-neural-networks-an-intuitive-approach-to-backpropagation-3b2ff958794c)\n\n- [Stanford Tutorials](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/)\n\n- [Train, Validation & Test in Artificial Neural Networks](http://stackoverflow.com/questions/2976452/whats-is-the-difference-between-train-validation-and-test-set-in-neural-networ)\n\n- [Artificial Neural Networks Tutorials](http://stackoverflow.com/questions/478947/what-are-some-good-resources-for-learning-about-artificial-neural-networks)\n\n- [Neural Networks FAQs on Stack Overflow](http://stackoverflow.com/questions/tagged/neural-network?sort=votes&pageSize=50)\n\n- [Deep Learning Tutorials on deeplearning.net](http://deeplearning.net/tutorial/index.html)\n\n- [Neural Networks and Deep Learning Online Book](http://neuralnetworksanddeeplearning.com/)\n\n- Neural Machine Translation\n\n    - **[Machine Translation Reading List](https://github.com/THUNLP-MT/MT-Reading-List#machine-translation-reading-list)**\n\n    - [Introduction to Neural Machine Translation with GPUs (part 1)](https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-with-gpus/), [Part 2](https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-2/), [Part 3](https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/)\n    \n    - [Deep Speech: Accurate Speech Recognition with GPU-Accelerated Deep Learning](https://devblogs.nvidia.com/parallelforall/deep-speech-accurate-speech-recognition-gpu-accelerated-deep-learning/)\n\n<a name="frame" />\n\n- Deep Learning Frameworks\n\n    - [Torch vs. Theano](http://fastml.com/torch-vs-theano/)\n    \n    - [dl4j vs. torch7 vs. theano](http://deeplearning4j.org/compare-dl4j-torch7-pylearn.html)\n    \n    - [Deep Learning Libraries by Language](http://www.teglor.com/b/deep-learning-libraries-language-cm569/)\n    \n\n    - [Theano](https://en.wikipedia.org/wiki/Theano_(software))\n    \n        - [Website](http://deeplearning.net/software/theano/)\n        \n        - [Theano Introduction](http://www.wildml.com/2015/09/speeding-up-your-neural-network-with-theano-and-the-gpu/)\n        \n        - [Theano Tutorial](http://outlace.com/Beginner-Tutorial-Theano/)\n        \n        - [Good Theano Tutorial](http://deeplearning.net/software/theano/tutorial/)\n        \n        - [Logistic Regression using Theano for classifying digits](http://deeplearning.net/tutorial/logreg.html#logreg)\n        \n        - [MLP using Theano](http://deeplearning.net/tutorial/mlp.html#mlp)\n        \n        - [CNN using Theano](http://deeplearning.net/tutorial/lenet.html#lenet)\n        \n        - [RNNs using Theano](http://deeplearning.net/tutorial/rnnslu.html#rnnslu)\n        \n        - [LSTM for Sentiment Analysis in Theano](http://deeplearning.net/tutorial/lstm.html#lstm)\n        \n        - [RBM using Theano](http://deeplearning.net/tutorial/rbm.html#rbm)\n        \n        - [DBNs using Theano](http://deeplearning.net/tutorial/DBN.html#dbn)\n        \n        - [All Codes](https://github.com/lisa-lab/DeepLearningTutorials)\n        \n        - [Deep Learning Implementation Tutorials - Keras and Lasagne](https://github.com/vict0rsch/deep_learning/)\n\n    - [Torch](http://torch.ch/)\n    \n        - [Torch ML Tutorial](http://code.madbits.com/wiki/doku.php), [Code](https://github.com/torch/tutorials)\n        \n        - [Intro to Torch](http://ml.informatik.uni-freiburg.de/_media/teaching/ws1415/presentation_dl_lect3.pdf)\n        \n        - [Learning Torch GitHub Repo](https://github.com/chetannaik/learning_torch)\n        \n        - [Awesome-Torch (Repository on GitHub)](https://github.com/carpedm20/awesome-torch)\n        \n        - [Machine Learning using Torch Oxford Univ](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/), [Code](https://github.com/oxford-cs-ml-2015)\n        \n        - [Torch Internals Overview](https://apaszke.github.io/torch-internals.html)\n        \n        - [Torch Cheatsheet](https://github.com/torch/torch7/wiki/Cheatsheet)\n        \n        - [Understanding Natural Language with Deep Neural Networks Using Torch](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/)\n\n    - Caffe\n        - [Deep Learning for Computer Vision with Caffe and cuDNN](https://devblogs.nvidia.com/parallelforall/deep-learning-computer-vision-caffe-cudnn/)\n\n    - TensorFlow\n        - [Website](http://tensorflow.org/)\n        \n        - [TensorFlow Examples for Beginners](https://github.com/aymericdamien/TensorFlow-Examples)\n        \n        - [Stanford Tensorflow for Deep Learning Research Course](https://web.stanford.edu/class/cs20si/syllabus.html)\n        \n            - [GitHub Repo](https://github.com/chiphuyen/tf-stanford-tutorials)\n            \n        - [Simplified Scikit-learn Style Interface to TensorFlow](https://github.com/tensorflow/skflow)\n        \n        - [Learning TensorFlow GitHub Repo](https://github.com/chetannaik/learning_tensorflow)\n        \n        - [Benchmark TensorFlow GitHub](https://github.com/soumith/convnet-benchmarks/issues/66)\n        \n        - [Awesome TensorFlow List](https://github.com/jtoy/awesome-tensorflow)\n        \n        - [TensorFlow Book](https://github.com/BinRoot/TensorFlow-Book)\n        \n        - [Android TensorFlow Machine Learning Example](https://blog.mindorks.com/android-tensorflow-machine-learning-example-ff0e9b2654cc)\n        \n            - [GitHub Repo](https://github.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample)\n        - [Creating Custom Model For Android Using TensorFlow](https://blog.mindorks.com/creating-custom-model-for-android-using-tensorflow-3f963d270bfb)\n            - [GitHub Repo](https://github.com/MindorksOpenSource/AndroidTensorFlowMNISTExample)            \n\n<a name="feed" />\n\n- Feed Forward Networks\n\n    - [A Quick Introduction to Neural Networks](https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/)\n    \n    - [Implementing a Neural Network from scratch](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/), [Code](https://github.com/dennybritz/nn-from-scratch)\n    \n    - [Speeding up your Neural Network with Theano and the gpu](http://www.wildml.com/2015/09/speeding-up-your-neural-network-with-theano-and-the-gpu/), [Code](https://github.com/dennybritz/nn-theano)\n    \n    - [Basic ANN Theory](https://takinginitiative.wordpress.com/2008/04/03/basic-neural-network-tutorial-theory/)\n    \n    - [Role of Bias in Neural Networks](http://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks)\n    \n    - [Choosing number of hidden layers and nodes](http://stackoverflow.com/questions/3345079/estimating-the-number-of-neurons-and-number-of-layers-of-an-artificial-neural-ne),[2](http://stackoverflow.com/questions/10565868/multi-layer-perceptron-mlp-architecture-criteria-for-choosing-number-of-hidde?lq=1),[3](http://stackoverflow.com/questions/9436209/how-to-choose-number-of-hidden-layers-and-nodes-in-neural-network/2#)\n    \n    - [Backpropagation in Matrix Form](http://sudeepraja.github.io/Neural/)\n    \n    - [ANN implemented in C++ | AI Junkie](http://www.ai-junkie.com/ann/evolved/nnt6.html)\n    \n    - [Simple Implementation](http://stackoverflow.com/questions/15395835/simple-multi-layer-neural-network-implementation)\n    \n    - [NN for Beginners](http://www.codeproject.com/Articles/16419/AI-Neural-Network-for-beginners-Part-of)\n    \n    - [Regression and Classification with NNs (Slides)](http://www.autonlab.org/tutorials/neural13.pdf)\n    \n    - [Another Intro](http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.html)\n\n<a name="rnn" />\n\n- Recurrent and LSTM Networks\n    - [awesome-rnn: list of resources (GitHub Repo)](https://github.com/kjw0612/awesome-rnn)\n    \n    - [Recurrent Neural Net Tutorial Part 1](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/), [Part 2](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/), [Part 3](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/), [Code](https://github.com/dennybritz/rnn-tutorial-rnnlm/)\n    \n    - [NLP RNN Representations](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)\n    \n    - [The Unreasonable effectiveness of RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), [Torch Code](https://github.com/karpathy/char-rnn), [Python Code](https://gist.github.com/karpathy/d4dee566867f8291f086)\n    \n    - [Intro to RNN](http://deeplearning4j.org/recurrentnetwork.html), [LSTM](http://deeplearning4j.org/lstm.html)\n    \n    - [An application of RNN](http://hackaday.com/2015/10/15/73-computer-scientists-created-a-neural-net-and-you-wont-believe-what-happened-next/)\n    \n    - [Optimizing RNN Performance](http://svail.github.io/)\n    \n    - [Simple RNN](http://outlace.com/Simple-Recurrent-Neural-Network/)\n    \n    - [Auto-Generating Clickbait with RNN](https://larseidnes.com/2015/10/13/auto-generating-clickbait-with-recurrent-neural-networks/)\n    \n    - [Sequence Learning using RNN (Slides)](http://www.slideshare.net/indicods/general-sequence-learning-with-recurrent-neural-networks-for-next-ml)\n    \n    - [Machine Translation using RNN (Paper)](http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf)\n    \n    - [Music generation using RNNs (Keras)](https://github.com/MattVitelli/GRUV)\n    \n    - [Using RNN to create on-the-fly dialogue (Keras)](http://neuralniche.com/post/tutorial/)\n    \n    - Long Short Term Memory (LSTM)\n    \n        - [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n        \n        - [LSTM explained](https://apaszke.github.io/lstm-explained.html)\n        \n        - [Beginnerâ€™s Guide to LSTM](http://deeplearning4j.org/lstm.html)\n        \n        - [Implementing LSTM from scratch](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/), [Python/Theano code](https://github.com/dennybritz/rnn-tutorial-gru-lstm)\n        \n        - [Torch Code for character-level language models using LSTM](https://github.com/karpathy/char-rnn)\n        \n        - [LSTM for Kaggle EEG Detection competition (Torch Code)](https://github.com/apaszke/kaggle-grasp-and-lift)\n        \n        - [LSTM for Sentiment Analysis in Theano](http://deeplearning.net/tutorial/lstm.html#lstm)\n        \n        - [Deep Learning for Visual Q&A | LSTM | CNN](http://avisingh599.github.io/deeplearning/visual-qa/), [Code](https://github.com/avisingh599/visual-qa)\n        \n        - [Computer Responds to email using LSTM | Google](http://googleresearch.blogspot.in/2015/11/computer-respond-to-this-email.html)\n        \n        - [LSTM dramatically improves Google Voice Search](http://googleresearch.blogspot.ch/2015/09/google-voice-search-faster-and-more.html), [Another Article](http://deeplearning.net/2015/09/30/long-short-term-memory-dramatically-improves-google-voice-etc-now-available-to-a-billion-users/)\n        \n        - [Understanding Natural Language with LSTM Using Torch](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/)\n        \n        - [Torch code for Visual Question Answering using a CNN+LSTM model](https://github.com/abhshkdz/neural-vqa)\n        \n        - [LSTM for Human Activity Recognition](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition/)\n        \n    - Gated Recurrent Units (GRU)\n    \n        - [LSTM vs GRU](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)\n    \n    - [Time series forecasting with Sequence-to-Sequence (seq2seq) rnn models](https://github.com/guillaume-chevalier/seq2seq-signal-prediction)\n\n\n<a name="rnn2" />\n\n- [Recursive Neural Network (not Recurrent)](https://en.wikipedia.org/wiki/Recursive_neural_network)\n\n    - [Recursive Neural Tensor Network (RNTN)](http://deeplearning4j.org/recursiveneuraltensornetwork.html)\n    \n    - [word2vec, DBN, RNTN for Sentiment Analysis ](http://deeplearning4j.org/zh-sentiment_analysis_word2vec.html)\n\n<a name="rbm" />\n\n- Restricted Boltzmann Machine\n\n    - [Beginner''s Guide about RBMs](http://deeplearning4j.org/restrictedboltzmannmachine.html)\n    \n    - [Another Good Tutorial](http://deeplearning.net/tutorial/rbm.html)\n    \n    - [Introduction to RBMs](http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/)\n    \n    - [Hinton''s Guide to Training RBMs](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)\n    \n    - [RBMs in R](https://github.com/zachmayer/rbm)\n    \n    - [Deep Belief Networks Tutorial](http://deeplearning4j.org/deepbeliefnetwork.html)\n    \n    - [word2vec, DBN, RNTN for Sentiment Analysis ](http://deeplearning4j.org/zh-sentiment_analysis_word2vec.html)\n\n<a name="auto" />\n\n- Autoencoders: Unsupervised (applies BackProp after setting target = input)\n\n    - [Andrew Ng Sparse Autoencoders pdf](https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf)\n    \n    - [Deep Autoencoders Tutorial](http://deeplearning4j.org/deepautoencoder.html)\n    \n    - [Denoising Autoencoders](http://deeplearning.net/tutorial/dA.html), [Theano Code](http://deeplearning.net/tutorial/code/dA.py)\n    \n    - [Stacked Denoising Autoencoders](http://deeplearning.net/tutorial/SdA.html#sda)\n\n\n<a name="cnn" />\n\n- Convolutional Neural Networks\n\n    - [An Intuitive Explanation of Convolutional Neural Networks](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)\n    \n    - [Awesome Deep Vision: List of Resources (GitHub)](https://github.com/kjw0612/awesome-deep-vision)\n    \n    - [Intro to CNNs](http://deeplearning4j.org/convolutionalnets.html)\n    \n    - [Understanding CNN for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)\n    \n    - [Stanford Notes](http://vision.stanford.edu/teaching/cs231n/), [Codes](http://cs231n.github.io/), [GitHub](https://github.com/cs231n/cs231n.github.io)\n    \n    - [JavaScript Library (Browser Based) for CNNs](http://cs.stanford.edu/people/karpathy/convnetjs/)\n    \n    - [Using CNNs to detect facial keypoints](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)\n    \n    - [Deep learning to classify business photos at Yelp](http://engineeringblog.yelp.com/2015/10/how-we-use-deep-learning-to-classify-business-photos-at-yelp.html)\n    \n    - [Interview with Yann LeCun | Kaggle](http://blog.kaggle.com/2014/12/22/convolutional-nets-and-cifar-10-an-interview-with-yan-lecun/)\n    \n    - [Visualising and Understanding CNNs](https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf)\n\n<a name="nrl" />\n\n- Network Representation Learning\n\n    - [Awesome Graph Embedding](https://github.com/benedekrozemberczki/awesome-graph-embedding)\n    \n    - [Awesome Network Embedding](https://github.com/chihming/awesome-network-embedding)\n    \n    - [Network Representation Learning Papers](https://github.com/thunlp)\n    \n    - [Knowledge Representation Learning Papers](https://github.com/thunlp/KRLPapers)\n    \n    - [Graph Based Deep Learning Literature](https://github.com/naganandy/graph-based-deep-learning-literature)\n\n<a name="nlp" />\n\n## Natural Language Processing\n\n- [A curated list of speech and natural language processing resources](https://github.com/edobashira/speech-language-processing)\n\n- [Understanding Natural Language with Deep Neural Networks Using Torch](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/)\n\n- [tf-idf explained](http://michaelerasm.us/post/tf-idf-in-10-minutes/)\n\n- [Interesting Deep Learning NLP Projects Stanford](http://cs224d.stanford.edu/reports.html), [Website](http://cs224d.stanford.edu/)\n\n- [The Stanford NLP Group](https://nlp.stanford.edu/)\n\n- [NLP from Scratch | Google Paper](https://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/35671.pdf)\n\n- [Graph Based Semi Supervised Learning for NLP](http://graph-ssl.wdfiles.com/local--files/blog%3A_start/graph_ssl_acl12_tutorial_slides_final.pdf)\n\n- [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model)\n\n    - [Classification text with Bag of Words](http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/)\n    \n<a name="topic" />\n\n- Topic Modeling\n    - [Topic Modeling Wikipedia](https://en.wikipedia.org/wiki/Topic_model) \n    - [**Probabilistic Topic Models Princeton PDF**](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf)\n\n    - [LDA Wikipedia](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation), [LSA Wikipedia](https://en.wikipedia.org/wiki/Latent_semantic_analysis), [Probabilistic LSA Wikipedia](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis)\n    \n    - [What is a good explanation of Latent Dirichlet Allocation (LDA)?](https://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation)\n    \n    - [**Introduction to LDA**](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/), [Another good explanation](http://confusedlanguagetech.blogspot.in/2012/07/jordan-boyd-graber-and-philip-resnik.html)\n    \n    - [The LDA Buffet - Intuitive Explanation](http://www.matthewjockers.net/2011/09/29/the-lda-buffet-is-now-open-or-latent-dirichlet-allocation-for-english-majors/)\n    \n    - [Your Guide to Latent Dirichlet Allocation (LDA)](https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d)\n    \n    - [Difference between LSI and LDA](https://www.quora.com/Whats-the-difference-between-Latent-Semantic-Indexing-LSI-and-Latent-Dirichlet-Allocation-LDA)\n    \n    - [Original LDA Paper](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf)\n    \n    - [alpha and beta in LDA](http://datascience.stackexchange.com/questions/199/what-does-the-alpha-and-beta-hyperparameters-contribute-to-in-latent-dirichlet-a)\n    \n    - [Intuitive explanation of the Dirichlet distribution](https://www.quora.com/What-is-an-intuitive-explanation-of-the-Dirichlet-distribution)\n    - [topicmodels: An R Package for Fitting Topic Models](https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf)\n\n    - [Topic modeling made just simple enough](https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/)\n    \n    - [Online LDA](http://alexminnaar.com/online-latent-dirichlet-allocation-the-best-option-for-topic-modeling-with-large-data-sets.html), [Online LDA with Spark](http://alexminnaar.com/distributed-online-latent-dirichlet-allocation-with-apache-spark.html)\n    \n    - [LDA in Scala](http://alexminnaar.com/latent-dirichlet-allocation-in-scala-part-i-the-theory.html), [Part 2](http://alexminnaar.com/latent-dirichlet-allocation-in-scala-part-ii-the-code.html)\n    \n    - [Segmentation of Twitter Timelines via Topic Modeling](https://alexisperrier.com/nlp/2015/09/16/segmentation_twitter_timelines_lda_vs_lsa.html)\n    \n    - [Topic Modeling of Twitter Followers](http://alexperrier.github.io/jekyll/update/2015/09/04/topic-modeling-of-twitter-followers.html)\n\n    - [Multilingual Latent Dirichlet Allocation (LDA)](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA). ([Tutorial here](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA/blob/master/Multilingual-LDA-Pipeline-Tutorial.ipynb))\n\n    - [Deep Belief Nets for Topic Modeling](https://github.com/larsmaaloee/deep-belief-nets-for-topic-modeling)\n    - [Gaussian LDA for Topic Models with Word Embeddings](http://www.cs.cmu.edu/~rajarshd/papers/acl2015.pdf)\n    - Python\n        - [Series of lecture notes for probabilistic topic models written in ipython notebook](https://github.com/arongdari/topic-model-lecture-note)\n        - [Implementation of various topic models in Python](https://github.com/arongdari/python-topic-model)\n           \n<a name="word2vec" />\n\n- word2vec\n\n    - [Google word2vec](https://code.google.com/archive/p/word2vec)\n    \n    - [Bag of Words Model Wiki](https://en.wikipedia.org/wiki/Bag-of-words_model)\n    \n    - [word2vec Tutorial](https://rare-technologies.com/word2vec-tutorial/)\n    \n    - [A closer look at Skip Gram Modeling](http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf)\n    \n    - [Skip Gram Model Tutorial](http://alexminnaar.com/word2vec-tutorial-part-i-the-skip-gram-model.html), [CBoW Model](http://alexminnaar.com/word2vec-tutorial-part-ii-the-continuous-bag-of-words-model.html)\n    \n    - [Word Vectors Kaggle Tutorial Python](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors), [Part 2](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors)\n    \n    - [Making sense of word2vec](http://rare-technologies.com/making-sense-of-word2vec/)\n    \n    - [word2vec explained on deeplearning4j](http://deeplearning4j.org/word2vec.html)\n    \n    - [Quora word2vec](https://www.quora.com/How-does-word2vec-work)\n    \n    - [Other Quora Resources](https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures-in-laymans-terms), [2](https://www.quora.com/What-is-the-difference-between-the-Bag-of-Words-model-and-the-Continuous-Bag-of-Words-model), [3](https://www.quora.com/Is-skip-gram-negative-sampling-better-than-CBOW-NS-for-word2vec-If-so-why)\n    \n    - [word2vec, DBN, RNTN for Sentiment Analysis ](http://deeplearning4j.org/zh-sentiment_analysis_word2vec.html)\n\n- Text Clustering\n\n    - [How string clustering works](http://stackoverflow.com/questions/8196371/how-clustering-works-especially-string-clustering)\n    \n    - [Levenshtein distance for measuring the difference between two sequences](https://en.wikipedia.org/wiki/Levenshtein_distance)\n    \n    - [Text clustering with Levenshtein distances](http://stackoverflow.com/questions/21511801/text-clustering-with-levenshtein-distances)\n\n- Text Classification\n\n    - [Classification Text with Bag of Words](http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/)\n\n- Named Entity Recognitation \n    \n     - [Stanford Named Entity Recognizer (NER)](https://nlp.stanford.edu/software/CRF-NER.shtml)\n\n     - [Named Entity Recognition: Applications and Use Cases- Towards Data Science](https://towardsdatascience.com/named-entity-recognition-applications-and-use-cases-acdbf57d595e)\n	\n- [Language learning with NLP and reinforcement learning](http://blog.dennybritz.com/2015/09/11/reimagining-language-learning-with-nlp-and-reinforcement-learning/)\n\n- [Kaggle Tutorial Bag of Words and Word vectors](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words), [Part 2](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors), [Part 3](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors)\n\n- [What would Shakespeare say (NLP Tutorial)](https://gigadom.wordpress.com/2015/10/02/natural-language-processing-what-would-shakespeare-say/)\n\n- [A closer look at Skip Gram Modeling](http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf)\n\n<a name="vision" />\n\n## Computer Vision\n- [Awesome computer vision (github)](https://github.com/jbhuang0604/awesome-computer-vision)\n\n- [Awesome deep vision (github)](https://github.com/kjw0612/awesome-deep-vision)\n\n\n<a name="svm" />\n\n## Support Vector Machine\n\n- [Highest Voted Questions about SVMs on Cross Validated](http://stats.stackexchange.com/questions/tagged/svm)\n\n- [Help me Understand SVMs!](http://stats.stackexchange.com/questions/3947/help-me-understand-support-vector-machines)\n\n- [SVM in Layman''s terms](https://www.quora.com/What-does-support-vector-machine-SVM-mean-in-laymans-terms)\n\n- [How does SVM Work | Comparisons](http://stats.stackexchange.com/questions/23391/how-does-a-support-vector-machine-svm-work)\n\n- [A tutorial on SVMs](http://alex.smola.org/papers/2003/SmoSch03b.pdf)\n\n- [Practical Guide to SVC](http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf), [Slides](http://www.csie.ntu.edu.tw/~cjlin/talks/freiburg.pdf)\n\n- [Introductory Overview of SVMs](http://www.statsoft.com/Textbook/Support-Vector-Machines)\n\n- Comparisons\n\n    - [SVMs > ANNs](http://stackoverflow.com/questions/6699222/support-vector-machines-better-than-artificial-neural-networks-in-which-learn?rq=1), [ANNs > SVMs](http://stackoverflow.com/questions/11632516/what-are-advantages-of-artificial-neural-networks-over-support-vector-machines), [Another Comparison](http://www.svms.org/anns.html)\n    \n    - [Trees > SVMs](http://stats.stackexchange.com/questions/57438/why-is-svm-not-so-good-as-decision-tree-on-the-same-data)\n    \n    - [Kernel Logistic Regression vs SVM](http://stats.stackexchange.com/questions/43996/kernel-logistic-regression-vs-svm)\n    \n    - [Logistic Regression vs SVM](http://stats.stackexchange.com/questions/58684/regularized-logistic-regression-and-support-vector-machine), [2](http://stats.stackexchange.com/questions/95340/svm-v-s-logistic-regression), [3](https://www.quora.com/Support-Vector-Machines/What-is-the-difference-between-Linear-SVMs-and-Logistic-Regression)\n    \n- [Optimization Algorithms in Support Vector Machines](http://pages.cs.wisc.edu/~swright/talks/sjw-complearning.pdf)\n\n- [Variable Importance from SVM](http://stats.stackexchange.com/questions/2179/variable-importance-from-svm)\n\n- Software\n\n    - [LIBSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)\n    \n    - [Intro to SVM in R](http://cbio.ensmp.fr/~jvert/svn/tutorials/practical/svmbasic/svmbasic_notes.pdf)\n    \n- Kernels\n    - [What are Kernels in ML and SVM?](https://www.quora.com/What-are-Kernels-in-Machine-Learning-and-SVM)\n    \n    - [Intuition Behind Gaussian Kernel in SVMs?](https://www.quora.com/Support-Vector-Machines/What-is-the-intuition-behind-Gaussian-kernel-in-SVM)\n    \n- Probabilities post SVM\n\n    - [Platt''s Probabilistic Outputs for SVM](http://www.csie.ntu.edu.tw/~htlin/paper/doc/plattprob.pdf)\n    \n    - [Platt Calibration Wiki](https://en.wikipedia.org/wiki/Platt_scaling)\n    \n    - [Why use Platts Scaling](http://stats.stackexchange.com/questions/5196/why-use-platts-scaling)\n    \n    - [Classifier Classification with Platt''s Scaling](http://fastml.com/classifier-calibration-with-platts-scaling-and-isotonic-regression/)\n\n\n<a name="rl" />\n\n## Reinforcement Learning\n\n- [Awesome Reinforcement Learning (GitHub)](https://github.com/aikorea/awesome-rl)\n\n- [RL Tutorial Part 1](http://outlace.com/Reinforcement-Learning-Part-1/), [Part 2](http://outlace.com/Reinforcement-Learning-Part-2/)\n\n<a name="dt" />\n\n## Decision Trees\n\n- [Wikipedia Page - Lots of Good Info](https://en.wikipedia.org/wiki/Decision_tree_learning)\n\n- [FAQs about Decision Trees](http://stats.stackexchange.com/questions/tagged/cart)\n\n- [Brief Tour of Trees and Forests](https://statistical-research.com/index.php/2013/04/29/a-brief-tour-of-the-trees-and-forests/)\n\n- [Tree Based Models in R](http://www.statmethods.net/advstats/cart.html)\n\n- [How Decision Trees work?](http://www.aihorizon.com/essays/generalai/decision_trees.htm)\n\n- [Weak side of Decision Trees](http://stats.stackexchange.com/questions/1292/what-is-the-weak-side-of-decision-trees)\n\n- [Thorough Explanation and different algorithms](http://www.ise.bgu.ac.il/faculty/liorr/hbchap9.pdf)\n\n- [What is entropy and information gain in the context of building decision trees?](http://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain)\n\n- [Slides Related to Decision Trees](http://www.slideshare.net/pierluca.lanzi/machine-learning-and-data-mining-11-decision-trees)\n\n- [How do decision tree learning algorithms deal with missing values?](http://stats.stackexchange.com/questions/96025/how-do-decision-tree-learning-algorithms-deal-with-missing-values-under-the-hoo)\n\n- [Using Surrogates to Improve Datasets with Missing Values](https://www.salford-systems.com/videos/tutorials/tips-and-tricks/using-surrogates-to-improve-datasets-with-missing-values)\n\n- [Good Article](https://www.mindtools.com/dectree.html)\n\n- [Are decision trees almost always binary trees?](http://stats.stackexchange.com/questions/12187/are-decision-trees-almost-always-binary-trees)\n\n- [Pruning Decision Trees](https://en.wikipedia.org/wiki/Pruning_(decision_trees)), [Grafting of Decision Trees](https://en.wikipedia.org/wiki/Grafting_(decision_trees))\n\n- [What is Deviance in context of Decision Trees?](http://stats.stackexchange.com/questions/6581/what-is-deviance-specifically-in-cart-rpart)\n\n- [Discover structure behind data with decision trees](http://vooban.com/en/tips-articles-geek-stuff/discover-structure-behind-data-with-decision-trees/) - Grow and plot a decision tree to automatically figure out hidden rules in your data\n\n- Comparison of Different Algorithms\n\n    - [CART vs CTREE](http://stats.stackexchange.com/questions/12140/conditional-inference-trees-vs-traditional-decision-trees)\n    \n    - [Comparison of complexity or performance](https://stackoverflow.com/questions/9979461/different-decision-tree-algorithms-with-comparison-of-complexity-or-performance)\n    \n    - [CHAID vs CART](http://stats.stackexchange.com/questions/61230/chaid-vs-crt-or-cart) , [CART vs CHAID](http://www.bzst.com/2006/10/classification-trees-cart-vs-chaid.html)\n    \n    - [Good Article on comparison](http://www.ftpress.com/articles/article.aspx?p=2248639&seqNum=11)\n    \n- CART\n\n    - [Recursive Partitioning Wikipedia](https://en.wikipedia.org/wiki/Recursive_partitioning)\n    \n    - [CART Explained](http://documents.software.dell.com/Statistics/Textbook/Classification-and-Regression-Trees)\n    \n    - [How to measure/rank â€œvariable importanceâ€ when using CART?](http://stats.stackexchange.com/questions/6478/how-to-measure-rank-variable-importance-when-using-cart-specifically-using)\n    \n    - [Pruning a Tree in R](http://stackoverflow.com/questions/15318409/how-to-prune-a-tree-in-r)\n    \n    - [Does rpart use multivariate splits by default?](http://stats.stackexchange.com/questions/4356/does-rpart-use-multivariate-splits-by-default)\n    \n    - [FAQs about Recursive Partitioning](http://stats.stackexchange.com/questions/tagged/rpart)\n    \n- CTREE\n\n    - [party package in R](https://cran.r-project.org/web/packages/party/party.pdf)\n    \n    - [Show volumne in each node using ctree in R](http://stackoverflow.com/questions/13772715/show-volume-in-each-node-using-ctree-plot-in-r)\n    \n    - [How to extract tree structure from ctree function?](http://stackoverflow.com/questions/8675664/how-to-extract-tree-structure-from-ctree-function)\n    \n- CHAID\n\n    - [Wikipedia Artice on CHAID](https://en.wikipedia.org/wiki/CHAID)\n    \n    - [Basic Introduction to CHAID](https://smartdrill.com/Introduction-to-CHAID.html)\n    \n    - [Good Tutorial on CHAID](http://www.statsoft.com/Textbook/CHAID-Analysis)\n    \n- MARS\n\n    - [Wikipedia Article on MARS](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines)\n    \n- Probabilistic Decision Trees\n\n    - [Bayesian Learning in Probabilistic Decision Trees](http://www.stats.org.uk/bayesian/Jordan.pdf)\n    \n    - [Probabilistic Trees Research Paper](http://people.stern.nyu.edu/adamodar/pdfiles/papers/probabilistic.pdf)\n\n<a name="rf" />\n\n## Random Forest / Bagging\n\n- [Awesome Random Forest (GitHub)**](https://github.com/kjw0612/awesome-random-forest)\n\n- [How to tune RF parameters in practice?](https://www.kaggle.com/forums/f/15/kaggle-forum/t/4092/how-to-tune-rf-parameters-in-practice)\n\n- [Measures of variable importance in random forests](http://stats.stackexchange.com/questions/12605/measures-of-variable-importance-in-random-forests)\n\n- [Compare R-squared from two different Random Forest models](http://stats.stackexchange.com/questions/13869/compare-r-squared-from-two-different-random-forest-models)\n\n- [OOB Estimate Explained | RF vs LDA](https://stat.ethz.ch/education/semesters/ss2012/ams/slides/v10.2.pdf)\n\n- [Evaluating Random Forests for Survival Analysis Using Prediction Error Curve](https://www.jstatsoft.org/index.php/jss/article/view/v050i11)\n\n- [Why doesn''t Random Forest handle missing values in predictors?](http://stats.stackexchange.com/questions/98953/why-doesnt-random-forest-handle-missing-values-in-predictors)\n\n- [How to build random forests in R with missing (NA) values?](http://stackoverflow.com/questions/8370455/how-to-build-random-forests-in-r-with-missing-na-values)\n\n- [FAQs about Random Forest](http://stats.stackexchange.com/questions/tagged/random-forest), [More FAQs](http://stackoverflow.com/questions/tagged/random-forest)\n\n- [Obtaining knowledge from a random forest](http://stats.stackexchange.com/questions/21152/obtaining-knowledge-from-a-random-forest)\n\n- [Some Questions for R implementation](http://stackoverflow.com/questions/20537186/getting-predictions-after-rfimpute), [2](http://stats.stackexchange.com/questions/81609/whether-preprocessing-is-needed-before-prediction-using-finalmodel-of-randomfore), [3](http://stackoverflow.com/questions/17059432/random-forest-package-in-r-shows-error-during-prediction-if-there-are-new-fact)\n\n<a name="gbm" />\n\n## Boosting\n\n- [Boosting for Better Predictions](http://www.datasciencecentral.com/profiles/blogs/boosting-algorithms-for-better-predictions)\n\n- [Boosting Wikipedia Page](https://en.wikipedia.org/wiki/Boosting_(machine_learning))\n\n- [Introduction to Boosted Trees | Tianqi Chen](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)\n\n- Gradient Boosting Machine\n\n    - [Gradiet Boosting Wiki](https://en.wikipedia.org/wiki/Gradient_boosting)\n    \n    - [Guidelines for GBM parameters in R](http://stats.stackexchange.com/questions/25748/what-are-some-useful-guidelines-for-gbm-parameters), [Strategy to set parameters](http://stats.stackexchange.com/questions/35984/strategy-to-set-the-gbm-parameters)\n    \n    - [Meaning of Interaction Depth](http://stats.stackexchange.com/questions/16501/what-does-interaction-depth-mean-in-gbm), [2](http://stats.stackexchange.com/questions/16501/what-does-interaction-depth-mean-in-gbm)\n    \n    - [Role of n.minobsinnode parameter of GBM in R](http://stats.stackexchange.com/questions/30645/role-of-n-minobsinnode-parameter-of-gbm-in-r)\n    \n    - [GBM in R](http://www.slideshare.net/mark_landry/gbm-package-in-r)\n    \n    - [FAQs about GBM](http://stats.stackexchange.com/tags/gbm/hot)\n    \n    - [GBM vs xgboost](https://www.kaggle.com/c/higgs-boson/forums/t/9497/r-s-gbm-vs-python-s-xgboost)\n\n- xgboost\n\n    - [xgboost tuning kaggle](https://www.kaggle.com/khozzy/rossmann-store-sales/xgboost-parameter-tuning-template/log)\n    \n    - [xgboost vs gbm](https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13012/question-to-experienced-kagglers-and-anyone-who-wants-to-take-a-shot/68296#post68296)\n    \n    - [xgboost survey](https://www.kaggle.com/c/higgs-boson/forums/t/10335/xgboost-post-competition-survey)\n    \n    - [Practical XGBoost in Python online course (free)](http://education.parrotprediction.teachable.com/courses/practical-xgboost-in-python)\n    \n- AdaBoost\n\n    - [AdaBoost Wiki](https://en.wikipedia.org/wiki/AdaBoost), [Python Code](https://gist.github.com/tristanwietsma/5486024)\n    \n    - [AdaBoost Sparse Input Support](http://hamzehal.blogspot.com/2014/06/adaboost-sparse-input-support.html)\n    \n    - [adaBag R package](https://cran.r-project.org/web/packages/adabag/adabag.pdf)\n    \n    - [Tutorial](http://math.mit.edu/~rothvoss/18.304.3PM/Presentations/1-Eric-Boosting304FinalRpdf.pdf)\n\n- CatBoost\n\n    - [CatBoost Documentation](https://catboost.ai/docs/)\n\n    - [Benchmarks](https://catboost.ai/#benchmark)\n\n    - [Tutorial](https://github.com/catboost/tutorials)\n\n    - [GitHub Project](https://github.com/catboost)\n\n    - [CatBoost vs. Light GBM vs. XGBoost](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)\n\n<a name="ensem" />\n\n## Ensembles\n\n- [Wikipedia Article on Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning)\n\n- [Kaggle Ensembling Guide](http://mlwave.com/kaggle-ensembling-guide/)\n\n- [The Power of Simple Ensembles](http://www.overkillanalytics.net/more-is-always-better-the-power-of-simple-ensembles/)\n\n- [Ensemble Learning Intro](http://machine-learning.martinsewell.com/ensembles/)\n\n- [Ensemble Learning Paper](http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf)\n\n- [Ensembling models with R](http://amunategui.github.io/blending-models/), [Ensembling Regression Models in R](http://stats.stackexchange.com/questions/26790/ensembling-regression-models), [Intro to Ensembles in R](http://www.vikparuchuri.com/blog/intro-to-ensemble-learning-in-r/)\n\n- [Ensembling Models with caret](http://stats.stackexchange.com/questions/27361/stacking-ensembling-models-with-caret)\n\n- [Bagging vs Boosting vs Stacking](http://stats.stackexchange.com/questions/18891/bagging-boosting-and-stacking-in-machine-learning)\n\n- [Good Resources | Kaggle Africa Soil Property Prediction](https://www.kaggle.com/c/afsis-soil-properties/forums/t/10391/best-ensemble-references)\n\n- [Boosting vs Bagging](http://www.chioka.in/which-is-better-boosting-or-bagging/)\n\n- [Resources for learning how to implement ensemble methods](http://stats.stackexchange.com/questions/32703/resources-for-learning-how-to-implement-ensemble-methods)\n\n- [How are classifications merged in an ensemble classifier?](http://stats.stackexchange.com/questions/21502/how-are-classifications-merged-in-an-ensemble-classifier)\n\n<a name="stack" />\n\n## Stacking Models\n\n- [Stacking, Blending and Stacked Generalization](http://www.chioka.in/stacking-blending-and-stacked-generalization/)\n\n- [Stacked Generalization (Stacking)](http://machine-learning.martinsewell.com/ensembles/stacking/)\n\n- [Stacked Generalization: when does it work?](http://www.ijcai.org/Proceedings/97-2/011.pdf)\n\n- [Stacked Generalization Paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.1533&rep=rep1&type=pdf)\n\n<a name="vc" />\n\n## Vapnikâ€“Chervonenkis Dimension\n\n- [Wikipedia article on VC Dimension](https://en.wikipedia.org/wiki/VC_dimension)\n\n- [Intuitive Explanantion of VC Dimension](https://www.quora.com/Explain-VC-dimension-and-shattering-in-lucid-Way)\n\n- [Video explaining VC Dimension](https://www.youtube.com/watch?v=puDzy2XmR5c)\n\n- [Introduction to VC Dimension](http://www.svms.org/vc-dimension/)\n\n- [FAQs about VC Dimension](http://stats.stackexchange.com/questions/tagged/vc-dimension)\n\n- [Do ensemble techniques increase VC-dimension?](http://stats.stackexchange.com/questions/78076/do-ensemble-techniques-increase-vc-dimension)\n\n\n<a name="bayes" />\n\n## Bayesian Machine Learning\n\n- [Bayesian Methods for Hackers (using pyMC)](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)\n\n- [Should all Machine Learning be Bayesian?](http://videolectures.net/bark08_ghahramani_samlbb/)\n\n- [Tutorial on Bayesian Optimisation for Machine Learning](http://www.iro.umontreal.ca/~bengioy/cifar/NCAP2014-summerschool/slides/Ryan_adams_140814_bayesopt_ncap.pdf)\n\n- [Bayesian Reasoning and Deep Learning](http://blog.shakirm.com/2015/10/bayesian-reasoning-and-deep-learning/), [Slides](http://blog.shakirm.com/wp-content/uploads/2015/10/Bayes_Deep.pdf)\n\n- [Bayesian Statistics Made Simple](http://greenteapress.com/wp/think-bayes/)\n\n- [Kalman & Bayesian Filters in Python](https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python)\n\n- [Markov Chain Wikipedia Page](https://en.wikipedia.org/wiki/Markov_chain)\n\n\n<a name="semi" />\n\n## Semi Supervised Learning\n\n- [Wikipedia article on Semi Supervised Learning](https://en.wikipedia.org/wiki/Semi-supervised_learning)\n\n- [Tutorial on Semi Supervised Learning](http://pages.cs.wisc.edu/~jerryzhu/pub/sslicml07.pdf)\n\n- [Graph Based Semi Supervised Learning for NLP](http://graph-ssl.wdfiles.com/local--files/blog%3A_start/graph_ssl_acl12_tutorial_slides_final.pdf)\n\n- [Taxonomy](http://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/taxo_[0].pdf)\n\n- [Video Tutorial Weka](https://www.youtube.com/watch?v=sWxcIjZFGNM)\n\n- [Unsupervised, Supervised and Semi Supervised learning](http://stats.stackexchange.com/questions/517/unsupervised-supervised-and-semi-supervised-learning)\n\n- [Research Papers 1](http://mlg.eng.cam.ac.uk/zoubin/papers/zglactive.pdf), [2](http://mlg.eng.cam.ac.uk/zoubin/papers/zgl.pdf), [3](http://icml.cc/2012/papers/616.pdf)\n\n\n<a name="opt" />\n\n## Optimization\n\n- [Mean Variance Portfolio Optimization with R and Quadratic Programming](http://www.wdiam.com/2012/06/10/mean-variance-portfolio-optimization-with-r-and-quadratic-programming/?utm_content=buffer04c12&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer)\n\n- [Algorithms for Sparse Optimization and Machine Learning](http://www.ima.umn.edu/2011-2012/W3.26-30.12/activities/Wright-Steve/sjw-ima12)\n\n- [Optimization Algorithms in Machine Learning](http://pages.cs.wisc.edu/~swright/nips2010/sjw-nips10.pdf), [Video Lecture](http://videolectures.net/nips2010_wright_oaml/)\n\n- [Optimization Algorithms for Data Analysis](http://www.birs.ca/workshops/2011/11w2035/files/Wright.pdf)\n\n- [Video Lectures on Optimization](http://videolectures.net/stephen_j_wright/)\n\n- [Optimization Algorithms in Support Vector Machines](http://pages.cs.wisc.edu/~swright/talks/sjw-complearning.pdf)\n\n- [The Interplay of Optimization and Machine Learning Research](http://jmlr.org/papers/volume7/MLOPT-intro06a/MLOPT-intro06a.pdf)\n\n- [Hyperopt tutorial for Optimizing Neural Networksâ€™ Hyperparameters](http://vooban.com/en/tips-articles-geek-stuff/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters/)\n\n\n<a name="other" />\n\n## Other Tutorials\n\n- For a collection of Data Science Tutorials using R, please refer to [this list](https://github.com/ujjwalkarn/DataScienceR).\n\n- For a collection of Data Science Tutorials using Python, please refer to [this list](https://github.com/ujjwalkarn/DataSciencePython).\n', '{"language":null,"stars":17250,"forks":3964,"watchers":17250,"open_issues":43,"topics":["awesome","awesome-list","deep-learning","deep-learning-tutorial","deep-neural-networks","deeplearning","list","machine-learning","machinelearning","neural-network","neural-networks"],"default_branch":"master","size_kb":9024,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:sindresorhus:awesome","source_url":"https://github.com/sindresorhus/awesome"},{"type":"has_code","target_id":"github:sindresorhus:awesome","source_url":"https://github.com/sindresorhus/awesome"},{"type":"has_code","target_id":"github:ujjwalkarn:Machine-Learning-Tutorials","source_url":"https://github.com/ujjwalkarn/Machine-Learning-Tutorials"},{"type":"has_code","target_id":"github:ujjwalkarn:DataScienceR","source_url":"https://github.com/ujjwalkarn/DataScienceR"},{"type":"has_code","target_id":"github:ujjwalkarn:DataSciencePython","source_url":"https://github.com/ujjwalkarn/DataSciencePython"},{"type":"has_code","target_id":"github:dair-ai:ML-YouTube-Courses","source_url":"https://github.com/dair-ai/ML-YouTube-Courses"},{"type":"has_code","target_id":"github:prakhar1989:awesome-courses","source_url":"https://github.com/prakhar1989/awesome-courses#machine-learning"},{"type":"has_code","target_id":"github:ZuzooVn:machine-learning-for-software-engineers","source_url":"https://github.com/ZuzooVn/machine-learning-for-software-engineers"},{"type":"has_code","target_id":"github:hangtwenty:dive-into-machine-learning","source_url":"https://github.com/hangtwenty/dive-into-machine-learning"},{"type":"has_code","target_id":"github:josephmisiti:awesome-machine-learning","source_url":"https://github.com/josephmisiti/awesome-machine-learning"},{"type":"has_code","target_id":"github:fasouto:awesome-dataviz","source_url":"https://github.com/fasouto/awesome-dataviz"},{"type":"has_code","target_id":"github:okulbilisim:awesome-datascience","source_url":"https://github.com/okulbilisim/awesome-datascience"},{"type":"has_code","target_id":"github:pedrosan:TheAnalyticsEdge","source_url":"https://github.com/pedrosan/TheAnalyticsEdge"},{"type":"has_code","target_id":"github:humphd:have-fun-with-machine-learning","source_url":"https://github.com/humphd/have-fun-with-machine-learning"},{"type":"has_code","target_id":"github:owainlewis:awesome-artificial-intelligence","source_url":"https://github.com/owainlewis/awesome-artificial-intelligence"},{"type":"has_code","target_id":"github:trevorstephens:gplearn","source_url":"https://github.com/trevorstephens/gplearn"},{"type":"has_code","target_id":"github:rouseguy:intro2stats","source_url":"https://github.com/rouseguy/intro2stats"},{"type":"has_code","target_id":"github:ethen8181:machine-learning","source_url":"https://github.com/ethen8181/machine-learning"},{"type":"has_code","target_id":"github:soulmachine:machine-learning-cheat-sheet","source_url":"https://github.com/soulmachine/machine-learning-cheat-sheet"},{"type":"has_code","target_id":"github:ChristosChristofidis:awesome-deep-learning","source_url":"https://github.com/ChristosChristofidis/awesome-deep-learning"},{"type":"has_code","target_id":"github:floodsung:Deep-Learning-Papers-Reading-Roadmap","source_url":"https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap"},{"type":"has_code","target_id":"github:rouseguy:intro2deeplearning","source_url":"https://github.com/rouseguy/intro2deeplearning"},{"type":"has_code","target_id":"github:THUNLP-MT:MT-Reading-List","source_url":"https://github.com/THUNLP-MT/MT-Reading-List#machine-translation-reading-list"},{"type":"has_code","target_id":"github:lisa-lab:DeepLearningTutorials","source_url":"https://github.com/lisa-lab/DeepLearningTutorials"},{"type":"has_code","target_id":"github:vict0rsch:deep_learning","source_url":"https://github.com/vict0rsch/deep_learning"},{"type":"has_code","target_id":"github:torch:tutorials","source_url":"https://github.com/torch/tutorials"},{"type":"has_code","target_id":"github:chetannaik:learning_torch","source_url":"https://github.com/chetannaik/learning_torch"},{"type":"has_code","target_id":"github:carpedm20:awesome-torch","source_url":"https://github.com/carpedm20/awesome-torch"},{"type":"has_code","target_id":"github:torch:torch7","source_url":"https://github.com/torch/torch7"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:chiphuyen:tf-stanford-tutorials","source_url":"https://github.com/chiphuyen/tf-stanford-tutorials"},{"type":"has_code","target_id":"github:tensorflow:skflow","source_url":"https://github.com/tensorflow/skflow"},{"type":"has_code","target_id":"github:chetannaik:learning_tensorflow","source_url":"https://github.com/chetannaik/learning_tensorflow"},{"type":"has_code","target_id":"github:soumith:convnet-benchmarks","source_url":"https://github.com/soumith/convnet-benchmarks"},{"type":"has_code","target_id":"github:jtoy:awesome-tensorflow","source_url":"https://github.com/jtoy/awesome-tensorflow"},{"type":"has_code","target_id":"github:BinRoot:TensorFlow-Book","source_url":"https://github.com/BinRoot/TensorFlow-Book"},{"type":"has_code","target_id":"github:MindorksOpenSource:AndroidTensorFlowMachineLearningExample","source_url":"https://github.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample"},{"type":"has_code","target_id":"github:MindorksOpenSource:AndroidTensorFlowMNISTExample","source_url":"https://github.com/MindorksOpenSource/AndroidTensorFlowMNISTExample"},{"type":"has_code","target_id":"github:dennybritz:nn-from-scratch","source_url":"https://github.com/dennybritz/nn-from-scratch"},{"type":"has_code","target_id":"github:dennybritz:nn-theano","source_url":"https://github.com/dennybritz/nn-theano"},{"type":"has_code","target_id":"github:kjw0612:awesome-rnn","source_url":"https://github.com/kjw0612/awesome-rnn"},{"type":"has_code","target_id":"github:dennybritz:rnn-tutorial-rnnlm","source_url":"https://github.com/dennybritz/rnn-tutorial-rnnlm"},{"type":"has_code","target_id":"github:karpathy:char-rnn","source_url":"https://github.com/karpathy/char-rnn"},{"type":"has_code","target_id":"github:MattVitelli:GRUV","source_url":"https://github.com/MattVitelli/GRUV"},{"type":"has_code","target_id":"github:dennybritz:rnn-tutorial-gru-lstm","source_url":"https://github.com/dennybritz/rnn-tutorial-gru-lstm"},{"type":"has_code","target_id":"github:karpathy:char-rnn","source_url":"https://github.com/karpathy/char-rnn"},{"type":"has_code","target_id":"github:apaszke:kaggle-grasp-and-lift","source_url":"https://github.com/apaszke/kaggle-grasp-and-lift"},{"type":"has_code","target_id":"github:avisingh599:visual-qa","source_url":"https://github.com/avisingh599/visual-qa"},{"type":"has_code","target_id":"github:abhshkdz:neural-vqa","source_url":"https://github.com/abhshkdz/neural-vqa"},{"type":"has_code","target_id":"github:guillaume-chevalier:LSTM-Human-Activity-Recognition","source_url":"https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition"},{"type":"has_code","target_id":"github:guillaume-chevalier:seq2seq-signal-prediction","source_url":"https://github.com/guillaume-chevalier/seq2seq-signal-prediction"},{"type":"has_code","target_id":"github:zachmayer:rbm","source_url":"https://github.com/zachmayer/rbm"},{"type":"has_code","target_id":"github:kjw0612:awesome-deep-vision","source_url":"https://github.com/kjw0612/awesome-deep-vision"},{"type":"has_code","target_id":"github:cs231n:cs231n.github.io","source_url":"https://github.com/cs231n/cs231n.github.io"},{"type":"has_code","target_id":"github:benedekrozemberczki:awesome-graph-embedding","source_url":"https://github.com/benedekrozemberczki/awesome-graph-embedding"},{"type":"has_code","target_id":"github:chihming:awesome-network-embedding","source_url":"https://github.com/chihming/awesome-network-embedding"},{"type":"has_code","target_id":"github:thunlp:KRLPapers","source_url":"https://github.com/thunlp/KRLPapers"},{"type":"has_code","target_id":"github:naganandy:graph-based-deep-learning-literature","source_url":"https://github.com/naganandy/graph-based-deep-learning-literature"},{"type":"has_code","target_id":"github:edobashira:speech-language-processing","source_url":"https://github.com/edobashira/speech-language-processing"},{"type":"has_code","target_id":"github:ArtificiAI:Multilingual-Latent-Dirichlet-Allocation-LDA","source_url":"https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA"},{"type":"has_code","target_id":"github:ArtificiAI:Multilingual-Latent-Dirichlet-Allocation-LDA","source_url":"https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA"},{"type":"has_code","target_id":"github:larsmaaloee:deep-belief-nets-for-topic-modeling","source_url":"https://github.com/larsmaaloee/deep-belief-nets-for-topic-modeling"},{"type":"has_code","target_id":"github:arongdari:topic-model-lecture-note","source_url":"https://github.com/arongdari/topic-model-lecture-note"},{"type":"has_code","target_id":"github:arongdari:python-topic-model","source_url":"https://github.com/arongdari/python-topic-model"},{"type":"has_code","target_id":"github:jbhuang0604:awesome-computer-vision","source_url":"https://github.com/jbhuang0604/awesome-computer-vision"},{"type":"has_code","target_id":"github:kjw0612:awesome-deep-vision","source_url":"https://github.com/kjw0612/awesome-deep-vision"},{"type":"has_code","target_id":"github:aikorea:awesome-rl","source_url":"https://github.com/aikorea/awesome-rl"},{"type":"has_code","target_id":"github:kjw0612:awesome-random-forest","source_url":"https://github.com/kjw0612/awesome-random-forest"},{"type":"has_code","target_id":"github:catboost:tutorials","source_url":"https://github.com/catboost/tutorials"},{"type":"has_code","target_id":"github:CamDavidsonPilon:Probabilistic-Programming-and-Bayesian-Methods-for-Hackers","source_url":"https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers"},{"type":"has_code","target_id":"github:rlabbe:Kalman-and-Bayesian-Filters-in-Python","source_url":"https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python"},{"type":"has_code","target_id":"github:ujjwalkarn:DataScienceR","source_url":"https://github.com/ujjwalkarn/DataScienceR"},{"type":"has_code","target_id":"github:ujjwalkarn:DataSciencePython","source_url":"https://github.com/ujjwalkarn/DataSciencePython"}]', NULL, 'CC0-1.0', 'approved', 80, '224a9c5fed89db501db5947318d8e208', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-ujjwalkarn-Machine-Learning-Tutorials from https://github.com/ujjwalkarn.png
Image converted to WebP: data/images/github-ujjwalkarn-Machine-Learning-Tutorials.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-dair-ai-ML-YouTube-Courses', 'github--dair-ai--ml-youtube-courses', 'ML-YouTube-Courses', 'dair-ai', 'At DAIR.AI we â¤ï¸ open AI education. In this repo, we index and organize some of the best and most recent machine learning courses available on YouTube. **Machine Learning** - Caltech CS156: Learning from Data - Stanford CS229: Machine Learning - Making Friends with Machine Learning - Applied Machine Learning - Introduction to Machine Learning (TÃ¼bingen) - Machine Learning Lecture (Stefan Harmeling) - Statistical Machine Learning (TÃ¼bingen) - Probabilistic Machine Learning - MIT 6.S897: Machin...', '["ai","data-science","deep-learning","machine-learning","natural-language-processing","nlp"]', 'other', 17048, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/dair-ai/ML-YouTube-Courses","fetched_at":"2025-12-08T10:39:52.044Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# ğŸ“º ML YouTube Courses\n\nAt DAIR.AI we â¤ï¸ open AI education. In this repo, we index and organize some of the best and most recent machine learning courses available on YouTube.\n\n**Machine Learning**\n\n- [Caltech CS156: Learning from Data](#caltech-cs156-learning-from-data)\n- [Stanford CS229: Machine Learning](#stanford-cs229-machine-learning)\n- [Making Friends with Machine Learning](#making-friends-with-machine-learning)\n- [Applied Machine Learning](#applied-machine-learning)\n- [Introduction to Machine Learning (TÃ¼bingen)](#introduction-to-machine-learning-TÃ¼bingen)\n- [Machine Learning Lecture (Stefan Harmeling)](#machine-learning-lecture-stefan-harmeling)\n- [Statistical Machine Learning (TÃ¼bingen)](#statistical-machine-learning-TÃ¼bingen)\n- [Probabilistic Machine Learning](#probabilistic-machine-learning)\n- [MIT 6.S897: Machine Learning for Healthcare (2019)](#mit-6s897-machine-learning-for-healthcare-2019)\n\n**Deep Learning**\n\n- [Neural Networks: Zero to Hero](#neural-networks-zero-to-hero-by-andrej-karpathy)\n- [MIT: Deep Learning for Art, Aesthetics, and Creativity](#mit-deep-learning-for-art-aesthetics-and-creativity)\n- [Stanford CS230: Deep Learning (2018)](#stanford-cs230-deep-learning-2018)\n- [Introduction to Deep Learning (MIT)](#introduction-to-deep-learning)\n- [CMU Introduction to Deep Learning (11-785)](#cmu-introduction-to-deep-learning-11-785)\n- [Deep Learning: CS 182](#deep-learning-cs-182)\n- [Deep Unsupervised Learning](#deep-unsupervised-learning)\n- [NYU Deep Learning SP21](#nyu-deep-learning-sp21)\n- [Foundation Models](#foundation-models)\n- [Deep Learning (TÃ¼bingen)](#deep-learning-TÃ¼bingen)\n\n**Scientific Machine Learning**\n\n- [Parallel Computing and Scientific Machine Learning](#parallel-computing-and-scientific-machine-learning)\n\n**Practical Machine Learning**\n\n- [LLMOps: Building Real-World Applications With Large Language Models](#llmops-building-real-world-applications-with-large-language-models)\n- [Evaluating and Debugging Generative AI](#evaluating-and-debugging-generative-ai)\n- [ChatGPT Prompt Engineering for Developers](#chatgpt-prompt-engineering-for-developers)\n- [LangChain for LLM Application Development](#langchain-for-llm-application-development)\n- [LangChain: Chat with Your Data](#langchain-chat-with-your-data)\n- [Building Systems with the ChatGPT API](#building-systems-with-the-chatgpt-api)\n- [LangChain & Vector Databases in Production](#langchain--vector-databases-in-production)\n- [Building LLM-Powered Apps](#building-llm-powered-apps)\n- [Full Stack LLM Bootcamp](#full-stack-llm-bootcamp)\n- [Full Stack Deep Learning](#full-stack-deep-learning)\n- [Practical Deep Learning for Coders](#practical-deep-learning-for-coders)\n- [Stanford MLSys Seminars](#stanford-mlsys-seminars)\n- [Machine Learning Engineering for Production (MLOps)](#machine-learning-engineering-for-production-mlops)\n- [MIT Introduction to Data-Centric AI](#mit-introduction-to-data-centric-ai)\n\n**Natural Language Processing**\n\n- [XCS224U: Natural Language Understanding (2023)](#xcs224u-natural-language-understanding-2023)\n- [Stanford CS25 - Transformers United](#stanford-cs25---transformers-united)\n- [NLP Course (Hugging Face)](#nlp-course-hugging-face)\n- [CS224N: Natural Language Processing with Deep Learning](#cs224n-natural-language-processing-with-deep-learning)\n- [CMU Neural Networks for NLP](#cmu-neural-networks-for-nlp)\n- [CS224U: Natural Language Understanding](#cs224u-natural-language-understanding)\n- [CMU Advanced NLP 2021/2022/2024](#cmu-advanced-nlp)\n- [Multilingual NLP](#multilingual-nlp)\n- [Advanced NLP](#advanced-nlp)\n\n**Computer Vision**\n\n- [CS231N: Convolutional Neural Networks for Visual Recognition](#cs231n-convolutional-neural-networks-for-visual-recognition)\n- [Deep Learning for Computer Vision](#deep-learning-for-computer-vision)\n- [Deep Learning for Computer Vision (DL4CV)](#deep-learning-for-computer-vision-dl4cv)\n- [Deep Learning for Computer Vision (neuralearn.ai)](#deep-learning-for-computer-vision-neuralearnai)\n\n**Reinforcement Learning**\n\n- [Deep Reinforcement Learning](#deep-reinforcement-learning)\n- [Reinforcement Learning Lecture Series (DeepMind)](#reinforcement-learning-lecture-series-deepmind)\n- [Reinforcement Learning (Polytechnique Montreal, Fall 2021)](#reinforcement-learning-polytechnique-montreal-fall-2021)\n- [Foundations of Deep RL](#foundations-of-deep-rl)\n- [Stanford CS234: Reinforcement Learning](#stanford-cs234-reinforcement-learning)\n\n**Graph Machine Learning**\n\n- [Machine Learning with Graphs (Stanford)](#machine-learning-with-graphs-stanford)\n- [AMMI Geometric Deep Learning Course](#ammi-geometric-deep-learning-course)\n\n**Multi-Task Learning**\n\n- [Multi-Task and Meta-Learning (Stanford)](#stanford-cs330-deep-multi-task-and-meta-learning)\n\n**Others**\n\n- [MIT Deep Learning in Life Sciences](#mit-deep-learning-in-life-sciences)\n- [Self-Driving Cars (TÃ¼bingen)](#self-driving-cars-TÃ¼bingen)\n- [Advanced Robotics (Berkeley)](#advanced-robotics-uc-berkeley)\n\n---\n\n## Caltech CS156: Learning from Data\n\nAn introductory course in machine learning that covers the basic theory, algorithms, and applications.\n\n- Lecture 1: The Learning Problem\n- Lecture 2: Is Learning Feasible?\n- Lecture 3: The Linear Model I\n- Lecture 4: Error and Noise\n- Lecture 5: Training versus Testing\n- Lecture 6: Theory of Generalization\n- Lecture 7: The VC Dimension\n- Lecture 8: Bias-Variance Tradeoff\n- Lecture 9: The Linear Model II\n- Lecture 10: Neural Networks\n- Lecture 11: Overfitting\n- Lecture 12: Regularization\n- Lecture 13: Validation\n- Lecture 14: Support Vector Machines\n- Lecture 15: Kernel Methods\n- Lecture 16: Radial Basis Functions\n- Lecture 17: Three Learning Principles\n- Lecture 18: Epilogue\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLD63A284B7615313A)\n\n## Stanford CS229: Machine Learning\n\nTo learn some of the basics of ML:\n\n- Linear Regression and Gradient Descent\n- Logistic Regression\n- Naive Bayes\n- SVMs\n- Kernels\n- Decision Trees\n- Introduction to Neural Networks\n- Debugging ML Models\n  ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)\n\n## Making Friends with Machine Learning\n\nA series of mini lectures covering various introductory topics in ML:\n\n- Explainability in AI\n- Classification vs. Regression\n- Precession vs. Recall\n- Statistical Significance\n- Clustering and K-means\n- Ensemble models\n  ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLRKtJ4IpxJpDxl0NTvNYQWKCYzHNuy2xG)\n\n## Neural Networks: Zero to Hero (by Andrej Karpathy)\n\nCourse providing an in-depth overview of neural networks.\n\n- Backpropagation\n- Spelled-out intro to Language Modeling\n- Activation and Gradients\n- Becoming a Backprop Ninja\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)\n\n## MIT: Deep Learning for Art, Aesthetics, and Creativity\n\nCovers the application of deep learning for art, aesthetics, and creativity.\n\n- Nostalgia -> Art -> Creativity -> Evolution as Data + Direction\n- Efficient GANs\n- Explorations in AI for Creativity\n- Neural Abstractions\n- Easy 3D Content Creation with Consistent Neural Fields\n  ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLCpMvp7ftsnIbNwRnQJbDNRqO6qiN3EyH)\n\n## Stanford CS230: Deep Learning (2018)\n\nCovers the foundations of deep learning, how to build different neural networks(CNNs, RNNs, LSTMs, etc...), how to lead machine learning projects, and career advice for deep learning practitioners.\n\n- Deep Learning Intuition\n- Adversarial examples - GANs\n- Full-cycle of a Deep Learning Project\n- AI and Healthcare\n- Deep Learning Strategy\n- Interpretability of Neural Networks\n- Career Advice and Reading Research Papers\n- Deep Reinforcement Learning\n\nğŸ”— [Link to Course](https://youtube.com/playlist?list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb) ğŸ”— [Link to Materials](https://cs230.stanford.edu/syllabus/)\n\n## Applied Machine Learning\n\nTo learn some of the most widely used techniques in ML:\n\n- Optimization and Calculus\n- Overfitting and Underfitting\n- Regularization\n- Monte Carlo Estimation\n- Maximum Likelihood Learning\n- Nearest Neighbours\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL2UML_KCiC0UlY7iCQDSiGDMovaupqc83)\n\n## Introduction to Machine Learning (TÃ¼bingen)\n\nThe course serves as a basic introduction to machine learning and covers key concepts in regression, classification, optimization, regularization, clustering, and dimensionality reduction.\n\n- Linear regression\n- Logistic regression\n- Regularization\n- Boosting\n- Neural networks\n- PCA\n- Clustering\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL05umP7R6ij35ShKLDqccJSDntugY4FQT)\n\n## Machine Learning Lecture (Stefan Harmeling)\n\nCovers many fundamental ML concepts:\n\n- Bayes rule\n- From logic to probabilities\n- Distributions\n- Matrix Differential Calculus\n- PCA\n- K-means and EM\n- Causality\n- Gaussian Processes\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLzrCXlf6ypbxS5OYOY3EN_0u2fDuIT6Gt)\n\n## Statistical Machine Learning (TÃ¼bingen)\n\nThe course covers the standard paradigms and algorithms in statistical machine learning.\n\n- KNN\n- Bayesian decision theory\n- Convex optimization\n- Linear and ridge regression\n- Logistic regression\n- SVM\n- Random Forests\n- Boosting\n- PCA\n- Clustering\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL05umP7R6ij2XCvrRzLokX6EoHWaGA2cC)\n\n## Practical Deep Learning for Coders\n\nThis course covers topics such as how to:\n\n- Build and train deep learning models for computer vision, natural language processing, tabular analysis, and collaborative filtering problems\n- Create random forests and regression models\n- Deploy models\n- Use PyTorch, the worldâ€™s fastest growing deep learning software, plus popular libraries like fastai and Hugging Face\n- Foundations and Deep Dive to Diffusion Models\n- ...\n\nğŸ”— [Link to Course - Part 1](https://www.youtube.com/playlist?list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU)\n\nğŸ”— [Link to Course - Part 2](https://www.youtube.com/watch?v=_7rMfsA24Ls&ab_channel=JeremyHoward)\n\n## Stanford MLSys Seminars\n\nA seminar series on all sorts of topics related to building machine learning systems.\n\nğŸ”— [Link to Lectures](https://www.youtube.com/playlist?list=PLSrTvUm384I9PV10koj_cqit9OfbJXEkq)\n\n## Machine Learning Engineering for Production (MLOps)\n\nSpecialization course on MLOPs by Andrew Ng.\n\nğŸ”— [Link to Lectures](https://www.youtube.com/playlist?list=PLkDaE6sCZn6GMoA0wbpJLi3t34Gd8l0aK)\n\n## MIT Introduction to Data-Centric AI\n\nCovers the emerging science of Data-Centric AI (DCAI) that studies techniques to improve datasets, which is often the best way to improve performance in practical ML applications. Topics include:\n\n- Data-Centric AI vs. Model-Centric AI\n- Label Errors\n- Dataset Creation and Curation\n- Data-centric Evaluation of ML Models\n- Class Imbalance, Outliers, and Distribution Shift\n- ...\n\nğŸ”— [Course Website](https://dcai.csail.mit.edu/)\n\nğŸ”— [Lecture Videos](https://www.youtube.com/watch?v=ayzOzZGHZy4&list=PLnSYPjg2dHQKdig0vVbN-ZnEU0yNJ1mo5)\n\nğŸ”— [Lab Assignments](https://github.com/dcai-course/dcai-lab)\n\n## Machine Learning with Graphs (Stanford)\n\nTo learn some of the latest graph techniques in machine learning:\n\n- PageRank\n- Matrix Factorizing\n- Node Embeddings\n- Graph Neural Networks\n- Knowledge Graphs\n- Deep Generative Models for Graphs\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn)\n\n## Probabilistic Machine Learning\n\nTo learn the probabilistic paradigm of ML:\n\n- Reasoning about uncertainty\n- Continuous Variables\n- Sampling\n- Markov Chain Monte Carlo\n- Gaussian Distributions\n- Graphical Models\n- Tuning Inference Algorithms\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL05umP7R6ij2YE8rRJSb-olDNbntAQ_Bx)\n\n## MIT 6.S897: Machine Learning for Healthcare (2019)\n\nThis course introduces students to machine learning in healthcare, including the nature of clinical data and the use of machine learning for risk stratification, disease progression modeling, precision medicine, diagnosis, subtype discovery, and improving clinical workflows.\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLUl4u3cNGP60B0PQXVQyGNdCyCTDU1Q5j)\n\n## Introduction to Deep Learning\n\nTo learn some of the fundamentals of deep learning:\n\n- Introduction to Deep Learning\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI)\n\n## CMU Introduction to Deep Learning (11-785)\n\nThe course starts off gradually from MLPs (Multi Layer Perceptrons) and then progresses into concepts like attention\nand sequence-to-sequence models.\n\nğŸ”— [Link to Course](https://deeplearning.cs.cmu.edu/F22/index.html) \\nğŸ”— [Lectures](https://www.youtube.com/playlist?list=PLp-0K3kfddPxRmjgjm0P1WT6H-gTqE8j9) \\nğŸ”— [Tutorials/Recitations](https://www.youtube.com/playlist?list=PLp-0K3kfddPz8WXg8RqH0sEN6X2L65HUZ)\n\n## Deep Learning: CS 182\n\nTo learn some of the widely used techniques in deep learning:\n\n- Machine Learning Basics\n- Error Analysis\n- Optimization\n- Backpropagation\n- Initialization\n- Batch Normalization\n- Style transfer\n- Imitation Learning\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A)\n\n## Deep Unsupervised Learning\n\nTo learn the latest and most widely used techniques in deep unsupervised learning:\n\n- Autoregressive Models\n- Flow Models\n- Latent Variable Models\n- Self-supervised learning\n- Implicit Models\n- Compression\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLwRJQ4m4UJjPiJP3691u-qWwPGVKzSlNP)\n\n## NYU Deep Learning SP21\n\nTo learn some of the advanced techniques in deep learning:\n\n- Neural Nets: rotation and squashing\n- Latent Variable Energy Based Models\n- Unsupervised Learning\n- Generative Adversarial Networks\n- Autoencoders\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI)\n\n## Foundation Models\n\nTo learn about foundation models like GPT-3, CLIP, Flamingo, Codex, and DINO.\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL9t0xVFP90GD8hox0KipBkJcLX_C3ja67)\n\n## Deep Learning (TÃ¼bingen)\n\nThis course introduces the practical and theoretical principles of deep neural networks.\n\n- Computation graphs\n- Activation functions and loss functions\n- Training, regularization and data augmentation\n- Basic and state-of-the-art deep neural network architectures including convolutional networks and graph neural networks\n- Deep generative models such as auto-encoders, variational auto-encoders and generative adversarial networks\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL05umP7R6ij3NTWIdtMbfvX7Z-4WEXRqD)\n\n## Parallel Computing and Scientific Machine Learning\n\n- The Basics of Scientific Simulators\n- Introduction to Parallel Computing\n- Continuous Dynamics\n- Inverse Problems and Differentiable Programming\n- Distributed Parallel Computing\n- Physics-Informed Neural Networks and Neural Differential Equations\n- Probabilistic Programming, AKA Bayesian Estimation on Programs\n- Globalizing the Understanding of Models\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLCAl7tjCwWyGjdzOOnlbGnVNZk0kB8VSa)\n\n## XCS224U: Natural Language Understanding (2023)\n\nThis course covers topics such as:\n\n- Contextual Word Representations\n- Information Retrieval\n- In-context learning\n- Behavioral Evaluation of NLU models\n- NLP Methods and Metrics\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLoROMvodv4rOwvldxftJTmoR3kRcWkJBp)\n\n## Stanford CS25 - Transformers United\n\nThis course consists of lectures focused on Transformers, providing a deep dive and their applications\n\n- Introduction to Transformers\n- Transformers in Language: GPT-3, Codex\n- Applications in Vision\n- Transformers in RL & Universal\n  Compute Engines\n- Scaling transformers\n- Interpretability with transformers\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM)\n\n## NLP Course (Hugging Face)\n\nLearn about different NLP concepts and how to apply language models and Transformers to NLP:\n\n- What is Transfer Learning?\n- BPE Tokenization\n- Batching inputs\n- Fine-tuning models\n- Text embeddings and semantic search\n- Model evaluation\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o)\n\n## CS224N: Natural Language Processing with Deep Learning\n\nTo learn the latest approaches for deep learning based NLP:\n\n- Dependency parsing\n- Language models and RNNs\n- Question Answering\n- Transformers and pretraining\n- Natural Language Generation\n- T5 and Large Language Models\n- Future of NLP\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)\n\n## CMU Neural Networks for NLP\n\nTo learn the latest neural network based techniques for NLP:\n\n- Language Modeling\n- Efficiency tricks\n- Conditioned Generation\n- Structured Prediction\n- Model Interpretation\n- Advanced Search Algorithms\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL8PYTP1V4I8AkaHEJ7lOOrlex-pcxS-XV)\n\n## CS224U: Natural Language Understanding\n\nTo learn the latest concepts in natural language understanding:\n\n- Grounded Language Understanding\n- Relation Extraction\n- Natural Language Inference (NLI)\n- NLU and Neural Information Extraction\n- Adversarial testing\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLoROMvodv4rPt5D0zs3YhbWSZA8Q_DyiJ)\n\n## CMU Advanced NLP\n\nTo learn:\n\n- Basics of modern NLP techniques\n- Multi-task, Multi-domain, multi-lingual learning\n- Prompting + Sequence-to-sequence pre-training\n- Interpreting and Debugging NLP Models\n- Learning from Knowledge-bases\n- Adversarial learning\n- ...\n\nğŸ”— [Link to 2021 Edition](https://www.youtube.com/playlist?list=PL8PYTP1V4I8AYSXn_GKVgwXVluCT9chJ6)\n\nğŸ”— [Link to 2022 Edition](https://www.youtube.com/playlist?list=PL8PYTP1V4I8D0UkqW2fEhgLrnlDW9QK7z)\n\nğŸ”— [Link to 2024 Edition](https://www.youtube.com/playlist?list=PL8PYTP1V4I8DZprnWryM4nR8IZl1ZXDjg)\n\n## Multilingual NLP\n\nTo learn the latest concepts for doing multilingual NLP:\n\n- Typology\n- Words, Part of Speech, and Morphology\n- Advanced Text Classification\n- Machine Translation\n- Data Augmentation for MT\n- Low Resource ASR\n- Active Learning\n- ...\n\nğŸ”— [Link to 2020 Course](https://www.youtube.com/playlist?list=PL8PYTP1V4I8CHhppU6n1Q9-04m96D9gt5)\n\nğŸ”— [Link to 2022 Course](https://www.youtube.com/playlist?list=PL8PYTP1V4I8BhCpzfdKKdd1OnTfLcyZr7)\n\n## Advanced NLP\n\nTo learn advanced concepts in NLP:\n\n- Attention Mechanisms\n- Transformers\n- BERT\n- Question Answering\n- Model Distillation\n- Vision + Language\n- Ethics in NLP\n- Commonsense Reasoning\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLWnsVgP6CzadmQX6qevbar3_vDBioWHJL)\n\n## CS231N: Convolutional Neural Networks for Visual Recognition\n\nStanford''s Famous CS231n course. The videos are only available for the Spring 2017 semester. The course is currently known as Deep Learning for Computer Vision, but the Spring 2017 version is titled Convolutional Neural Networks for Visual Recognition.\n\n- Image Classification\n- Loss Functions and Optimization\n- Introduction to Neural Networks\n- Convolutional Neural Networks\n- Training Neural Networks\n- Deep Learning Software\n- CNN Architectures\n- Recurrent Neural Networks\n- Detection and Segmentation\n- Visualizing and Understanding\n- Generative Models\n- Deep Reinforcement Learning\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) ğŸ”— [Link to Materials](http://cs231n.stanford.edu/2017/)\n\n## Deep Learning for Computer Vision\n\nTo learn some of the fundamental concepts in CV:\n\n- Introduction to deep learning for CV\n- Image Classification\n- Convolutional Networks\n- Attention Networks\n- Detection and Segmentation\n- Generative Models\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r)\n\n## Deep Learning for Computer Vision (DL4CV)\n\nTo learn modern methods for computer vision:\n\n- CNNs\n- Advanced PyTorch\n- Understanding Neural Networks\n- RNN, Attention and ViTs\n- Generative Models\n- GPU Fundamentals\n- Self-Supervision\n- Neural Rendering\n- Efficient Architectures\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL_Z2_U9MIJdNgFM7-f2fZ9ZxjVRP_jhJv)\n\n## Deep Learning for Computer Vision (neuralearn.ai)\n\nTo learn modern methods for computer vision:\n\n- Self-Supervised Learning\n- Neural Rendering\n- Efficient Architectures\n- Machine Learning Operations (MLOps)\n- Modern Convolutional Neural Networks\n- Transformers in Vision\n- Model Deployment\n\nğŸ”— [Link to Course](https://www.youtube.com/watch?v=IA3WxTTPXqQ)\n\n## AMMI Geometric Deep Learning Course\n\nTo learn about concepts in geometric deep learning:\n\n- Learning in High Dimensions\n- Geometric Priors\n- Grids\n- Manifolds and Meshes\n- Sequences and Time Warping\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLn2-dEmQeTfSLXW8yXP4q_Ii58wFdxb3C)\n\n## Deep Reinforcement Learning\n\nTo learn the latest concepts in deep RL:\n\n- Intro to RL\n- RL algorithms\n- Real-world sequential decision making\n- Supervised learning of behaviors\n- Deep imitation learning\n- Cost functions and reward functions\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc)\n\n## Reinforcement Learning Lecture Series (DeepMind)\n\nThe Deep Learning Lecture Series is a collaboration between DeepMind and the UCL Centre for Artificial Intelligence.\n\n- Introduction to RL\n- Dynamic Programming\n- Model-free algorithms\n- Deep reinforcement learning\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm)\n\n\n## LLMOps: Building Real-World Applications With Large Language Models\n\nLearn to build modern software with LLMs using the newest tools and techniques in the field.\n\nğŸ”— [Link to Course](https://www.comet.com/site/llm-course/)\n\n## Evaluating and Debugging Generative AI\n\nYou''ll learn:\n\n- Instrument A Jupyter Notebook\n- Manage Hyperparameters Config\n- Log Run Metrics\n- Collect artifacts for dataset and model versioning\n- Log experiment results\n- Trace prompts and responses for LLMs\n- ...\n\nğŸ”— [Link to Course](https://www.deeplearning.ai/short-courses/evaluating-debugging-generative-ai/)\n\n## ChatGPT Prompt Engineering for Developers\n\nLearn how to use a large language model (LLM) to quickly build new and powerful applications.\n\nğŸ”— [Link to Course](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)\n\n## LangChain for LLM Application Development\n\nYou''ll learn:\n\n- Models, Prompt, and Parsers\n- Memories for LLMs\n- Chains\n- Question Answering over Documents\n- Agents\n\nğŸ”— [Link to Course](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)\n\n## LangChain: Chat with Your Data\n\nYou''ll learn about:\n\n- Document Loading\n- Document Splitting\n- Vector Stores and Embeddings\n- Retrieval\n- Question Answering\n- Chat\n\nğŸ”— [Link to Course](https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/)\n\n## Building Systems with the ChatGPT API\n\nLearn how to automate complex workflows using chain calls to a large language model.\n\nğŸ”— [Link to Course](https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/)\n\n## LangChain & Vector Databases in Production\n\nLearn how to use LangChain and Vector DBs in Production:\n\n- LLMs and LangChain\n- Learning how to Prompt\n- Keeping Knowledge Organized with Indexes\n- Combining Components Together with Chains\n- ...\n\nğŸ”— [Link to Course](https://learn.activeloop.ai/courses/langchain)\n\n## Building LLM-Powered Apps\n\nLearn how to build LLM-powered applications using LLM APIs\n\n- Unpacking LLM APIs\n- Building a Baseline LLM Application\n- Enhancing and Optimizing LLM Applications\n- ...\n\nğŸ”— [Link to Course](https://www.wandb.courses/courses/building-llm-powered-apps)\n\n## Full Stack LLM Bootcamp\n\nTo learn how to build and deploy LLM-powered applications:\n\n- Learn to Spell: Prompt Engineering\n- LLMOPs\n- UX for Language User Interfaces\n- Augmented Language Models\n- Launch an LLM App in One Hour\n- LLM Foundations\n- Project Walkthrough: askFSDL\n- ...\n\nğŸ”— [Link to Course](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/)\n\n## Full Stack Deep Learning\n\nTo learn full-stack production deep learning:\n\n- ML Projects\n- Infrastructure and Tooling\n- Experiment Managing\n- Troubleshooting DNNs\n- Data Management\n- Data Labeling\n- Monitoring ML Models\n- Web deployment\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL1T8fO7ArWlcWg04OgNiJy91PywMKT2lv)\n\n## Introduction to Deep Learning and Deep Generative Models\n\nCovers the fundamental concepts of deep learning\n\n- Single-layer neural networks and gradient descent\n- Multi-layer neural networks and backpropagation\n- Convolutional neural networks for images\n- Recurrent neural networks for text\n- Autoencoders, variational autoencoders, and generative adversarial networks\n- Encoder-decoder recurrent neural networks and transformers\n- PyTorch code examples\n\nğŸ”— [Link to Course](https://www.youtube.com/watch?v=1nqCZqDYPp0&list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51) ğŸ”— [Link to Materials](https://sebastianraschka.com/blog/2021/dl-course.html)\n\n## Self-Driving Cars (TÃ¼bingen)\n\nCovers the most dominant paradigms of self-driving cars: modular pipeline-based approaches as well as deep-learning based end-to-end driving techniques.\n\n- Camera, lidar and radar-based perception\n- Localization, navigation, path planning\n- Vehicle modeling/control\n- Deep Learning\n- Imitation learning\n- Reinforcement learning\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL05umP7R6ij321zzKXK6XCQXAaaYjQbzr)\n\n## Reinforcement Learning (Polytechnique Montreal, Fall 2021)\n\nDesigning autonomous decision making systems is one of the longstanding goals of Artificial Intelligence. Such decision making systems, if realized, can have a big impact in machine learning for robotics, game playing, control, health care to name a few. This course introduces Reinforcement Learning as a general framework to design such autonomous decision making systems.\n\n- Introduction to RL\n- Multi-armed bandits\n- Policy Gradient Methods\n- Contextual Bandits\n- Finite Markov Decision Process\n- Dynamic Programming\n- Policy Iteration, Value Iteration\n- Monte Carlo Methods\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLImtCgowF_ES_JdF_UcM60EXTcGZg67Ua) ğŸ”— [Link to Materials](https://chandar-lab.github.io/INF8953DE/)\n\n## Foundations of Deep RL\n\nA mini 6-lecture series by Pieter Abbeel.\n\n- MDPs, Exact Solution Methods, Max-ent RL\n- Deep Q-Learning\n- Policy Gradients and Advantage Estimation\n- TRPO and PPO\n- DDPG and SAC\n- Model-based RL\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0)\n\n## Stanford CS234: Reinforcement Learning\n\nCovers topics from basic concepts of Reinforcement Learning to more advanced ones:\n\n- Markov decision processes & planning\n- Model-free policy evaluation\n- Model-free control\n- Reinforcement learning with function approximation & Deep RL\n- Policy Search\n- Exploration\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u) ğŸ”— [Link to Materials](https://web.stanford.edu/class/cs234/)\n\n## Stanford CS330: Deep Multi-Task and Meta Learning\n\nThis is a graduate-level course covering different aspects of deep multi-task and meta learning.\n\n- Multi-task learning, transfer learning basics\n- Meta-learning algorithms\n- Advanced meta-learning topics\n- Multi-task RL, goal-conditioned RL\n- Meta-reinforcement learning\n- Hierarchical RL\n- Lifelong learning\n- Open problems\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5) ğŸ”— [Link to Materials](https://cs330.stanford.edu/)\n\n## MIT Deep Learning in Life Sciences\n\nA course introducing foundations of ML for applications in genomics and the life sciences more broadly.\n\n- Interpreting ML Models\n- DNA Accessibility, Promoters and Enhancers\n- Chromatin and gene regulation\n- Gene Expression, Splicing\n- RNA-seq, Splicing\n- Single cell RNA-sequencing\n- Dimensionality Reduction, Genetics, and Variation\n- Drug Discovery\n- Protein Structure Prediction\n- Protein Folding\n- Imaging and Cancer\n- Neuroscience\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLypiXJdtIca5ElZMWHl4HMeyle2AzUgVB)\n\nğŸ”— [Link to Materials](https://mit6874.github.io/)\n\n## Advanced Robotics: UC Berkeley\n\nThis is course is from Peter Abbeel and covers a review on reinforcement learning and continues to applications in robotics.\n\n- MDPs: Exact Methods\n- Discretization of Continuous State Space MDPs\n- Function Approximation / Feature-based Representations\n- LQR, iterative LQR / Differential Dynamic Programming\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNBPJdt8WamRAt4XKc639wF) ğŸ”— [Link to Materials](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa19/)\n\n---\n\nReach out on [Twitter](https://twitter.com/omarsar0) if you have any questions.\n\nIf you are interested to contribute, feel free to open a PR with a link to the course. It will take a bit of time, but I have plans to do many things with these individual lectures. We can summarize the lectures, include notes, provide additional reading material, include difficulty of content, etc.\n\nYou can now find ML Course notes [here](https://github.com/dair-ai/ML-Course-Notes).\n', '{"language":null,"stars":17048,"forks":2082,"watchers":17048,"open_issues":18,"topics":["ai","data-science","deep-learning","machine-learning","natural-language-processing","nlp"],"default_branch":"main","size_kb":114,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:dcai-course:dcai-lab","source_url":"https://github.com/dcai-course/dcai-lab"},{"type":"has_code","target_id":"github:dair-ai:ML-Course-Notes","source_url":"https://github.com/dair-ai/ML-Course-Notes"}]', NULL, 'CC0-1.0', 'approved', 80, 'b5de36445314c51d7f137232fa167576', NULL, NULL, CURRENT_TIMESTAMP);
