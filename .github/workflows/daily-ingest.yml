name: Daily Ingest

on:
  schedule:
    - cron: '0 3 * * *' # Daily at 03:00 UTC
  workflow_dispatch:

jobs:
  ingest:
    runs-on: ubuntu-latest
    steps:
      # Step 1: Checkout repository
      - name: Checkout repository
        uses: actions/checkout@v4

      # Step 2: Setup Node.js environment
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 20

      # Step 3: Install Dependencies (Node & Rust)
      - name: Install Dependencies
        run: |
          npm ci
          # Rust is pre-installed on ubuntu-latest

      # Step 4: V3.2 Universal Adapter System
      - name: Fetch & Enrich Model Data (V3.2 Pipeline)
        env:
          CF_PROXY_URL: ${{ secrets.CF_PROXY_URL }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "üöÄ Running V3.2 Universal Ingestion Pipeline..."
          node scripts/ingestion/orchestrator.js
          echo "‚úÖ Data fetched with full README, compliance checks, and quality scores"

      # Step 6: Process Data with Rust (Core Step)
      - name: Process Data (Rust)
        env:
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_PUBLIC_URL_PREFIX: "https://cdn.free2aitools.com"
        run: |
          echo "Building and running Rust optimizer..."
          cd tools/rust-img-optimizer
          cargo run --release -- --input ../../data/merged.json
          # Move SQL files to repository root data directory
          mv data/upsert.sql ../../data/upsert.sql
          mv data/update_urls.sql ../../data/update_urls.sql
          echo "‚úÖ Data processed. Generated upsert.sql and update_urls.sql"

      # Step 7: Upload Images to R2 (WebP format - V3.2)
      - name: Upload Images to R2
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "Uploading images to R2..."
          # V3.2: Upload WebP images (primary format)
          for file in data/images/*.webp; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              echo "Uploading $filename..."
              npx wrangler r2 object put "ai-nexus-assets/models/$filename" --file="$file"
            fi
          done
          # Legacy: Also upload any JPG files for backward compatibility
          for file in data/images/*.jpg; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              echo "Uploading $filename..."
              npx wrangler r2 object put "ai-nexus-assets/models/$filename" --file="$file"
            fi
          done
          echo "‚úÖ Images uploaded successfully"
      
      # Step 7b: Upload Docs to R2 (V3.1 Constitution Pillar III: Data Integrity)
      - name: Upload Docs to R2
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "üìÑ Uploading docs to R2 (full README storage)..."
          doc_count=0
          for file in data/docs/*.md; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              echo "Uploading $filename..."
              npx wrangler r2 object put "ai-nexus-assets/docs/$filename" --file="$file" --content-type="text/markdown"
              doc_count=$((doc_count + 1))
            fi
          done
          echo "‚úÖ Uploaded $doc_count docs to R2"
      
      # QUALITY GATE: Verify SQL contains image URLs
      - name: Validate SQL Output (Quality Gate)
        run: |
          echo "üîç Validating SQL quality..."
          
          # Check files exist
          if [ ! -s data/upsert.sql ]; then
            echo "‚ùå FATAL: upsert.sql is missing or empty!"
            exit 1
          fi
          if [ ! -s data/update_urls.sql ]; then
            echo "‚ùå FATAL: update_urls.sql is missing or empty!"
            exit 1
          fi
          
          # Strict check: At least 80% of UPDATE statements must have a valid image URL
          TOTAL_UPDATES=$(grep -c 'UPDATE models' data/update_urls.sql || echo 0)
          URL_UPDATES=$(grep -c 'https://cdn.free2aitools.com' data/update_urls.sql || echo 0)
          SUCCESS_RATE=0
          if [ "$TOTAL_UPDATES" -gt 0 ]; then
            SUCCESS_RATE=$((URL_UPDATES * 100 / TOTAL_UPDATES))
          fi

          echo " SQL Quality Metrics:"
          echo "  Total UPDATE statements: $TOTAL_UPDATES"
          echo "  With image URLs: $URL_UPDATES"
          echo "  Success Rate: $SUCCESS_RATE%"

          if [ "$SUCCESS_RATE" -lt 80 ]; then
            echo "‚ùå FATAL: Image URL success rate ($SUCCESS_RATE%) is below the 80% threshold."
            exit 1
          fi
          
          echo "‚úÖ Quality Gate PASSED: SQL contains valid image URLs"
        
      # Step 8: Execute D1 Updates (Direct - SQL is now small with R2 content storage)
      - name: Execute D1 Updates
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "üîÑ Executing D1 updates (R2 stores full content, D1 only metadata)..."
          
          echo "  üìù Executing upsert.sql..."
          npx wrangler d1 execute ai-nexus-db --remote --file="./data/upsert.sql"
          
          echo "  üìù Executing update_urls.sql..."
          npx wrangler d1 execute ai-nexus-db --remote --file="./data/update_urls.sql"
          
          echo "‚úÖ All D1 updates completed"


      # Step 9: Upload artifacts
      - name: Upload ingest artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ingest-logs
          path: |
            data/raw.json
            data/merged.json
            data/upsert.sql
            data/update_urls.sql
