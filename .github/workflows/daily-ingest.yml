name: Daily Ingest

on:
  schedule:
    - cron: '0 3 * * *' # Daily at 03:00 UTC
  workflow_dispatch:

jobs:
  ingest:
    runs-on: ubuntu-latest
    steps:
      # Step 1: Checkout repository
      - name: Checkout repository
        uses: actions/checkout@v4

      # Step 2: Setup Node.js environment
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 20

      # Step 3: Install Dependencies (Node & Rust)
      - name: Install Dependencies
        run: |
          npm ci
          # Rust is pre-installed on ubuntu-latest

      # Step 4: Fetch Metadata (Multi-Source)
      # Step 4: Fetch Metadata (Multi-Source)
      - name: Fetch Model Metadata
        env:
          CF_PROXY_URL: ${{ secrets.CF_PROXY_URL }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: node scripts/run-collectors.js

      # Step 5: Process Raw Data (Loop 1.5)
      - name: Process Raw Data (Loop 1.5)
        run: node scripts/process-raw-data.js

      # Step 6: Process Data with Rust (Core Step)
      - name: Diagnose R2 Connectivity
        run: |
          echo "--- Diagnosing R2 Endpoint Connectivity ---"
          ACCOUNT_ID=${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_URL="https://${ACCOUNT_ID}.r2.cloudflarestorage.com"
          echo "--- Running curl verbose connection test to ${R2_URL} ---"
          curl -v --fail --connect-timeout 15 "${R2_URL}"

      - name: Process Data (Rust)
        run: |
          echo "Building and running Rust optimizer..."
          (cd tools/rust-img-optimizer && cargo run --release -- --input ../../data/merged.json) > data/upsert.sql
          echo "‚úÖ Data processed and upsert.sql generated"
        env:
          R2_ACCESS_KEY: ${{ secrets.R2_ACCESS_KEY }}
          R2_SECRET_KEY: ${{ secrets.R2_SECRET_KEY }}
          R2_BUCKET: "ai-nexus-assets"
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_PUBLIC_URL_PREFIX: "https://cdn.free2aitools.com"
      
      # QUALITY GATE: Verify SQL contains image URLs
      - name: Validate SQL Output (Quality Gate)
        run: |
          echo "üîç Validating upsert.sql quality..."
          
          # Check file exists and has content
          if [ ! -s data/upsert.sql ]; then
            echo "‚ùå FATAL: upsert.sql is missing or empty!"
            exit 1
          fi
          
          # Strict check: At least 80% of INSERT statements must have a valid image URL
          TOTAL_INSERTS=$(grep -c 'INSERT OR REPLACE' data/upsert.sql || echo 0)
          URL_INSERTS=$(grep -c 'https://cdn.free2aitools.com' data/upsert.sql || echo 0)
          SUCCESS_RATE=0
          if [ "$TOTAL_INSERTS" -gt 0 ]; then
            SUCCESS_RATE=$((URL_INSERTS * 100 / TOTAL_INSERTS))
          fi

          echo " SQL Quality Metrics:"
          echo "  Total INSERT statements: $TOTAL_INSERTS"
          echo "  With image URLs: $URL_INSERTS"
          echo "  Success Rate: $SUCCESS_RATE%"

          if [ "$SUCCESS_RATE" -lt 80 ]; then
            echo "‚ùå FATAL: Image URL success rate ($SUCCESS_RATE%) is below the 80% threshold."
            exit 1
          fi
          
          echo "‚úÖ Quality Gate PASSED: SQL contains valid image URLs"
        
      - name: Execute D1 Updates
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          npx wrangler d1 execute ai-nexus-db --remote --file=./data/upsert.sql



      # Step 7: Upload artifacts
      - name: Upload ingest artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ingest-logs
          path: |
            data/raw.json
            data/merged.json
            data/upsert.sql
