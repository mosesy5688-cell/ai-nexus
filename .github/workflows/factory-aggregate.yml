# Factory 3/4 - Aggregate
# V14.5 Phase 4: Modular Factory Architecture
# 
# Triggers: workflow_run from Process, or manual dispatch
# Inputs: shard-* artifacts from Process stage
# Outputs: trending.json, search-*.json, sitemaps, rankings
# 
# Constitution: Art 3.1 (Aggregator), Art 5 (Weekly), Art 6.3 (Search)

name: Factory 3/4 - Aggregate

on:
  workflow_run:
    workflows: ["Factory 2/4 - Process"]
    types: [completed]
  workflow_dispatch:
    inputs:
      run_id:
        description: 'Run ID of Process workflow to get shard artifacts from'
        required: false
        type: string

jobs:
  aggregate:
    name: Aggregate Shard Results
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install Dependencies
        run: npm ci

      # Download all shard artifacts with merge
      - name: Download All Shard Artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: shard-*
          path: ./downloaded/
          merge-multiple: true
          run-id: ${{ inputs.run_id || github.event.workflow_run.id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}

      # Organize downloaded files
      - name: Organize Downloaded Files
        run: |
          mkdir -p artifacts output
          
          # Move shard JSON files to artifacts/
          find ./downloaded -name "shard-*.json" -exec mv {} artifacts/ \; 2>/dev/null || true
          
          # Merge output directories from all shards
          if [ -d "./downloaded/output" ]; then
            cp -r ./downloaded/output/* output/ 2>/dev/null || true
          fi
          
          # Also check for cache/ directories
          find ./downloaded -type d -name "cache" -exec cp -r {}/* output/ \; 2>/dev/null || true
          
          echo "ðŸ“‚ Artifacts structure:"
          ls -la artifacts/ || echo "No artifacts found"
          echo "ðŸ“‚ Output structure:"
          find output -type f | head -20 || echo "No output files"

      # Restore FNI history
      - name: Restore FNI History
        uses: actions/cache@v4
        with:
          path: cache/fni-history.json
          key: fni-history-${{ github.run_id }}
          restore-keys: |
            fni-history-

      # Restore weekly accumulator
      - name: Restore Weekly Accumulator
        uses: actions/cache@v4
        with:
          path: cache/weekly-accum.json
          key: weekly-accum-${{ github.run_id }}
          restore-keys: |
            weekly-accum-

      # Run aggregator
      - name: Run Aggregator
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          # V16.2.5: R2 secrets removed. Aggregator now saves state to output/ for 4/4 to upload.
        run: |
          echo "ðŸ”„ Running Aggregator..."
          node scripts/factory/aggregator.js

      # Validate critical files
      - name: Validate Output
        run: |
          echo "ðŸ” Validating output..."
          
          CRITICAL_FILES=(
            "output/cache/trending.json"
            "output/cache/search-core.json"
            "output/cache/category_stats.json"
          )
          
          MISSING=0
          for f in "${CRITICAL_FILES[@]}"; do
            if [ -f "$f" ]; then
              echo "âœ… $f"
            else
              echo "âŒ Missing: $f"
              MISSING=$((MISSING + 1))
            fi
          done
          
          if [ $MISSING -gt 0 ]; then
            echo "âš ï¸ Warning: $MISSING critical files missing"
          fi
          
          FILE_COUNT=$(find output -name "*.json" | wc -l)
          echo "ðŸ“Š Total JSON files: $FILE_COUNT"

      # Save updated caches
      - name: Save FNI History
        uses: actions/cache/save@v4
        with:
          path: cache/fni-history.json
          key: fni-history-${{ github.run_id }}

      - name: Save Weekly Accumulator
        uses: actions/cache/save@v4
        with:
          path: cache/weekly-accum.json
          key: weekly-accum-${{ github.run_id }}

      # V16.2.5: Compress output to handle 300k+ files (Upload Artifact limit workaround)
      - name: Compress Output
        run: |
          echo "ðŸ“¦ Compressing 300k+ files..."
          tar -czf factory-output.tar.gz output/
          echo "âœ… Compression complete: $(du -sh factory-output.tar.gz)"

      # Upload final output artifact (Compressed)
      - name: Upload Factory Output
        uses: actions/upload-artifact@v4
        with:
          name: factory-output
          path: factory-output.tar.gz
          retention-days: 7

      # V14.5.2: Save entity data to Cache for downstream workflows (factory-linker)
      - name: Save Entity Data to Cache (for Linker)
        uses: actions/cache/save@v4
        with:
          path: |
            output/entities.json
            output/cache
          key: entity-data-${{ github.run_id }}

      # Trigger next stage (via workflow_run - automatic)
      - name: Notify Upload Stage
        run: |
          echo "âœ… Aggregate complete. Factory 4/4 - Upload and Linker will start automatically."

      - name: Summary
        run: |
          echo "## Factory 3/4 - Aggregate Complete ðŸ“Š" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- Shards merged: 20" >> $GITHUB_STEP_SUMMARY
          echo "- JSON files: $(find output -name '*.json' | wc -l)" >> $GITHUB_STEP_SUMMARY
          echo "- Artifact: factory-output" >> $GITHUB_STEP_SUMMARY
          echo "- Cache: entity-data-${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- Next: Factory 4/4 - Upload + Linker" >> $GITHUB_STEP_SUMMARY
