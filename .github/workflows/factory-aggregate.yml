# Factory 3/4 - Aggregate
# V14.5 Phase 4: Modular Factory Architecture
# 
# Triggers: workflow_run from Process, or manual dispatch
# Inputs: shard-* artifacts from Process stage
# Outputs: trending.json, search-*.json, sitemaps, rankings
# 
# Constitution: Art 3.1 (Aggregator), Art 5 (Weekly), Art 6.3 (Search)

name: Factory 3/4 - Aggregate

on:
  workflow_run:
    workflows: ["Factory 2/4 - Process"]
    types: [completed]
  workflow_dispatch:
    inputs:
      run_id:
        description: 'Run ID of Process workflow to get shard artifacts from'
        required: false
        type: string

jobs:
  # V14.5.3: Identify upstream Harvest run for full data restoration
  check-upstream:
    name: Check Upstream Harvest
    runs-on: ubuntu-latest
    outputs:
      harvest-id: ${{ steps.get-ids.outputs.harvest-id }}
      process-id: ${{ steps.get-ids.outputs.process-id }}
    steps:
      - name: Verify Upstream Conclusion
        if: ${{ github.event_name == 'workflow_run' && github.event.workflow_run.conclusion != 'success' }}
        run: |
          echo "âŒ ERROR: Upstream workflow '${{ github.event.workflow_run.name }}' failed with conclusion '${{ github.event.workflow_run.conclusion }}'."
          echo "Aborting Factory 3/4 to prevent data corruption."
          exit 1
      - name: Checkout Repository
        uses: actions/checkout@v4
      - name: Get Run IDs (Harvest & Process)
        id: get-ids
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # 1. Start with the Process Run ID (Triggering event or manual input)
          PROCESS_RUN_ID=${{ inputs.run_id || github.event.workflow_run.id }}
          
          # 2. If PROCESS_RUN_ID is missing, find the latest successful Process run on MAIN
          if [ -z "$PROCESS_RUN_ID" ] || [ "$PROCESS_RUN_ID" == "null" ]; then
            echo "ðŸ” Manual dispatch: Finding latest successful Process run on main branch..."
            PROCESS_RUN_ID=$(gh run list --workflow factory-process.yml --branch main --status success --limit 1 --json databaseId --jq '.[0].databaseId' | tr -d '[:space:]')
          fi
          
          # 3. Find the latest successful Harvest run on MAIN (The Production Source of Truth)
          HARVEST_RUN_ID=$(gh run list --workflow factory-harvest.yml --branch main --status success --limit 1 --json databaseId --jq '.[0].databaseId' | tr -d '[:space:]')
          
          echo "harvest-id=$HARVEST_RUN_ID" >> $GITHUB_OUTPUT
          echo "process-id=$PROCESS_RUN_ID" >> $GITHUB_OUTPUT
          echo "âœ… Resolved Harvest ID (Main): $HARVEST_RUN_ID"
          echo "âœ… Resolved Process ID (Main): $PROCESS_RUN_ID"

  # Job 1: Core Merge (The Pivot - SPEC V2.0)
  merge-core:
    name: Merge Core Registry
    needs: check-upstream
    runs-on: ubuntu-latest
    env:
      NODE_OPTIONS: "--max-old-space-size=8192"
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      - name: Install Dependencies
        run: npm ci

      # V4.1: Restore Shards from Cache (Not Artifact)
      - name: Restore Shards from Cache
        id: cache-shards
        uses: actions/cache/restore@v4
        with:
          path: |
            output/shards
          key: cycle-${{ needs.check-upstream.outputs.process-id }}-shards
          restore-keys: |
            cycle-

      - name: Clean Environment Residue
        run: |
          echo "ðŸ§¹ Cleaning any residue from previous failed runs..."
          rm -rf cache/ data/ output/meta/backup/
          mkdir -p cache data output/meta/backup

      - name: Organize Shards & Context
        run: |
          mkdir -p artifacts output/cache/shards
          # Copy shards for local aggregator use
          if [ -d "output/shards" ]; then
            SHARD_COUNT=$(find output/shards -name "shard-*.json.gz" | wc -l)
            echo "ðŸ“¦ Preparing $SHARD_COUNT shards for aggregation..."
            
            # V18.2.3: Integrity Check (Sniff Magic Bytes)
            for f in output/shards/shard-*.json.gz; do
              [ -e "$f" ] || continue
              if ! node -e "const b=require('fs').readFileSync('$f'); if(b[0]!==0x1f || b[1]!==0x8b) process.exit(1)" 2>/dev/null; then
                echo "  - âš ï¸ Fake .gz detected: $f. Removing to prevent crash."
                rm "$f"
              else
                cp "$f" artifacts/
              fi
            done
          fi
          
          # Final validation
          FINAL_COUNT=$(ls artifacts/shard-*.json.gz 2>/dev/null | wc -l)
          echo "âœ… Final compressed shards for aggregator: $FINAL_COUNT"
          
          if [ "$FINAL_COUNT" -eq 0 ]; then
            echo "âŒ ERROR: No valid compressed shard files found!"
            ls -R output/shards || echo "output/shards is empty"
            exit 1
          fi

      # V4.1: Restore Harvest Context from Cache
      - name: Restore Harvest Context from Cache
        uses: actions/cache/restore@v4
        with:
          path: |
            data/merged.json.gz
            data/merged_shard_*.json.gz
            data/manifest.json
            cache/
          key: cycle-${{ needs.check-upstream.outputs.harvest-id }}-harvest
          restore-keys: |
            cycle-

      - name: Validate Context
        run: |
          if [ -f "data/merged.json.gz" ]; then
            echo "âœ… Harvest context restored from cache."
          else
            echo "âš ï¸ Harvest context not in cache. Will use shards only."
          fi

      - name: Run Core Merge & Health
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ai-nexus-assets
          STRICT_R2_LOCKDOWN: 'true'
        run: |
            echo "ðŸ”„ Running Merge-Core (V2.0 Sharding)..."
            node scripts/factory/aggregator.js --task=health
            node scripts/factory/aggregator.js --task=core

      - name: Consolidate Final Artifacts & Context
        if: success()
        run: |
          mkdir -p output/cache output/meta/backup cache output/cache/shards
          # Preserve shards in output/cache/shards for the intra-cycle and upload artifacts
          [ -d "output/shards" ] && cp output/shards/shard-*.json.gz output/cache/shards/ || true
          
          # 1. Mirror production artifacts for R2 distribution (Stage 4/4)
          [ -d "meta" ] && cp -r meta/* output/meta/ 2>/dev/null || true
          
          # 2. Mirror BACK to root cache/ for persistent memory (Stage 1/4)
          if [ -d "output/meta/backup" ] && [ "$(ls -A output/meta/backup)" ]; then
            cp -r output/meta/backup/* cache/ 2>/dev/null || true
            # V18.3 Fix: Populate output/cache for Satellite Jobs (Zero-Loss Handoff)
            echo "ðŸ“¦ Populating output/cache/ for satellites..."
            cp -r output/meta/backup/* output/cache/ 2>/dev/null || true
          fi
          
          # V18.12.5.23: Copy registry shards to output/cache/ for satellite access
          # Satellites set CACHE_DIR=output/cache and need registry/part-*.json.gz
          if [ -d "cache/registry" ] && [ "$(ls -A cache/registry)" ]; then
            echo "ðŸ“¦ Copying registry shards to output/cache/registry/ for satellites..."
            mkdir -p output/cache/registry
            cp -r cache/registry/* output/cache/registry/
            echo "âœ… Copied $(ls cache/registry/*.json.gz 2>/dev/null | wc -l) registry shards"
          fi
          # Also copy global registry monolith for loadGlobalRegistry fallback
          [ -f "cache/global-registry.json.gz" ] && cp cache/global-registry.json.gz output/cache/ || true
          
          echo "ðŸ“¦ Final output structure for R2:"
          ls -R output/meta/backup/ | head -20
          echo "ðŸ“¦ Final cache structure for Stage 1/4:"
          ls -R cache/ | head -20

      # V17.0: Cleaning relocated to finalize step after cache save to prevent Fusion data loss
      - name: Registry Handoff
        run: |
          echo "âœ… Core registry and fragments verified. Ready for handoff."

      # V4.1: Explicitly upload shards as artifact to satisfy pattern downloader
      - name: Upload Shard Artifact (Bridge)
        uses: actions/upload-artifact@v4
        with:
          name: agg-shards
          path: output/cache/shards/shard-*.json.gz
          retention-days: 1

      # V4.1: Internal Cache for same-workflow satellite jobs
      - name: Save Intra-Cycle Core to Cache
        uses: actions/cache/save@v4
        with:
          path: output/
          key: intra-cycle-${{ github.run_id }}-core

      # V4.1: Keep Artifact for same-workflow satellite jobs (Safety Fallback)
      - name: Upload Registry Artifact (Intra-Chain)
        uses: actions/upload-artifact@v4
        with:
          name: core-registry-v2
          path: |
            output/
            artifacts/
          retention-days: 1

      # V4.1: Save Global Registry for next cycle's 1/4 Harvest
      - name: Save Global Registry to Cache
        uses: actions/cache/save@v4
        with:
          path: |
            cache/global-registry.json.gz
            cache/registry/
          key: global-registry-${{ github.run_id }}

      - name: Save FNI History to Cache (Self-Healing)
        uses: actions/cache/save@v4
        with:
          path: |
            cache/fni-history.json.gz
            cache/fni-history/
          key: fni-history-${{ github.run_id }}

      - name: Save Daily Accumulator to Cache (Self-Healing)
        uses: actions/cache/save@v4
        with:
          path: |
            cache/daily-accum.json.gz
            cache/daily-accum/
          key: daily-accum-${{ github.run_id }}

      - name: Save Checksums to Cache
        uses: actions/cache/save@v4
        with:
          path: |
            cache/entity-checksums.json.gz
            cache/task-checksums.json.gz
          key: checksums-${{ github.run_id }}

  # Parallel Satellite Jobs (SPEC V2.0)
  aggregate-search:
    name: "Agg: Search Index"
    needs: merge-core
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with: { node-version: '20', cache: 'npm' }
      - run: npm ci
      - name: Restore Core Registry from Cache
        uses: actions/cache/restore@v4
        with:
          path: output/
          key: intra-cycle-${{ github.run_id }}-core
      - name: Download Shard Artifacts
        uses: actions/download-artifact@v4
        with:
          name: agg-shards
          path: artifacts/
      - name: Generate Search Indices
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ai-nexus-assets
          CACHE_DIR: output/cache
          STRICT_R2_LOCKDOWN: 'true'
        run: |
          export NODE_OPTIONS="--max-old-space-size=6144"
          node scripts/factory/aggregator.js --task=search
      - name: Upload Search Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: agg-search
          path: output/cache/search*
          if-no-files-found: warn
          retention-days: 1

  aggregate-rankings:
    name: "Agg: Rankings"
    needs: merge-core
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with: { node-version: '20', cache: 'npm' }
      - run: npm ci
      - name: Restore Core Registry from Cache
        uses: actions/cache/restore@v4
        with:
          path: output/
          key: intra-cycle-${{ github.run_id }}-core
      - name: Download Shard Artifacts
        uses: actions/download-artifact@v4
        with:
          name: agg-shards
          path: artifacts/
      - name: Generate Rankings
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ai-nexus-assets
          CACHE_DIR: output/cache
          STRICT_R2_LOCKDOWN: 'true'
        run: |
          export NODE_OPTIONS="--max-old-space-size=6144"
          node scripts/factory/aggregator.js --task=rankings
          node scripts/factory/aggregator.js --task=category
      - name: Upload Rankings Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: agg-rankings
          # V17.4: Ensure directory hierarchy is preserved
          path: |
            output/cache/rankings/
            output/cache/category_stats.json.gz
          if-no-files-found: warn
          retention-days: 1


  aggregate-trending:
    name: "Agg: Trending"
    needs: merge-core
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with: { node-version: '20', cache: 'npm' }
      - run: npm ci
      - name: Restore Core Registry from Cache
        uses: actions/cache/restore@v4
        with:
          path: output/
          key: intra-cycle-${{ github.run_id }}-core
      - name: Download Shard Artifacts
        uses: actions/download-artifact@v4
        with:
          name: agg-shards
          path: artifacts/
      - name: Restore FNI History from Cache
        uses: actions/cache/restore@v4
        with:
          path: |
            cache/fni-history.json.gz
            cache/fni-history/
          key: fni-history-${{ github.run_id }}
      - name: Generate Trending & Trends
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ai-nexus-assets
          CACHE_DIR: output/cache
          STRICT_R2_LOCKDOWN: 'true'
        run: |
          export NODE_OPTIONS="--max-old-space-size=6144"
          node scripts/factory/aggregator.js --task=trending
          node scripts/factory/aggregator.js --task=trend
      - name: Upload Trending & Trend Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: agg-trending
          path: |
            output/cache/trending.json.gz
            output/cache/trend-data.json.gz
          if-no-files-found: warn
          retention-days: 1

  aggregate-relations:
    name: "Agg: Knowledge Mesh"
    needs: merge-core
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with: { node-version: '20', cache: 'npm' }
      - run: npm ci
      - name: Restore Core Registry from Cache
        uses: actions/cache/restore@v4
        with:
          path: output/
          key: intra-cycle-${{ github.run_id }}-core
      - name: Download Shard Artifacts
        uses: actions/download-artifact@v4
        with:
          name: agg-shards
          path: artifacts/
      - name: Generate Relations
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ai-nexus-assets
          CACHE_DIR: output/cache
          STRICT_R2_LOCKDOWN: 'true'
        run: |
          export NODE_OPTIONS="--max-old-space-size=7168"
          node scripts/factory/aggregator.js --task=relations
      - name: Upload Relations Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: agg-relations
          path: |
            output/cache/relations.json.gz
            output/cache/relations/
            output/cache/mesh/
            output/cache/knowledge/
          if-no-files-found: warn
          retention-days: 1

  finalize:
    name: Finalize Aggregation
    needs: [aggregate-search, aggregate-rankings, aggregate-trending, aggregate-relations]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
      - name: Consolidate All Results
        uses: actions/download-artifact@v4
        with:
          pattern: agg-*
          path: output/
          merge-multiple: true
      - name: Run Health Consolidation
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ai-nexus-assets
        run: node scripts/factory/consolidate-health.js

      # V17.1: Restore core-registry from Cache (Restores directly to output/)
      - name: Restore Core Registry
        uses: actions/cache/restore@v4
        with:
          path: output/
          key: intra-cycle-${{ github.run_id }}-core

      - name: Merge Core with Satellite Outputs
        run: |
          # V17.5: Entities.json and meta are already restored to output/ by the previous step
          # We just need to ensure directory structure for non-standard satellite outputs
          mkdir -p output/cache
          # Move satellite outputs into cache if needed
          [ -f "output/trending.json.gz" ] && mv output/trending.json.gz output/cache/ || true
          [ -f "output/trending.json" ] && mv output/trending.json output/cache/ || true
          [ -f "output/trend-data.json.gz" ] && mv output/trend-data.json.gz output/cache/ || true
          [ -f "output/trend-data.json" ] && mv output/trend-data.json output/cache/ || true
          [ -f "output/category_stats.json.gz" ] && mv output/category_stats.json.gz output/cache/ || true
          [ -f "output/category_stats.json" ] && mv output/category_stats.json output/cache/ || true
          [ -f "output/relations.json.gz" ] && mv output/relations.json.gz output/cache/ || true
          [ -f "output/relations.json" ] && mv output/relations.json output/cache/ || true
          # V17.4: Ensure rankings and search indices are nested in cache/
          [ -d "output/rankings" ] && mv output/rankings output/cache/ || true
          [ -d "output/relations" ] && mv output/relations output/cache/ || true
          [ -d "output/mesh" ] && mv output/mesh output/cache/ || true
          [ -d "output/knowledge" ] && mv output/knowledge output/cache/ || true
          [ -f "output/search-core.json.gz" ] && mv output/search-core.json.gz output/cache/ || true
          [ -f "output/search-core.json" ] && mv output/search-core.json output/cache/ || true
          [ -f "output/search-full.json.gz" ] && mv output/search-full.json.gz output/cache/ || true
          [ -f "output/search-full.json" ] && mv output/search-full.json output/cache/ || true
          [ -f "output/search-manifest.json" ] && mv output/search-manifest.json output/cache/ || true
          
          # V19.2: content.db and bundles preservation
          [ -d "output/data" ] && echo "âœ… data/ directory found" || mkdir -p output/data
          [ -f "output/content.db" ] && mv output/content.db output/data/ || true
          [ -d "output/bundles" ] && mv output/bundles output/data/ || true
          
          # V17.8: Shard Bridge - Shards from agg-shards download will be in output/*.json.gz
          mkdir -p output/cache/shards
          find output -maxdepth 1 -name "shard-*.json.gz" -exec mv {} output/cache/shards/ \; || true
          
          # Check legacy and intra-cycle locations
          [ -d "output/shards" ] && mv output/shards/* output/cache/shards/ 2>/dev/null || true
          
          # List final structure
          echo "ðŸ“¦ Final output structure:"
          find output -type f | head -50

      # V17.1: Save COMPLETE output (with all satellite results) to Cache for Factory 4/4
      - name: Save Complete Cycle Output to Cache
        uses: actions/cache/save@v4
        with:
          path: output/
          key: cycle-${{ github.run_id }}-output
      - name: Summary
        run: |
          echo "## Factory 3/4 - Aggregator V2.0 Complete ðŸš€" >> $GITHUB_STEP_SUMMARY
          echo "Scale: 1M+ Readiness Verified" >> $GITHUB_STEP_SUMMARY
          echo "Jobs: Parallel (Search, Rankings, Trending, Relations)" >> $GITHUB_STEP_SUMMARY
          echo "Cache: Complete output saved for Factory 4/4" >> $GITHUB_STEP_SUMMARY
