# Factory 3/4 - Aggregate
# V14.5 Phase 4: Modular Factory Architecture
# 
# Triggers: workflow_run from Process, or manual dispatch
# Inputs: shard-* artifacts from Process stage
# Outputs: trending.json, search-*.json, sitemaps, rankings
# 
# Constitution: Art 3.1 (Aggregator), Art 5 (Weekly), Art 6.3 (Search)

name: Factory 3/4 - Aggregate

on:
  workflow_run:
    workflows: ["Factory 2/4 - Process"]
    types: [completed]
  workflow_dispatch:
    inputs:
      run_id:
        description: 'Run ID of Process workflow to get shard artifacts from'
        required: false
        type: string

jobs:
  # V14.5.3: Identify upstream Harvest run for full data restoration
  check-upstream:
    name: Check Upstream Harvest
    runs-on: ubuntu-latest
    outputs:
      harvest-run-id: ${{ steps.get-id.outputs.id }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Get Harvest Run ID
        id: get-id
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # The 3/4 Aggregate is triggered by 2/4 Process. 
          # 2/4 Process was triggered by 1/4 Harvest.
          # We need to find the original 1/4 Harvest run that created the 'factory-entity-data' cache.
          
          # 1. Get 2/4 Process Run ID (Triggering event)
          PROCESS_RUN_ID=${{ github.event.workflow_run.id }}
          
          # 2. Get the 1/4 Harvest Run ID (Parent of Process Run)
          # Note: In our dispatch logic, we pass run_id as input to Process.
          # If we don't have it explicitly, we look for the most recent successful Harvest run.
          HARVEST_RUN_ID=$(gh run list --workflow "Factory 1/4 - Harvest" --status success --limit 1 --json databaseId --jq '.[0].databaseId')
          
          echo "id=$HARVEST_RUN_ID" >> $GITHUB_OUTPUT
          echo "âœ… Found Upstream Harvest Run: $HARVEST_RUN_ID"

  aggregate:
    name: Aggregate Shard Results
    needs: check-upstream
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    env:
      CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
      CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
      R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
      R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
      R2_BUCKET: ai-nexus-assets
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install Dependencies
        run: npm ci

      # Download all shard artifacts with merge
      - name: Download All Shard Artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: shard-*
          path: ./downloaded/
          merge-multiple: true
          run-id: ${{ inputs.run_id || github.event.workflow_run.id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}

      # Organize downloaded files
      - name: Organize Downloaded Files
        run: |
          mkdir -p artifacts output
          
          # Move shard JSON files to artifacts/
          find ./downloaded -name "shard-*.json" -exec mv {} artifacts/ \; 2>/dev/null || true
          
          # Merge output directories from all shards
          if [ -d "./downloaded/output" ]; then
            cp -r ./downloaded/output/* output/ 2>/dev/null || true
          fi
          
          # Also check for cache/ directories
          find ./downloaded -type d -name "cache" -exec cp -r {}/* output/ \; 2>/dev/null || true
          
          echo "ðŸ“‚ Artifacts structure:"
          ls -la artifacts/ || echo "No artifacts found"
          echo "ðŸ“‚ Output structure:"
          find output -type f | head -20 || echo "No output files"

      # Restore FNI history
      - name: Restore FNI History
        uses: actions/cache@v4
        with:
          path: cache/fni-history.json
          key: fni-history-${{ github.run_id }}
          restore-keys: |
            fni-history-

      # Restore weekly accumulator
      - name: Restore Weekly Accumulator
        uses: actions/cache@v4
        with:
          path: cache/weekly-accum.json
          key: weekly-accum-${{ github.run_id }}
          restore-keys: |
            weekly-accum-

      # V14.5.3: Restore the full 295k dataset from the original Harvest run
      - name: Restore Entity Data from Harvest Cache
        uses: actions/cache/restore@v4
        with:
          path: |
            data/merged.json
            data/merged_shard_*.json
          key: factory-entity-data-${{ needs.check-upstream.outputs.harvest-run-id }}
          fail-on-cache-miss: true

      # Run aggregator
      - name: Run Aggregator
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          # V16.2.5: R2 secrets removed. Aggregator now saves state to output/ for 4/4 to upload.
        run: |
          echo "ðŸ”„ Running Aggregator..."
          node scripts/factory/aggregator.js

      # Validate critical files
      - name: Validate Output
        run: |
          echo "ðŸ” Validating output..."
          
          CRITICAL_FILES=(
            "output/cache/trending.json"
            "output/cache/search-core.json"
            "output/cache/category_stats.json"
          )
          
          MISSING=0
          for f in "${CRITICAL_FILES[@]}"; do
            if [ -f "$f" ]; then
              echo "âœ… $f"
            else
              echo "âŒ Missing: $f"
              MISSING=$((MISSING + 1))
            fi
          done
          
          if [ $MISSING -gt 0 ]; then
            echo "âš ï¸ Warning: $MISSING critical files missing"
          fi
          
          FILE_COUNT=$(find output -name "*.json" | wc -l)
          echo "ðŸ“Š Total JSON files: $FILE_COUNT"

      # Save updated caches
      - name: Save FNI History
        uses: actions/cache/save@v4
        with:
          path: cache/fni-history.json
          key: fni-history-${{ github.run_id }}

      - name: Save Weekly Accumulator
        uses: actions/cache/save@v4
        with:
          path: cache/weekly-accum.json
          key: weekly-accum-${{ github.run_id }}

      # V16.3 FIX: Explicitly save global registry to cache for Harvest (1/4) to see
      - name: Save Global Registry
        uses: actions/cache/save@v4
        with:
          path: |
            cache/global-registry*.json
            cache/entity-checksums.json
          key: global-registry-${{ github.run_id }}

      # V16.2.6: Restore 'internal cache' transfer (Bypasses 313k file upload crash)
      - name: Save Entity Data to Cache (for Upload & Linker)
        uses: actions/cache/save@v4
        with:
          path: output/
          key: entity-data-${{ github.run_id }}


      # Trigger next stage (via workflow_run - automatic)
      - name: Notify Upload Stage
        run: |
          echo "âœ… Aggregate complete. Factory 4/4 - Upload and Linker will start automatically."

      - name: Summary
        run: |
          echo "## Factory 3/4 - Aggregate Complete ðŸ“Š" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- Shards merged: 20" >> $GITHUB_STEP_SUMMARY
          echo "- JSON files: $(find output -name '*.json' | wc -l)" >> $GITHUB_STEP_SUMMARY
          echo "- Artifact: factory-output" >> $GITHUB_STEP_SUMMARY
          echo "- Cache: entity-data-${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- Next: Factory 4/4 - Upload + Linker" >> $GITHUB_STEP_SUMMARY
