name: L1 Harvester - V4.2 Parallel Collection

# V4.2 Constitution: Pillar VIII - Cloudflare-First Processing
# Phase A.3b: Parallel harvesting for 3x speedup
# GitHub: Collection + Asset Processing ONLY
# Cloudflare Unified Workflow: Data cleaning + FNI + D1 ingestion

on:
  schedule:
    - cron: '0 3 * * *'  # Daily at 03:00 UTC
  workflow_dispatch:
    inputs:
      sources:
        description: 'Sources to harvest (comma-separated or "all")'
        default: 'all'
        type: string

concurrency:
  group: l1-harvester-v42
  cancel-in-progress: true

jobs:
  # ============================================================
  # PHASE 1: PARALLEL SOURCE HARVESTING
  # Each source runs independently in parallel
  # ============================================================
  
  harvest-huggingface:
    name: Harvest HuggingFace
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: 20
      - run: npm ci
      
      - name: Harvest HuggingFace Models
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "üì• [Parallel] Harvesting HuggingFace..."
          node scripts/ingestion/harvest-single.js huggingface --limit 10000
          
      - name: Upload Batch Artifact
        uses: actions/upload-artifact@v4
        with:
          name: batch-huggingface
          path: data/raw_batch_huggingface.json
          retention-days: 1

  harvest-github:
    name: Harvest GitHub
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: 20
      - run: npm ci
      
      - name: Harvest GitHub Repos
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "üì• [Parallel] Harvesting GitHub..."
          node scripts/ingestion/harvest-single.js github --limit 5000
          
      - name: Upload Batch Artifact
        uses: actions/upload-artifact@v4
        with:
          name: batch-github
          path: data/raw_batch_github.json
          retention-days: 1

  harvest-academic:
    name: Harvest Academic (ArXiv + Papers)
    runs-on: ubuntu-latest
    timeout-minutes: 90
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: 20
      - run: npm ci
      
      - name: Harvest ArXiv Papers
        run: |
          echo "üì• [Parallel] Harvesting ArXiv..."
          node scripts/ingestion/harvest-single.js arxiv --limit 5000 || echo "ArXiv adapter skipped"
          
      - name: Harvest HuggingFace Papers
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "üì• [Parallel] Harvesting HF Papers..."
          node scripts/ingestion/harvest-single.js huggingface-papers --limit 3000 || echo "HF Papers adapter skipped"

      - name: Upload Batch Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: batch-academic
          path: data/raw_batch_*.json
          retention-days: 1

  harvest-ecosystem:
    name: Harvest Ecosystem (Ollama, MCP, Replicate, Kaggle)
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: 20
      - run: npm ci
      
      # ABANDONED 2025-12-21: ModelScope has closed API architecture, no public model list endpoint
      # - name: Harvest ModelScope
      #   env:
      #     MODELSCOPE_API_TOKEN: ${{ secrets.MODELSCOPE_API_TOKEN }}
      #   run: |
      #     echo "üì• [Parallel] Harvesting ModelScope..."
      #     node scripts/ingestion/harvest-single.js modelscope --limit 2000 || echo "ModelScope adapter skipped"
          
      - name: Harvest Ollama Registry
        run: |
          echo "üì• [Parallel] Harvesting Ollama..."
          node scripts/ingestion/harvest-single.js ollama --limit 1000 || echo "Ollama adapter skipped"
          
      - name: Harvest MCP Servers
        run: |
          echo "üì• [Parallel] Harvesting MCP..."
          node scripts/ingestion/harvest-single.js mcp --limit 500 || echo "MCP adapter skipped"

      - name: Harvest Replicate
        env:
          REPLICATE_API_TOKEN: ${{ secrets.REPLICATE_API_TOKEN }}
        run: |
          echo "üì• [Parallel] Harvesting Replicate..."
          node scripts/ingestion/harvest-single.js replicate --limit 5000 || echo "Replicate adapter skipped"

      - name: Harvest Kaggle
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          echo "üì• [Parallel] Harvesting Kaggle..."
          node scripts/ingestion/harvest-single.js kaggle --limit 10000 || echo "Kaggle adapter skipped"

      - name: Upload Batch Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: batch-ecosystem
          path: data/raw_batch_*.json
          retention-days: 1

  # ============================================================
  # PHASE 2: MERGE AND UPLOAD
  # Runs after all parallel harvests complete
  # ============================================================
  
  merge-and-upload:
    name: Merge Batches & Upload to R2
    runs-on: ubuntu-latest
    needs: [harvest-huggingface, harvest-github, harvest-academic, harvest-ecosystem]
    timeout-minutes: 30
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: 20
      - run: npm ci
      
      - name: Download All Batch Artifacts
        uses: actions/download-artifact@v4
        with:
          path: data/
          pattern: batch-*
          merge-multiple: true
          
      - name: List Downloaded Batches
        run: |
          echo "üì¶ Downloaded batches:"
          ls -la data/raw_batch_*.json 2>/dev/null || echo "No batch files found"
          
      - name: Merge Batches
        run: |
          echo "üîÑ Merging batches..."
          node scripts/ingestion/merge-batches.js
          
          if [ -f "data/merged.json" ]; then
            MODEL_COUNT=$(node -e "console.log(JSON.parse(require('fs').readFileSync('data/merged.json')).length)")
            echo "‚úÖ Merged: $MODEL_COUNT total entities"
          else
            echo "‚ùå Error: merged.json not generated"
            exit 1
          fi
          
      - name: Upload Raw Data to R2 (V7.1 Batch Upload)
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "üì¶ V7.1: Batch Upload to R2..."
          TIMESTAMP=$(date -u +"%Y%m%dT%H%M%SZ")
          
          # Install gzip tools
          npm install --save-dev zlib 2>/dev/null || true
          
          # Create manifest
          BATCH_COUNT=0
          TOTAL_ENTITIES=0
          BATCH_FILES=""
          
          # Upload each batch with gzip compression
          for batch_file in data/raw_batch_*.json; do
            if [ -f "$batch_file" ]; then
              BATCH_COUNT=$((BATCH_COUNT + 1))
              batch_name=$(basename "$batch_file" .json)
              
              # Gzip compress
              gzip -c "$batch_file" > "${batch_file}.gz"
              
              # Upload to V7.1 path: ingest/batches/
              r2_key="ingest/batches/${batch_name}.json.gz"
              npx wrangler r2 object put "ai-nexus-assets/$r2_key" \
                --file="${batch_file}.gz" \
                --content-type="application/json" \
                --content-encoding="gzip" \
                --remote
              echo "‚úÖ Uploaded: $r2_key"
              
              # Track batch info
              BATCH_ENTITIES=$(node -e "console.log(JSON.parse(require('fs').readFileSync('$batch_file')).length)")
              TOTAL_ENTITIES=$((TOTAL_ENTITIES + BATCH_ENTITIES))
              BATCH_FILES="${BATCH_FILES}\"${r2_key}\","
            fi
          done
          
          # Remove trailing comma
          BATCH_FILES="${BATCH_FILES%,}"
          
          # Create and upload manifest.json
          cat > data/manifest.json << EOF
          {
            "version": "7.1",
            "timestamp": "${TIMESTAMP}",
            "batchCount": ${BATCH_COUNT},
            "totalEntities": ${TOTAL_ENTITIES},
            "batches": [${BATCH_FILES}],
            "status": "ready"
          }
          EOF
          
          npx wrangler r2 object put "ai-nexus-assets/ingest/manifest.json" \
            --file="data/manifest.json" \
            --content-type="application/json" \
            --remote
          echo "‚úÖ Uploaded: ingest/manifest.json"
          
          # Also upload merged.json for L5 compatibility (gzipped)
          if [ -f "data/merged.json" ]; then
            echo "üì¶ Uploading entities.json.gz for L5 Heavy Compute..."
            gzip -c "data/merged.json" > "data/entities.json.gz"
            npx wrangler r2 object put "ai-nexus-assets/ingest/current/entities.json.gz" \
              --file="data/entities.json.gz" \
              --content-type="application/json" \
              --content-encoding="gzip" \
              --remote
            echo "‚úÖ Uploaded: ingest/current/entities.json.gz"
          fi
          
          echo ""
          echo "üîÑ V7.1 Batch upload complete: ${BATCH_COUNT} batches, ${TOTAL_ENTITIES} entities"
          echo "üîî L8 Queue Consumer will process batches"
          
      - name: Pipeline Summary
        run: |
          echo ""
          echo "=================================================="
          echo "üéâ V4.2 L1 Parallel Harvester Complete!"
          echo "=================================================="
          echo ""
          echo "üìä Summary:"
          if [ -f "data/merged.json" ]; then
            MODEL_COUNT=$(node -e "console.log(JSON.parse(require('fs').readFileSync('data/merged.json')).length)")
            echo "   Total entities: $MODEL_COUNT"
          fi
          echo "   Batches: $(ls -1 data/raw_batch_*.json 2>/dev/null | wc -l)"
          echo ""
          echo "‚è≠Ô∏è Next: Cloudflare Unified Workflow (every 5 min)"
          echo "   - Ingest raw-data/ ‚Üí D1"
          echo "   - Calculate FNI scores"
          echo "=================================================="
