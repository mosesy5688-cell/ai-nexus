name: L1 Harvester - V4.2 Parallel Collection

# V4.2 Constitution: Pillar VIII - Cloudflare-First Processing
# Phase A.3b: Parallel harvesting for 3x speedup
# GitHub: Collection + Asset Processing ONLY
# Cloudflare Unified Workflow: Data cleaning + FNI + D1 ingestion

on:
  schedule:
    - cron: '0 3 * * *'  # Daily at 03:00 UTC
  workflow_dispatch:
    inputs:
      sources:
        description: 'Sources to harvest (comma-separated or "all")'
        default: 'all'
        type: string

concurrency:
  group: l1-harvester-v42
  cancel-in-progress: true

jobs:
  # ============================================================
  # PHASE 1: PARALLEL SOURCE HARVESTING
  # Each source runs independently in parallel
  # ============================================================
  
  harvest-huggingface:
    name: Harvest HuggingFace
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: 20
      - run: npm ci
      
      - name: Harvest HuggingFace Models
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "üì• [Parallel] Harvesting HuggingFace..."
          node scripts/ingestion/harvest-single.js huggingface --limit 10000
          
      - name: Upload Batch Artifact
        uses: actions/upload-artifact@v4
        with:
          name: batch-huggingface
          path: data/raw_batch_huggingface.json
          retention-days: 1

  harvest-github:
    name: Harvest GitHub
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: 20
      - run: npm ci
      
      - name: Harvest GitHub Repos
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "üì• [Parallel] Harvesting GitHub..."
          node scripts/ingestion/harvest-single.js github --limit 5000
          
      - name: Upload Batch Artifact
        uses: actions/upload-artifact@v4
        with:
          name: batch-github
          path: data/raw_batch_github.json
          retention-days: 1

  harvest-academic:
    name: Harvest Academic (ArXiv + Papers)
    runs-on: ubuntu-latest
    timeout-minutes: 90
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: 20
      - run: npm ci
      
      - name: Harvest ArXiv Papers
        run: |
          echo "üì• [Parallel] Harvesting ArXiv..."
          node scripts/ingestion/harvest-single.js arxiv --limit 5000 || echo "ArXiv adapter skipped"
          
      - name: Harvest HuggingFace Papers
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "üì• [Parallel] Harvesting HF Papers..."
          node scripts/ingestion/harvest-single.js huggingface-papers --limit 3000 || echo "HF Papers adapter skipped"

      - name: Upload Batch Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: batch-academic
          path: data/raw_batch_*.json
          retention-days: 1

  harvest-ecosystem:
    name: Harvest Ecosystem (ModelScope, Ollama, CivitAI, MCP)
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: 20
      - run: npm ci
      
      # ABANDONED 2025-12-21: ModelScope has closed API architecture, no public model list endpoint
      # - name: Harvest ModelScope
      #   env:
      #     MODELSCOPE_API_TOKEN: ${{ secrets.MODELSCOPE_API_TOKEN }}
      #   run: |
      #     echo "üì• [Parallel] Harvesting ModelScope..."
      #     node scripts/ingestion/harvest-single.js modelscope --limit 2000 || echo "ModelScope adapter skipped"
          
      - name: Harvest Ollama Registry
        run: |
          echo "üì• [Parallel] Harvesting Ollama..."
          node scripts/ingestion/harvest-single.js ollama --limit 1000 || echo "Ollama adapter skipped"
          
      - name: Harvest MCP Servers
        run: |
          echo "üì• [Parallel] Harvesting MCP..."
          node scripts/ingestion/harvest-single.js mcp --limit 500 || echo "MCP adapter skipped"

      - name: Upload Batch Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: batch-ecosystem
          path: data/raw_batch_*.json
          retention-days: 1

  # ============================================================
  # PHASE 2: MERGE AND UPLOAD
  # Runs after all parallel harvests complete
  # ============================================================
  
  merge-and-upload:
    name: Merge Batches & Upload to R2
    runs-on: ubuntu-latest
    needs: [harvest-huggingface, harvest-github, harvest-academic, harvest-ecosystem]
    timeout-minutes: 30
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: 20
      - run: npm ci
      
      - name: Download All Batch Artifacts
        uses: actions/download-artifact@v4
        with:
          path: data/
          pattern: batch-*
          merge-multiple: true
          
      - name: List Downloaded Batches
        run: |
          echo "üì¶ Downloaded batches:"
          ls -la data/raw_batch_*.json 2>/dev/null || echo "No batch files found"
          
      - name: Merge Batches
        run: |
          echo "üîÑ Merging batches..."
          node scripts/ingestion/merge-batches.js
          
          if [ -f "data/merged.json" ]; then
            MODEL_COUNT=$(node -e "console.log(JSON.parse(require('fs').readFileSync('data/merged.json')).length)")
            echo "‚úÖ Merged: $MODEL_COUNT total entities"
          else
            echo "‚ùå Error: merged.json not generated"
            exit 1
          fi
          
      - name: Upload Raw Data to R2
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "üì¶ V4.2: Uploading to R2..."
          TIMESTAMP=$(date -u +"%Y%m%dT%H%M%SZ")
          
          for batch_file in data/raw_batch_*.json; do
            if [ -f "$batch_file" ]; then
              batch_name=$(basename "$batch_file" .json)
              r2_key="raw-data/${TIMESTAMP}_${batch_name}.json"
              npx wrangler r2 object put "ai-nexus-assets/$r2_key" --file="$batch_file" --content-type="application/json" --remote
              echo "‚úÖ Uploaded: $r2_key"
            fi
          done
          
          echo ""
          echo "üîÑ Raw data uploaded to R2"
          echo "üîî Unified Workflow will process within 5 minutes"
          
      - name: Pipeline Summary
        run: |
          echo ""
          echo "=================================================="
          echo "üéâ V4.2 L1 Parallel Harvester Complete!"
          echo "=================================================="
          echo ""
          echo "üìä Summary:"
          if [ -f "data/merged.json" ]; then
            MODEL_COUNT=$(node -e "console.log(JSON.parse(require('fs').readFileSync('data/merged.json')).length)")
            echo "   Total entities: $MODEL_COUNT"
          fi
          echo "   Batches: $(ls -1 data/raw_batch_*.json 2>/dev/null | wc -l)"
          echo ""
          echo "‚è≠Ô∏è Next: Cloudflare Unified Workflow (every 5 min)"
          echo "   - Ingest raw-data/ ‚Üí D1"
          echo "   - Calculate FNI scores"
          echo "=================================================="
