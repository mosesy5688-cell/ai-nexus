name: L1 Harvester - V4.2 Parallel Collection

# V4.2 Constitution: Pillar VIII - Cloudflare-First Processing
# Phase A.3b: Parallel harvesting for 3x speedup
# GitHub: Collection + Asset Processing ONLY
# Cloudflare Unified Workflow: Data cleaning + FNI + D1 ingestion

on:
  schedule:
    - cron: '0 3 * * *'  # Daily at 03:00 UTC
  workflow_dispatch:
    inputs:
      sources:
        description: 'Sources to harvest (comma-separated or "all")'
        default: 'all'
        type: string

concurrency:
  group: l1-harvester-v42
  cancel-in-progress: true

jobs:
  # ============================================================
  # PHASE 1: PARALLEL SOURCE HARVESTING
  # Each source runs independently in parallel
  # ============================================================
  
  harvest-huggingface:
    name: Harvest HuggingFace
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: 20
      - run: npm ci
      
      - name: Harvest HuggingFace Models
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "üì• [Parallel] Harvesting HuggingFace..."
          node scripts/ingestion/harvest-single.js huggingface --limit 10000
          
      - name: Upload Batch Artifact
        uses: actions/upload-artifact@v4
        with:
          name: batch-huggingface
          path: data/raw_batch_huggingface.json
          retention-days: 1

  harvest-github:
    name: Harvest GitHub
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: 20
      - run: npm ci
      
      - name: Harvest GitHub Repos
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "üì• [Parallel] Harvesting GitHub..."
          node scripts/ingestion/harvest-single.js github --limit 5000
          
      - name: Upload Batch Artifact
        uses: actions/upload-artifact@v4
        with:
          name: batch-github
          path: data/raw_batch_github.json
          retention-days: 1

  harvest-academic:
    name: Harvest Academic (ArXiv + Papers + Datasets)
    runs-on: ubuntu-latest
    timeout-minutes: 180
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: 20
      - run: npm ci
      
      - name: Harvest ArXiv Papers
        run: |
          echo "üì• [Parallel] Harvesting ArXiv..."
          node scripts/ingestion/harvest-single.js arxiv --limit 50000 || echo "ArXiv adapter skipped"
          
      - name: Harvest HuggingFace Papers
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "üì• [Parallel] Harvesting HF Papers..."
          node scripts/ingestion/harvest-single.js huggingface-papers --limit 3000 || echo "HF Papers adapter skipped"

      - name: Harvest HuggingFace Datasets
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "üì• [Parallel] Harvesting HF Datasets..."
          node scripts/ingestion/harvest-single.js huggingface-datasets --limit 10000 || echo "HF Datasets adapter skipped"

      - name: Upload Batch Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: batch-academic
          path: data/raw_batch_*.json
          retention-days: 1

  harvest-ecosystem:
    name: Harvest Ecosystem (Ollama, MCP, Replicate, Kaggle, CivitAI, SemanticScholar, OpenLLM, DeepSpec)
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: 20
      - run: npm ci
      
      # ABANDONED 2025-12-21: ModelScope has closed API architecture, no public model list endpoint
      # - name: Harvest ModelScope
      #   env:
      #     MODELSCOPE_API_TOKEN: ${{ secrets.MODELSCOPE_API_TOKEN }}
      #   run: |
      #     echo "üì• [Parallel] Harvesting ModelScope..."
      #     node scripts/ingestion/harvest-single.js modelscope --limit 2000 || echo "ModelScope adapter skipped"
          
      - name: Harvest Ollama Registry
        run: |
          echo "üì• [Parallel] Harvesting Ollama..."
          node scripts/ingestion/harvest-single.js ollama --limit 1000 || echo "Ollama adapter skipped"
          
      - name: Harvest MCP Servers
        run: |
          echo "üì• [Parallel] Harvesting MCP..."
          node scripts/ingestion/harvest-single.js mcp --limit 500 || echo "MCP adapter skipped"

      - name: Harvest Replicate
        env:
          REPLICATE_API_TOKEN: ${{ secrets.REPLICATE_API_TOKEN }}
        run: |
          echo "üì• [Parallel] Harvesting Replicate..."
          node scripts/ingestion/harvest-single.js replicate --limit 5000 || echo "Replicate adapter skipped"

      - name: Harvest Kaggle
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          echo "üì• [Parallel] Harvesting Kaggle..."
          node scripts/ingestion/harvest-single.js kaggle --limit 10000 || echo "Kaggle adapter skipped"

      # ABANDONED 2025-12-21: LangChain Hub API migrated to LangSmith, no public list endpoint
      # - name: Harvest LangChain Hub
      #   run: |
      #     echo "üì• [Parallel] Harvesting LangChain Hub (Agents + Prompts)..."
      #     node scripts/ingestion/harvest-single.js langchain --limit 2000 || echo "LangChain adapter skipped"

      - name: Harvest CivitAI
        run: |
          echo "üì• [Parallel] Harvesting CivitAI (Image/Video Models)..."
          node scripts/ingestion/harvest-single.js civitai --limit 5000 || echo "CivitAI adapter skipped"

      - name: Harvest Semantic Scholar
        run: |
          echo "üì• [Parallel] Harvesting Semantic Scholar (Papers/Citations)..."
          node scripts/ingestion/harvest-single.js semanticscholar --limit 3000 || echo "Semantic Scholar adapter skipped"

      # B.1: OpenLLM Leaderboard (Benchmark data for model evaluation)
      # Verified: meets Art.XI admission rules (exempt: outputs benchmarks, not entities)
      - name: Harvest OpenLLM Benchmarks
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "üìä [Parallel] Harvesting OpenLLM Leaderboard Benchmarks..."
          # V2.1: Increased from 500 to 1000 for better benchmark coverage
          node scripts/ingestion/harvest-single.js openllm --limit 1000 || echo "OpenLLM adapter skipped"

      # B.1: DeepSpec Extractor (Model specifications from config.json)
      # Verified: meets Art.XI admission rules (exempt: outputs specs, not entities)
      - name: Harvest DeepSpec Specifications
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "üìã [Parallel] Extracting Model Specifications..."
          # V2.1: Increased from 100 to 2000 for params_billions coverage
          node scripts/ingestion/harvest-single.js deepspec --limit 2000 || echo "DeepSpec adapter skipped"

      - name: Upload Batch Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: batch-ecosystem
          path: data/raw_batch_*.json
          retention-days: 1

  # ============================================================
  # PHASE 2: MERGE AND UPLOAD
  # Runs after all parallel harvests complete
  # ============================================================
  
  merge-and-upload:
    name: Merge Batches & Upload to R2
    runs-on: ubuntu-latest
    needs: [harvest-huggingface, harvest-github, harvest-academic, harvest-ecosystem]
    timeout-minutes: 30
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: 20
      - run: npm ci
      
      - name: Download All Batch Artifacts
        uses: actions/download-artifact@v4
        with:
          path: data/
          pattern: batch-*
          merge-multiple: true
          
      - name: List Downloaded Batches
        run: |
          echo "üì¶ Downloaded batches:"
          ls -la data/raw_batch_*.json 2>/dev/null || echo "No batch files found"
          
      - name: Merge Batches
        run: |
          echo "üîÑ Merging batches..."
          node scripts/ingestion/merge-batches.js
          
          if [ -f "data/merged.json" ]; then
            MODEL_COUNT=$(node -e "console.log(JSON.parse(require('fs').readFileSync('data/merged.json')).length)")
            echo "‚úÖ Merged: $MODEL_COUNT total entities"
          else
            echo "‚ùå Error: merged.json not generated"
            exit 1
          fi
          
      - name: Upload Raw Data to R2 (V7.2 Batch Upload with Validation)
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "üì¶ V7.2: Batch Upload to R2 with Validation..."
          TIMESTAMP=$(date -u +"%Y%m%dT%H%M%SZ")
          
          # V7.2 Configuration
          MAX_BATCH_SIZE_MB=50  # Maximum batch size
          MAX_ENTITIES_PER_BATCH=5000  # Per V7.1 spec
          
          # Create manifest
          BATCH_COUNT=0
          TOTAL_ENTITIES=0
          BATCH_FILES=""
          VALIDATION_PASSED=true
          
          # Upload each batch with gzip compression and validation
          for batch_file in data/raw_batch_*.json; do
            if [ -f "$batch_file" ]; then
              BATCH_COUNT=$((BATCH_COUNT + 1))
              batch_name=$(basename "$batch_file" .json)
              
              # V7.2: Batch size validation
              FILE_SIZE=$(stat -c%s "$batch_file" 2>/dev/null || stat -f%z "$batch_file")
              FILE_SIZE_MB=$((FILE_SIZE / 1024 / 1024))
              BATCH_ENTITIES=$(node -e "console.log(JSON.parse(require('fs').readFileSync('$batch_file')).length)")
              
              if [ "$FILE_SIZE_MB" -gt "$MAX_BATCH_SIZE_MB" ]; then
                echo "‚ö†Ô∏è WARNING: $batch_name exceeds ${MAX_BATCH_SIZE_MB}MB limit (${FILE_SIZE_MB}MB)"
                VALIDATION_PASSED=false
              fi
              
              if [ "$BATCH_ENTITIES" -gt "$MAX_ENTITIES_PER_BATCH" ]; then
                echo "‚ö†Ô∏è WARNING: $batch_name exceeds ${MAX_ENTITIES_PER_BATCH} entities limit (${BATCH_ENTITIES})"
                VALIDATION_PASSED=false
              fi
              
              # Gzip compress
              gzip -c "$batch_file" > "${batch_file}.gz"
              
              # V7.2: Calculate SHA-256 hash
              BATCH_HASH=$(sha256sum "${batch_file}.gz" | cut -d' ' -f1)
              COMPRESSED_SIZE=$(stat -c%s "${batch_file}.gz" 2>/dev/null || stat -f%z "${batch_file}.gz")
              
              # Upload to V7.1 path: ingest/batches/
              r2_key="ingest/batches/${batch_name}.json.gz"
              npx wrangler r2 object put "ai-nexus-assets/$r2_key" \
                --file="${batch_file}.gz" \
                --content-type="application/json" \
                --content-encoding="gzip" \
                --remote
              echo "‚úÖ Uploaded: $r2_key (${BATCH_ENTITIES} entities, hash: ${BATCH_HASH:0:8}...)"
              
              # Track batch info with hash
              TOTAL_ENTITIES=$((TOTAL_ENTITIES + BATCH_ENTITIES))
              BATCH_FILES="${BATCH_FILES}{\"key\":\"${r2_key}\",\"entities\":${BATCH_ENTITIES},\"hash\":\"${BATCH_HASH}\",\"size\":${COMPRESSED_SIZE}},"
            fi
          done
          
          # Remove trailing comma
          BATCH_FILES="${BATCH_FILES%,}"
          
          # V1.1-LOCK: INTEGRITY-V1.1 manifest format with total_hash
          # Calculate ordered-concat total_hash
          TOTAL_HASH=""
          for hash in $(echo "$BATCH_FILES" | grep -oP '"hash":"[^"]+' | cut -d'"' -f4 | sort); do
            TOTAL_HASH="${TOTAL_HASH}${hash}"
          done
          TOTAL_HASH_COMPUTED=$(echo -n "$TOTAL_HASH" | sha256sum | cut -d' ' -f1)
          
          # V1.1-LOCK: Generate INTEGRITY-V1.1 manifest using node (avoids heredoc issues)
          node -e "
            const fs = require('fs');
            const manifest = {
              version: 'INTEGRITY-V1.1',
              job_id: '${GITHUB_RUN_ID}',
              stage: 'L1',
              started_at: '${TIMESTAMP}',
              completed_at: new Date().toISOString(),
              status: 'complete',
              source: {
                type: 'multi-source',
                adapters: ['huggingface', 'github', 'arxiv', 'papers', 'ecosystem']
              },
              output: {
                type: 'r2',
                path: 'ingest/batches/',
                total_files: ${BATCH_COUNT},
                total_entities: ${TOTAL_ENTITIES}
              },
              batches: [${BATCH_FILES}],
              checksum: {
                algorithm: 'sha256',
                mode: 'ordered-concat',
                total_hash: 'sha256:${TOTAL_HASH_COMPUTED}'
              },
              config: {
                maxBatchSizeMB: ${MAX_BATCH_SIZE_MB},
                maxEntitiesPerBatch: ${MAX_ENTITIES_PER_BATCH}
              },
              validation_passed: ${VALIDATION_PASSED}
            };
            fs.writeFileSync('data/manifest.json', JSON.stringify(manifest, null, 2));
          "
          
          npx wrangler r2 object put "ai-nexus-assets/ingest/manifest.json" \
            --file="data/manifest.json" \
            --content-type="application/json" \
            --remote
          echo "‚úÖ Uploaded: ingest/manifest.json (V7.2 with hash verification)"
          
          # Also upload merged.json for L5 compatibility (gzipped)
          if [ -f "data/merged.json" ]; then
            echo "üì¶ Uploading entities.json.gz for L5 Heavy Compute..."
            gzip -c "data/merged.json" > "data/entities.json.gz"
            npx wrangler r2 object put "ai-nexus-assets/ingest/current/entities.json.gz" \
              --file="data/entities.json.gz" \
              --content-type="application/json" \
              --content-encoding="gzip" \
              --remote
            echo "‚úÖ Uploaded: ingest/current/entities.json.gz"
          fi
          
          echo ""
          echo "üîÑ V7.1 Batch upload complete: ${BATCH_COUNT} batches, ${TOTAL_ENTITIES} entities"
          echo "üîî L8 Queue Consumer will process batches"
          
      - name: Pipeline Summary
        run: |
          echo ""
          echo "=================================================="
          echo "üéâ V4.2 L1 Parallel Harvester Complete!"
          echo "=================================================="
          echo ""
          echo "üìä Summary:"
          if [ -f "data/merged.json" ]; then
            MODEL_COUNT=$(node -e "console.log(JSON.parse(require('fs').readFileSync('data/merged.json')).length)")
            echo "   Total entities: $MODEL_COUNT"
          fi
          echo "   Batches: $(ls -1 data/raw_batch_*.json 2>/dev/null | wc -l)"
          echo ""
          echo "‚è≠Ô∏è Next: Cloudflare Unified Workflow (every 5 min)"
          echo "   - Ingest raw-data/ ‚Üí D1"
          echo "   - Calculate FNI scores"
          echo "=================================================="
