# Image Processor Workflow V14.5
# Constitution Reference: Art 3.4 (Image Processing), V14.5 Architecture
# 
# Runs after Factory to download, convert (WebP), and upload entity images to R2

name: Image Processor V14.5

on:
  # Trigger after Factory Aggregate completes (To use GitHub Cache)
  workflow_run:
    workflows: ["Factory 3/4 - Aggregate"]
    types: [completed]
  workflow_dispatch:
    inputs:
      batch_size:
        description: 'Number of images to process per run'
        type: number
        default: 2000
      run_id:
        description: 'Aggregate run ID to restore cache from (Manual only)'
        type: string
        required: false

env:
  R2_BUCKET: ai-nexus-assets
  R2_ENDPOINT: https://${{ secrets.CLOUDFLARE_ACCOUNT_ID }}.r2.cloudflarestorage.com
  CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
  CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
  R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
  R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}

jobs:
  # V16.7: Identify upstream Aggregate run for reliable cache restoration
  check-upstream:
    name: Check Upstream
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    outputs:
      upstream-run-id: ${{ steps.get-id.outputs.id }}
    steps:
      - name: Get ID
        id: get-id
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ] && [ -n "${{ inputs.run_id }}" ]; then
            echo "id=${{ inputs.run_id }}" >> $GITHUB_OUTPUT
          else
            # 3.5/4 LINKER and Image Processor are both triggered by 3/4 Aggregate.
            # We must resolve the parent Aggregate run ID to restore the correct cache.
            ID=${{ github.event.workflow_run.id }}
            echo "id=$ID" >> $GITHUB_OUTPUT
            echo "âœ… Targeting Parent Run: $ID"
          fi

  process-images:
    name: Process Entity Images
    needs: check-upstream
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache Rust Dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            tools/rust-img-optimizer/target/
          key: rust-img-${{ hashFiles('tools/rust-img-optimizer/Cargo.lock') }}

      - name: Build Rust Optimizer
        working-directory: tools/rust-img-optimizer
        run: cargo build --release

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install Dependencies
        run: npm ci

      # V16.7: Correct identity restoration using Resolved Upstream ID
      - name: Restore Registry from GitHub Cache
        uses: actions/cache/restore@v4
        with:
          path: output/
          key: entity-data-${{ needs.check-upstream.outputs.upstream-run-id }}
          restore-keys: |
            entity-data-

      # V16.4.1: Fetch Global Entity Registry (Cache Priority)
      - name: Fetch Global Entity Registry
        run: |
          mkdir -p data/images
          
          # Check if registry exists in cache
          if [ -f "output/meta/backup/global-registry.json" ]; then
            echo "âœ… Verified: Registry exists in GitHub Cache. Using local copy."
            cp output/meta/backup/global-registry.json data/registry.json
          else
            echo "âš ï¸ Cache miss: Registry not found in output/meta/backup/. Falling back to R2..."
            npx -y wrangler r2 object get $R2_BUCKET/meta/backup/global-registry.json --file=data/registry.json --remote
          fi
          
          # Process in Node.js
          node -e "
          const fs = require('fs');
          try {
            if (!fs.existsSync('data/registry.json')) {
               console.error('âŒ Registry file missing both in cache and R2 backup');
               process.exit(1);
            }
            const registry = JSON.parse(fs.readFileSync('data/registry.json', 'utf8'));
            const entities = registry.entities || [];
            
            console.log('ðŸ“Š Registry loaded:', entities.length, 'entities');
            
            // Extract image URLs needing processing (Prioritize by FNI score)
            const imageUrls = entities.filter(e => e.image_url && !e.image_url.includes('free2aitools.com'))
              .sort((a, b) => (b.fni_score || 0) - (a.fni_score || 0))
              .slice(0, ${{ inputs.batch_size || 2000 }})
              .map(e => ({
                id: e.id || e.slug,
                source_url: e.image_url,
                type: e.type || 'model'
              }));

            fs.writeFileSync('data/image-queue.json', JSON.stringify(imageUrls, null, 2));
            console.log('âœ… Image queue created:', imageUrls.length, 'tasks');
          } catch (e) {
            console.error('âŒ Failed to process registry:', e.message);
            process.exit(1);
          }
          "

      - name: Download and Convert Images
        run: |
          echo "ðŸ–¼ï¸ Processing images..."
          
          node -e "
          const fs = require('fs');
          const https = require('https');
          const http = require('http');
          const path = require('path');
          
          const queue = JSON.parse(fs.readFileSync('data/image-queue.json', 'utf-8'));
          const results = { success: 0, failed: 0, skipped: 0 };
          
          async function downloadImage(url, dest, depth = 0) {
            if (depth > 5) throw new Error('Too many redirects');
            return new Promise((resolve, reject) => {
              const protocol = url.startsWith('https') ? https : http;
              protocol.get(url, { timeout: 15000 }, (res) => {
                if (res.statusCode === 200) {
                  const file = fs.createWriteStream(dest);
                  res.pipe(file);
                  file.on('finish', () => { file.close(); resolve(true); });
                } else if (res.statusCode >= 300 && res.statusCode < 400 && res.headers.location) {
                  const absoluteUrl = new URL(res.headers.location, url).toString();
                  downloadImage(absoluteUrl, dest, depth + 1).then(resolve).catch(reject);
                } else {
                  reject(new Error('Status: ' + res.statusCode));
                }
              }).on('error', reject);
            });
          }

          const sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms));
          
          async function processQueue() {
            for (const item of queue) {
              const safeId = (item.id || 'unknown').replace(/[/:]/g, '--');
              const type = item.type || 'model';
              const destDir = path.join('data/images', type);
              if (!fs.existsSync(destDir)) fs.mkdirSync(destDir, { recursive: true });
              
              const destPath = path.join(destDir, safeId + '.jpg');
              
              try {
                if (fs.existsSync(destPath)) { results.skipped++; continue; }
                
                // V16.4.4: Polite Crawling (500ms - 1500ms jitter)
                const delay = 500 + Math.random() * 1000;
                await sleep(delay);

                await downloadImage(item.source_url, destPath);
                results.success++;
                console.log('âœ… [' + type + '] ' + safeId);
              } catch (err) {
                results.failed++;
                console.log('âŒ [' + type + '] ' + safeId + ': ' + err.message);
              }
            }
            console.log('Results:', results);
            fs.writeFileSync('data/image-results.json', JSON.stringify(results, null, 2));
          }
          
          (async () => {
            try {
              await processQueue();
              console.log('âœ… Queue processing finished');
            } catch (e) {
              console.error('âŒ Queue processing failed:', e.message);
              process.exit(1);
            }
          })();
          "

      - name: Convert to WebP (Rust)
        working-directory: tools/rust-img-optimizer
        run: |
          echo "ðŸ”„ Converting to WebP..."
          mkdir -p ../../output/images
          find ../../data/images -name "*.jpg" | while read img; do
            type_dir=$(basename $(dirname "$img"))
            mkdir -p "../../output/images/$type_dir"
            base_name=$(basename "$img" .jpg)
            ./target/release/rust-img-optimizer convert --input "$img" --output "../../output/images/$type_dir/${base_name}.webp" --format webp --quality 75 --max-width 640 2>/dev/null || true
          done

      # V16.7: Standardized R2 Upload (WebP Only Policy)
      - name: Upload Images to R2 (S3 API)
        env:
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ai-nexus-assets
        run: |
          echo "ðŸ“¤ Uploading WebP images to R2..."
          node scripts/factory/r2-upload-s3.js

      - name: Update Global Registry (Progressive Drain)
        run: |
          echo "ðŸ“ Updating registry with CDN URLs..."
          # Move output/images to data/images locally for the processor to see them as 'processed'
          cp -r output/images/* data/images/
          node scripts/factory/lib/image-post-processor.js data/registry.json data/image-results.json
          
          # Sync back to output for persistence
          cp data/registry.json output/meta/backup/global-registry.json
          
      - name: Persist Updated Registry to GitHub Cache
        uses: actions/cache/save@v4
        with:
          path: output/
          key: entity-data-${{ needs.check-upstream.outputs.upstream-run-id }}-img

      - name: Summary
        run: |
          echo "## ðŸ–¼ï¸ Image Processor Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f "data/image-results.json" ]; then
            cat data/image-results.json >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          WEBP_COUNT=$(find output/images -name "*.webp" | wc -l)
          echo "- WebP images created/uploaded: $WEBP_COUNT" >> $GITHUB_STEP_SUMMARY
