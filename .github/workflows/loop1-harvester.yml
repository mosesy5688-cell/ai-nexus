name: L1 Harvester - Daily Data Ingestion

# V3.1 Constitution: Loop 1 - Harvester
# Frequency: Daily
# Purpose: Multi-source fetch (HuggingFace/GitHub/ArXiv) â†’ Process â†’ Rankings â†’ Publish

on:
  schedule:
    - cron: '0 3 * * *' # Daily at 03:00 UTC (11:00 Beijing Time)
  workflow_dispatch:

jobs:
  harvest:
    name: Harvest & Process Data
    runs-on: ubuntu-latest
    steps:
      # === PHASE 1: SETUP ===
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 20

      - name: Install Dependencies
        run: npm ci

      # === PHASE 2: DATA COLLECTION (L1 Core) ===
      - name: Fetch & Enrich Model Data (V3.2 Pipeline)
        env:
          CF_PROXY_URL: ${{ secrets.CF_PROXY_URL }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "ðŸš€ L1 Harvester: Running V3.2 Universal Ingestion Pipeline..."
          node scripts/ingestion/orchestrator.js
          echo "âœ… Data fetched with full README, compliance checks, and quality scores"

      # === PHASE 3: RUST PROCESSING ===
      - name: Process Data (Rust)
        env:
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_PUBLIC_URL_PREFIX: "https://cdn.free2aitools.com"
        run: |
          echo "Building and running Rust optimizer..."
          cd tools/rust-img-optimizer
          cargo run --release -- --input ../../data/merged.json
          mv data/upsert.sql ../../data/upsert.sql
          mv data/update_urls.sql ../../data/update_urls.sql
          echo "âœ… Data processed. Generated upsert.sql and update_urls.sql"

      # === PHASE 4: R2 UPLOAD ===
      - name: Upload Images to R2
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "ðŸ“· Uploading images to R2..."
          for file in data/images/*.webp; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              npx wrangler r2 object put "ai-nexus-assets/models/$filename" --file="$file"
            fi
          done
          echo "âœ… Images uploaded successfully"
      
      - name: Upload Docs to R2 (Full README - No Truncation)
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "ðŸ“„ Uploading docs to R2 (V3.1 Pillar III: Data Integrity)..."
          doc_count=0
          for file in data/docs/*.md; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              npx wrangler r2 object put "ai-nexus-assets/docs/$filename" --file="$file" --content-type="text/markdown"
              doc_count=$((doc_count + 1))
            fi
          done
          echo "âœ… Uploaded $doc_count docs to R2"

      # === PHASE 5: QUALITY GATE ===
      - name: Validate SQL Output (Quality Gate)
        run: |
          echo "ðŸ” Validating SQL quality..."
          
          if [ ! -s data/upsert.sql ]; then
            echo "âŒ FATAL: upsert.sql is missing or empty!"
            exit 1
          fi
          
          TOTAL_UPDATES=$(grep -c 'UPDATE models' data/update_urls.sql || echo 0)
          URL_UPDATES=$(grep -c 'https://cdn.free2aitools.com' data/update_urls.sql || echo 0)
          SUCCESS_RATE=0
          if [ "$TOTAL_UPDATES" -gt 0 ]; then
            SUCCESS_RATE=$((URL_UPDATES * 100 / TOTAL_UPDATES))
          fi

          echo "  Total UPDATE statements: $TOTAL_UPDATES"
          echo "  With image URLs: $URL_UPDATES"
          echo "  Success Rate: $SUCCESS_RATE%"

          if [ "$SUCCESS_RATE" -lt 80 ]; then
            echo "âŒ FATAL: Image URL success rate ($SUCCESS_RATE%) is below 80% threshold."
            exit 1
          fi
          
          echo "âœ… Quality Gate PASSED"

      # === PHASE 6: D1 UPDATE (TRANSACTIONAL MICRO-BATCHES) ===
      # Optimized per Architect review:
      # - Transaction wrapping: BEGIN/COMMIT for atomicity + 10x speed
      # - 50 rows/batch: Conservative for FTS5 CPU overhead
      # - Retry logic: 3 attempts with 5s cooldown
      # - Breathing room: 2s between batches
      - name: Execute D1 Updates (Batched with Retry)
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "ðŸ”„ Executing D1 updates with transactional micro-batches..."
          
          # Split SQL files into transaction-wrapped batches (50 rows, 100KB max)
          node scripts/split-sql-batch.js ./data/upsert.sql ./data/upsert_batches
          node scripts/split-sql-batch.js ./data/update_urls.sql ./data/url_batches
          
          # Function to execute batch with retry
          execute_with_retry() {
            local batch=$1
            local max_attempts=3
            
            for i in $(seq 1 $max_attempts); do
              echo "   Executing $(basename $batch) (attempt $i/$max_attempts)..."
              if npx wrangler d1 execute ai-nexus-db --remote --file="$batch"; then
                echo "   âœ… Success: $(basename $batch)"
                return 0
              else
                echo "   âš ï¸ Failed: $(basename $batch) (attempt $i/$max_attempts)"
                if [ $i -eq $max_attempts ]; then
                  echo "   âŒ FATAL: Failed after $max_attempts attempts"
                  return 1
                fi
                echo "   â³ Cooling down for 5 seconds..."
                sleep 5
              fi
            done
          }
          
          # Execute upsert batches with retry and breathing room
          echo "ðŸ“¦ Executing upsert batches..."
          for batch in ./data/upsert_batches/batch_*.sql; do
            # Check existence and size
            # Valid INSERT header is >350 chars, so any valid batch must be >500 bytes
            if [ -f "$batch" ]; then
              FILE_SIZE=$(wc -c <"$batch")
              if [ "$FILE_SIZE" -lt 500 ]; then
                echo "   âš ï¸ Skipping small/invalid batch: $(basename $batch) ($FILE_SIZE bytes)"
                continue
              fi
              
              if ! execute_with_retry "$batch"; then
                exit 1
              fi
              # Breathing room for D1 between batches
              sleep 2
            fi
          done
          
          # Execute URL update batches with retry and breathing room
          echo "ðŸ“¦ Executing URL update batches..."
          for batch in ./data/url_batches/batch_*.sql; do
            if [ -f "$batch" ]; then
              FILE_SIZE=$(wc -c <"$batch")
              if [ "$FILE_SIZE" -lt 500 ]; then
                echo "   âš ï¸ Skipping small/invalid batch: $(basename $batch) ($FILE_SIZE bytes)"
                continue
              fi

              if ! execute_with_retry "$batch"; then
                exit 1
              fi
              sleep 2
            fi
          done
          
          echo "âœ… D1 updates completed successfully"

      # === PHASE 6B: R2-FIRST LAKEHOUSE (SCALABLE D1 INGESTION) ===
      # V3.2 Architecture: JSON batches via R2 + Astro API
      # Benefits: 
      # - Bypasses D1 SQL parsing limits
      # - Internal CF network (10x faster)
      # - Parameterized queries (safer, no SQL injection)
      # - Scales to 50K+ entities (Sprint 2 ready)
      - name: R2-First Lakehouse Ingestion
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          ADMIN_SECRET: ${{ secrets.ADMIN_SECRET }}
          SITE_URL: ${{ vars.SITE_URL }}
        run: |
          echo "ðŸ—ï¸ R2-First Lakehouse: Uploading JSON batches..."
          
          # Check if ingest directory has files
          if [ ! -d data/ingest ] || [ -z "$(ls -A data/ingest/*.json 2>/dev/null)" ]; then
            echo "âš ï¸ No JSON batches found in data/ingest/, skipping Lakehouse ingestion"
            exit 0
          fi
          
          # Upload JSON batches to R2
          batch_count=0
          for file in data/ingest/*.json; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              echo "  ðŸ“¤ Uploading $filename to R2..."
              npx wrangler r2 object put "ai-nexus-assets/ingest/$filename" --file="$file" --content-type="application/json"
              batch_count=$((batch_count + 1))
            fi
          done
          
          echo "ðŸ“¦ Uploaded $batch_count JSON batches to R2"
          
          # Trigger Astro API to ingest each batch
          echo "ðŸ”„ Triggering D1 ingestion via API..."
          
          SITE_URL="${SITE_URL:-https://free2aitools.com}"
          success_count=0
          fail_count=0
          
          for file in data/ingest/*.json; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              echo "  ðŸ“¥ Ingesting $filename..."
              
              # Call API with retry
              response=$(curl -s -w "\n%{http_code}" -X POST "${SITE_URL}/api/admin/ingest" \
                -H "X-Admin-Key: ${ADMIN_SECRET}" \
                -H "Content-Type: application/json" \
                -d "{\"filename\": \"$filename\"}" || echo "CURL_FAILED")
              
              http_code=$(echo "$response" | tail -n1)
              body=$(echo "$response" | sed '$d')
              
              if [ "$http_code" = "200" ]; then
                echo "    âœ… Success: $filename"
                success_count=$((success_count + 1))
              else
                echo "    âš ï¸ Failed: $filename (HTTP $http_code)"
                echo "    Response: $body"
                fail_count=$((fail_count + 1))
              fi
              
              # Breathing room
              sleep 1
            fi
          done
          
          echo ""
          echo "ðŸ R2-First Lakehouse Summary:"
          echo "  âœ… Successful: $success_count batches"
          echo "  âŒ Failed: $fail_count batches"
          
          if [ $fail_count -gt 0 ]; then
            echo "âš ï¸ Some batches failed, but continuing..."
          fi

      # === PHASE 7: POST-PROCESS (Rankings & Tags) ===
      # CRITICAL: Integrated from post-process.yml for data freshness
      - name: Run Post-Processing (Rankings & Tags)
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "ðŸ† Running Post-Processing for fresh rankings..."
          node scripts/run-post-processing.js
          echo "âœ… Rankings, tags, and search index generated"

      # === PHASE 8: ARTIFACTS ===
      - name: Upload ingest artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: l1-harvester-logs
          path: |
            data/raw.json
            data/merged.json
            data/upsert.sql
            data/update_urls.sql
            public/data/keywords.json
            public/data/rankings.json
            public/data/search-index.json
          retention-days: 30

      - name: Summary
        if: always()
        run: |
          echo "## ðŸŒ¾ L1 Harvester Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Phase | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Data Collection | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| Rust Processing | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| R2 Upload | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| D1 Update | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| Post-Process | âœ… |" >> $GITHUB_STEP_SUMMARY
