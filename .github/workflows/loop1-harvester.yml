name: L1 Harvester - Daily Data Ingestion

# V3.1 Constitution: Loop 1 - Harvester
# Frequency: Daily
# Purpose: Multi-source fetch (HuggingFace/GitHub/ArXiv) â†’ Process â†’ Rankings â†’ Publish

on:
  schedule:
    - cron: '0 3 * * *' # Daily at 03:00 UTC (11:00 Beijing Time)
  workflow_dispatch:

jobs:
  harvest:
    name: Harvest & Process Data
    runs-on: ubuntu-latest
    steps:
      # === PHASE 1: SETUP ===
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 20

      - name: Install Dependencies
        run: npm ci

      # === PHASE 2: DATA COLLECTION (L1 Core) ===
      - name: Fetch & Enrich Model Data (V3.2 Pipeline)
        env:
          CF_PROXY_URL: ${{ secrets.CF_PROXY_URL }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "ðŸš€ L1 Harvester: Running V3.2 Universal Ingestion Pipeline..."
          node scripts/ingestion/orchestrator.js
          echo "âœ… Data fetched with full README, compliance checks, and quality scores"

      # === PHASE 3: RUST PROCESSING ===
      - name: Process Data (Rust)
        env:
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_PUBLIC_URL_PREFIX: "https://cdn.free2aitools.com"
        run: |
          echo "Building and running Rust optimizer..."
          cd tools/rust-img-optimizer
          cargo run --release -- --input ../../data/merged.json
          mv data/upsert.sql ../../data/upsert.sql
          mv data/update_urls.sql ../../data/update_urls.sql
          echo "âœ… Data processed. Generated upsert.sql and update_urls.sql"

      # === PHASE 4: R2 UPLOAD ===
      - name: Upload Images to R2
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "ðŸ“· Uploading images to R2..."
          for file in data/images/*.webp; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              npx wrangler r2 object put "ai-nexus-assets/models/$filename" --file="$file"
            fi
          done
          echo "âœ… Images uploaded successfully"
      
      - name: Upload Docs to R2 (Full README - No Truncation)
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "ðŸ“„ Uploading docs to R2 (V3.1 Pillar III: Data Integrity)..."
          doc_count=0
          for file in data/docs/*.md; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              npx wrangler r2 object put "ai-nexus-assets/docs/$filename" --file="$file" --content-type="text/markdown"
              doc_count=$((doc_count + 1))
            fi
          done
          echo "âœ… Uploaded $doc_count docs to R2"

      # === PHASE 5: QUALITY GATE ===
      - name: Validate SQL Output (Quality Gate)
        run: |
          echo "ðŸ” Validating SQL quality..."
          
          if [ ! -s data/upsert.sql ]; then
            echo "âŒ FATAL: upsert.sql is missing or empty!"
            exit 1
          fi
          
          TOTAL_UPDATES=$(grep -c 'UPDATE models' data/update_urls.sql || echo 0)
          URL_UPDATES=$(grep -c 'https://cdn.free2aitools.com' data/update_urls.sql || echo 0)
          SUCCESS_RATE=0
          if [ "$TOTAL_UPDATES" -gt 0 ]; then
            SUCCESS_RATE=$((URL_UPDATES * 100 / TOTAL_UPDATES))
          fi

          echo "  Total UPDATE statements: $TOTAL_UPDATES"
          echo "  With image URLs: $URL_UPDATES"
          echo "  Success Rate: $SUCCESS_RATE%"

          if [ "$SUCCESS_RATE" -lt 80 ]; then
            echo "âŒ FATAL: Image URL success rate ($SUCCESS_RATE%) is below 80% threshold."
            exit 1
          fi
          
          echo "âœ… Quality Gate PASSED"

      # === PHASE 6: D1 UPDATE ===
      - name: Execute D1 Updates
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "ðŸ”„ Executing D1 updates..."
          npx wrangler d1 execute ai-nexus-db --remote --file="./data/upsert.sql"
          npx wrangler d1 execute ai-nexus-db --remote --file="./data/update_urls.sql"
          echo "âœ… D1 updates completed"

      # === PHASE 7: POST-PROCESS (Rankings & Tags) ===
      # CRITICAL: Integrated from post-process.yml for data freshness
      - name: Run Post-Processing (Rankings & Tags)
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "ðŸ† Running Post-Processing for fresh rankings..."
          node scripts/run-post-processing.js
          echo "âœ… Rankings, tags, and search index generated"

      # === PHASE 8: ARTIFACTS ===
      - name: Upload ingest artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: l1-harvester-logs
          path: |
            data/raw.json
            data/merged.json
            data/upsert.sql
            data/update_urls.sql
            public/data/keywords.json
            public/data/rankings.json
            public/data/search-index.json
          retention-days: 30

      - name: Summary
        if: always()
        run: |
          echo "## ðŸŒ¾ L1 Harvester Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Phase | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Data Collection | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| Rust Processing | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| R2 Upload | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| D1 Update | âœ… |" >> $GITHUB_STEP_SUMMARY
          echo "| Post-Process | âœ… |" >> $GITHUB_STEP_SUMMARY
