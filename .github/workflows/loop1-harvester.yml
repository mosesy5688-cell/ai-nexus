name: L1 Harvester - V4.1 Collection Pipeline

# V4.1 Constitution: Pillar VIII - Cloudflare-First Processing
# GitHub: Collection + Asset Processing ONLY
# Cloudflare Unified Workflow: Data cleaning + FNI + D1 ingestion

on:
  schedule:
    - cron: '0 3 * * *'  # Daily at 03:00 UTC
  workflow_dispatch:

concurrency:
  group: l1-harvester
  cancel-in-progress: true

jobs:
  harvest:
    name: V4.1 Harvest & Upload
    runs-on: ubuntu-latest
    
    steps:
      # ============================================================
      # PHASE 1: SETUP
      # ============================================================
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 20

      - name: Install Dependencies
        run: npm ci

      # ============================================================
      # PHASE 2: DATA COLLECTION (Orchestrator)
      # ============================================================
      - name: Fetch Model Data (Multi-Source)
        env:
          CF_PROXY_URL: ${{ secrets.CF_PROXY_URL }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          MODELSCOPE_API_TOKEN: ${{ secrets.MODELSCOPE_API_TOKEN }}
        run: |
          echo "üöÄ V4.1 L1 Harvester: Collection Phase"
          echo "=================================================="
          node scripts/ingestion/orchestrator.js
          
          # Verify output
          if [ -f "data/merged.json" ]; then
            MODEL_COUNT=$(node -e "console.log(JSON.parse(require('fs').readFileSync('data/merged.json')).length)")
            echo "‚úÖ Collected $MODEL_COUNT models"
          else
            echo "‚ùå Error: merged.json not generated"
            exit 1
          fi

      # ============================================================
      # PHASE 2B: V6.2 SPACES COLLECTION
      # ============================================================
      - name: Fetch Spaces Data (V6.2)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "üöÄ V6.2 Spaces Harvester"
          echo "=================================================="
          node scripts/harvest-spaces.js --limit=200
          
          if [ -f "data/spaces.json" ]; then
            SPACE_COUNT=$(node -e "console.log(JSON.parse(require('fs').readFileSync('data/spaces.json')).length)")
            echo "‚úÖ Collected $SPACE_COUNT spaces"
          else
            echo "‚ö†Ô∏è Warning: spaces.json not generated (non-fatal)"
          fi

      # ============================================================
      # PHASE 2C: V6.2 DATASETS COLLECTION
      # ============================================================
      - name: Fetch Datasets Data (V6.2)
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "üöÄ V6.2 Datasets Harvester"
          echo "=================================================="
          node scripts/harvest-datasets.js --limit=500
          
          if [ -f "data/datasets.json" ]; then
            DATASET_COUNT=$(node -e "console.log(JSON.parse(require('fs').readFileSync('data/datasets.json')).length)")
            echo "‚úÖ Collected $DATASET_COUNT datasets"
          else
            echo "‚ö†Ô∏è Warning: datasets.json not generated (non-fatal)"
          fi

      # ============================================================
      # PHASE 3: RUST IMAGE PROCESSING (Optional - if images exist)
      # ============================================================
      - name: Setup Rust (if images need processing)
        uses: dtolnay/rust-toolchain@stable
        if: hashFiles('data/images_to_process.json') != ''

      - name: Process Images with Rust
        if: hashFiles('data/images_to_process.json') != ''
        env:
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_PUBLIC_URL_PREFIX: "https://cdn.free2aitools.com"
        run: |
          echo "üì∑ Processing images with Rust optimizer..."
          cd tools/rust-img-optimizer
          cargo run --release -- --input ../../data/merged.json
          echo "‚úÖ Image processing complete"

      # ============================================================
      # PHASE 4: R2 UPLOAD (Dump & Go)
      # V4.1: Upload everything to R2, let Cloudflare Workflow handle D1
      # ============================================================
      - name: Upload Images to R2
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "üì∑ Uploading images to R2..."
          if [ -d "data/images" ]; then
            for file in data/images/*.webp; do
              if [ -f "$file" ]; then
                filename=$(basename "$file")
                npx wrangler r2 object put "ai-nexus-assets/models/$filename" --file="$file" --remote
              fi
            done
            echo "‚úÖ Images uploaded"
          else
            echo "‚ÑπÔ∏è No images to upload"
          fi
      
      - name: Upload Docs to R2 (Full README)
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "üìÑ Uploading docs to R2..."
          doc_count=0
          if [ -d "data/docs" ]; then
            for file in data/docs/*.md; do
              if [ -f "$file" ]; then
                filename=$(basename "$file")
                npx wrangler r2 object put "ai-nexus-assets/docs/$filename" --file="$file" --content-type="text/markdown" --remote
                doc_count=$((doc_count + 1))
              fi
            done
          fi
          echo "‚úÖ Uploaded $doc_count docs to R2"

      # ============================================================
      # PHASE 5: UPLOAD RAW DATA TO R2 (‚≠ê V4.1 Core)
      # The Unified Cloudflare Workflow will consume this
      # ============================================================
      - name: Upload Raw Data Batches to R2
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "üì¶ V4.1: Uploading raw data to R2..."
          echo "=================================================="
          
          # Generate timestamp for batch naming
          TIMESTAMP=$(date -u +"%Y%m%dT%H%M%SZ")
          
          # Split merged.json into batches and upload
          node -e "
          const fs = require('fs');
          const models = JSON.parse(fs.readFileSync('data/merged.json'));
          const BATCH_SIZE = 50;
          
          for (let i = 0; i < models.length; i += BATCH_SIZE) {
            const batch = models.slice(i, i + BATCH_SIZE);
            const batchNum = String(Math.floor(i/BATCH_SIZE)).padStart(3, '0');
            const filename = 'data/raw_batch_' + batchNum + '.json';
            fs.writeFileSync(filename, JSON.stringify(batch, null, 2));
            console.log('Created: ' + filename + ' (' + batch.length + ' models)');
          }
          console.log('Total batches: ' + Math.ceil(models.length/BATCH_SIZE));
          "
          
          # Upload each batch to R2 raw-data/
          for batch_file in data/raw_batch_*.json; do
            if [ -f "$batch_file" ]; then
              batch_name=$(basename "$batch_file" .json)
              r2_key="raw-data/${TIMESTAMP}_${batch_name}.json"
              npx wrangler r2 object put "ai-nexus-assets/$r2_key" --file="$batch_file" --content-type="application/json" --remote
              echo "‚úÖ Uploaded: $r2_key"
            fi
          done
          
          echo ""
          echo "üîÑ Raw data uploaded to R2"
          echo "üîî Unified Workflow will process within 5 minutes"

      # ============================================================
      # PHASE 6: SUMMARY (No D1 writes from GitHub!)
      # ============================================================
      - name: Pipeline Summary
        run: |
          echo ""
          echo "=================================================="
          echo "üéâ V4.1 L1 Harvester Complete!"
          echo "=================================================="
          echo ""
          echo "üìä Summary:"
          if [ -f "data/merged.json" ]; then
            MODEL_COUNT=$(node -e "console.log(JSON.parse(require('fs').readFileSync('data/merged.json')).length)")
            echo "   Models collected: $MODEL_COUNT"
          fi
          echo "   Images uploaded: $(ls -1 data/images/*.webp 2>/dev/null | wc -l || echo 0)"
          echo "   Docs uploaded: $(ls -1 data/docs/*.md 2>/dev/null | wc -l || echo 0)"
          echo "   Raw batches: $(ls -1 data/raw_batch_*.json 2>/dev/null | wc -l || echo 0)"
          echo ""
          echo "‚è≥ Next: Cloudflare Unified Workflow (every 5 min)"
          echo "   - Ingest raw-data/ ‚Üí D1"
          echo "   - Calculate FNI scores"
          echo "   - Log to workflow_logs"
          echo ""
          echo "üîó Constitution: V4.1 Pillar VIII (Cloudflare-First)"
          echo "=================================================="
