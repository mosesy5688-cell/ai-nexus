name: L1 Harvester - Daily Data Ingestion

# V3.1 Constitution: Loop 1 - Harvester
# Frequency: Daily
# Purpose: Multi-source fetch (HuggingFace/GitHub/ArXiv) ‚Üí Process ‚Üí Rankings ‚Üí Publish

on:
  schedule:
    - cron: '0 3 * * *' # Daily at 03:00 UTC (11:00 Beijing Time)
  workflow_dispatch:

jobs:
  harvest:
    name: Harvest & Process Data
    runs-on: ubuntu-latest
    steps:
      # === PHASE 1: SETUP ===
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 20

      - name: Install Dependencies
        run: npm ci

      # === PHASE 2: DATA COLLECTION (L1 Core) ===
      - name: Fetch & Enrich Model Data (V3.2 Pipeline)
        env:
          CF_PROXY_URL: ${{ secrets.CF_PROXY_URL }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "üöÄ L1 Harvester: Running V3.2 Universal Ingestion Pipeline..."
          node scripts/ingestion/orchestrator.js
          echo "‚úÖ Data fetched with full README, compliance checks, and quality scores"

      # === PHASE 3: RUST PROCESSING ===
      - name: Process Data (Rust)
        env:
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_PUBLIC_URL_PREFIX: "https://cdn.free2aitools.com"
        run: |
          echo "Building and running Rust optimizer..."
          cd tools/rust-img-optimizer
          cargo run --release -- --input ../../data/merged.json
          mv data/upsert.sql ../../data/upsert.sql
          mv data/update_urls.sql ../../data/update_urls.sql
          echo "‚úÖ Data processed. Generated upsert.sql and update_urls.sql"

      # === PHASE 4: R2 UPLOAD ===
      - name: Upload Images to R2
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "üì∑ Uploading images to R2..."
          for file in data/images/*.webp; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              npx wrangler r2 object put "ai-nexus-assets/models/$filename" --file="$file"
            fi
          done
          echo "‚úÖ Images uploaded successfully"
      
      - name: Upload Docs to R2 (Full README - No Truncation)
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "üìÑ Uploading docs to R2 (V3.1 Pillar III: Data Integrity)..."
          doc_count=0
          for file in data/docs/*.md; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              npx wrangler r2 object put "ai-nexus-assets/docs/$filename" --file="$file" --content-type="text/markdown"
              doc_count=$((doc_count + 1))
            fi
          done
          echo "‚úÖ Uploaded $doc_count docs to R2"

      # === PHASE 5: QUALITY GATE ===
      - name: Validate SQL Output (Quality Gate)
        run: |
          echo "üîç Validating SQL quality..."
          
          if [ ! -s data/upsert.sql ]; then
            echo "‚ùå FATAL: upsert.sql is missing or empty!"
            exit 1
          fi
          
          TOTAL_UPDATES=$(grep -c 'UPDATE models' data/update_urls.sql || echo 0)
          URL_UPDATES=$(grep -c 'https://cdn.free2aitools.com' data/update_urls.sql || echo 0)
          SUCCESS_RATE=0
          if [ "$TOTAL_UPDATES" -gt 0 ]; then
            SUCCESS_RATE=$((URL_UPDATES * 100 / TOTAL_UPDATES))
          fi

          echo "  Total UPDATE statements: $TOTAL_UPDATES"
          echo "  With image URLs: $URL_UPDATES"
          echo "  Success Rate: $SUCCESS_RATE%"

          if [ "$SUCCESS_RATE" -lt 80 ]; then
            echo "‚ùå FATAL: Image URL success rate ($SUCCESS_RATE%) is below 80% threshold."
            exit 1
          fi
          
          echo "‚úÖ Quality Gate PASSED"

      # === PHASE 6: D1 UPDATE (TRANSACTIONAL MICRO-BATCHES) ===
      # Optimized per Architect review:
      # - Transaction wrapping: BEGIN/COMMIT for atomicity + 10x speed
      # - 50 rows/batch: Conservative for FTS5 CPU overhead
      # - Retry logic: 3 attempts with 5s cooldown
      # - Breathing room: 2s between batches
      - name: Execute D1 Updates (Batched with Retry)
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "üîÑ Executing D1 updates with transactional micro-batches..."
          
          # Split SQL files into transaction-wrapped batches (50 rows, 100KB max)
          node scripts/split-sql-batch.js ./data/upsert.sql ./data/upsert_batches
          node scripts/split-sql-batch.js ./data/update_urls.sql ./data/url_batches
          
          # Function to execute batch with retry
          execute_with_retry() {
            local batch=$1
            local max_attempts=3
            
            for i in $(seq 1 $max_attempts); do
              echo "   Executing $(basename $batch) (attempt $i/$max_attempts)..."
              if npx wrangler d1 execute ai-nexus-db --remote --file="$batch"; then
                echo "   ‚úÖ Success: $(basename $batch)"
                return 0
              else
                echo "   ‚ö†Ô∏è Failed: $(basename $batch) (attempt $i/$max_attempts)"
                if [ $i -eq $max_attempts ]; then
                  echo "   ‚ùå FATAL: Failed after $max_attempts attempts"
                  return 1
                fi
                echo "   ‚è≥ Cooling down for 5 seconds..."
                sleep 5
              fi
            done
          }
          
          # Execute upsert batches with retry and breathing room
          echo "üì¶ Executing upsert batches..."
          for batch in ./data/upsert_batches/batch_*.sql; do
            # Check existence and size (skip empty/small files < 25 bytes)
            if [ -f "$batch" ]; then
              FILE_SIZE=$(wc -c <"$batch")
              if [ "$FILE_SIZE" -lt 25 ]; then
                echo "   ‚ö†Ô∏è Skipping empty/small batch: $(basename $batch) ($FILE_SIZE bytes)"
                continue
              fi
              
              if ! execute_with_retry "$batch"; then
                exit 1
              fi
              # Breathing room for D1 between batches
              sleep 2
            fi
          done
          
          # Execute URL update batches with retry and breathing room
          echo "üì¶ Executing URL update batches..."
          for batch in ./data/url_batches/batch_*.sql; do
            if [ -f "$batch" ]; then
              FILE_SIZE=$(wc -c <"$batch")
              if [ "$FILE_SIZE" -lt 25 ]; then
                echo "   ‚ö†Ô∏è Skipping empty/small batch: $(basename $batch) ($FILE_SIZE bytes)"
                continue
              fi

              if ! execute_with_retry "$batch"; then
                exit 1
              fi
              sleep 2
            fi
          done
          
          echo "‚úÖ D1 updates completed successfully"

      # === PHASE 7: POST-PROCESS (Rankings & Tags) ===
      # CRITICAL: Integrated from post-process.yml for data freshness
      - name: Run Post-Processing (Rankings & Tags)
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
        run: |
          echo "üèÜ Running Post-Processing for fresh rankings..."
          node scripts/run-post-processing.js
          echo "‚úÖ Rankings, tags, and search index generated"

      # === PHASE 8: ARTIFACTS ===
      - name: Upload ingest artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: l1-harvester-logs
          path: |
            data/raw.json
            data/merged.json
            data/upsert.sql
            data/update_urls.sql
            public/data/keywords.json
            public/data/rankings.json
            public/data/search-index.json
          retention-days: 30

      - name: Summary
        if: always()
        run: |
          echo "## üåæ L1 Harvester Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Phase | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Data Collection | ‚úÖ |" >> $GITHUB_STEP_SUMMARY
          echo "| Rust Processing | ‚úÖ |" >> $GITHUB_STEP_SUMMARY
          echo "| R2 Upload | ‚úÖ |" >> $GITHUB_STEP_SUMMARY
          echo "| D1 Update | ‚úÖ |" >> $GITHUB_STEP_SUMMARY
          echo "| Post-Process | ‚úÖ |" >> $GITHUB_STEP_SUMMARY
