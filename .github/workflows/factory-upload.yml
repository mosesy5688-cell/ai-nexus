# Factory 4/4 - Upload
# V14.5 Phase 4: Modular Factory Architecture
# 
# Triggers: workflow_run from Aggregate, or manual dispatch
# Inputs: factory-output artifact from Aggregate stage
# Actions: Upload to R2, CDN purge, Health check
# 
# Constitution: Art 13.4 (Non-Destructive), Art 6.2 (CDN)

name: Factory 4/4 - Upload

on:
  workflow_run:
    workflows: ["Factory 3/4 - Aggregate"]
    types: [completed]
  workflow_dispatch:
    inputs:
      run_id:
        description: 'Aggregate workflow run ID'
        required: false
        type: string

env:
  CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
  CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
  CLOUDFLARE_ZONE_ID: ${{ secrets.CLOUDFLARE_ZONE_ID }}
  R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
  R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
  R2_BUCKET: ai-nexus-assets

jobs:
  check-upstream:
    name: Check Upstream
    runs-on: ubuntu-latest
    outputs:
      upstream-run-id: ${{ steps.get-id.outputs.id }}
    steps:
      - name: Verify Upstream Conclusion
        if: ${{ github.event_name == 'workflow_run' && github.event.workflow_run.conclusion != 'success' }}
        run: |
          echo "âŒ ERROR: Upstream workflow '${{ github.event.workflow_run.name }}' failed with conclusion '${{ github.event.workflow_run.conclusion }}'."
          echo "Aborting Factory 4/4 to prevent data corruption."
          exit 1
      - name: Get ID
        id: get-id
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # V17.4: Resolution logic for chained workflows
          # 1. Start with the manual input
          AGGREGATE_RUN_ID=${{ inputs.run_id }}
          
          # 2. If missing (automatic trigger), find the latest successful Aggregate run on MAIN
          # This works regardless of whether triggered by Linker or Aggregate, as Linker is downstream of Aggregate.
          if [ -z "$AGGREGATE_RUN_ID" ] || [ "$AGGREGATE_RUN_ID" == "null" ]; then
            echo "ðŸ” Resolving latest successful Aggregate run from main branch..."
            AGGREGATE_RUN_ID=$(gh run list --workflow factory-aggregate.yml --branch main --status success --limit 1 --json databaseId --jq '.[0].databaseId' | tr -d '[:space:]')
          fi
          
          echo "id=$AGGREGATE_RUN_ID" >> $GITHUB_OUTPUT
          echo "âœ… Resolved Aggregate ID (Source of Truth): $AGGREGATE_RUN_ID"

  upload:
    name: Upload to R2
    needs: check-upstream
    runs-on: ubuntu-latest
    env:
      # V17.2: Safety margin for large file operations
      NODE_OPTIONS: '--max-old-space-size=4096'
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install Dependencies
        run: npm ci

      # V4.1: Restore from Cache (Not Artifact)
      - name: Restore Cycle Output from Cache
        uses: actions/cache/restore@v4
        with:
          path: output/
          key: cycle-${{ needs.check-upstream.outputs.upstream-run-id }}-output
          restore-keys: |
            cycle-

      - name: Validate Output Structure
        run: |
          if [ ! -f "output/entities.json" ]; then
            echo "âŒ ERROR: output/entities.json not found!"
            exit 1
          fi
          echo "âœ… Output structure validated"
          ls -la output/

      # V17.2: Restore Linker output and merge with Aggregate output
      - name: Restore Linker Output from Cache
        uses: actions/cache/restore@v4
        id: linker-cache
        with:
          path: linker-output/
          key: linker-${{ needs.check-upstream.outputs.upstream-run-id }}-output
          restore-keys: |
            linker-
          
      - name: Merge Linker Output
        run: |
          if [ -d "linker-output" ]; then
            echo "ðŸ“¦ Merging Linker output into main output..."
            # Copy Linker's cache/* to output/cache/
            cp -r linker-output/cache/* output/cache/ 2>/dev/null || true
            # Copy Linker's rss/ to output/rss/
            cp -r linker-output/rss output/ 2>/dev/null || true
            echo "âœ… Linker output merged"
            ls -la output/cache/
          else
            echo "âš ï¸ No Linker cache found, continuing without knowledge mesh data"
          fi

      # V4.1: R2 Dual-Version Rotation (BEFORE upload)
      # NOTE: entities.json is too large for Wrangler (300MiB limit).
      # The S3 API script (r2-upload-s3.js) handles this file with multipart upload.
      - name: R2 Registry Version Rotation
        run: |
          echo "ðŸ”„ Rotating R2 registry versions..."
          
          # Check file size first
          FILE_SIZE=$(stat -c%s "output/entities.json" 2>/dev/null || echo "0")
          SIZE_MB=$((FILE_SIZE / 1048576))
          echo "ðŸ“Š entities.json size: ${SIZE_MB}MB"
          
          # Wrangler limit is 300MB. Skip backup if file is too large.
          if [ "$SIZE_MB" -gt 290 ]; then
            echo "âš ï¸ entities.json exceeds Wrangler 300MiB limit (${SIZE_MB}MB)."
            echo "ðŸ“¦ Backup will be handled by r2-upload-s3.js (S3 multipart upload)."
          else
            # 1. current -> previous (Only if file is manageable)
            if npx -y wrangler r2 object get $R2_BUCKET/meta/backup/global-registry-current.json --file=./temp-current.json --remote 2>/dev/null; then
              echo "ðŸ“¦ Moving current -> previous"
              npx -y wrangler r2 object put $R2_BUCKET/meta/backup/global-registry-previous.json --file=./temp-current.json --remote
              rm ./temp-current.json
            else
              echo "âš ï¸ No current registry found (first run)"
            fi
            
            # 2. Upload new as current
            if [ -f "output/entities.json" ]; then
              echo "ðŸ“¤ Uploading new registry as current"
              npx -y wrangler r2 object put $R2_BUCKET/meta/backup/global-registry-current.json --file=output/entities.json --remote
            fi
          fi


      # V16.3.3: Verify Registry Backup Integrity (Robust Encoding)
      - name: Verify Registry Backup
        run: |
          echo "ðŸ” Verifying Registry Backup files..."
          if [ -f "output/meta/backup/global-registry.json" ]; then
            SIZE=$(du -h "output/meta/backup/global-registry.json" | cut -f1)
            # Use Node.js for more tolerant JSON reading and counting
            COUNT=$(node -e "
              try {
                const fs = require('fs');
                const data = JSON.parse(fs.readFileSync('output/meta/backup/global-registry.json', 'utf8'));
                console.log(data.count || (Array.isArray(data.entities) ? data.entities.length : 'unknown'));
              } catch (e) {
                console.log('error');
              }
            ")
            
            if [ "$COUNT" = "error" ]; then
              echo "âš ï¸ Warning: Could not parse registry count due to encoding/memory, but file exists."
              echo "âœ… global-registry.json found ($SIZE)"
            else
              echo "âœ… global-registry.json found ($SIZE, $COUNT entities)"
            fi
          else
            echo "âŒ ERROR: global-registry.json missing from backup!"
            exit 1
          fi

      # V16.2.6: R2 Upload (SPEC V14.5)
      - name: Upload to R2
        env:
          # V17.2: Complete list - includes all data from 3/4 and 3.5/4
          # V17.3: Added daily/ for daily AI reports
          R2_PREFIX_FILTER: checkpoint.json,entities.json,entities/,cache/entities/,cache/rankings/,cache/search/,cache/trending.json,cache/category_stats.json,cache/search-core.json,cache/search-full.json,cache/search-manifest.json,cache/relations/,cache/relations.json,cache/trend-data.json,cache/knowledge/,cache/mesh/,cache/graph.json,cache/reports/,daily/,meta/,sitemaps/,rss/,sitemap.xml,sitemap.xsl

        run: |
          echo "ðŸ“¤ Uploading to R2 via S3 API..."
          node scripts/factory/r2-upload-s3.js

       # CDN Purge (V16.2.3: Added search manifest and shards)
      - name: Purge CDN Cache
        run: |
          echo "ðŸ”„ Purging CDN cache..."
          
          # We purge the manifest and core index. Shards are usually unique-named or updated.
          PURGE_PATHS='["https://cdn.free2aitools.com/cache/trending.json","https://cdn.free2aitools.com/cache/search-core.json","https://cdn.free2aitools.com/cache/search-manifest.json","https://cdn.free2aitools.com/cache/category_stats.json","https://cdn.free2aitools.com/cache/relations.json","https://cdn.free2aitools.com/cache/rankings/tool/p1.json","https://cdn.free2aitools.com/sitemap.xml"]'

          
          curl -s -X POST "https://api.cloudflare.com/client/v4/zones/$CLOUDFLARE_ZONE_ID/purge_cache" \
            -H "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
            -H "Content-Type: application/json" \
            --data "{\"files\":$PURGE_PATHS}" \
            | jq '.success'

      # CDN Warming (V16.2.3: Added search manifest)
      - name: Warm CDN Cache
        run: |
          echo "ðŸ”¥ Warming CDN cache..."
          
          WARM_URLS=(
            "https://cdn.free2aitools.com/cache/trending.json"
            "https://cdn.free2aitools.com/cache/search-core.json"
            "https://cdn.free2aitools.com/cache/search-manifest.json"
            "https://cdn.free2aitools.com/cache/category_stats.json"
            "https://cdn.free2aitools.com/cache/rankings/tool/p1.json"
            "https://cdn.free2aitools.com/sitemap.xml"
          )

          
          for url in "${WARM_URLS[@]}"; do
            STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$url")
            echo "$url â†’ $STATUS"
          done

      # Trigger Health Check
      - name: Trigger Health Check
        run: |
          echo "ðŸ¥ Triggering Health Check..."
          gh workflow run daily-health-check.yml || echo "Health check workflow not found"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        continue-on-error: true

      - name: Summary
        run: |
          echo "## Factory 4/4 - Upload Complete ðŸš€" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Upload Statistics" >> $GITHUB_STEP_SUMMARY
          echo "- Files uploaded: $(find output -type f | wc -l)" >> $GITHUB_STEP_SUMMARY
          echo "- CDN purged: âœ…" >> $GITHUB_STEP_SUMMARY
          echo "- CDN warmed: âœ…" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Next Steps" >> $GITHUB_STEP_SUMMARY
          echo "- Health Check triggered" >> $GITHUB_STEP_SUMMARY
          echo "- Image Processor (Automatic via workflow_run)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Factory Pipeline Complete!** ðŸŽ‰" >> $GITHUB_STEP_SUMMARY
