# Factory 4/4 - Upload
# V14.5 Phase 4: Modular Factory Architecture
# 
# Triggers: workflow_run from Aggregate, or manual dispatch
# Inputs: factory-output artifact from Aggregate stage
# Actions: Upload to R2, CDN purge, Health check
# 
# Constitution: Art 13.4 (Non-Destructive), Art 6.2 (CDN)

name: Factory 4/4 - Upload

on:
  workflow_run:
    workflows: ["Factory 3/4 - Aggregate"]
    types: [completed]
  workflow_dispatch:
    inputs:
      run_id:
        description: 'Aggregate workflow run ID to download artifact from'
        required: true
        type: string

env:
  CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
  CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
  CLOUDFLARE_ZONE_ID: ${{ secrets.CLOUDFLARE_ZONE_ID }}
  R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
  R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
  R2_BUCKET: ai-nexus-assets

jobs:
  upload:
    name: Upload to R2
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install Dependencies
        run: npm ci

      # V16.2.6: Restore 'internal cache' transfer
      - name: Restore Factory Output from Cache
        uses: actions/cache/restore@v4
        with:
          path: output/
          key: entity-data-${{ inputs.run_id || github.event.workflow_run.id }}
          fail-on-cache-miss: true

      # V16.3.3: Verify Registry Backup Integrity (Robust Encoding)
      - name: Verify Registry Backup
        run: |
          echo "ðŸ” Verifying Registry Backup files..."
          if [ -f "output/meta/backup/global-registry.json" ]; then
            SIZE=$(du -h "output/meta/backup/global-registry.json" | cut -f1)
            # Use Node.js for more tolerant JSON reading and counting
            COUNT=$(node -e "
              try {
                const fs = require('fs');
                const data = JSON.parse(fs.readFileSync('output/meta/backup/global-registry.json', 'utf8'));
                console.log(data.count || (Array.isArray(data.entities) ? data.entities.length : 'unknown'));
              } catch (e) {
                console.log('error');
              }
            ")
            
            if [ "$COUNT" = "error" ]; then
              echo "âš ï¸ Warning: Could not parse registry count due to encoding/memory, but file exists."
              echo "âœ… global-registry.json found ($SIZE)"
            else
              echo "âœ… global-registry.json found ($SIZE, $COUNT entities)"
            fi
          else
            echo "âŒ ERROR: global-registry.json missing from backup!"
            exit 1
          fi

      # V16.2.6: R2 Upload (SPEC V14.5)
      - name: Upload to R2
        run: |
          echo "ðŸ“¤ Uploading to R2 via S3 API..."
          node scripts/factory/r2-upload-s3.js

       # CDN Purge (V16.2.3: Added search manifest and shards)
      - name: Purge CDN Cache
        run: |
          echo "ðŸ”„ Purging CDN cache..."
          
          # We purge the manifest and core index. Shards are usually unique-named or updated.
          PURGE_PATHS='["https://cdn.free2aitools.com/cache/trending.json","https://cdn.free2aitools.com/cache/search-core.json","https://cdn.free2aitools.com/cache/search-manifest.json","https://cdn.free2aitools.com/cache/category_stats.json","https://cdn.free2aitools.com/cache/relations.json"]'
          
          curl -s -X POST "https://api.cloudflare.com/client/v4/zones/$CLOUDFLARE_ZONE_ID/purge_cache" \
            -H "Authorization: Bearer $CLOUDFLARE_API_TOKEN" \
            -H "Content-Type: application/json" \
            --data "{\"files\":$PURGE_PATHS}" \
            | jq '.success'

      # CDN Warming (V16.2.3: Added search manifest)
      - name: Warm CDN Cache
        run: |
          echo "ðŸ”¥ Warming CDN cache..."
          
          WARM_URLS=(
            "https://cdn.free2aitools.com/cache/trending.json"
            "https://cdn.free2aitools.com/cache/search-core.json"
            "https://cdn.free2aitools.com/cache/search-manifest.json"
            "https://cdn.free2aitools.com/cache/category_stats.json"
          )
          
          for url in "${WARM_URLS[@]}"; do
            STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$url")
            echo "$url â†’ $STATUS"
          done

      # Trigger Health Check
      - name: Trigger Health Check
        run: |
          echo "ðŸ¥ Triggering Health Check..."
          gh workflow run daily-health-check.yml || echo "Health check workflow not found"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        continue-on-error: true

      - name: Summary
        run: |
          echo "## Factory 4/4 - Upload Complete ðŸš€" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Upload Statistics" >> $GITHUB_STEP_SUMMARY
          echo "- Files uploaded: $(find output -type f | wc -l)" >> $GITHUB_STEP_SUMMARY
          echo "- CDN purged: âœ…" >> $GITHUB_STEP_SUMMARY
          echo "- CDN warmed: âœ…" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Next Steps" >> $GITHUB_STEP_SUMMARY
          echo "- Health Check triggered" >> $GITHUB_STEP_SUMMARY
          echo "- Image Processor (Automatic via workflow_run)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Factory Pipeline Complete!** ðŸŽ‰" >> $GITHUB_STEP_SUMMARY
