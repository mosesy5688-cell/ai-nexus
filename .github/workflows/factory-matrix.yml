# Factory Matrix Workflow V14.4
# Constitution Reference: Art 3.1 (Matrix), Art 2.3 (Cache Safety)
# 
# Phase 1: 20 parallel shards process entities
# Phase 2: Aggregator combines results

name: Factory Matrix V14.4

on:
  schedule:
    # Run Mon/Thu at 03:00 UTC
    - cron: '0 3 * * 1,4'
  workflow_dispatch:
    inputs:
      force_full:
        description: 'Force full rebuild (ignore cache)'
        type: boolean
        default: false

env:
  R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
  R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
  R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
  R2_BUCKET: ai-nexus-assets

jobs:
  # ============================================================
  # PHASE 0: Prepare Data
  # ============================================================
  prepare:
    name: Prepare Entity Data
    runs-on: ubuntu-latest
    outputs:
      entity-count: ${{ steps.count.outputs.count }}
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      # Restore FNI history from cache (Art 2.3)
      - name: Restore FNI History Cache
        uses: actions/cache@v3
        id: fni-cache
        with:
          path: cache/fni-history.json
          key: fni-history-${{ github.run_id }}
          restore-keys: |
            fni-history-

      - name: Restore Weekly Accumulator Cache
        uses: actions/cache@v3
        id: weekly-cache
        with:
          path: cache/weekly-accum.json
          key: weekly-accum-${{ github.run_id }}
          restore-keys: |
            weekly-accum-

      # If cache miss, restore from R2 (Art 2.3 Safety Net)
      - name: Restore from R2 if cache miss
        if: steps.fni-cache.outputs.cache-hit != 'true'
        run: |
          echo "Cache miss - restoring from R2 backup..."
          mkdir -p cache
          aws s3 cp s3://$R2_BUCKET/meta/backup/fni-history.json cache/fni-history.json --endpoint-url $R2_ENDPOINT || echo "{}" > cache/fni-history.json
          aws s3 cp s3://$R2_BUCKET/meta/backup/weekly-accum.json cache/weekly-accum.json --endpoint-url $R2_ENDPOINT || echo '{"entries":[]}' > cache/weekly-accum.json

      # Fetch latest entity data using proven L1 orchestrator
      - name: Fetch Entity Data
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          node scripts/ingestion/orchestrator.js
          # Orchestrator outputs to data/merged.json, copy to expected path
          cp data/merged.json data/entities.json

      - name: Count Entities
        id: count
        run: |
          # V14.4: More robust entity counting with fallback
          if [ -f "data/entities.json" ]; then
            COUNT=$(jq length data/entities.json 2>/dev/null || echo "0")
          elif [ -f "data/merged.json" ]; then
            cp data/merged.json data/entities.json
            COUNT=$(jq length data/entities.json 2>/dev/null || echo "0")
          else
            echo "[]" > data/entities.json
            COUNT=0
          fi
          echo "count=$COUNT" >> $GITHUB_OUTPUT
          echo "Entity count: $COUNT"
          
          # Fail if no entities collected
          if [ "$COUNT" -eq "0" ]; then
            echo "WARNING: No entities collected. Check ingestion logs."
          fi

      - name: Upload Entity Data
        uses: actions/upload-artifact@v4
        with:
          name: entity-data
          path: |
            data/entities.json
            cache/fni-history.json
            cache/weekly-accum.json
          retention-days: 1

  # ============================================================
  # PHASE 1: Matrix Shards (Art 3.1)
  # ============================================================
  matrix-shard:
    name: Shard ${{ matrix.shard }}
    needs: prepare
    runs-on: ubuntu-latest
    strategy:
      matrix:
        shard: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
      fail-fast: false
      max-parallel: 20
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Download Entity Data
        uses: actions/download-artifact@v4
        with:
          name: entity-data
          path: ./

      - name: Process Shard ${{ matrix.shard }}
        env:
          ENTITIES_PATH: data/entities.json
          FNI_HISTORY_PATH: cache/fni-history.json
          ARTIFACT_DIR: artifacts
        run: |
          node scripts/factory/shard-processor.js --shard=${{ matrix.shard }} --total=20

      - name: Upload Shard Artifact
        uses: actions/upload-artifact@v4
        with:
          name: shard-${{ matrix.shard }}
          path: |
            artifacts/
            output/
          retention-days: 1

  # ============================================================
  # PHASE 2: Aggregator (Art 3.1)
  # ============================================================
  aggregate:
    name: Aggregator
    needs: matrix-shard
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      # Download all shard artifacts
      - name: Download Shard 0
        uses: actions/download-artifact@v4
        with:
          name: shard-0
          path: ./
        continue-on-error: true

      - name: Download Shard 1
        uses: actions/download-artifact@v4
        with:
          name: shard-1
          path: ./
        continue-on-error: true

      - name: Download Shard 2
        uses: actions/download-artifact@v4
        with:
          name: shard-2
          path: ./
        continue-on-error: true

      - name: Download Shard 3
        uses: actions/download-artifact@v4
        with:
          name: shard-3
          path: ./
        continue-on-error: true

      - name: Download Shard 4
        uses: actions/download-artifact@v4
        with:
          name: shard-4
          path: ./
        continue-on-error: true

      - name: Download Shard 5
        uses: actions/download-artifact@v4
        with:
          name: shard-5
          path: ./
        continue-on-error: true

      - name: Download Shard 6
        uses: actions/download-artifact@v4
        with:
          name: shard-6
          path: ./
        continue-on-error: true

      - name: Download Shard 7
        uses: actions/download-artifact@v4
        with:
          name: shard-7
          path: ./
        continue-on-error: true

      - name: Download Shard 8
        uses: actions/download-artifact@v4
        with:
          name: shard-8
          path: ./
        continue-on-error: true

      - name: Download Shard 9
        uses: actions/download-artifact@v4
        with:
          name: shard-9
          path: ./
        continue-on-error: true

      - name: Download Shard 10
        uses: actions/download-artifact@v4
        with:
          name: shard-10
          path: ./
        continue-on-error: true

      - name: Download Shard 11
        uses: actions/download-artifact@v4
        with:
          name: shard-11
          path: ./
        continue-on-error: true

      - name: Download Shard 12
        uses: actions/download-artifact@v4
        with:
          name: shard-12
          path: ./
        continue-on-error: true

      - name: Download Shard 13
        uses: actions/download-artifact@v4
        with:
          name: shard-13
          path: ./
        continue-on-error: true

      - name: Download Shard 14
        uses: actions/download-artifact@v4
        with:
          name: shard-14
          path: ./
        continue-on-error: true

      - name: Download Shard 15
        uses: actions/download-artifact@v4
        with:
          name: shard-15
          path: ./
        continue-on-error: true

      - name: Download Shard 16
        uses: actions/download-artifact@v4
        with:
          name: shard-16
          path: ./
        continue-on-error: true

      - name: Download Shard 17
        uses: actions/download-artifact@v4
        with:
          name: shard-17
          path: ./
        continue-on-error: true

      - name: Download Shard 18
        uses: actions/download-artifact@v4
        with:
          name: shard-18
          path: ./
        continue-on-error: true

      - name: Download Shard 19
        uses: actions/download-artifact@v4
        with:
          name: shard-19
          path: ./
        continue-on-error: true

      # Download cache data
      - name: Download Cache Data
        uses: actions/download-artifact@v4
        with:
          name: entity-data
          path: ./

      - name: Run Aggregator
        run: node scripts/factory/aggregator.js

      # Save updated cache (Art 2.3)
      - name: Save FNI History Cache
        uses: actions/cache/save@v3
        with:
          path: cache/fni-history.json
          key: fni-history-${{ github.run_id }}

      - name: Save Weekly Accumulator Cache
        uses: actions/cache/save@v3
        with:
          path: cache/weekly-accum.json
          key: weekly-accum-${{ github.run_id }}

      - name: Upload Output Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: factory-output
          path: output/
          retention-days: 7

  # ============================================================
  # PHASE 3: Upload to R2 (V14.4 Phoenix Protocol)
  # Art 13.4: Non-Destructive - NEVER use --delete
  # ============================================================
  upload-to-r2:
    name: Upload to R2 (Phoenix)
    needs: aggregate
    runs-on: ubuntu-latest
    env:
      CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
      CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Download Output
        uses: actions/download-artifact@v4
        with:
          name: factory-output
          path: output/

      - name: Restore Upload Checkpoint
        uses: actions/cache@v4
        with:
          path: upload-checkpoint.json
          key: upload-checkpoint-${{ github.run_id }}
          restore-keys: |
            upload-checkpoint-

      - name: Pre-flight Validation
        run: |
          echo "ð V14.4 Phoenix Protocol - Pre-flight Validation"
          echo "================================================="
          
          # Validate Cloudflare secrets
          if [ -z "$CLOUDFLARE_API_TOKEN" ] || [ -z "$CLOUDFLARE_ACCOUNT_ID" ]; then
            echo "â?ERROR: Missing CLOUDFLARE_API_TOKEN or CLOUDFLARE_ACCOUNT_ID"
            exit 1
          fi
          
          # Count files
          FILE_COUNT=$(find output -type f -name "*.json" 2>/dev/null | wc -l)
          echo "ð JSON files: $FILE_COUNT"
          
          if [ "$FILE_COUNT" -lt 100 ]; then
            echo "â?ERROR: Only $FILE_COUNT files. Expected 100+. Aborting."
            echo "This prevents uploading empty/incomplete data to R2."
            exit 1
          fi
          
          # Check for critical files
          if [ ! -f "output/cache/trending.json" ]; then
            echo "â ï¸ WARNING: Missing trending.json"
          fi
          if [ ! -f "output/cache/search-core.json" ]; then
            echo "â ï¸ WARNING: Missing search-core.json"
          fi
          
          echo "â?Pre-flight validation passed"

      - name: Upload to R2 (Wrangler)
        run: node scripts/factory/r2-upload.js

      - name: Save Upload Checkpoint
        uses: actions/cache/save@v4
        if: always()
        with:
          path: upload-checkpoint.json
          key: upload-checkpoint-${{ github.run_id }}

      # CDN Cache Purge (Art 6.2)
      - name: Purge CDN Cache
        if: success()
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ZONE_ID: ${{ secrets.CLOUDFLARE_ZONE_ID }}
        run: |
          echo "ð Purging CDN cache for critical paths..."
          
          # Purge specific critical files
          curl -X POST "https://api.cloudflare.com/client/v4/zones/${CLOUDFLARE_ZONE_ID}/purge_cache" \
            -H "Authorization: Bearer ${CLOUDFLARE_API_TOKEN}" \
            -H "Content-Type: application/json" \
            --data '{
              "files": [
                "https://free2aitools.com/api/cache/trending.json",
                "https://free2aitools.com/api/cache/search-core.json",
                "https://free2aitools.com/api/cache/category_stats.json"
              ]
            }' || echo "â ï¸ CDN purge failed (non-critical)"
          
          echo "â?CDN cache purge requested"

      # V14.5: Warm CDN Cache (pre-populate for first users)
      - name: Warm CDN Cache
        run: |
          echo "ð¥ Warming CDN cache..."
          sleep 5  # Wait for purge to propagate
          
          # Request critical files to populate CDN cache
          curl -s -o /dev/null -w "%{http_code}" https://free2aitools.com/api/cache/trending.json || true
          curl -s -o /dev/null -w "%{http_code}" https://free2aitools.com/api/cache/search-core.json || true
          curl -s -o /dev/null -w "%{http_code}" https://free2aitools.com/api/cache/category_stats.json || true
          curl -s -o /dev/null -w "%{http_code}" https://free2aitools.com/ || true
          
          echo "â?CDN cache warmed"

      - name: Summary
        run: |
          echo "## ð¥ V14.4 Phoenix Protocol Complete!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Upload Summary" >> $GITHUB_STEP_SUMMARY
          echo "- â?Used Wrangler (not aws sync --delete)" >> $GITHUB_STEP_SUMMARY
          echo "- â?Non-destructive upload (Art 13.4)" >> $GITHUB_STEP_SUMMARY
          echo "- â?Checkpoint recovery enabled" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          FILE_COUNT=$(find output -type f | wc -l)
          echo "- ð Files uploaded: $FILE_COUNT" >> $GITHUB_STEP_SUMMARY
