# Factory 1/4 - Harvest (V14.5 Parallel Architecture)
# 
# V14.5: 4 parallel job groups for 3x faster harvesting
# Based on proven L1 Harvester pattern (loop1-harvester-v42.yml)
# 
# Triggers: Schedule Mon/Thu 03:00 UTC (11:00 Beijing)
# Outputs: merged.json artifact for downstream processing
# 
# Constitution: Art 3.1 (Factory Pipeline)

name: Factory 1/4 - Harvest

on:
  schedule:
    - cron: '0 3 * * *'  # Daily 03:00 UTC
  workflow_dispatch:
    inputs:
      skip_harvest:
        description: 'Skip harvesting and use solidified batches from cache'
        required: false
        type: boolean
        default: false
      source_run_id:
        description: 'Run ID to recover batches from (if skip_harvest is true)'
        required: false
        type: string

concurrency:
  group: factory-harvest
  cancel-in-progress: true

# ============================================================
# JOB 1: HUGGINGFACE MODELS (10K)
# ============================================================
jobs:
  harvest-huggingface:
    name: Harvest HuggingFace
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      
      - name: Harvest HuggingFace Models
        if: github.event.inputs.skip_harvest != 'true'
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          NODE_OPTIONS: '--max-old-space-size=6144'
        run: |
          echo "ðŸ“¥ [Parallel] Harvesting HuggingFace Models (V19: 30K)..."
          node scripts/ingestion/harvest-single.js huggingface --limit 30000
          
      - name: Solidify HuggingFace Batches (Cache)
        if: github.event.inputs.skip_harvest != 'true'
        uses: actions/cache/save@v4
        with:
          path: data/raw_batch_*.json
          key: harvest-raw-huggingface-${{ github.run_id }}

      - name: Upload Batch Artifact
        uses: actions/upload-artifact@v4
        with:
          name: batch-huggingface
          path: data/raw_batch_*.json
          retention-days: 1

  # ============================================================
  # JOB 2: GITHUB AGENTS (5K)
  # ============================================================
  harvest-github:
    name: Harvest GitHub
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      
      - name: Harvest GitHub Repos
        if: github.event.inputs.skip_harvest != 'true'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "ðŸ“¥ [Parallel] Harvesting GitHub (V19: 10K)..."
          node scripts/ingestion/harvest-single.js github --limit 10000
          
      - name: Solidify GitHub Batches (Cache)
        if: github.event.inputs.skip_harvest != 'true'
        uses: actions/cache/save@v4
        with:
          path: data/raw_batch_*.json
          key: harvest-raw-github-${{ github.run_id }}

      - name: Upload Batch Artifact
        uses: actions/upload-artifact@v4
        with:
          name: batch-github
          path: data/raw_batch_*.json
          retention-days: 1

  # ============================================================
  # JOB 3: ACADEMIC SOURCES (ArXiv + Papers + Datasets)
  # ============================================================
  harvest-academic:
    name: Harvest Academic
    runs-on: ubuntu-latest
    timeout-minutes: 180
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      
      - name: Harvest ArXiv Papers
        if: github.event.inputs.skip_harvest != 'true'
        run: |
          echo "ðŸ“¥ [Parallel] Harvesting ArXiv (V19: 100K)..."
          node scripts/ingestion/harvest-single.js arxiv --limit 100000 || echo "ArXiv adapter skipped"
          
      - name: Harvest HuggingFace Papers
        if: github.event.inputs.skip_harvest != 'true'
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "ðŸ“¥ [Parallel] Harvesting HF Papers..."
          node scripts/ingestion/harvest-single.js huggingface-papers --limit 3000 || echo "HF Papers adapter skipped"

      - name: Harvest HuggingFace Datasets
        if: github.event.inputs.skip_harvest != 'true'
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          NODE_OPTIONS: '--max-old-space-size=4096'
        run: |
          echo "ðŸ“¥ [Parallel] Harvesting HF Datasets (V19: 20K)..."
          node scripts/ingestion/harvest-single.js huggingface-datasets --limit 20000 || echo "HF Datasets adapter skipped"

      - name: Solidify Academic Batches (Cache)
        if: github.event.inputs.skip_harvest != 'true'
        uses: actions/cache/save@v4
        with:
          path: data/raw_batch_*.json
          key: harvest-raw-academic-${{ github.run_id }}

      - name: Upload Batch Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: batch-academic
          path: data/raw_batch_*.json
          retention-days: 1

  # ============================================================
  # JOB 4: ECOSYSTEM (All remaining sources)
  # ============================================================
  harvest-ecosystem:
    name: Harvest Ecosystem
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      
      - name: Harvest Ollama Registry
        if: github.event.inputs.skip_harvest != 'true'
        run: |
          echo "ðŸ“¥ Harvesting Ollama..."
          node scripts/ingestion/harvest-single.js ollama --limit 1000 || echo "Ollama adapter skipped"
          
      - name: Harvest MCP Servers
        if: github.event.inputs.skip_harvest != 'true'
        run: |
          echo "ðŸ“¥ Harvesting MCP..."
          node scripts/ingestion/harvest-single.js mcp --limit 500 || echo "MCP adapter skipped"

      - name: Harvest Replicate
        if: github.event.inputs.skip_harvest != 'true'
        env:
          REPLICATE_API_TOKEN: ${{ secrets.REPLICATE_API_TOKEN }}
        run: |
          echo "ðŸ“¥ Harvesting Replicate..."
          node scripts/ingestion/harvest-single.js replicate --limit 5000 || echo "Replicate adapter skipped"

      # V14.5.2: Setup Python for Kaggle models sidecar (REST API deprecated)
      - name: Setup Python for Kaggle
        if: github.event.inputs.skip_harvest != 'true'
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          
      - name: Install Kaggle CLI
        if: github.event.inputs.skip_harvest != 'true'
        run: pip install kaggle

      - name: Harvest Kaggle
        if: github.event.inputs.skip_harvest != 'true'
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          echo "ðŸ“¥ Harvesting Kaggle..."
          node scripts/ingestion/harvest-single.js kaggle --limit 10000 || echo "Kaggle adapter skipped"

      - name: Harvest CivitAI
        if: github.event.inputs.skip_harvest != 'true'
        run: |
          echo "ðŸ“¥ Harvesting CivitAI..."
          node scripts/ingestion/harvest-single.js civitai --limit 5000 || echo "CivitAI adapter skipped"

      - name: Harvest Semantic Scholar
        if: github.event.inputs.skip_harvest != 'true'
        run: |
          echo "ðŸ“¥ Harvesting Semantic Scholar..."
          node scripts/ingestion/harvest-single.js semanticscholar --limit 3000 || echo "Semantic Scholar adapter skipped"

      - name: Harvest OpenLLM Benchmarks
        if: github.event.inputs.skip_harvest != 'true'
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "ðŸ“Š Harvesting OpenLLM Leaderboard..."
          node scripts/ingestion/harvest-single.js openllm --limit 1000 || echo "OpenLLM adapter skipped"

      - name: Harvest DeepSpec Specifications
        if: github.event.inputs.skip_harvest != 'true'
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "ðŸ“‹ Extracting Model Specifications..."
          node scripts/ingestion/harvest-single.js deepspec --limit 2000 || echo "DeepSpec adapter skipped"

      - name: Harvest HuggingFace Spaces
        if: github.event.inputs.skip_harvest != 'true'
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "ðŸ“¥ Harvesting HuggingFace Spaces..."
          node scripts/ingestion/harvest-single.js huggingface-spaces --limit 2000 || echo "Spaces adapter skipped"

      - name: Harvest AI Agents
        if: github.event.inputs.skip_harvest != 'true'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "ðŸ“¥ Harvesting AI Agents..."
          node scripts/ingestion/harvest-single.js agents --limit 200 || echo "Agents adapter skipped"

      - name: Solidify Ecosystem Batches (Cache)
        if: github.event.inputs.skip_harvest != 'true'
        uses: actions/cache/save@v4
        with:
          path: data/raw_batch_*.json
          key: harvest-raw-ecosystem-${{ github.run_id }}

      - name: Upload Batch Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: batch-ecosystem
          path: data/raw_batch_*.json
          retention-days: 1

  # ============================================================
  # JOB 5: MERGE AND UPLOAD
  # ============================================================
  merge-and-upload:
    name: Merge & Upload
    runs-on: ubuntu-latest
    needs: [harvest-huggingface, harvest-github, harvest-academic, harvest-ecosystem]
    timeout-minutes: 30
    env:
      # V18.12.5.1: 10GB Heap to handle 168k registry merge (V8 peak safety)
      NODE_OPTIONS: '--max-old-space-size=10240'
    
    steps:
      - uses: actions/checkout@v4
      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      
      # V16.96.2: Pre-merge Cache Maintenance
      - name: Cache Cleanup
        run: |
          echo "ðŸ§¹ [V16.96.2] Purging stale shards to stay under 10GB limit..."
          node scripts/factory/cleanup-cache.js
      
      # V18.3: Solidification Recovery Logic
      - name: Create Data Directory
        run: mkdir -p data/
      - name: Recover Solidified Batches (Cache Fallback)
        if: github.event.inputs.skip_harvest == 'true'
        uses: actions/cache/restore@v4
        with:
          path: data/raw_batch_*.json
          key: harvest-raw-huggingface-${{ github.event.inputs.source_run_id }}
          restore-keys: harvest-raw-huggingface-
      - name: Recover Solidified Batches (GitHub)
        if: github.event.inputs.skip_harvest == 'true'
        uses: actions/cache/restore@v4
        with:
          path: data/raw_batch_*.json
          key: harvest-raw-github-${{ github.event.inputs.source_run_id }}
          restore-keys: harvest-raw-github-
      - name: Recover Solidified Batches (Academic)
        if: github.event.inputs.skip_harvest == 'true'
        uses: actions/cache/restore@v4
        with:
          path: data/raw_batch_*.json
          key: harvest-raw-academic-${{ github.event.inputs.source_run_id }}
          restore-keys: harvest-raw-academic-
      - name: Recover Solidified Batches (Ecosystem)
        if: github.event.inputs.skip_harvest == 'true'
        uses: actions/cache/restore@v4
        with:
          path: data/raw_batch_*.json
          key: harvest-raw-ecosystem-${{ github.event.inputs.source_run_id }}
          restore-keys: harvest-raw-ecosystem-

      # Download current batch artifacts (if not skipping)
      - name: Download All Batch Artifacts
        if: github.event.inputs.skip_harvest != 'true'
        uses: actions/download-artifact@v4
        with:
          path: data/
          pattern: batch-*
          merge-multiple: true
          
      - name: List Batches (Current or Recovered)
        run: |
          echo "ðŸ“¦ Entity batches ready for merge:"
          ls -la data/
          ls -la data/raw_batch_*.json 2>/dev/null || echo "âš ï¸ No batch files found in data/"
          
      - name: Restore FNI History
        uses: actions/cache@v4
        with:
          path: |
            cache/fni-history.json.gz
            cache/fni-history/
          key: fni-history-${{ github.run_id }}
          restore-keys: |
            fni-history-

      - name: Restore Daily Accumulator
        uses: actions/cache@v4
        with:
          path: |
            cache/daily-accum.json.gz
            cache/daily-accum/
          key: daily-accum-${{ github.run_id }}
          restore-keys: |
            daily-accum-

      - name: Restore Checksums
        uses: actions/cache@v4
        with:
          path: |
            cache/entity-checksums.json.gz
            cache/task-checksums.json.gz
          key: checksums-${{ github.run_id }}
          restore-keys: |
            checksums-
          
      # V4.1: Restore global registry (Immutable Key + Prefix Match)
      - name: Restore Global Registry (Cache-First)
        id: cache-registry
        uses: actions/cache/restore@v4
        with:
          path: |
            cache/global-registry.json.gz
            cache/registry/
          key: global-registry-${{ github.run_id }}  # Won't match (not saved yet)
          restore-keys: |
            global-registry-
          # GitHub auto-selects the most recently created match

      # V4.2: R2 Zero-Loss Restoration (Cache-Preservation V1.0)
      - name: R2 Fail-Safe Restoration
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ai-nexus-assets
        run: |
          echo "ðŸŒ [V18.2.3] Auditing Registry Context..."
          mkdir -p cache/registry cache/fni-history cache/daily-accum
          
          # 1. Monolith Recovery (Additive)
          if [ ! -s "cache/global-registry.json.gz" ]; then
             echo "  - Missing local monolith. Fetching from R2..."
             npx -y wrangler r2 object get $R2_BUCKET/meta/backup/global-registry.json.gz --file=cache/global-registry.json.gz --remote 2>/dev/null || true
          fi

          # 2. Sharded Recovery (Hard Preservation)
          # RULES: 
          # - NEVER delete existing cache files.
          # - ONLY download from R2 if local is missing or empty.
          # - ABORT if R2 returns a 0B file to prevent corruption.
          
          echo "ðŸ§© Auditing Shards (Registry)..."
          for i in {000..015}; do
            TARGET="cache/registry/part-$i.json.gz"
            if [ ! -s "$TARGET" ]; then
               echo "  - Shard $i missing/empty. Attemping R2 restoration..."
               npx -y wrangler r2 object get $R2_BUCKET/meta/backup/registry/part-$i.json.gz --file=$TARGET --remote 2>/dev/null || true
               
               # Integrity Guard (V18.2.3): Check magic number (1f 8b)
               if [ -f "$TARGET" ] && [ -s "$TARGET" ]; then
                  if ! node -e "const b=require('fs').readFileSync('$TARGET'); if(b[0]!==0x1f || b[1]!==0x8b) process.exit(1)" 2>/dev/null; then
                     echo "  - âš ï¸ Shard $i: Fake .gz detected (missing magic bytes). Purging."
                     rm "$TARGET"
                  fi
               fi
            else
               echo "  - âœ… Shard $i: Existing healthy cache found."
            fi
          done

          echo "ðŸ§© Auditing Shards (FNI History)..."
          for i in {000..015}; do
            TARGET="cache/fni-history/part-$i.json.gz"
            if [ ! -s "$TARGET" ]; then
               npx -y wrangler r2 object get $R2_BUCKET/meta/backup/fni-history/part-$i.json.gz --file=$TARGET --remote 2>/dev/null || true
               if [ -f "$TARGET" ] && [ -s "$TARGET" ]; then
                  node -e "const b=require('fs').readFileSync('$TARGET'); if(b[0]!==0x1f || b[1]!==0x8b) process.exit(1)" 2>/dev/null || rm "$TARGET"
               fi
            fi
          done

          # 3. Critical metadata (Checksums)
          if [ ! -s "cache/entity-checksums.json.gz" ]; then
             npx -y wrangler r2 object get $R2_BUCKET/meta/backup/entity-checksums.json.gz --file=cache/entity-checksums.json.gz --remote 2>/dev/null || true
          fi

      - name: Merge Batches
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ai-nexus-assets
          ALLOW_R2_RECOVERY: 'true'
        run: |
          echo "ðŸ”„ Merging batches with ID deduplication..."
          node scripts/ingestion/merge-batches.js
          
          if [ -f "data/manifest.json" ]; then
            ENTITY_COUNT=$(jq -r '.total_entities' data/manifest.json)
            echo "âœ… Merged: $ENTITY_COUNT total entities (via manifest)"
            
            if [ "$ENTITY_COUNT" -lt 80000 ]; then
              echo "âŒ ERROR: Only $ENTITY_COUNT entities found. Expected 80,000+."
              echo "This indicates corrupted data or missing batches."
              exit 1
            fi
          else
            echo "âŒ Error: manifest.json not generated"
            exit 1
          fi

      # V4.1: Save Entity Data to Cache (Cycle-Specific Key)
      - name: Save Entity Data to Cache
        uses: actions/cache/save@v4
        with:
          path: |
            data/merged.json.gz
            data/merged_shard_*.json.gz
            data/manifest.json
            cache/
          key: cycle-${{ github.run_id }}-harvest

      # Pipeline Summary
      - name: Summary
        run: |
          ENTITY_COUNT=$(jq -r '.total_entities' data/manifest.json)
          BATCH_COUNT=$(ls -1 data/raw_batch_*.json 2>/dev/null | wc -l)
          
          echo "## Factory Stage 1/4 (Harvest) - Complete ðŸš€" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“Š Ingestion Data Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Total Entities**: $ENTITY_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "- **Batch Files**: $BATCH_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "- **Manifest State**: Verified âœ…" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… Next: Factory 2/4 - Process" >> $GITHUB_STEP_SUMMARY
