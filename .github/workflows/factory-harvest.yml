# Factory 1/4 - Harvest (V14.5 Parallel Architecture)
# 
# V14.5: 4 parallel job groups for 3x faster harvesting
# Based on proven L1 Harvester pattern (loop1-harvester-v42.yml)
# 
# Triggers: Schedule Mon/Thu 03:00 UTC (11:00 Beijing)
# Outputs: merged.json artifact for downstream processing
# 
# Constitution: Art 3.1 (Factory Pipeline)

name: Factory 1/4 - Harvest

on:
  schedule:
    - cron: '0 3 * * *'  # Daily 03:00 UTC
  workflow_dispatch:

concurrency:
  group: factory-harvest
  cancel-in-progress: true

# ============================================================
# JOB 1: HUGGINGFACE MODELS (10K)
# ============================================================
jobs:
  harvest-huggingface:
    name: Harvest HuggingFace
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      
      - name: Harvest HuggingFace Models
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          NODE_OPTIONS: '--max-old-space-size=4096'
        run: |
          echo "ðŸ“¥ [Parallel] Harvesting HuggingFace Models..."
          node scripts/ingestion/harvest-single.js huggingface --limit 10000
          
      - name: Upload Batch Artifact
        uses: actions/upload-artifact@v4
        with:
          name: batch-huggingface
          path: data/raw_batch_*.json
          retention-days: 1

  # ============================================================
  # JOB 2: GITHUB AGENTS (5K)
  # ============================================================
  harvest-github:
    name: Harvest GitHub
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      
      - name: Harvest GitHub Repos
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "ðŸ“¥ [Parallel] Harvesting GitHub..."
          node scripts/ingestion/harvest-single.js github --limit 5000
          
      - name: Upload Batch Artifact
        uses: actions/upload-artifact@v4
        with:
          name: batch-github
          path: data/raw_batch_*.json
          retention-days: 1

  # ============================================================
  # JOB 3: ACADEMIC SOURCES (ArXiv + Papers + Datasets)
  # ============================================================
  harvest-academic:
    name: Harvest Academic
    runs-on: ubuntu-latest
    timeout-minutes: 180
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      
      - name: Harvest ArXiv Papers
        run: |
          echo "ðŸ“¥ [Parallel] Harvesting ArXiv..."
          node scripts/ingestion/harvest-single.js arxiv --limit 50000 || echo "ArXiv adapter skipped"
          
      - name: Harvest HuggingFace Papers
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "ðŸ“¥ [Parallel] Harvesting HF Papers..."
          node scripts/ingestion/harvest-single.js huggingface-papers --limit 3000 || echo "HF Papers adapter skipped"

      - name: Harvest HuggingFace Datasets
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          NODE_OPTIONS: '--max-old-space-size=4096'
        run: |
          echo "ðŸ“¥ [Parallel] Harvesting HF Datasets..."
          node scripts/ingestion/harvest-single.js huggingface-datasets --limit 10000 || echo "HF Datasets adapter skipped"

      - name: Upload Batch Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: batch-academic
          path: data/raw_batch_*.json
          retention-days: 1

  # ============================================================
  # JOB 4: ECOSYSTEM (All remaining sources)
  # ============================================================
  harvest-ecosystem:
    name: Harvest Ecosystem
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      
      - name: Harvest Ollama Registry
        run: |
          echo "ðŸ“¥ Harvesting Ollama..."
          node scripts/ingestion/harvest-single.js ollama --limit 1000 || echo "Ollama adapter skipped"
          
      - name: Harvest MCP Servers
        run: |
          echo "ðŸ“¥ Harvesting MCP..."
          node scripts/ingestion/harvest-single.js mcp --limit 500 || echo "MCP adapter skipped"

      - name: Harvest Replicate
        env:
          REPLICATE_API_TOKEN: ${{ secrets.REPLICATE_API_TOKEN }}
        run: |
          echo "ðŸ“¥ Harvesting Replicate..."
          node scripts/ingestion/harvest-single.js replicate --limit 5000 || echo "Replicate adapter skipped"

      # V14.5.2: Setup Python for Kaggle models sidecar (REST API deprecated)
      - name: Setup Python for Kaggle
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          
      - name: Install Kaggle CLI
        run: pip install kaggle

      - name: Harvest Kaggle
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          echo "ðŸ“¥ Harvesting Kaggle..."
          node scripts/ingestion/harvest-single.js kaggle --limit 10000 || echo "Kaggle adapter skipped"

      - name: Harvest CivitAI
        run: |
          echo "ðŸ“¥ Harvesting CivitAI..."
          node scripts/ingestion/harvest-single.js civitai --limit 5000 || echo "CivitAI adapter skipped"

      - name: Harvest Semantic Scholar
        run: |
          echo "ðŸ“¥ Harvesting Semantic Scholar..."
          node scripts/ingestion/harvest-single.js semanticscholar --limit 3000 || echo "Semantic Scholar adapter skipped"

      - name: Harvest OpenLLM Benchmarks
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "ðŸ“Š Harvesting OpenLLM Leaderboard..."
          node scripts/ingestion/harvest-single.js openllm --limit 1000 || echo "OpenLLM adapter skipped"

      - name: Harvest DeepSpec Specifications
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "ðŸ“‹ Extracting Model Specifications..."
          node scripts/ingestion/harvest-single.js deepspec --limit 2000 || echo "DeepSpec adapter skipped"

      - name: Harvest HuggingFace Spaces
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "ðŸ“¥ Harvesting HuggingFace Spaces..."
          node scripts/ingestion/harvest-single.js huggingface-spaces --limit 2000 || echo "Spaces adapter skipped"

      - name: Harvest AI Agents
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "ðŸ“¥ Harvesting AI Agents..."
          node scripts/ingestion/harvest-single.js agents --limit 200 || echo "Agents adapter skipped"

      - name: Upload Batch Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: batch-ecosystem
          path: data/raw_batch_*.json
          retention-days: 1

  # ============================================================
  # JOB 5: MERGE AND UPLOAD
  # ============================================================
  merge-and-upload:
    name: Merge & Upload
    runs-on: ubuntu-latest
    needs: [harvest-huggingface, harvest-github, harvest-academic, harvest-ecosystem]
    timeout-minutes: 30
    env:
      # V17.2: Increased heap for 200K+ entity merge (default 512MB insufficient)
      NODE_OPTIONS: '--max-old-space-size=6144'
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      
      # Download all batch artifacts
      - name: Download All Batch Artifacts
        uses: actions/download-artifact@v4
        with:
          path: data/
          pattern: batch-*
          merge-multiple: true
          
      - name: List Downloaded Batches
        run: |
          echo "ðŸ“¦ Downloaded batches:"
          ls -la data/raw_batch_*.json 2>/dev/null || echo "No batch files found"
          
      - name: Restore FNI History
        uses: actions/cache@v4
        with:
          path: |
            cache/fni-history.json
            cache/fni-history/
          key: fni-history-${{ github.run_id }}
          restore-keys: |
            fni-history-

      - name: Restore Daily Accumulator
        uses: actions/cache@v4
        with:
          path: |
            cache/daily-accum.json
            cache/daily-accum/
          key: daily-accum-${{ github.run_id }}
          restore-keys: |
            daily-accum-

      - name: Restore Checksums
        uses: actions/cache@v4
        with:
          path: |
            cache/entity-checksums.json
            cache/task-checksums.json
          key: checksums-${{ github.run_id }}
          restore-keys: |
            checksums-
          
      # V4.1: Restore global registry (Immutable Key + Prefix Match)
      - name: Restore Global Registry (Cache-First)
        id: cache-registry
        uses: actions/cache/restore@v4
        with:
          path: |
            cache/global-registry.json
            cache/registry/
          key: global-registry-${{ github.run_id }}  # Won't match (not saved yet)
          restore-keys: |
            global-registry-
          # GitHub auto-selects the most recently created match

      # V4.1: R2 Dual-Version Fallback (current â†’ previous)
      - name: R2 Fallback & Pre-validation
        if: steps.cache-registry.outputs.cache-hit != 'true'
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ai-nexus-assets
        run: |
          echo "âš ï¸ Cache miss. Attempting R2 dual-version fallback..."
          mkdir -p cache
          
          # 1. Restore Registry (Current/Previous)
          if npx -y wrangler r2 object get $R2_BUCKET/meta/backup/global-registry.json --file=cache/global-registry.json --remote 2>/dev/null; then
            echo "âœ… Restored from R2: global-registry.json"
          elif npx -y wrangler r2 object get $R2_BUCKET/meta/backup/global-registry-previous.json --file=cache/global-registry.json --remote 2>/dev/null; then
            echo "âš ï¸ Restored from R2: global-registry-previous.json (fallback)"
          fi
          
          # 2. Restore FNI History
          npx -y wrangler r2 object get $R2_BUCKET/meta/backup/fni-history.json --file=cache/fni-history.json --remote 2>/dev/null && echo "âœ… Restored FNI History from R2" || true
          
          # 3. Restore Daily Accum
          npx -y wrangler r2 object get $R2_BUCKET/meta/backup/daily-accum.json --file=cache/daily-accum.json --remote 2>/dev/null && echo "âœ… Restored Daily Accum from R2" || true
          
          # 4. Restore Entity Checksums
          npx -y wrangler r2 object get $R2_BUCKET/meta/backup/entity-checksums.json --file=cache/entity-checksums.json --remote 2>/dev/null && echo "âœ… Restored Checksums from R2" || true

          # 5. Restore Shards (Robust Sharded Restoration V2.1)
          echo "ðŸ§© Fetching sharded registry baseline..."
          mkdir -p cache/registry
          # Sync registry folder recursively (if monolith is corrupted/missing)
          # We use a pattern-based move or sequential get for known shards
          for i in {000..015}; do
            npx -y wrangler r2 object get $R2_BUCKET/meta/backup/registry/part-$i.json --file=cache/registry/part-$i.json --remote 2>/dev/null && echo "  - Registry Shard $i restored" || break
          done

          echo "ðŸ§© Fetching sharded FNI history baseline..."
          mkdir -p cache/fni-history
          for i in {000..015}; do
            npx -y wrangler r2 object get $R2_BUCKET/meta/backup/fni-history/part-$i.json --file=cache/fni-history/part-$i.json --remote 2>/dev/null && echo "  - FNI Shard $i restored" || break
          done

          echo "ðŸ§© Fetching sharded Accumulator baseline..."
          mkdir -p cache/daily-accum
          for i in {000..015}; do
            npx -y wrangler r2 object get $R2_BUCKET/meta/backup/daily-accum/part-$i.json --file=cache/daily-accum/part-$i.json --remote 2>/dev/null && echo "  - Accum Shard $i restored" || break
          done

      # Merge batches with ID deduplication
      - name: Merge Batches
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ai-nexus-assets
        run: |
          echo "ðŸ”„ Merging batches with ID deduplication..."
          node scripts/ingestion/merge-batches.js
          
          if [ -f "data/merged.json" ]; then
            ENTITY_COUNT=$(node -e "console.log(JSON.parse(require('fs').readFileSync('data/merged.json')).length)")
            echo "âœ… Merged: $ENTITY_COUNT total entities"
            
            if [ "$ENTITY_COUNT" -lt 1000 ]; then
              echo "âš ï¸ Warning: Low entity count ($ENTITY_COUNT)"
            fi
          else
            echo "âŒ Error: merged.json not generated"
            exit 1
          fi

      # V4.1: Save Entity Data to Cache (Cycle-Specific Key)
      - name: Save Entity Data to Cache
        uses: actions/cache/save@v4
        with:
          path: |
            data/merged.json
            data/merged_shard_*.json
            data/manifest.json
            cache/
          key: cycle-${{ github.run_id }}-harvest


      # Pipeline Summary
      - name: Summary
        run: |
          ENTITY_COUNT=$(node -e "console.log(JSON.parse(require('fs').readFileSync('data/merged.json')).length)")
          BATCH_COUNT=$(ls -1 data/raw_batch_*.json 2>/dev/null | wc -l)
          
          echo "## Factory 1/4 - Harvest Complete ðŸŒ¾" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### V14.5 Parallel Harvest Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Total Entities | $ENTITY_COUNT |" >> $GITHUB_STEP_SUMMARY
          echo "| Batch Files | $BATCH_COUNT |" >> $GITHUB_STEP_SUMMARY
          echo "| Architecture | 4 Parallel Jobs |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… Next: Factory 2/4 - Process" >> $GITHUB_STEP_SUMMARY
