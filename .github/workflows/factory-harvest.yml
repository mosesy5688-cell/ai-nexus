# Factory 1/4 - Harvest (V14.5 Parallel Architecture)
# 
# V14.5: 4 parallel job groups for 3x faster harvesting
# Based on proven L1 Harvester pattern (loop1-harvester-v42.yml)
# 
# Triggers: Schedule Mon/Thu 03:00 UTC (11:00 Beijing)
# Outputs: merged.json artifact for downstream processing
# 
# Constitution: Art 3.1 (Factory Pipeline)

name: Factory 1/4 - Harvest

on:
  schedule:
    - cron: '0 3 * * *'  # Daily 03:00 UTC
  workflow_dispatch:

concurrency:
  group: factory-harvest
  cancel-in-progress: true

# ============================================================
# JOB 1: HUGGINGFACE MODELS (10K)
# ============================================================
jobs:
  harvest-huggingface:
    name: Harvest HuggingFace
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      
      - name: Harvest HuggingFace Models
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          NODE_OPTIONS: '--max-old-space-size=4096'
        run: |
          echo "ðŸ“¥ [Parallel] Harvesting HuggingFace Models..."
          node scripts/ingestion/harvest-single.js huggingface --limit 10000
          
      - name: Upload Batch Artifact
        uses: actions/upload-artifact@v4
        with:
          name: batch-huggingface
          path: data/raw_batch_*.json
          retention-days: 1

  # ============================================================
  # JOB 2: GITHUB AGENTS (5K)
  # ============================================================
  harvest-github:
    name: Harvest GitHub
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      
      - name: Harvest GitHub Repos
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "ðŸ“¥ [Parallel] Harvesting GitHub..."
          node scripts/ingestion/harvest-single.js github --limit 5000
          
      - name: Upload Batch Artifact
        uses: actions/upload-artifact@v4
        with:
          name: batch-github
          path: data/raw_batch_*.json
          retention-days: 1

  # ============================================================
  # JOB 3: ACADEMIC SOURCES (ArXiv + Papers + Datasets)
  # ============================================================
  harvest-academic:
    name: Harvest Academic
    runs-on: ubuntu-latest
    timeout-minutes: 180
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      
      - name: Harvest ArXiv Papers
        run: |
          echo "ðŸ“¥ [Parallel] Harvesting ArXiv..."
          node scripts/ingestion/harvest-single.js arxiv --limit 50000 || echo "ArXiv adapter skipped"
          
      - name: Harvest HuggingFace Papers
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "ðŸ“¥ [Parallel] Harvesting HF Papers..."
          node scripts/ingestion/harvest-single.js huggingface-papers --limit 3000 || echo "HF Papers adapter skipped"

      - name: Harvest HuggingFace Datasets
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          NODE_OPTIONS: '--max-old-space-size=4096'
        run: |
          echo "ðŸ“¥ [Parallel] Harvesting HF Datasets..."
          node scripts/ingestion/harvest-single.js huggingface-datasets --limit 10000 || echo "HF Datasets adapter skipped"

      - name: Upload Batch Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: batch-academic
          path: data/raw_batch_*.json
          retention-days: 1

  # ============================================================
  # JOB 4: ECOSYSTEM (All remaining sources)
  # ============================================================
  harvest-ecosystem:
    name: Harvest Ecosystem
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      
      - name: Harvest Ollama Registry
        run: |
          echo "ðŸ“¥ Harvesting Ollama..."
          node scripts/ingestion/harvest-single.js ollama --limit 1000 || echo "Ollama adapter skipped"
          
      - name: Harvest MCP Servers
        run: |
          echo "ðŸ“¥ Harvesting MCP..."
          node scripts/ingestion/harvest-single.js mcp --limit 500 || echo "MCP adapter skipped"

      - name: Harvest Replicate
        env:
          REPLICATE_API_TOKEN: ${{ secrets.REPLICATE_API_TOKEN }}
        run: |
          echo "ðŸ“¥ Harvesting Replicate..."
          node scripts/ingestion/harvest-single.js replicate --limit 5000 || echo "Replicate adapter skipped"

      # V14.5.2: Setup Python for Kaggle models sidecar (REST API deprecated)
      - name: Setup Python for Kaggle
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          
      - name: Install Kaggle CLI
        run: pip install kaggle

      - name: Harvest Kaggle
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          echo "ðŸ“¥ Harvesting Kaggle..."
          node scripts/ingestion/harvest-single.js kaggle --limit 10000 || echo "Kaggle adapter skipped"

      - name: Harvest CivitAI
        run: |
          echo "ðŸ“¥ Harvesting CivitAI..."
          node scripts/ingestion/harvest-single.js civitai --limit 5000 || echo "CivitAI adapter skipped"

      - name: Harvest Semantic Scholar
        run: |
          echo "ðŸ“¥ Harvesting Semantic Scholar..."
          node scripts/ingestion/harvest-single.js semanticscholar --limit 3000 || echo "Semantic Scholar adapter skipped"

      - name: Harvest OpenLLM Benchmarks
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "ðŸ“Š Harvesting OpenLLM Leaderboard..."
          node scripts/ingestion/harvest-single.js openllm --limit 1000 || echo "OpenLLM adapter skipped"

      - name: Harvest DeepSpec Specifications
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "ðŸ“‹ Extracting Model Specifications..."
          node scripts/ingestion/harvest-single.js deepspec --limit 2000 || echo "DeepSpec adapter skipped"

      - name: Harvest HuggingFace Spaces
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "ðŸ“¥ Harvesting HuggingFace Spaces..."
          node scripts/ingestion/harvest-single.js huggingface-spaces --limit 2000 || echo "Spaces adapter skipped"

      - name: Harvest AI Agents
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "ðŸ“¥ Harvesting AI Agents..."
          node scripts/ingestion/harvest-single.js agents --limit 200 || echo "Agents adapter skipped"

      - name: Upload Batch Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: batch-ecosystem
          path: data/raw_batch_*.json
          retention-days: 1

  # ============================================================
  # JOB 5: MERGE AND UPLOAD
  # ============================================================
  merge-and-upload:
    name: Merge & Upload
    runs-on: ubuntu-latest
    needs: [harvest-huggingface, harvest-github, harvest-academic, harvest-ecosystem]
    timeout-minutes: 30
    env:
      # V17.2: Increased heap for 200K+ entity merge (default 512MB insufficient)
      NODE_OPTIONS: '--max-old-space-size=6144'
    
    steps:
      - uses: actions/checkout@v4
      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      
      # V16.96.2: Pre-merge Cache Maintenance
      - name: Cache Cleanup
        run: |
          echo "ðŸ§¹ [V16.96.2] Purging stale shards to stay under 10GB limit..."
          node scripts/factory/cleanup-cache.js
      
      # Download all batch artifacts
      - name: Download All Batch Artifacts
        uses: actions/download-artifact@v4
        with:
          path: data/
          pattern: batch-*
          merge-multiple: true
          
      - name: List Downloaded Batches
        run: |
          echo "ðŸ“¦ Downloaded batches:"
          ls -la data/raw_batch_*.json 2>/dev/null || echo "No batch files found"
          
      - name: Restore FNI History
        uses: actions/cache@v4
        with:
          path: |
            cache/fni-history.json.gz
            cache/fni-history/
          key: fni-history-${{ github.run_id }}
          restore-keys: |
            fni-history-

      - name: Restore Daily Accumulator
        uses: actions/cache@v4
        with:
          path: |
            cache/daily-accum.json.gz
            cache/daily-accum/
          key: daily-accum-${{ github.run_id }}
          restore-keys: |
            daily-accum-

      - name: Restore Checksums
        uses: actions/cache@v4
        with:
          path: |
            cache/entity-checksums.json.gz
            cache/task-checksums.json.gz
          key: checksums-${{ github.run_id }}
          restore-keys: |
            checksums-
          
      # V4.1: Restore global registry (Immutable Key + Prefix Match)
      - name: Restore Global Registry (Cache-First)
        id: cache-registry
        uses: actions/cache/restore@v4
        with:
          path: |
            cache/global-registry.json.gz
            cache/registry/
          key: global-registry-${{ github.run_id }}  # Won't match (not saved yet)
          restore-keys: |
            global-registry-
          # GitHub auto-selects the most recently created match

      # V4.2: R2 Zero-Loss Restoration (Cache-Preservation V1.0)
      - name: R2 Fail-Safe Restoration
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ai-nexus-assets
        run: |
          echo "ðŸŒ [V18.2.3] Auditing Registry Context..."
          mkdir -p cache/registry cache/fni-history cache/daily-accum
          
          # 1. Monolith Recovery (Additive)
          if [ ! -s "cache/global-registry.json.gz" ]; then
             echo "  - Missing local monolith. Fetching from R2..."
             npx -y wrangler r2 object get $R2_BUCKET/meta/backup/global-registry.json.gz --file=cache/global-registry.json.gz --remote 2>/dev/null || true
          fi

          # 2. Sharded Recovery (Hard Preservation)
          # RULES: 
          # - NEVER delete existing cache files.
          # - ONLY download from R2 if local is missing or empty.
          # - ABORT if R2 returns a 0B file to prevent corruption.
          
          echo "ðŸ§© Auditing Shards (Registry)..."
          for i in {000..015}; do
            TARGET="cache/registry/part-$i.json.gz"
            if [ ! -s "$TARGET" ]; then
               echo "  - Shard $i missing/empty. Attemping R2 restoration..."
               npx -y wrangler r2 object get $R2_BUCKET/meta/backup/registry/part-$i.json.gz --file=$TARGET --remote 2>/dev/null || true
               
               # Integrity Guard: If R2 returns a 0B file, delete it so it doesn't break zlib
               if [ -f "$TARGET" ] && [ ! -s "$TARGET" ]; then
                  rm "$TARGET"
                  echo "  - âš ï¸ Shard $i: R2 returned empty file. Skipped to prevent corruption."
               fi
            else
               echo "  - âœ… Shard $i: Existing healthy cache found."
            fi
          done

          echo "ðŸ§© Auditing Shards (FNI History)..."
          for i in {000..015}; do
            TARGET="cache/fni-history/part-$i.json.gz"
            if [ ! -s "$TARGET" ]; then
               npx -y wrangler r2 object get $R2_BUCKET/meta/backup/fni-history/part-$i.json.gz --file=$TARGET --remote 2>/dev/null || true
               [ -f "$TARGET" ] && [ ! -s "$TARGET" ] && rm "$TARGET" || true
            fi
          done

          # 3. Critical metadata (Checksums)
          if [ ! -s "cache/entity-checksums.json.gz" ]; then
             npx -y wrangler r2 object get $R2_BUCKET/meta/backup/entity-checksums.json.gz --file=cache/entity-checksums.json.gz --remote 2>/dev/null || true
          fi

      - name: Merge Batches
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ai-nexus-assets
          ALLOW_R2_RECOVERY: 'true'
        run: |
          echo "ðŸ”„ Merging batches with ID deduplication..."
          node scripts/ingestion/merge-batches.js
          
          if [ -f "data/merged.json.gz" ]; then
            ENTITY_COUNT=$(node -e "const zlib = require('zlib'); const fs = require('fs'); const data = zlib.gunzipSync(fs.readFileSync('data/merged.json.gz')); console.log(JSON.parse(data).length)")
            echo "âœ… Merged: $ENTITY_COUNT total entities"
            
            if [ "$ENTITY_COUNT" -lt 1000 ]; then
              echo "âš ï¸ Warning: Low entity count ($ENTITY_COUNT)"
            fi
          else
            echo "âŒ Error: merged.json not generated"
            exit 1
          fi

      # V4.1: Save Entity Data to Cache (Cycle-Specific Key)
      # V4.1: Save Entity Data to Cache (Cycle-Specific Key)
      - name: Save Entity Data to Cache
        uses: actions/cache/save@v4
        with:
          path: |
            data/merged.json.gz
            data/merged_shard_*.json.gz
            data/manifest.json
            cache/
          key: cycle-${{ github.run_id }}-harvest

      # Pipeline Summary
      - name: Summary
        run: |
          ENTITY_COUNT=$(node -e "const zlib = require('zlib'); const fs = require('fs'); const data = zlib.gunzipSync(fs.readFileSync('data/merged.json.gz')); console.log(JSON.parse(data).length)")
          BATCH_COUNT=$(ls -1 data/raw_batch_*.json 2>/dev/null | wc -l)
          
          echo "## Factory 1/4 - Harvest Complete ðŸŒ¾" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### V18.2 Compressed Pipeline" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Total Entities | $ENTITY_COUNT |" >> $GITHUB_STEP_SUMMARY
          echo "| Batch Files | $BATCH_COUNT |" >> $GITHUB_STEP_SUMMARY
          echo "| Compression | Gzip Enabled (V18.2) |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… Next: Factory 2/4 - Process" >> $GITHUB_STEP_SUMMARY
